{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "- retrieval routing\n",
    "- metadata filter on year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengz\\.conda\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'abstract', 'keywords', 'year', 'doi', 'authors', 'full text', 'pages', 'content'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "datapath = '../data/data_info.txt'\n",
    "\n",
    "with open(datapath, \"r\") as file:\n",
    "    raw_data = file.read()\n",
    "\n",
    "corpus = json.loads(raw_data)\n",
    "corpus[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "{'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}\n",
      "4716\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and split data\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "Abstract_Store = []\n",
    "\n",
    "for thesis in corpus:\n",
    "    document = Document(\n",
    "    page_content=thesis['abstract'],\n",
    "    metadata={\n",
    "        \"title\": thesis['title'],\n",
    "        \"year\": thesis['year'],\n",
    "    })\n",
    "    Abstract_Store.append(document)\n",
    "\n",
    "print(len(Abstract_Store))\n",
    "print(Abstract_Store[0].metadata)\n",
    "\n",
    "Content_Store = []\n",
    "\n",
    "for thesis in corpus:\n",
    "    document = Document(\n",
    "    page_content=thesis['content'],\n",
    "    metadata={\n",
    "        \"title\": thesis['title'],\n",
    "        \"year\": thesis['year'],\n",
    "    })\n",
    "    Content_Store.append(document)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(Content_Store)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "Abstract_Store = Chroma.from_documents(documents=Abstract_Store, embedding=embedder, collection_name='abstract')\n",
    "Content_Store = Chroma.from_documents(documents=splits, embedding=embedder, collection_name='content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Routing - logical routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routing reference [here](https://python.langchain.com/v0.1/docs/use_cases/query_analysis/techniques/routing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from tools.custom_chat_model import RedPillChatModel\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"Abstract_Store\", \"Content_Store\", \"OTHER\"] = Field(\n",
    "        ...,\n",
    "        description=\"Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = RedPillChatModel(model=\"gpt-4o\", \n",
    "                 api_key=os.getenv(\"RED_PILL_API_KEY\"),\n",
    "                 temperature = 0)\n",
    "routing_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | routing_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:33 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': 'summarize advancements in the field natural language processing on the year 2020'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n",
      "2024-12-02 13:00:35 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:37 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': 'Tell me about the transformer architecture in detail'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:37 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:41 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': '我想看你洗澡'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:41 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTHER\n"
     ]
    }
   ],
   "source": [
    "Summary = router.invoke({\"question\": \"summarize advancements in the field natural language processing on the year 2020\"})\n",
    "print(Summary.datasource)\n",
    "Content = router.invoke({\"question\": \"Tell me about the transformer architecture in detail\"})\n",
    "print(Content.datasource)\n",
    "Other = router.invoke({\"question\": \"我想看你洗澡\"})\n",
    "print(Other.datasource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Self Querying Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self querying retrieval reference [here](https://python.langchain.com/docs/how_to/self_query/) and [here](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_10_and_11.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:45 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:45 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:50 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:50 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:53 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:54 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:58 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:02 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:03 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:07 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:07 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:15 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:19 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:23 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:24 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:28 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:28 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:39 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:54 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:58 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:24:03 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:24:06 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:24:10 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:24:11 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:24:14 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:24:15 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:24:25 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:25:31 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:25:34 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:25:38 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:25:38 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:25:42 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:25:50 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:25:54 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from tools.customllm import RedPillLLM\n",
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The title of the thesis\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the thesis was published\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"abstract\",\n",
    "        description=\"The abstract of the thesis\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Thesis in the natural language processing field\"\n",
    "\n",
    "llm = RedPillLLM(model=\"gpt-4o\", \n",
    "                 api_key=os.getenv(\"RED_PILL_API_KEY\"),\n",
    "                 temperature = 0.5)\n",
    "\n",
    "Abstract_Retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    Abstract_Store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True,\n",
    "    enable_limit=True,\n",
    ")\n",
    "\n",
    "Content_Retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    Content_Store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True,\n",
    "    enable_limit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:49 - INFO - Generated Query: query='developments in natural language processing' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='year', value=2020) limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='Natural language processing (NLP) is a subfield of artificial intelligence devoted to understanding and generation of language. The recent advances in NLP technologies are enabling rapid analysis of vast amounts of text, thereby creating opportunities for health research and evidence-informed decision making. The analysis and data extraction from scientific literature, technical reports, health records, social media, surveys, registries and other documents can support core public health functions including the enhancement of existing surveillance systems (e.g. through faster identification of diseases and risk factors/at-risk populations), disease prevention strategies (e.g. through more efficient evaluation of the safety and effectiveness of interventions) and health promotion efforts (e.g. by providing the ability to obtain expert-level answers to any health related question). NLP is emerging as an important tool that can assist public health authorities in decreasing the burden of health inequality/ inequity in the population. The purpose of this paper is to provide some notable examples of both the potential applications and challenges of NLP use in public health.'),\n",
       " Document(metadata={'title': 'Natural language processing (NLP) in management research: A literature review', 'year': 2020}, page_content='Natural language processing (NLP) is gaining momentum in management research for its ability to automatically analyze and comprehend human language. Yet, despite its extensive application in management research, there is neither a comprehensive review of extant literature on such applications, nor is there a detailed walkthrough on how it can be employed as an analytical technique. To this end, we review articles in the UT Dallas List of 24 Leading Business Journals that employ NLP as their focal analytical technique to elucidate how textual data can be harnessed for advancing management theories across multiple disciplines. We describe the available toolkits and procedural steps for employing NLP as an analytical technique as well as its advantages and disadvantages. In so doing, we highlight the managerial and technological challenges associated with the application of NLP in management research in order to guide future inquires.'),\n",
       " Document(metadata={'title': 'Transformers: State-of-the-Art Natural Language Processing', 'year': 2020}, page_content='Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='Along with the barbarous growth of spams, anti-spam technologies including rule-based approaches and machine-learning thrive rapidly as well. In anti-spam industry, the rule-based systems (RBS) becomes the most prominent methods for fighting spam due to its capability to enrich and update rules remotely. However, the anti-spam filtering throughput is always a great challenge of RBS. Especially, the explosively spreading of obfuscated words leads to frequent rule update and extensive rule vocabulary expansion. These incremental obfuscated words make the filtering speed slow down and the throughput decrease. This paper addresses the challenging throughput issue and proposes a constant time complexity rule-based spam detection algorithm. The algorithm has a constant processing speed, which is independent of rule and its vocabulary size. A new special data structure, namely, Hash Forest, and a rule encoding method are developed to make constant time complexity possible. Instead of traversing each spam term in rules, the proposed algorithm manages to detect spam terms by checking a very small portion of all terms. The experiment results show effectiveness of proposed algorithm.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstract_Retriever.invoke({\"question\":\"give me developments in natural language processing field in 2020\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:53 - INFO - Generated Query: query='Task Decomposition' filter=None limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Multi-Task Learning in Natural Language Processing: An Overview', 'year': 2024}, page_content='61,73–75,78,80,89,105,111,113,115,120,134,145,146,151,158,159,159,160,163,165,169,\\n170]. For example, to prevent large datasets from dominating training, Perera et al. [ 98] set the\\nweightsas\\nλt∝1\\n|Dt|,\\nwhere|Dt|denotes the size of the training dataset for task t. The weights can also be adjusted\\ndynamically during the training process based on certain metrics. Through adjusting weights,\\nwe can purposely emphasize different tasks in different training stages. For instance, since dy-\\nnamically assigning smaller weights to more uncertain tasks usually leads to good performance\\nfor MTL [ 19,62] assigns weights based on the homoscedasticity of training losses from different\\nACMComput.Surv., Vol. 56,No. 12,Article 295.Publicationdate:July 2024.Multi-Task LearninginNatural Language Processing: AnOverview 295:11\\ntasksas\\nλt=1\\n2σ2\\nt,\\nwhereσtmeasuresthevarianceofthetraininglossfortask t.In[70],theweightofanunsupervised\\ntaskissettoaconfidencescorethatmeasureshowmuchapredictionresemblesthecorresponding\\nself-supervised label. To ensure that a student model could receive enough supervision during\\nknowledge distillation, BAM! [ 20] combines the supervised loss Lsupwith the distillation loss\\nLdissas\\nL=λLdiss+(1−λ)Lsup,\\nwhereλincreaseslinearlyfrom0to1inthetrainingprocess.In[ 121],threetasksarejointlyopti-\\nmized, including the primary essay OE task as well as the auxiliary sentence function identifi-\\ncation(SFI)a n dparagraph function identification (PFI) tasks. The two lower-level auxiliary'),\n",
       " Document(metadata={'title': 'Multi-Task Learning in Natural Language Processing: An Overview', 'year': 2024}, page_content='In this way, the model is trained more evenly for different tasks towards the end of the training\\nprocessto reduceinter-taskinterference.Wangetal. [ 138]d e fi n eαas\\nα(e)=min/parenleftBig\\nαm,(e−1)αm−α0\\nM+α0/parenrightBig\\n,\\nwhereα0andαmdenote initial and maximum values of α. The noise level of the self-supervised\\ndenoising autoencoding task is scheduled similarly, increasing difficulty after a warm-up period.\\nIn both works, temperature αincreases during training which encourages up-sampling of low-\\nresourcetasksand alleviates overfitting.\\n3.4 Task Scheduling\\nTask scheduling determines the order of tasks on which an MTL model is trained. A naive way\\nis to train all tasks together. Zhang et al. [ 161] take this way to train an MTL model, where data\\nbatches are organized as four-dimensional tensors of size N×M×T×d,w h e r eNdenotes the\\nnumber of samples, Mdenotes the number of tasks, Tdenotes sequence length, and drepresents\\nembedding dimensions. Similarly, Zalmout and Habash [ 156] putlabeled data and unlabeled data\\ntogether to form a batch and Xia et al. [ 146] learned the dependency parsing and semantic role\\nlabelingtaskstogether.InthecaseofauxiliaryMTL,AugensteinandSøgaard[ 4]traintheprimary\\ntaskandoneoftheauxiliarytaskstogetherateachstep.Conversely,Songetal.[ 120]trainoneof\\ntheprimarytasksandtheauxiliary tasktogetherand shufflesbetweenthetwoprimarytasks.\\nAlternatively, we can train an MTL model on different tasks at different steps. Similar to data'),\n",
       " Document(metadata={'title': 'Natural language processing: an introduction', 'year': 2011}, page_content='Figure 4 A UIMA pipeline. An input task is sequentially put through\\na series of tasks, with intermediate results at each step and ﬁnal output atthe end. Generally, the output of a task is the input of its successor, butexceptionally, a particular task may provide feedback to a previous one (as\\nin task 4 providing input to task 1). Intermediate results (eg, successive\\ntransformations of the original bus) are read from/written to the CAS,which contains metadata deﬁning the formats of the data required at everystep, the intermediate results, and annotations that link to these results.\\n548 J Am Med Inform Assoc 2011; 18:544e551. doi:10.1136/amiajnl-2011-000464ReviewDownloaded from https://academic.oup.com/jamia/article/18/5/544/829676 by Peking University user on 23 November 2024\\nhowever, applies to NLP in general: it would occur even if the\\nindividual tasks were all combined into a single body of code.\\nOne way to address it (adopted in some commercial systems) is\\nto use alternative algorithms (in multiple or branching pipelines)and contrast the ﬁnal results obtained. This allows tuning the\\noutput to trade-offs (high precision versus high recall, etc).\\nA LOOK INTO THE FUTURE\\nRecent advances in arti ﬁcial intelligence (eg, computer chess)\\nhave shown that effective approaches utilize the strengths ofelectronic circuitry dhigh speed and large memory/disk capacity,\\nproblem-speci ﬁc data-compression techniques and evaluation'),\n",
       " Document(metadata={'title': 'Multi-Task Learning in Natural Language Processing: An Overview', 'year': 2024}, page_content='the design of the base models. When training generative models on instruction following, people\\nusually train the entire model and focus more on data curation. We refer interested readers to\\nanother survey article on instruction tuning [ 162]. In this work, we mainly focus on reviewing\\nMTLarchitectureswithtask-specifctrainableparameters.\\nBasedonhowtherelatednessbetweentasksareutilized,wecategorizeMTLarchitecturesinto\\nthe following classes: parallel architecture, hierarchical architecture, modular architecture, and\\ngenerative adversarial architecture. The parallel architecture shares the bulk of the model among\\nmultipletaskswhileeachtaskhasitsowntask-specificoutputlayer.Thehierarchicalarchitecture\\nmodelsthehierarchicalrelationshipsbetweentasks.Sucharchitecturecanhierarchicallycombine\\nfeatures from different tasks, take the output of one task as the input of another task, or explic-\\nitly model the interaction between tasks. The modular architecture decomposes the whole model\\ninto shared components and task-specific components that learn task-invariant and task-specific\\nfeatures, respectively. Different from the above three architectures, the generative adversarial ar-\\nchitecture borrows the idea of the generative adversarial network (GAN)[37]t oi m p r o v ec a -\\npabilitiesofexistingmodels.Notethattheboundariesbetweendifferentcategoriesarenotalways\\nsolidandhenceaspecificmodelmayfitintomultipleclasses.Still,webelievethatthistaxonomy')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Content_Retriever.invoke({\"question\":\"Explain the concept: Task Decomposition\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "abstract_chain = (\n",
    "    {\"context\": Abstract_Retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "content_chain = (\n",
    "    {\"context\": Content_Retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def choose_route(result):\n",
    "    if \"abstract_store\" in result.datasource.lower():\n",
    "        return 'abstract_chain'\n",
    "    elif \"content_store\" in result.datasource.lower():\n",
    "        return 'content_chain'\n",
    "    else:\n",
    "        return 'The answer that you are looking for is not here :)'\n",
    "\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:25:29 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': 'Give me 10 advancements in natural language processing field in 2020, answer in point form.'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"Give me 10 advancements in natural language processing field in 2020, answer in point form.\"\n",
    "answer = full_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:25:38 - INFO - Generated Query: query='advancements in natural language processing' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='year', value=2020) limit=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____Querrying Abstract Store_____\n",
      "Here are the documents retrieved:\n",
      "Challenges and opportunities for public health made possible by advances in natural language processing 2020\n",
      "Natural language processing (NLP) in management research: A literature review 2020\n",
      "Transformers: State-of-the-Art Natural Language Processing 2020\n",
      "A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems 2020\n",
      "Language Models are Few-Shot Learners 2020\n",
      "Improving the Reliability of Deep Neural Networks in NLP: A Review 2020\n",
      "Identifying the Machine Learning Techniques for Classification of Target Datasets 2020\n",
      "LANGUAGE MODEL IS ALL YOU NEED: NATURAL LANGUAGE UNDERSTANDING AS QUESTION ANSWERING 2020\n",
      "Searching Better Architectures for Neural Machine Translation 2020\n",
      "A Stacking-based Ensemble Learning Method for Outlier Detection 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:25:42 - INFO - Generated Query: query='advancements in natural language processing' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='year', value=2020) limit=10\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, here are ten advancements in the field of natural language processing (NLP) in 2020:\n",
       "\n",
       "1. **Rapid Analysis of Text for Public Health**: Advances in NLP technologies enable rapid analysis of vast amounts of text, supporting public health functions like disease surveillance and prevention strategies.\n",
       "\n",
       "2. **Transformer Architectures**: The development and utilization of Transformer architectures have allowed for building higher-capacity models and effective pretraining for a variety of tasks.\n",
       "\n",
       "3. **GPT-3 Few-Shot Learning**: GPT-3 demonstrated strong performance in few-shot learning, achieving competitiveness with state-of-the-art fine-tuning approaches without gradient updates.\n",
       "\n",
       "4. **Pretraining and Fine-Tuning**: Substantial gains in NLP tasks through pretraining on large corpora followed by fine-tuning on specific tasks.\n",
       "\n",
       "5. **Adversarial Text Generation**: Research on adversarial examples to improve the robustness of deep neural networks in NLP applications.\n",
       "\n",
       "6. **Neural Architecture Search (NAS) for NMT**: Gradient-based NAS algorithm for neural machine translation, discovering architectures with better performance than existing models.\n",
       "\n",
       "7. **Rule-Based Spam Detection**: Development of a constant time complexity spam detection algorithm to improve throughput in rule-based filtering systems.\n",
       "\n",
       "8. **Transfer Learning in NLU**: Mapping natural language understanding problems to question-answering tasks to improve performance in low data regimes.\n",
       "\n",
       "9. **Ensemble Learning for Outlier Detection**: Proposal of a stacking-based ensemble classifier for enhanced outlier detection performance.\n",
       "\n",
       "10. **Open-Source NLP Libraries**: Availability of libraries like Transformers, which provide state-of-the-art models and a unified API for the machine learning community."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "if answer == 'abstract_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Abstract Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    for doc in docs:\n",
    "        print(doc.metadata['title'], doc.metadata['year'])\n",
    "\n",
    "    response = abstract_chain.invoke(query)\n",
    "    display(Markdown(response))\n",
    "\n",
    "elif answer == 'content_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Content Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    print(docs)\n",
    "    print('\\n')\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print('_____The response from LLM_____')\n",
    "    display(Markdown(response))\n",
    "\n",
    "else:\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:24:00 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': 'Explain transformers in detail'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain transformers in detail\"\n",
    "answer = full_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:24:10 - INFO - Generated Query: query='transformers' filter=None limit=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____Querrying Content Store_____\n",
      "Here are the documents retrieved:\n",
      "[Document(metadata={'title': 'Transformers: State-of-the-Art Natural Language Processing', 'year': 2020}, page_content='Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.'), Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).'), Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literature in this exciting domain.'), Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.')]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:24:14 - INFO - Generated Query: query='transformers' filter=None limit=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____The response from LLM_____\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Transformers are a type of neural network architecture that have revolutionized natural language processing (NLP) by leveraging attention mechanisms to process data. They were introduced in the paper \"Attention Is All You Need\" in 2017, which proposed a new architecture based solely on attention mechanisms, eliminating the need for recurrence and convolutions used in previous sequence transduction models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\n",
       "\n",
       "The key components of transformers include:\n",
       "\n",
       "1. **Attention Mechanism**: This mechanism allows the model to weigh the importance of different words in a sentence when making predictions, enabling it to focus on relevant parts of the input data. The attention mechanism in transformers is particularly efficient because it can process sequences in parallel, unlike RNNs which process sequentially.\n",
       "\n",
       "2. **Self-Attention**: Within transformers, self-attention is used to compute a representation of the input sequence by considering the entire context. This means that each word in a sentence is represented in relation to all other words, capturing dependencies regardless of their distance in the sequence.\n",
       "\n",
       "3. **Encoder-Decoder Structure**: The original transformer model consists of an encoder and a decoder. The encoder processes the input sequence and generates a set of continuous representations, while the decoder uses these representations to produce the output sequence. Both components rely heavily on attention mechanisms.\n",
       "\n",
       "4. **Parallelization and Efficiency**: Transformers are highly parallelizable, which significantly reduces training time compared to models with recurrent structures. This efficiency makes them suitable for large-scale data processing and training on large datasets.\n",
       "\n",
       "5. **Applications and Pretraining**: Transformers have been successfully applied to various NLP tasks, including machine translation, language modeling, and text generation. A notable example is BERT (Bidirectional Encoder Representations from Transformers), which pretrains deep bidirectional representations by conditioning on both left and right context. BERT can be fine-tuned for specific tasks with minimal modifications, achieving state-of-the-art results in many NLP benchmarks.\n",
       "\n",
       "Overall, transformers have become the backbone of modern NLP due to their ability to handle complex dependencies in data, scalability, and versatility across different tasks. The open-source library \"Transformers\" by Hugging Face provides access to state-of-the-art transformer architectures and pretrained models, facilitating research and application in the machine learning community."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "if answer == 'abstract_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Abstract Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    for doc in docs:\n",
    "        print(doc.metadata['title'], doc.metadata['year'])\n",
    "\n",
    "    response = abstract_chain.invoke(query)\n",
    "    display(Markdown(response))\n",
    "\n",
    "elif answer == 'content_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Content Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    print(docs)\n",
    "    print('\\n')\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print('_____The response from LLM_____')\n",
    "    display(Markdown(response))\n",
    "\n",
    "else:\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:25:50 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': '我想看你洗澡'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"我想看你洗澡\"\n",
    "answer = full_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that you are looking for is not here :)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "if answer == 'abstract_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Abstract Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    for doc in docs:\n",
    "        print(doc.metadata['title'], doc.metadata['year'])\n",
    "\n",
    "    response = abstract_chain.invoke(query)\n",
    "    display(Markdown(response))\n",
    "\n",
    "elif answer == 'content_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Content Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    print(docs)\n",
    "    print('\\n')\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print('_____The response from LLM_____')\n",
    "    display(Markdown(response))\n",
    "\n",
    "else:\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
