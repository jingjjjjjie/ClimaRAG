{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "- retrieval routing\n",
    "- metadata filter on year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengz\\.conda\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'abstract', 'keywords', 'year', 'doi', 'authors', 'full text', 'pages', 'content'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "datapath = '../data/data_info.txt'\n",
    "\n",
    "with open(datapath, \"r\") as file:\n",
    "    raw_data = file.read()\n",
    "\n",
    "corpus = json.loads(raw_data)\n",
    "corpus[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "{'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}\n",
      "4716\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and split data\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "Abstract_Store = []\n",
    "\n",
    "for thesis in corpus:\n",
    "    document = Document(\n",
    "    page_content=thesis['abstract'],\n",
    "    metadata={\n",
    "        \"title\": thesis['title'],\n",
    "        \"year\": thesis['year'],\n",
    "    })\n",
    "    Abstract_Store.append(document)\n",
    "\n",
    "print(len(Abstract_Store))\n",
    "print(Abstract_Store[0].metadata)\n",
    "\n",
    "Content_Store = []\n",
    "\n",
    "for thesis in corpus:\n",
    "    document = Document(\n",
    "    page_content=thesis['content'],\n",
    "    metadata={\n",
    "        \"title\": thesis['title'],\n",
    "        \"year\": thesis['year'],\n",
    "    })\n",
    "    Content_Store.append(document)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(Content_Store)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "Abstract_Store = Chroma.from_documents(documents=Abstract_Store, embedding=embedder, collection_name='abstract')\n",
    "Content_Store = Chroma.from_documents(documents=splits, embedding=embedder, collection_name='content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Routing - logical routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routing reference [here](https://python.langchain.com/v0.1/docs/use_cases/query_analysis/techniques/routing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from tools.custom_chat_model import RedPillChatModel\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"Abstract_Store\", \"Content_Store\", \"OTHER\"] = Field(\n",
    "        ...,\n",
    "        description=\"Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = RedPillChatModel(model=\"gpt-4o\", \n",
    "                 api_key=os.getenv(\"RED_PILL_API_KEY\"),\n",
    "                 temperature = 0)\n",
    "routing_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | routing_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:33 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': 'summarize advancements in the field natural language processing on the year 2020'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n",
      "2024-12-02 13:00:35 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:37 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': 'Tell me about the transformer architecture in detail'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:37 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:41 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': '我想看你洗澡'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:41 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTHER\n"
     ]
    }
   ],
   "source": [
    "Summary = router.invoke({\"question\": \"summarize advancements in the field natural language processing on the year 2020\"})\n",
    "print(Summary.datasource)\n",
    "Content = router.invoke({\"question\": \"Tell me about the transformer architecture in detail\"})\n",
    "print(Content.datasource)\n",
    "Other = router.invoke({\"question\": \"我想看你洗澡\"})\n",
    "print(Other.datasource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Self Querying Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self querying retrieval reference [here](https://python.langchain.com/docs/how_to/self_query/) and [here](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_10_and_11.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:45 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:45 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:50 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:50 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:53 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:54 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:00:58 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:02 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:03 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:07 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:07 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:15 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:19 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:23 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:24 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:28 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:28 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:39 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:54 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
      "2024-12-02 13:01:58 - WARNING - Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from tools.customllm import RedPillLLM\n",
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The title of the thesis\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the thesis was published\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"abstract\",\n",
    "        description=\"The abstract of the thesis\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Thesis in the natural language processing field\"\n",
    "\n",
    "llm = RedPillLLM(model=\"gpt-4o\", \n",
    "                 api_key=os.getenv(\"RED_PILL_API_KEY\"),\n",
    "                 temperature = 0.5)\n",
    "\n",
    "Abstract_Retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    Abstract_Store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True,\n",
    "    enable_limit=True,\n",
    ")\n",
    "\n",
    "Content_Retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    Content_Store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True,\n",
    "    enable_limit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:49 - INFO - Generated Query: query='developments in natural language processing' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='year', value=2020) limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='Natural language processing (NLP) is a subfield of artificial intelligence devoted to understanding and generation of language. The recent advances in NLP technologies are enabling rapid analysis of vast amounts of text, thereby creating opportunities for health research and evidence-informed decision making. The analysis and data extraction from scientific literature, technical reports, health records, social media, surveys, registries and other documents can support core public health functions including the enhancement of existing surveillance systems (e.g. through faster identification of diseases and risk factors/at-risk populations), disease prevention strategies (e.g. through more efficient evaluation of the safety and effectiveness of interventions) and health promotion efforts (e.g. by providing the ability to obtain expert-level answers to any health related question). NLP is emerging as an important tool that can assist public health authorities in decreasing the burden of health inequality/ inequity in the population. The purpose of this paper is to provide some notable examples of both the potential applications and challenges of NLP use in public health.'),\n",
       " Document(metadata={'title': 'Natural language processing (NLP) in management research: A literature review', 'year': 2020}, page_content='Natural language processing (NLP) is gaining momentum in management research for its ability to automatically analyze and comprehend human language. Yet, despite its extensive application in management research, there is neither a comprehensive review of extant literature on such applications, nor is there a detailed walkthrough on how it can be employed as an analytical technique. To this end, we review articles in the UT Dallas List of 24 Leading Business Journals that employ NLP as their focal analytical technique to elucidate how textual data can be harnessed for advancing management theories across multiple disciplines. We describe the available toolkits and procedural steps for employing NLP as an analytical technique as well as its advantages and disadvantages. In so doing, we highlight the managerial and technological challenges associated with the application of NLP in management research in order to guide future inquires.'),\n",
       " Document(metadata={'title': 'Transformers: State-of-the-Art Natural Language Processing', 'year': 2020}, page_content='Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='Along with the barbarous growth of spams, anti-spam technologies including rule-based approaches and machine-learning thrive rapidly as well. In anti-spam industry, the rule-based systems (RBS) becomes the most prominent methods for fighting spam due to its capability to enrich and update rules remotely. However, the anti-spam filtering throughput is always a great challenge of RBS. Especially, the explosively spreading of obfuscated words leads to frequent rule update and extensive rule vocabulary expansion. These incremental obfuscated words make the filtering speed slow down and the throughput decrease. This paper addresses the challenging throughput issue and proposes a constant time complexity rule-based spam detection algorithm. The algorithm has a constant processing speed, which is independent of rule and its vocabulary size. A new special data structure, namely, Hash Forest, and a rule encoding method are developed to make constant time complexity possible. Instead of traversing each spam term in rules, the proposed algorithm manages to detect spam terms by checking a very small portion of all terms. The experiment results show effectiveness of proposed algorithm.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstract_Retriever.invoke({\"question\":\"give me developments in natural language processing field in 2020\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:53 - INFO - Generated Query: query='Task Decomposition' filter=None limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Multi-Task Learning in Natural Language Processing: An Overview', 'year': 2024}, page_content='61,73–75,78,80,89,105,111,113,115,120,134,145,146,151,158,159,159,160,163,165,169,\\n170]. For example, to prevent large datasets from dominating training, Perera et al. [ 98] set the\\nweightsas\\nλt∝1\\n|Dt|,\\nwhere|Dt|denotes the size of the training dataset for task t. The weights can also be adjusted\\ndynamically during the training process based on certain metrics. Through adjusting weights,\\nwe can purposely emphasize different tasks in different training stages. For instance, since dy-\\nnamically assigning smaller weights to more uncertain tasks usually leads to good performance\\nfor MTL [ 19,62] assigns weights based on the homoscedasticity of training losses from different\\nACMComput.Surv., Vol. 56,No. 12,Article 295.Publicationdate:July 2024.Multi-Task LearninginNatural Language Processing: AnOverview 295:11\\ntasksas\\nλt=1\\n2σ2\\nt,\\nwhereσtmeasuresthevarianceofthetraininglossfortask t.In[70],theweightofanunsupervised\\ntaskissettoaconfidencescorethatmeasureshowmuchapredictionresemblesthecorresponding\\nself-supervised label. To ensure that a student model could receive enough supervision during\\nknowledge distillation, BAM! [ 20] combines the supervised loss Lsupwith the distillation loss\\nLdissas\\nL=λLdiss+(1−λ)Lsup,\\nwhereλincreaseslinearlyfrom0to1inthetrainingprocess.In[ 121],threetasksarejointlyopti-\\nmized, including the primary essay OE task as well as the auxiliary sentence function identifi-\\ncation(SFI)a n dparagraph function identification (PFI) tasks. The two lower-level auxiliary'),\n",
       " Document(metadata={'title': 'Multi-Task Learning in Natural Language Processing: An Overview', 'year': 2024}, page_content='In this way, the model is trained more evenly for different tasks towards the end of the training\\nprocessto reduceinter-taskinterference.Wangetal. [ 138]d e fi n eαas\\nα(e)=min/parenleftBig\\nαm,(e−1)αm−α0\\nM+α0/parenrightBig\\n,\\nwhereα0andαmdenote initial and maximum values of α. The noise level of the self-supervised\\ndenoising autoencoding task is scheduled similarly, increasing difficulty after a warm-up period.\\nIn both works, temperature αincreases during training which encourages up-sampling of low-\\nresourcetasksand alleviates overfitting.\\n3.4 Task Scheduling\\nTask scheduling determines the order of tasks on which an MTL model is trained. A naive way\\nis to train all tasks together. Zhang et al. [ 161] take this way to train an MTL model, where data\\nbatches are organized as four-dimensional tensors of size N×M×T×d,w h e r eNdenotes the\\nnumber of samples, Mdenotes the number of tasks, Tdenotes sequence length, and drepresents\\nembedding dimensions. Similarly, Zalmout and Habash [ 156] putlabeled data and unlabeled data\\ntogether to form a batch and Xia et al. [ 146] learned the dependency parsing and semantic role\\nlabelingtaskstogether.InthecaseofauxiliaryMTL,AugensteinandSøgaard[ 4]traintheprimary\\ntaskandoneoftheauxiliarytaskstogetherateachstep.Conversely,Songetal.[ 120]trainoneof\\ntheprimarytasksandtheauxiliary tasktogetherand shufflesbetweenthetwoprimarytasks.\\nAlternatively, we can train an MTL model on different tasks at different steps. Similar to data'),\n",
       " Document(metadata={'title': 'Natural language processing: an introduction', 'year': 2011}, page_content='Figure 4 A UIMA pipeline. An input task is sequentially put through\\na series of tasks, with intermediate results at each step and ﬁnal output atthe end. Generally, the output of a task is the input of its successor, butexceptionally, a particular task may provide feedback to a previous one (as\\nin task 4 providing input to task 1). Intermediate results (eg, successive\\ntransformations of the original bus) are read from/written to the CAS,which contains metadata deﬁning the formats of the data required at everystep, the intermediate results, and annotations that link to these results.\\n548 J Am Med Inform Assoc 2011; 18:544e551. doi:10.1136/amiajnl-2011-000464ReviewDownloaded from https://academic.oup.com/jamia/article/18/5/544/829676 by Peking University user on 23 November 2024\\nhowever, applies to NLP in general: it would occur even if the\\nindividual tasks were all combined into a single body of code.\\nOne way to address it (adopted in some commercial systems) is\\nto use alternative algorithms (in multiple or branching pipelines)and contrast the ﬁnal results obtained. This allows tuning the\\noutput to trade-offs (high precision versus high recall, etc).\\nA LOOK INTO THE FUTURE\\nRecent advances in arti ﬁcial intelligence (eg, computer chess)\\nhave shown that effective approaches utilize the strengths ofelectronic circuitry dhigh speed and large memory/disk capacity,\\nproblem-speci ﬁc data-compression techniques and evaluation'),\n",
       " Document(metadata={'title': 'Multi-Task Learning in Natural Language Processing: An Overview', 'year': 2024}, page_content='the design of the base models. When training generative models on instruction following, people\\nusually train the entire model and focus more on data curation. We refer interested readers to\\nanother survey article on instruction tuning [ 162]. In this work, we mainly focus on reviewing\\nMTLarchitectureswithtask-specifctrainableparameters.\\nBasedonhowtherelatednessbetweentasksareutilized,wecategorizeMTLarchitecturesinto\\nthe following classes: parallel architecture, hierarchical architecture, modular architecture, and\\ngenerative adversarial architecture. The parallel architecture shares the bulk of the model among\\nmultipletaskswhileeachtaskhasitsowntask-specificoutputlayer.Thehierarchicalarchitecture\\nmodelsthehierarchicalrelationshipsbetweentasks.Sucharchitecturecanhierarchicallycombine\\nfeatures from different tasks, take the output of one task as the input of another task, or explic-\\nitly model the interaction between tasks. The modular architecture decomposes the whole model\\ninto shared components and task-specific components that learn task-invariant and task-specific\\nfeatures, respectively. Different from the above three architectures, the generative adversarial ar-\\nchitecture borrows the idea of the generative adversarial network (GAN)[37]t oi m p r o v ec a -\\npabilitiesofexistingmodels.Notethattheboundariesbetweendifferentcategoriesarenotalways\\nsolidandhenceaspecificmodelmayfitintomultipleclasses.Still,webelievethatthistaxonomy')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Content_Retriever.invoke({\"question\":\"Explain the concept: Task Decomposition\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "abstract_chain = (\n",
    "    {\"context\": Abstract_Retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "content_chain = (\n",
    "    {\"context\": Content_Retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def choose_route(result):\n",
    "    if \"abstract_store\" in result.datasource.lower():\n",
    "        return 'abstract_chain'\n",
    "    elif \"content_store\" in result.datasource.lower():\n",
    "        return 'content_chain'\n",
    "    else:\n",
    "        return 'The answer that you are looking for is not here :)'\n",
    "\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:00:53 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': 'Give me 10 advancements in natural language processing field in 2020, answer in point form.'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"Give me 10 advancements in natural language processing field in 2020, answer in point form.\"\n",
    "answer = full_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:01:02 - INFO - Generated Query: query='advancements in natural language processing' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='year', value=2020) limit=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____Querrying Abstract Store_____\n",
      "Here are the documents retrieved:\n",
      "Challenges and opportunities for public health made possible by advances in natural language processing 2020\n",
      "Natural language processing (NLP) in management research: A literature review 2020\n",
      "Transformers: State-of-the-Art Natural Language Processing 2020\n",
      "A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems 2020\n",
      "Language Models are Few-Shot Learners 2020\n",
      "Improving the Reliability of Deep Neural Networks in NLP: A Review 2020\n",
      "Identifying the Machine Learning Techniques for Classification of Target Datasets 2020\n",
      "LANGUAGE MODEL IS ALL YOU NEED: NATURAL LANGUAGE UNDERSTANDING AS QUESTION ANSWERING 2020\n",
      "Searching Better Architectures for Neural Machine Translation 2020\n",
      "A Stacking-based Ensemble Learning Method for Outlier Detection 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:01:07 - INFO - Generated Query: query='advancements in natural language processing' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='year', value=2020) limit=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are advancements in the field of natural language processing (NLP) in 2020:\n",
      "\n",
      "1. **Rapid Analysis of Text**: Advances in NLP technologies enabled the rapid analysis of vast amounts of text, facilitating health research and evidence-informed decision-making.\n",
      "\n",
      "2. **Transformer Architectures**: The development of Transformer architectures allowed for building higher-capacity models, improving the effectiveness and efficiency of NLP tasks.\n",
      "\n",
      "3. **Pretrained Models**: Pretraining on large corpora has enhanced the performance of NLP models, allowing them to perform a wide variety of tasks more effectively.\n",
      "\n",
      "4. **Few-Shot Learning with GPT-3**: The introduction of GPT-3, a large-scale language model with 175 billion parameters, improved task-agnostic, few-shot performance, enabling the model to perform new language tasks with minimal examples.\n",
      "\n",
      "5. **Adversarial Texts and Robustness**: Research focused on the vulnerability of deep neural networks to adversarial examples, leading to the development of defensive strategies to improve model robustness in NLP applications.\n",
      "\n",
      "6. **Transfer Learning in NLU**: Mapping Natural Language Understanding (NLU) problems to Question Answering (QA) problems showed significant improvements, especially in low data regimes.\n",
      "\n",
      "7. **Neural Architecture Search (NAS) for NMT**: Gradient-based NAS algorithms were used to discover better-performing architectures for neural machine translation, achieving superior results compared to existing models.\n",
      "\n",
      "8. **Ensemble Learning for Outlier Detection**: A stacking-based ensemble learning method improved outlier detection performance, showcasing the potential of ensemble methods in NLP-related tasks.\n",
      "\n",
      "9. **Rule-Based Spam Detection**: Development of a constant time complexity rule-based spam detection algorithm addressed throughput challenges, enhancing the efficiency of spam filtering systems.\n",
      "\n",
      "10. **Toolkits and Procedural Steps for NLP in Management**: Comprehensive reviews and descriptions of toolkits and procedural steps for employing NLP in management research were provided, aiding in the advancement of management theories.\n"
     ]
    }
   ],
   "source": [
    "if answer == 'abstract_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Abstract Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    for doc in docs:\n",
    "        print(doc.metadata['title'], doc.metadata['year'])\n",
    "\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print(response)\n",
    "\n",
    "elif answer == 'content_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Content Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    print(docs)\n",
    "    print('\\n')\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print('_____The response from LLM_____')\n",
    "    print(response)\n",
    "\n",
    "else:\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:01:15 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': 'Explain transformers in detail'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain transformers in detail\"\n",
    "answer = full_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:01:23 - INFO - Generated Query: query='transformers' filter=None limit=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____Querrying Abstract Store_____\n",
      "Here are the documents retrieved:\n",
      "Transformers: State-of-the-Art Natural Language Processing 2020\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 2018\n",
      "Attention in Natural Language Processing 2021\n",
      "Attention Is All You Need 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:01:27 - INFO - Generated Query: query='transformers' filter=None limit=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a type of neural network architecture that have become fundamental in the field of natural language processing (NLP). They were introduced in the paper \"Attention Is All You Need\" and are characterized by their use of attention mechanisms without relying on recurrence or convolutions, which were common in previous models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\n",
      "\n",
      "### Key Features of Transformers:\n",
      "\n",
      "1. **Attention Mechanism**: The core innovation of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when encoding a particular word. This mechanism makes it possible to capture long-range dependencies and context in text more effectively than RNNs or CNNs.\n",
      "\n",
      "2. **Parallelization**: Unlike RNNs, which process data sequentially, transformers process all input data simultaneously. This parallelization significantly reduces training time and allows transformers to scale to large datasets and complex tasks.\n",
      "\n",
      "3. **Encoder-Decoder Structure**: Transformers typically consist of an encoder and a decoder. The encoder processes the input data to create a representation, while the decoder uses this representation to generate the output. Both the encoder and decoder are composed of layers that include self-attention and feedforward neural networks.\n",
      "\n",
      "4. **Pretraining and Fine-tuning**: Transformers have enabled advances in model pretraining, where a model is first trained on a large corpus of text to learn general language representations. These pretrained models, such as BERT (Bidirectional Encoder Representations from Transformers), can then be fine-tuned on specific tasks like question answering or language inference, often achieving state-of-the-art performance with minimal task-specific modifications.\n",
      "\n",
      "5. **Unified API and Pretrained Models**: The Transformers library, developed by Hugging Face, provides access to a wide range of transformer architectures and pretrained models. It is designed to be extensible for researchers, simple for practitioners, and robust for industrial applications.\n",
      "\n",
      "Overall, transformers have revolutionized NLP by providing a powerful and flexible framework that can be adapted to a variety of language tasks, leading to significant improvements in performance across numerous benchmarks.\n"
     ]
    }
   ],
   "source": [
    "if answer == 'abstract_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Abstract Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    for doc in docs:\n",
    "        print(doc.metadata['title'], doc.metadata['year'])\n",
    "\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print(response)\n",
    "\n",
    "elif answer == 'content_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Content Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    print(docs)\n",
    "    print('\\n')\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print('_____The response from LLM_____')\n",
    "    print(response)\n",
    "\n",
    "else:\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:01:54 - INFO - Sending request to Red Pill AI: {'model': 'gpt-4o', 'messages': [{'role': 'system', 'content': 'You are an expert at routing a user question to the appropriate data source.'}, {'role': 'user', 'content': '我想看你洗澡'}], 'temperature': 0, 'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Abstract_Store is a database with abstracts of papers in the natural language field, Content_Store is a database with the full text of papers in the natural language field. Given a user question choose which datasource would be most relevant for answering their question. For Summarization or more general use cases, route to Abstract_Store, only if asked on concepts or specific content route to Content_Store. Otherwise, if you encounter something wierd or not in the field of nlp, return OTHER', 'enum': ['Abstract_Store', 'Content_Store', 'OTHER'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"我想看你洗澡\"\n",
    "answer = full_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer that you are looking for is not here :)\n"
     ]
    }
   ],
   "source": [
    "if answer == 'abstract_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Abstract Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    for doc in docs:\n",
    "        print(doc.metadata['title'], doc.metadata['year'])\n",
    "\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print(response)\n",
    "\n",
    "elif answer == 'content_chain':\n",
    "    docs  = Abstract_Retriever.invoke(query)\n",
    "    print('_____Querrying Content Store_____')\n",
    "    print('Here are the documents retrieved:')\n",
    "    print(docs)\n",
    "    print('\\n')\n",
    "    response = abstract_chain.invoke(query)\n",
    "    print('_____The response from LLM_____')\n",
    "    print(response)\n",
    "\n",
    "else:\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
