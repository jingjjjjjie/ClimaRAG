{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "\n",
    "# embedder = SpacyEmbeddings(model_name=\"en_core_web_sm\")\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# testing the embedder\n",
    "embeddings = embedder.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'abstract', 'keywords', 'year', 'doi', 'authors', 'full text', 'pages', 'content'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "datapath = '../data/data_info.txt'\n",
    "\n",
    "with open(datapath, \"r\") as file:\n",
    "    raw_data = file.read()\n",
    "\n",
    "corpus = json.loads(raw_data)\n",
    "corpus[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "{'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for thesis in corpus:\n",
    "    document = Document(\n",
    "    page_content=thesis['content'],\n",
    "    metadata={\n",
    "        \"title\": thesis['title'],\n",
    "        #\"authors\": thesis['authors'],\n",
    "        \"year\": thesis['year'],\n",
    "        #\"keywords\": thesis['keywords']\n",
    "    })\n",
    "    docs.append(document)\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='A Critical Survey on the use of Fuzzy Sets in Speech \\nand Natural Language Processing \\n \\nJoao P. Carvalho \\nTULisbon – Instituto Superior \\nTécnico \\nL2F - INESC -ID \\nR. Alves Redol 9, 1000 -029 Lisboa  \\njoao.carvalho@inesc -id.pt Fernando  Batista  \\nISCTE -IUL – Lisbon University \\nInstitute  \\nL2F - INESC -ID \\nR. Alves Redol 9, 1000- 029 Lisboa  \\nfernando .batista@inesc -id.pt Luisa Coheur  \\nTULisbon – Instituto Superior \\nTécnico \\nL2F - INESC -ID \\nR. Alves Redol 9, 1000 -029 Lisboa  \\nluisa.coheur@inesc -id.pt \\n \\n \\nAbstract  — This paper  shows how the use and applications of \\nFuzzy Sets (FS) in S peech and Natural Language Processing \\n(SNLP) have  seen a steady decline to a point where  FS are \\nvirtually unknown or unappealing for most of the researchers \\ncurrently working in the SNLP  field, trie s to find t he reasons \\nbehind this decline , and proposes some guidelines on what could \\nbe done to reverse it and make FS assume a relevant role  in \\nSNLP . \\nKeywords:  Fuzzy Sets, Natural Language Processing, Speech \\nproce ssing.  \\nI.  INTRODUCTION1 \\nSpeech and natural language p rocessing ( SNLP) is a \\nresearch field that joins linguistics , computer science and signal \\nprocessing and is concerned with the interactions between \\ncomputers and human languages. This interdisciplinary field is \\noften referred  by less encompassing names like  computational  \\nlinguistics – usually used by linguistics  – or simply by natural'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='linguistics – usually used by linguistics  – or simply by natural  \\nlanguage processing – the term  originally preferred in computer \\nscience –, even when making extensive use of speech \\nprocessing . The ultimate goal of SNLP would be to develop  a \\nsystem to simulate a conversational agent that would be able to \\nmaintain a spoken dialogue or answer questions made in \\nhuman-like form. Such agent would recognize and understand \\nalmost any  question or discourse in a given spoken language, \\nand would provide  an appropriate spoken sequence to the \\nconversation (or a reply to the question). Automatic language \\ntranslation could also have to eventually be addressed \\nsomewhere in mid -process.  As it will be seen in section 2., t he \\ncomplexity and number of tasks involved in obtaining such \\nagent is even more daunting than it appears to be , at least to \\nsomeone not directly involved in those tasks . \\nFrom a fuzzy sets “ point of view ”, i.e., for someone directly \\ninvolved in fuzzy sets research,  it would seem very natural for \\nsuch agent to use fuzzy sets. Actually, the first thought would be “why shouldn’t it be implemented mainly using fuzzy systems?” The first reason is that SNLP is a huge field \\ncurrently tackling dozens of different problems for which \\n                                                             \\n1 This work was in part supported by FCT (INESC -ID multi annual funding) \\nthrough the PIDDAC Program funds .'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='through the PIDDAC Program funds . \\n specific evaluati on metric s exist, and it is not possible  to reduce  \\nthe whole field into a specific problem, as it was done by \\nNovak in  [36]. D espite some potentially interesting w orks and \\nresults ( See section III.), fuzzy sets are virtually unknown or \\nunappealing for most of the researchers currently working  and \\npublishing in SNLP journals and conferences. This could \\nsomehow be understandable if the SNLP community was \\nsmall, or if SNLP was a niche research field. However, several \\nyearly SNLP conferences attract hundreds of participants, and a \\nfew ones even a few thousands (e.g.: INTERSPEECH ; \\nICASSP; COLING ). Therefore, from a fuzzy sets point of \\nview,  it should b e expected to find a considerable  number of \\n“fuzzy” contributions . \\nThis paper presents an overview of the current use of fuzzy \\nsets in SNLP, focusing on works that have been accepted by \\nthe SNLP community, and presents some suggestions and \\ndirections in how can fuzzy sets be useful and accepted by the \\nsame community.  \\nII. SPEECH AND NATURAL LANGUAGE PROCESSING : A \\nSHORT OVERVIEW  \\nSNLP originated in the late 1940s, and despite long term \\ndivergences on the best way to approach SNLP, like the \\nfamous “Two Camps” in the 1960s – the symbolic paradigm \\nassociated with Chomsky formal lang uage theory and \\ngenerative syntax,  and the stochastic paradigm associated with \\nAI names like John McCarthy, Marvin Minsky and Claude'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='AI names like John McCarthy, Marvin Minsky and Claude \\nShannon , focused on stochastic, statistic and probabilistic \\nmodels  –, the field has lately converge to  the use of \\nprobabili stic finite state models and other machine learning \\ntechniques, especially unsupervised statistical approaches made possible and motivated  by the creation and availability of \\nlarge/huge linguistic and text corpora .   \\nIn order to understand the complexity o f the tasks involved \\nin SNLP let us an alyze the major steps involved in obtaining an \\nagent similar to what is described in section 1 , and what are \\ncurrently the major techniques used to approach them . \\nA. From Speech to Text  \\nWhen in the presence of an audio si gnal, the  agent would \\nneed to recognize a sequence of words . If each word was \\npronounced the same in all contexts and by all speakers, speech \\nU.S. Government work not protected by U.S. copyright\\nWCCI 2012 IEEE World Congress on Computational Intelligence \\nJune, 10-15, 2012 - Brisbane, Australia\\nFUZZ IEEE\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.                                                                                         \\n recognition would be really easy . But even if one focus in a \\nsingle language like English, what might seem an easy task \\nbecomes incredibly complex when one has to deal with gender'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='becomes incredibly complex when one has to deal with gender \\nand age variations, different accents  and ways to pronounce \\nand produce a word , different emotional states, rate of speech, \\ndisfluencies, noisy environmen ts, etc. One must not forget that \\nwords are produced by  the movement of the speech articulators \\n(tongue, lips, velum) during speech production , which is \\ndifferent from person to person, continuously changes  and is \\nsubject to physical constraints like momentum.  The same word \\ncan be represented by an infinite numbe r of different signals, \\nand the number of different words and word variations \\n(number, gender, etc.) to be recognized is huge. The modern algorithms to approac h this problem use smaller units of \\nspeech, called phones, instead of words. Phonology is the science that studies phones.  Note that even phones are very \\ncomplex signals.  \\nIn addition to word recognition, there are additional \\nproblems associated with speech  to text transcription.  Namely, \\none must deal with word boundary detection, sentence \\nboundary detection, punctuation detection, truecasing, speaker \\nrecognition (which can be important when several participants \\nare involved), etc. The recognition and marking of the above \\nitems is a problem usually known as rich transcription. \\nSpeech recognition has improved and nowadays works  \\nrather well under restricted environments, like commands over \\na telephone, or recognition of tv news. B ut even these tasks'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='a telephone, or recognition of tv news. B ut even these tasks \\nhave problems under  noisy environments.  On the last 3 decades \\nthe bottleneck with natural speech recognition has always been \\nthe availability of data, and the abilit y to process it \\nefficiently.  This availability  has evolved a lot in the last years,  \\nand so have speech recognition methods  and computational \\nsystems . The results obtained by  Dragon technologies,  the \\narrival of S iri on the iPhone 4s, and the spread of Google voice  \\nrecognition, changed the public perception that speech \\nrecognition does not work. However one must not forget that \\nsuch technologies  imply an online connection , lots of remote \\nprocessing  power , and corpora access.  \\nClearly, one can say that Speech -to-text is mostly \\ncomposed of  classification problem s, and the techniques \\ncurrently u sed to approach it are classification techniques. The \\nfollowing keywords can be used to access the most used techniques in speech recognition and rich transcription – \\nHidden Markov Models  (HMM);  Neural  Networks;  Gaussian \\nMixture Models (GMM); Support Vecto r Machines (SVM); \\nViterbi;  Multipass decoding; LPC  features; N-gram features; \\nMel Frequency Cepstral Coefficients (MFCC ) features. \\nB. Words, Utterances and Grammars  \\nIdentifying sentences (or utterances) and the respective \\ntokens  is one of the first step s involved in text processing, and \\nits impact is well known  for subsequent processing tasks .'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='its impact is well known  for subsequent processing tasks . \\nAlthough this might seem to be a trivial task, it is not. As an example, one cannot consider  that a f ull stop represents the end \\nof a sentence, since expressions such as Mr. or John F. \\nKennedy , need special attention. But it gets worse if we \\nconsider, for instance, Chinese. As it is written without spaces between words, the segmentation process is extreme ly \\ncomplicated (there are even competitions on this subject).  Other major text processing steps include stemming, \\nlemmatization, part of speech tagging (POS), syntactic analysis \\nand Named Entity Recognition (NER).  \\nThe first two apply some transformation to  words: \\nstemming rules usually discard some suffixes; lemmatization \\nmap variant forms of the same word into the same expression. \\nThis process is particularly interesting in tasks such as \\nInformation Retrieval, as the information we are seeking is \\nprobably not stated with the same words of our query. Stems \\nand lemmas might help. \\n In POS tagging each word is associated with one or more \\nmorpho -syntactic labels (name, proper name, verb, etc.). \\nSeveral POS taggers exist for many languages and some can reach almo st a perfect accuracy (considering that even linguists \\ndo not agree in every tag). Some of these taggers are rule -\\nbased, others use labeled corpora to train, for instance, HMM \\nmodels [24], and unsupervised POS tagging is also being \\nexplored [20].'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='models [24], and unsupervised POS tagging is also being \\nexplored [20]. \\nConcerning syntactic analysis, there  is an impressive \\ncollection of grammar formalisms, algorithms and tools. Grammars can be roughly split into constituency and \\ndependency grammars (and hybrids). The first set tries to group \\nsequences of words (phrases) together; the second set intends \\nto establish relations (dependencies) between words. Examples \\nof grammars from the first set are the well-known Context -Free \\nGrammars (CFG)  [9]; an example o f the second set is the Link \\nGrammar [43]. Many grammars are hand crafted, but there are \\nalso systems that try to infer rules. Algorithms for parsing \\nsentences are  defined according with the involved formalism. \\nFor instance, Earley algorithm  [14] can be applied to CFGs, but \\nCKY only to CFGs in some special form. Tools implementing these algorithms can be found in many programming \\nlanguages.  \\nNamed Entity Recognition is also a task that  should be \\ndetached. It consists in labeling “entities” in text, \\nusually  locations, organizations and people  names. HMMs and \\nits successors are widely used in this so useful task in so many \\nSNLP applications.  \\nTo conclude, one should note that a text e xtracted from  an \\nautomatic speech transcript contains errors, is characterized by \\na weaker syntactic structure, and contains phenomena specific \\nfrom speech data, such  as, repetitions, and other disfluencies ,'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='from speech data, such  as, repetitions, and other disfluencies , \\nthat may have impact in further processing . Most tasks \\nperformed for written texts are also performed for speech texts,  \\nbut additional specific problems need  to be addressed . For \\ninstance a grammar that suits for well -formed sentences might \\nbe inappropriate for speech input.  \\nC. Natural Language Understa nding \\nNatural Language Understanding (NLU) is the process of \\nconverting natural language utterances into a structure that the \\ncomputer can deal with. This structure, which abstracts the \\nsemantics of the utterance, is highly dependent on the involved \\napplication and can move from keywords to complex \\nrepresentations such as logical forms or frames (sets of \\nattribute/value pairs.) The process of mapping a given utterance \\ninto its semantic representation flows from keyword spotting to \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.                                                                                         \\n linguistically motivated a pproaches, but machine learning also \\nfeeds current NLU research.  \\nConsidering early systems, we detach ELIZA [49] and \\nLUNAR [51]. ELIZA, which aims at emulating a psychologist, \\nhad a great impact at its time, as many people believed it to be \\nhuman despite the simplicity of the underlying framework, \\nbased on regular expressions . ELIZA can be seen as the'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='based on regular expressions . ELIZA can be seen as the \\npredecessor of nowadays chatbots, such as Cleverbo t2 and \\nALICE3, which try to simulate human conversation. Many of \\nthese systems target to pass the Turing test and win the Lobner \\nprize4. In what concerns LUNAR, this natural language \\ninterface capable of answering questions about moon rocks also deserves some detach, as it started the line of research \\nresponsible for a panoply of Natural Language Interfaces with \\nDatabases (NLIDB), in the 80 s and 90s,  which ended up \\nconverging in question/answering (QA) systems. Both NLIDBs \\nand QA systems share a process of question understanding, but \\nin the former the target information is in the database and in the \\nlater information needs to be extracted fr om the web or \\ncollections of documents, that is, from possibly unstructured texts.  \\nIn early times, NLU was linguistically motivated and \\nusually based on the following paradigm: each syntactic rule \\nwas associated with a semantic rule, and logical forms were  \\ngenerated in a bottom -up, compositional process, where the \\nsemantic of each level was the result of combining the semantic \\nof the elements of previous levels through the correspondent \\nsemantic rules [24][1]. Nevertheless, this approach presents \\nseveral drawbacks, as for instance, syntactic elements, significant for semantic analysis, could be spread in different \\nsyntactic rules. In addition, all these rules were hand crafted,'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='syntactic rules. In addition, all these rules were hand crafted, \\nwhich made  them difficult to develop and maintain.  \\nSub-symbolic techniques, as in so many other research \\nfields, are currently being widely  applied to N LU. For \\ninstance,  Bhagat [4] treats NLU as a  classification problem, \\ni.e., his final goal is to be able to classify an utterance. Thus, \\nMaximum Entropy and Suppor t Vector Machines are used in \\nsome of their experiments. Other approaches try to label parts \\nof the utterance that can be later used to create frames. HMMs \\ncan be used in such approaches. However, despite the claim of \\nBhagat of training his system with lit tle data, all these \\napproaches rely on large annotated corpora, which are used for \\ntraining.  \\nNLU remains one of the most complicated problems to be \\nsolved in SNLP. Not only because of the so many possibilities \\nin stating something (which should all be mapp ed into the same \\nsemantic representation), but also because it is almost impossible to find a representation that perfectly suits even the \\nsimplest problem.  \\nD. Text to Speech  \\nThe final stage would involve generating speech from the \\ntext representing the agent  reply. Speech is usually generated \\nrecurring to phones, but before even considering  how to \\ngenerate and concatenate phones, this stage invo lves multiple \\n                                                             \\n2 http://Cleverbot.com \\n3 http://alice.pandorabots.com/'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='2 http://Cleverbot.com \\n3 http://alice.pandorabots.com/  \\n4 http://www.loebner.net/Prizef/loebner -prize.html  difficult steps since  ones intention it to produce human, instead \\nof robotic, sounding  speech.   \\nThe fi rst step is text normalization, and includes \\ntokenization and dealing with non standard words . \\nTokenization uses any supervised machine learning method to \\ntrain a classifier to mark s entence boundaries. Non-standard \\nwords are ofte n ambiguous, i.e., can be spoken in different \\nways a ccording to context. Examples : 3/4  (should it be spoken \\nas March 4th or three quarters ?); should  an acronym  be read as \\na word ( IKEA ), or using each letter in a sequence ( IBM)? lead \\n(refers to leash  or metal ? Note that each is pro nounced \\ndifferently ); Dr. (drive or doctor ?); St. (saint  or street ?) \\nDealing with non -standard words requires at least three steps: \\ntokenization  to separate out and identify potential non -standard \\nwords, classification  to classify them into a given type , and \\nexpansion  to convert each type into a string of standard words. \\nClassification itself might be difficult but is not ambiguous . \\nDisambiguation of  homographs can be done using POS \\ntagging except for a few special cases that are usually ignore d \\nin such  systems.  \\nThe second step consists in phonetic analysis. Current \\ntechniques u se dictionaries for known words. Problem lies with  \\nout to deal with proper names and with unkown words. Some'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='out to deal with proper names and with unkown words. Some \\nsystems have been proposed for names but there is room for \\nimprovement . Nonstandard words use machine learning based \\ngrapheme-to -phoneme conversion (g2p).  \\nThe next  steps are related with p rosodic features and \\nprosodic prominence – the rhythm,  intonation and level of \\nwords in phrases. Regarding features, p hrases are usually \\ndivided into parts w hen spoken,  and the last vowels of the parts \\nare usually longer. Natural sounding speech must include  these \\nfeatures. P hrase boundary prediction is generally treated as a \\nbinary classification task, where a word is given and the system \\nhas to decide whether or not to put a prosodic boundary after it. \\nDeterministic rules can also be included, like for example inserting boundaries after punctuation. More sophisticated \\nmodels are based on machine learning classifiers trained using  \\ncorpora  annotated with prosodic boundaries. Prominence \\ninvolves making some words sound more louder, other slower, slightly changing the tone, etc. The duration of phones is an \\nissue that is not only  related with the phone itself, but also with \\nprosodics and a wide  variety of contextual factors that can be \\nmodelled by rule-based or statistical methods. \\nE. Other Problems  \\nSNLP also deals with  several other problems not described \\nin the previous sections.  Among others, these include : author \\nrecognition (who wrote a given  text? ); speaker r ecognition'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='recognition (who wrote a given  text? ); speaker r ecognition  \\n(who is talking?); summarization  (extract the main contents of \\na given text);  machine translation  (automatic translation \\nbetween a pair of languages); detecting emotions in speech \\n(and also in texts); etc.  Currently most of these problems are \\nalso addressed recurring to statistical approaches and machine \\nlearning techniques.  \\nIII. A\\n SURVEY OF FUZZY SETS RELATED WORKS IN SPEECH \\nAND NATURAL LANGUAGE PROCESSING  \\nResearch in the various subareas of SNLP is spread across a \\nwide number of journals and conference proceedings. Some \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.                                                                                         \\n have more emphasis on natural language processing and \\ncomputational linguistics, others, related with speech \\nprocessing , are usually associated with the signal processing \\nfield. There are also a few books that can be considered as \\nreferences in the field . In this section one surveys the impact of \\nfuzzy sets in the most relevant SNLP books, journals and conference proceedings. It is also possible to find some SNLP \\nworks in conferences related with artificial intellig ence, like the \\nAAAI, for example, but those were not considered in this work.  \\nA. SNLP Books  \\nCurrently the most influential book in SNLP is Jurafsky and'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='A. SNLP Books  \\nCurrently the most influential book in SNLP is Jurafsky and \\nMartin’s “Speech and Language Processing  - An introduction \\nto natural language processing, computational linguistics, and speech recognition ” [24], which is used as the main textbook in \\nmany (most) graduation and post graduation courses  in SNLP . \\nThis work was printed originally in 2000, and had a revised 2\\nnd \\nedition in 2009. The book presents a very extensive overview of the field, giving an emphasis to the statistical and machine \\nlearning approach to SNLP. “Fuzzy” is referen ced in the book \\nexactly once . However, the w ord is present only as an example \\nof how plurals are constructed in English. Fuzzy sets are never \\neven mentioned as a scientific area. \\nOther relevant books can be mentioned, like for example: \\n“Challenges in Natural Language Processing”, by Bates and \\nWeisch edel [2]; “Natural Language Processing for Online \\nApplications”, by Jackson and Moulinier  [23]; “Computational \\nLinguistics: Models, Resources, Applications”, by Igor A. \\nBolshakov and Alexander Gelbukh  [5]; or “Natural Language \\nUnderstan ding”, by Allen . Total number of times “fuzzy” is \\nmentioned in al l these books? One: in [2], chapter 2., Atkins \\nwrites about “fuzzy boundaries”. However , she never  uses any \\ntechnique related with the fuzzy sets field  when dealing with \\nthese “fuzzy boundaries” . Finally one can mention the only \\nbook where FS are indeed referred as a proper technique, even'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='book where FS are indeed referred as a proper technique, even \\nif no details are given, the “Springer Handbook of Speech \\nProce ssing ” [3], where there are references to Fuzzy queries, \\nand Fuzzy Vector Quantization (FVQ).  \\nB. SNLP Journals  \\nSNLP Journals include “Computational Linguistics ”, \\n“Natural Language Engineering ”, “ACM Transactions on \\nSpeech and Language Processing ”, “Speech Communication ”, \\n“IEEE Transactions on Acoustics, Speech and Signal \\nProcessing”, “ IEEE Transactions on Speech and  Audio  \\nProcessing” and the “IEEE Transactions on Au dio, Speech and \\nLanguage Processing .  \\nThe Computational Linguistics  journal (CL) is the primary \\narchival forum for research on computational linguistics and \\nnatural language processing. The journal, sponsored by  the \\nAssociation for Computational Linguistic s (ACL) , has been \\npublished for the ACL by MIT Press since 1988. It was \\nfounded in 1974, publishes 4 issues p er year, and has currently \\nan ISI impact factor of 2.9. During a period of 37 years, only \\n15 papers mentioning “fuzzy”  have appeared in CL. Of those \\n15, 9 are short reviews of textbooks where fuzzy is mentioned. \\nOf the other 5 articles, there is one that ha s dozens of citations,  \\nalmost re volves around fuzzy, but uses the term “fuzzy”  as a \\nnovel ty, ignoring that “fuzzy” is a coined word in the sc ientific'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='novel ty, ignoring that “fuzzy” is a coined word in the sc ientific \\ncommunity for almost half a century!  [48]; other 2 papers also use the term “fuzzy” a couple of times (as in “language is fuzzy”), but never us e it has a scientific technique; another \\npaper refers to fuzzy when listing  the amount of AI papers \\nrefering to lan guage, where he cites  “Linguistic quantiﬁers \\nmodeled by Sugano integrals knowledge representation” (yes, he wrote SugAno instead of Sugeno), a work that has no \\nparticular relevance  from a  computational linguistics  point of \\nview ; in one paper  the author mentions he has proved in a \\nprevious conference paper that fuzzy sets can have a small role in CL, but never uses fuzzy sets in the paper th at was accepted \\nfor the journal; finally, the remaining paper [15] uses fuzzy sets \\nto try to find near-synoms, but never makes any reference to \\nany fuzzy sets work or even mentions  fuzzy sets as the \\nscientific theory that supports most of the presented research.  \\nConcluding, ther e is only one  paper  using/applying fuzzy sets  \\nin the major computational linguistics related journal , and even \\nin that paper, Fuzzy Sets are not acknoeledge d as a relevant \\nsicentific technique . \\nIn Natural Language Engineering  (NLE), in 18 volumes  \\none can find  half a dozen papers where fuzzy sets are somehow \\nreferenced, and two  works  based on fuzzy sets theory, one \\npublished in 1996 related  with POS tagging [27], and a rather'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='published in 1996 related  with POS tagging [27], and a rather \\nrecent paper on detecting emotions on short texts (SMS, \\ntweets) [35]. \\nIn the ACM Transactions on Speech and Language \\nProcessing, a journal currently in its 8\\nth volume,  only two \\npaper s related to fuzzy sets  have  been published , one where \\nfuzzy boundaries are applied in morpheme segmentation  [10], \\nand another concerning stemming in the Arabic language [16]. \\nThe balance in what concerns fuzzy sets in articles \\npublished in  more speech processing oriented journals is not \\nparticularly  better  (in between parenthesis the publication \\nyear) : \\n• IEEE Trans. Acoustics, Speech and Signal Proc essing, 2  \\npapers in 38  volumes (1979, 1983) [12][13]; \\n• In IEEE Trans. Speech and Audio  Processing, 3  papers  in \\n13 volumes   (1994, 2000, 2000)  [32][54][8]; \\n• In IEEE Trans. On Audio, Speech and Language \\nProcessing, 1 paper in 7 volumes  (2011) [47];  \\n• In\\n Speech Communication, 3 papers in 53 volumes (1996, \\n2001, 2002) [7][53][52]. \\nThe above papers usually address small (but important) \\nparts of speech processing proble ms by introducing fuzzy \\ntechniques such as neural fuzzy networks [32][54], fuzzy \\nmaximum  entropy  (ME)  smoothing of n-gram models [8], \\nfuzzy relations for evaluating phonemic hypotheses  [12], fuzzy \\npossibilities for plosive consonan ts recognition [13], fuzzy \\nmasks in missing feature t heory [47], a two stage neural fuzzy'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='masks in missing feature t heory [47], a two stage neural fuzzy \\ndecision classifier for speaker identification [7], telephone \\nspeech keyword fuzzy search algorithm [53], or a corpus-based \\nfuzzy fragmen t-class Markov model (FFCMM) proposed to \\nmodel the syntactic characteristics of a speech act and used to choose the speech act candidates  [52]. \\nGiven that only 16 papers in 184 journal volumes  were \\nfound , which means 16 paper s in more than 6 000, it is easy to \\nconclude that the influence of fuzzy sets in the SNLP community is close to null.  \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.                                                                                         \\n C. SNLP Conferences  \\nThe most important conferences concerned with natural \\nlanguage proces sing and computational linguis tics are  naturally \\nassociated with the ACL.  Computational Lin guistics \\n(COLING) and include t he annual proceedings of ACL, \\nNAACL, an d EACL, and the biennial COLING conference . \\nRelated conferences include various proceedings of AC L \\nSpecial Interest Groups  such as the Conference on Natural \\nLanguage Learning (CoNLL), as well as the conference on \\nEmpirical Methods in Natural Language Proce ssing (EMNLP) , \\nand also other conferences such as the Conference \\non Intelligent  Text Processing and Computational  Linguistics'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='on Intelligent  Text Processing and Computational  Linguistics \\n(CICLing)  or the International Conference on Computational \\nLinguistics  (ICCL) . \\nResearch on speech recognition, understanding, and \\nsynthesis is presented at the annual INTERSPEECH \\nconference, which is called the International Conference on \\nSpoken Language Processing (ICSLP) and the Eur opean \\nConference on Speech Com munication and Technology \\n(EUROSPEECH) in alternating years, or the ann ual IEEE \\nInternational Conference on Acoustics, Speech, and S ignal \\nProcessing (IEEE ICASSP), among others.  \\nThe e -library of L2F – the Spoken Language Laboratory at \\nInesc -ID – was used as a basis  to study  the influence of fuzzy \\nsets in SNLP conferences. Eve n if the library d oes not \\nobviously contain all the editions of all the conferences in the \\narea, it is very extensive and can be used as a good indicator. \\nThe total number of indexed papers in the library is over \\n160000, of which around 90000 were consider ed in this study.  \\nFigure 1 shows the number fuzzy sets related published papers \\nper year since 1980.  Figure 2 shows the percentage of fuzzy \\nsets related papers among all papers.  \\n \\nFigure 1 – Number of Fuzzy Sets related papers in the L2F SNLP conference \\nlibrary per year  \\nThe first noticeable  factor in the figures is the very low \\npercentage of fuzzy sets related papers (the max value is \\n0.54%). The second  relevant factor is the declining  tendency'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='0.54%). The second  relevant factor is the declining  tendency \\nsince the year 2000. The start of this declining period coincides with the publica tion of Jurafsky’s referential textbook  [24], and \\nthe declination appears to be co -related with the rise in \\ninfluence of machine learning in SNLP.  \\nIt should also be mentioned that a relevant number of the \\nworks referenced in the table are related to, and were published in speech processing conferences. The numbers and \\npercentages in conferences closer to NLP are much lo wer \\noverall . For example, only 1 paper related with fuzzy sets [18] \\nhas been accepted in the last four editions of COLING (see \\nT\\nABLE  I). This low number is even more impressive if one \\nmentions that COLING2010 had 335 papers and was held in \\nBeijing, China, a traditional “fuzzy sets friendly” country with \\na large fuzzy sets community. \\n \\nFigure 2 – Percentage of Fuzzy Sets related papers in the L2F SNLP \\nconference library from 1980 -81 until 2010 -11  \\nTABLE I : Number of papers in last COLING editions  \\nYear  2004 2006 2008 2010 \\n#papers  204 284 181 335 \\n#fuzzy sets related papers  0 0 0 1 \\nThe papers that were found cover a large number of \\ninteresting topics. Due to lack of space it is not possible to cite \\nall the works, but an Internet  keyword web search  should \\nprovide the necessary references. The following list of topics is \\nrepresentative but not exhaustive: fuzzy s peech recognition;'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='representative but not exhaustive: fuzzy s peech recognition; \\nfuzzy vector quantizatio n; fuzzy HMM;  fuzzy neuro HMM; \\nmodel  fuzzy estimation for HMM ; fuzzy entropy HMM ; fuzzy \\nspeech segmentation using HMM ; fuzzy GMM ; fuzzy speech \\nsynthesis; rule based fuzzy  formant speech synthesis; vowel \\nrecognition using fuzzy sets ; auditory representation  \\ncombination using fuzzy sets; f uzzy multilevel acoustic \\nphonetic decoder; speech recognition using fuzzy clustering ; \\nnoise adaptation algorithms  for roubust speech recognition \\nusing fuzzy clustering and f uzzy post correction rules; fuzzy \\nlogic based speech detection algorithms under n oisy \\nenvironments ; fuzzy speaker recognition ; fuzzy emotion \\nrecognition in v oice and in speech; fuzzy c -means clustering \\nfor speaker recognition ; fuzzy SVM for Age  and Gender \\nclassification in speech;  fuzzy statistical languag e; fuzzy topic \\ndetection; topic similarities  using fuzzy; fuzzy word match in \\ndocument retrieval; speaker adaptation me thods;  fuzzy \\nprosodics; fusion of different classification  outputs using fuzzy  \\nintegral s; audio visual speech perception (fuzzy logical model \\nof perception);  rule-based Translation Corpus where  queries \\nuse fuzzy matches; fuzzy natural languages inte rfaces for \\ndatabases ; N-gram Fuzzy Matching . \\n0\\t\\n \\xa05\\t\\n \\xa010\\t\\n \\xa015\\t\\n \\xa020\\t\\n \\xa025\\t\\n \\xa0\\n1980\\t\\n \\xa0\\n1983\\t\\n \\xa0\\n1986\\t\\n \\xa0\\n1989\\t\\n \\xa0\\n1992\\t\\n \\xa0\\n1995\\t\\n \\xa0\\n1998\\t\\n \\xa0\\n2001\\t\\n \\xa0\\n2004\\t\\n \\xa0\\n2007\\t\\n \\xa0\\n2010\\t\\n \\xa0\\n0.00%\\t\\n \\xa00.10%\\t\\n \\xa00.20%\\t\\n \\xa00.30%\\t\\n \\xa00.40%'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='1995\\t\\n \\xa0\\n1998\\t\\n \\xa0\\n2001\\t\\n \\xa0\\n2004\\t\\n \\xa0\\n2007\\t\\n \\xa0\\n2010\\t\\n \\xa0\\n0.00%\\t\\n \\xa00.10%\\t\\n \\xa00.20%\\t\\n \\xa00.30%\\t\\n \\xa00.40%\\t\\n \\xa00.50%\\t\\n \\xa00.60%\\t\\n \\xa0\\n1980-\\xad‐81\\t\\n \\xa0\\n1982-\\xad‐83\\t\\n \\xa0\\n1984-\\xad‐85\\t\\n \\xa0\\n1986-\\xad‐87\\t\\n \\xa0\\n1988-\\xad‐89\\t\\n \\xa0\\n1990-\\xad‐91\\t\\n \\xa0\\n1992-\\xad‐93\\t\\n \\xa0\\n1994-\\xad‐95\\t\\n \\xa0\\n1996-\\xad‐97\\t\\n \\xa0\\n1998-\\xad‐99\\t\\n \\xa0\\n2000-\\xad‐01\\t\\n \\xa0\\n2002-\\xad‐03\\t\\n \\xa0\\n2004-\\xad‐05\\t\\n \\xa0\\n2006-\\xad‐07\\t\\n \\xa0\\n2008-\\xad‐09\\t\\n \\xa0\\n2010-\\xad‐11\\t\\n \\xa0\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.                                                                                         \\n Some of the works compared  fuzzy techniques with \\nbaselines and state of the art techniques, but th e results were \\nnot always very conclusive  or the comparisons extensive. Some \\nexamples: \\n• In [29], speaker adaptation methods  were compared , and \\nfuzzy HMM produced worse results (1990) , while in [34], \\nfuzzy HMM were deemed at least as good in what \\nconcerns robustness of speech recognition algorithms \\n(1990); \\n• In [11], robust s peech recognition using fuz zy clustering, \\nproduced much better results  than hard clustering  (but no \\nother methods were compared); \\n• In [46], Fuzzy GMM is found to be more efective than \\nGMM for speaker recognitions; \\n• In [21], fuzzy sets  yeld better results  than MFCC when \\ncombining auditory representation s (1994); \\n• In [45], fuzzy techniques for speech segmentation, \\nproduced results equal to NN but slightly worse than \\nmanual segmentation  (2001).'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='produced results equal to NN but slightly worse than \\nmanual segmentation  (2001).  \\nNote that all of the above works  are more than 10 years old . \\nA sp ecial mention should be done to the works of B. B. \\nRieger  [39][38][40] [41] (among others), one of the pionneers \\nof the use of FS in Comput ational Linguistics.   \\nD. SNLP Works published outside the SNLP  community \\nFuzzy SNLP works have also been published outside the \\nSNLP community, and not necessarily within the FS \\ncommunity. It is our contention that in order to gain impact in \\nthe SNLP community , researchers  should focus on the forums \\ndirectly related  with SNLP . Therefore although we mention \\nsome works in this section, the  coverage was not meant to be as  \\nexhaustive  as in the previous sec tions. \\nIn Fuzzy Sets related conferences the number of SNLP \\nrelated works is  rather low. As an example, one can find only 7  \\npapers [50][17][44][42][30][6][37]  in the la st two Fuzz-IEEE \\nconference proceedings , of a total of around  1000 papers . One \\nshould note that these numbers do not i nclude works  that are \\nsomehow related to some natural language topics and that have been given a particular attention in the fuzzy sets community, \\nbut are usually ignored in SNLP. These topics include: \\n• Fuzzy Query -Answering (not to confuse with Question \\nAnswering) – Use of a “controlled” natural language to \\nquery databases. The “controlled” term is used to indicate'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='query databases. The “controlled” term is used to indicate \\nthat these systems are usually not prepared to accept \\ntotally open NL questions, as in QA. There should be a \\nhuge overlap and interaction between researchers in this \\ncommunity and NLU community that is not usually \\nverified. A good source for papers in this area could be R. \\nYager FQAS - Flexible Query Answering Systems \\nconference series, usually published in Springer’s LNCS – Lecture Notes in computer Science.  \\n• Zadeh’s Computing with words and Precisiated Natural Languages – A very well known topic in FS [56] that \\ndeals with how is  it possible to treat propositions drawn \\nfrom a natural language as objects of compu tation.  This \\ntopic is definitely related with Natural Language Understanding, but not yet associated with SNLP because it has not been integrated in the SNLP chain or compared \\nwith the SNLP NLU  methods.  \\n• Linguistic data mining and summarization – Deals with \\ncreating linguistic summaries from numeric data [55][26] \\n[25]. \\nMaybe an effort should be done to advertize/publish these \\nworks , where FS have assumed a very important role, in the \\nSNLP process area.  \\nThere are also some other  works that shou ld be referenced \\nsince they explicitely address the connection of FS with SNLP,  \\neven if they only focus on some limited aspects of SNLP:   \\nNovak [36]; Ma [33]; Inoue work on the treatment of fuzziness \\nin Natural Language [19]; Langanke work on Fuzzy Sets an d \\nChomsky grammars [31].'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='in Natural Language [19]; Langanke work on Fuzzy Sets an d \\nChomsky grammars [31]. \\nIV. C\\nAN FUZZY SETS BE USEFUL TO SNLP?  \\nIn the previous sections we have seen that , despite some \\nattempts to use FS in SNLP, fact is that, especially in the last \\ndecade, the SNLP community has basically ignored FS. \\nTherefore the big question is, “Can FS be useful to SNLP?”  \\nWe be lieve that the answer is affirmative, but it is imperative t o \\nfollow some guidelines in order for FS to be accepted and used \\nin the SNLP community:  \\n• Publish in the right places  – SNLP researchers will never \\ngive credit to works publi shed out of the main journals \\nand conferences of their community; \\n• It is wrong to ass ume that something is better just because \\nuses fuzzy  – Simply stating that fuzzy is naturally adapted \\nto deal with SNLP is worthless. Compare results against  \\nestablished and accepted baselines and gold standards using well -known and publicly available corpora. Use \\nstatistical tests. Be aware that SNLP community will not \\naccept  the proposed approach unless there is an \\nimprovement  over the state of the art methods . \\n• Fuzzy Sets cannot solve everything – FS \\n can have \\nobvious advantages when dealing with specific problems but using them does not make much sense in several \\napproaches. One good example consists in using FS when tagging  text: it is well known that the same word can'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='belong to different syntactic classes depending'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='on the context . E.g., “married” can  be the past participle \\nof the verb “marry”, as in “Bob married Alice”, or an \\nadjective, as in “Bob is married ”. Howe ver, when \\nsomeone uses “married” in an utterance it should be \\nassumed that he  knows exact ly how he is using that word. \\nThere is no ambiguity in the author intention. When classifying “married” within a n utterance using \\nprobabilistic methods, there is a given probability that the word belongs to a class or the other, but never (or rarely \\nin some languages) it belongs to more than one class. FS \\nwould only be useful if there was a possibility that the \\nword would somehow fit partially into both classes \\nsimultaneously within the sam e utterance , or if the classes \\nwould overlap . On the other hand there are problems \\nwhere FS make much more sense than probabilities, such \\nas, for example, when classifying phones in speech \\nrecognition, since a given phone can fit partially to \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.                                                                                         \\n several class es, and phones are subject to all sorts of \\nvariations, e.g., a “k” can be heard sometimes like a “g”  \\n(see section II.A .)   \\nRegarding what can FS bring to SNLP, lots  of work on \\ndifferent topics has already been done, especially during the'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='different topics has already been done, especially during the \\n90’s (see section III. C.) The rea son why most of those works \\nwas not well received was probably because of the lack of \\nproper testing and comparisons to other approaches. The \\nsound est of those “old” ideas could certainly be put up to date  \\nand properly tested in order to check for their validity.  An \\neffort should also be made to bring the FS works mentioned in \\nsection III.D  to the SNLP community, especially to the NLU \\ncommunity . \\nTo conclude we suggest a list of s ome particular ar eas \\nwhere  the application of FS could provide interesting results : \\n• Speech phone recognition – Phonetic variation occurs \\nnot only from speaker to speaker but even within the \\nsame speaker depending on multiple factors and \\ncontexts, like for example the short ening or reduction of \\nsounds as a speaker talks faster or colloquially . The \\nreason these changes happen is that the move ment of \\nthe speech articulators during speech production is \\ncontinuous and is subject to phy sical. Based on the fact \\nthat a speaker repr esents in his mind categories instead \\nof phones with all details , one could use fuzzy \\ndistinctive features for phones classification (instead of using binary variables ) capturing generalizations across \\nphone categories), in order t o improve the detection o f \\nphones in speech recognition . \\n• Fuzzy Syllabification  – Syllabification, the task of'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='phones in speech recognition . \\n• Fuzzy Syllabification  – Syllabification, the task of \\nsegmenting a sequence of phones into syllables  is \\nextremely important in several tasks of both speech \\nsynthesis and recognition.  One reason syllabification is \\na difficult computational task , is that there is no \\ncompletely agreed -upon def inition of syllable \\nboundaries . The most common syllabification methods \\nare rule-based and could possibly be improved if fuzzy \\nrule-bases were used  to deal with the uncertainty \\nregarding the syllable boundaries.  \\n• Multipass decoding  in speech recognition – Use FS to \\nimprove/complement later recognition  stages,  by using  \\ncontexts or word association s, as humans  do when they \\nmisunderstand a word due , for example , to a noisy \\nenvironment – humans  tends to think what of the word  \\nhipothesis make  more sense given the sentence context.  \\n• Speech emotion detection – FS make  sense since \\nemotions cannot be separated into crisp separate  classes. \\n• Speech synthesis involves several steps where FS could be useful to give a speech sinthetizer a more human like \\nappearance, especially in what concerns prosodics.  \\n• Text Authorship – Very good preliminary results using \\nFS have been presented recently  [22]. \\n• Fuzzy tagging of unknown words – Usually unkown \\nwords are tagged based on  statistics about hapax \\nlegomena  (words that have occurred only once), and need to use additional morphological infor mation that'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='could make use of fuzzy rule bases.  \\n• Fusion and combination of  features  is needed in several \\nSNLP problems  and can make good use of fuzzy \\nrulebases (and more), instead of using probability estimators (and HMM), or Maximum entropy models.  \\nV. C\\nONCLUSIONS  \\nThe use and applications of Fuzzy Sets in SNLP have seen \\na steady decline on the first decade of the 21st century, and this \\nwork as shown that nowadays FS  are virtually unknown or \\nunappealing for most of the researchers currently working and \\npublish ing in SNLP journals and conferences . This decline \\ncoincided with the publication of Jurafsky’s textbook [24], and \\nappears t o be co-related with the rise in  influence of machine \\nlearning methods in SNLP . Despite this fact it is our belief that \\nFS can have a relevant role in several areas of SNLP, as long as \\nFS researchers make an effort to publish their works in the \\nSNLP community. In order to accomplish it, researchers should \\nfocus on the assets that FS could bring to SNLP and follow \\nsome important guidelines in what concerns the results  they \\nobtain .  \\nREFERENCES  \\n[1] Allen, J., “Natural Language Understanding”, Benjamin/Cummings Pub. \\nCo.. 1995 \\n[2] Bates , M.,  Weischedel , R.,  “Challenges in Natural Language \\nProcessing”, Cambridge University Press, 1993  \\n[3] Benesty , Sondhi, Huang (Eds.),  “Springer Handbook of Speech \\nProcessing”, Springer, 2008 \\n[4] Bhagat, R., Leuski A., Hovy E., \"Statistical shallow semantic parsing'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='[4] Bhagat, R., Leuski A., Hovy E., \"Statistical shallow semantic parsing \\ndespite little training data\". In Proc. of the 9th International Workshop \\non Parsing Technology (Parsing \\'05). ACL, Stroudsburg, PA, USA, 186-187, 2005.  \\n[5] Bolshakov, I., Gelbukh , A., “Computational Linguistics: Models, \\nResources, Applications”, IPN-UNM -FCE, 20 04 \\n[6] Cao, J., Kubota, N., Liu, H., “A Two Stage Pattern Matching Method for \\nSpeaking Recognition of Partner Robots”, Procceedings of the 2010  \\nFuzz -IEEE , WCCI2010, Barcelona , Spain, 499- 504, 2010 \\n[7] Castellano, P., Sridharan, S., “A two stage fuzzy decision clas sifier for \\nspeaker identification“ , Speech Communication, Vol. 18 (2) (1996) pp. \\n139-149 \\n[8] Chen, S. F., Rosenfeld, R., “A survey of smoothing techniques for ME \\nmodels ”, IEEE Trans. Speech Audio Processing, vol. 8, pp. 37 – 50, \\n2000 \\n[9] Chomsky, N.,  “Three models for the description of lan guage ”, IRI \\nTransactions on Information Theory , 2(3), 113 – 124, 1956  \\n[10] Creutz, M., Lagus, K., “Unsupervised models for morpheme segmentation and morphology learning”, ACM Transactions on Speech \\nand Language Processing (TSLP), Vol.4  (1), ACM, 2007 \\n[11] Cung, H. M., Normandin, Y.  \"Noise adaptation algorithms fo r robust \\nspeech recognition\", Proceddings of the  SPAC1992, ETRW on Speech \\nProcessing in Adverse Conditions,  Cannes, France, 171-174, 1992  \\n[12] De Mori, R., et al., “ Inference of a knowledge source for the recognition'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='[12] De Mori, R., et al., “ Inference of a knowledge source for the recognition \\nof nasals in continuous speech ”, IEEE Trans. Acoust., Speech, Signal \\nProcessing, vol. 27, pp. 538 – 549, 1979   \\n[13] DeMichelis, P., et al., “ Computer recognition of plosive sounds using \\ncontextual information”, IEEE Trans. A coust., Speech, Signal \\nProcessing, vol. 31, pp. 359 - 377, 1983  \\n[14] Earley J., \"An efficient context -free parsing algorithm\", Commun. ACM \\n13, 2, 94 -102, 1970  \\n[15] Edmonds, P., Hirst, G., “ Near -Synonymy and Lexical Choice ”, \\nComputational Linguistics, Vol 28 (2), 105- 144, 2002 \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.                                                                                         \\n [16] El-Beltagy, S.R., Rafea, A., “ An accuracy -enhanced light stemmer for \\narabic text”, ACM Transactions on Speech and Language Processing \\n(TSLP), Vol. 4, (2), Article 2, ACM, 2011  \\n[17] El-Fiqui, H., Petraki, E., Hussein, A., “ A Computational Linguistic  \\nApproach for the Identification of Translator Stylometry using Arabic -\\nEnglish Text ”, Procceedings of the 2011 IEEE International Conference \\non Fuzzy Systems”, Taipei, Taiwan, 2029 -2045, 2011 \\n[18] Fu, G., Wang , X., “ Chinese Sentence -Level Sentiment Classification \\nBased on Fuzzy Sets ”, Proceedings of the COLING2010, Beijing, \\nChina, 312 -319, 2010'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='Based on Fuzzy Sets ”, Proceedings of the COLING2010, Beijing, \\nChina, 312 -319, 2010 \\n[19] Fujioka  R., et.al, ”Treatment of Fuzziness in natural Language by Fuzzy \\nLingual System - FLINS,”  Proc. of the 5th IEEE International \\nConference on Fuzzy Syste ms and the 2nd International Fuzzy \\nEngineering Symposium (IFES95 and FUZZ -IEEE95) , Yokohama, \\nJapan, vol. 2, pp. 1045 -1050, 1995.  \\n[20] Graca, J.,   Ganchev, K., Coheur, L., Pereira, F., and Taskar, B., \\n\"Controlling Complexity in Part -of-Speech Induction\", Journal  of \\nArtificial Intelligence Research, Vol.41, 527 -551, 2011 \\n[21] Gransden, L.R.,  Beet,  S.W., \"Combining auditory repre sentations using \\nfuzzy sets\", Proc. of the  ICSLP 94, 1047 -1050, 1994  \\n[22] Homem, N., Carvalho  J.P., “ Authorship Identification and Author Fuzzy \\nFinge rprints”, Proc. of the NAFIPS2011 - 30th Annual Conference of \\nthe North American Fuzzy Information Processing Society , El Paso, TX, \\nUSA,  2011 \\n[23] Jackson , P., Moulinier , I., “Natural Language Processi ng for Online \\nApplications”, John Benjamins Publishing Company, 2007 \\n[24] Jurafsky , D., Martin, J.,  “Speech and Language Processing - An \\nintroduction to natural language processing, computational linguistics, \\nand speech recognition” , 2nd Edition, Prentice -Hall, 2009  \\n[25] Kacprzyk, J., Wilbik,  A., Zadro żny, S., “ Linguist ic summarization of \\ntime series using a fuzz y quantifier driven aggregation”,  Fuzzy Sets and \\nSystems, 159(12):1485 –1499, 2008.'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='Systems, 159(12):1485 –1499, 2008.  \\n[26] Kacprzyk, J., Zadrozny, S., “Computing with Words and Systemic \\nFunctional Linguistics: Linguistic Data Summaries and Natural \\nLang uage Generation”, Proc. of the IPMU, Advances in Intelligent and \\nSoft Computing, 2010, Vol 68 23-36, 2010  \\n[27] Kim, J -H., KIM  G-C., “Fuzzy network model for part- of-speech tagging \\nunder small training data  “, Natural Language Engineering , vol 2:  pp 95 \\n– 110, 1996 \\n[28] Kleene, S.,  “Representation of events in nerve nets and finite automata”,  \\nIn Shannon, C. and McCarthy, J. (Eds.), Automata Studies , pp. 3 –41. \\nPrinceton University Press, Princeton, NJ.  \\n[29] Koo, M. W., et al,  \"A comparative study of speaker a daptation methods \\nfor H MM- based speech recognition\", Proc. of the  ICSLP90 , 145-148, \\n1990 \\n[30] Lai, L- F. et al, “ Developing a Fuzzy Search Engine Based on Fuzzy \\nOntology and Semantic Search ”, Procceedings of the 2011 IEEE \\nInternational Conference on Fuzzy Systems”, Taipei, Taiwan, 2684 -\\n2689, 2011 \\n[31] Langanke, U., “Language technology — chomsky grammars and/or \\nfuzzy sets? ”, Proc. of the SAMI2008, 6th International Symposium on \\napplied Machine Intelligence and Informatics, Kosice, Slovakia, 85 -89, \\n2008 \\n[32] Le Cerf, P., et al., “Multilayer perceptrons as labelers for hidden Markov \\nmodels ”, IEEE Trans. Speech Audio Processing, vol. 2, pp. 185 – 193, \\n1994 \\n[33] Ma, L., “ Clarification on Linguistic Applications of Fuzzy Set Theory to'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='1994 \\n[33] Ma, L., “ Clarification on Linguistic Applications of Fuzzy Set Theory to \\nNatural Language Analysis ”, Proc. of the 20 11 International Conference \\non fuzzy systems and Knowledge Discovery (FSKD), 811 -815, 2011 \\n[34]  Minami, Y. et al,, \"On the robustness of HMM and ANN speech \\nrecognition algorithms\", Proc. of the ICSLP90 , 1345- 1348, 1990  \\n[35] Neviarouskaya A., Prendinger, H., Ishizuka, M., “Affect Analysis \\nModel: novel rule based approach to affect sensing from text” , Natural \\nLanguage Engineering , vol 17 : pp 95 -135, 2010  \\n[36] Novak, V., “Fuzzy Sets in Natural Language Processing”, in An \\nIntroduction to Fuzzy Logic Applications in I ntelligent Systems , R. R. \\nYager et al. (eds.), , Kluwer Academic Publishers , 185 -200, 1992 [37] Okamoto , W., Tano,  S., Inoue, A., Fujioka  A., \"A Generalized Four -Step \\nInference Method for Fuzzy Quantified and Truth -Qualifie d Natural \\nLanguage Propositions \" Procceedings of the 2010  Fuzz -IEEE , \\nWCCI2010 , Barcelona , Spain, 191- 198, 2010. \\n[38] Rieger , B.B., “Feasible Fuzzy Semantics”, Procs. of the  COLING 1978, \\n41-43, Bergen, Denmark, 1978  \\n[39] Rieger , B.B., “Fuzzy Structural Semantics”, Proc. of the 3rd European \\nMeeting o n Cybernetics, Vienna, Austria, 1976  \\n[40] Rieger , B.B., “ Fuzzy Word Meaning Analysis And Representation In \\nLinguistic Semantics. An Empirical Approach To The Reconstruction \\nOf Lexical Meanings In East - And West -German Newspaper Texts ”,'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='Of Lexical Meanings In East - And West -German Newspaper Texts ”, \\nProcs. of the  COLING 1980 , 76-84, Tokyo, Japan,1980  \\n[41] Rieger , B.B., “ Semantic Relevance And Aspect Dependency In A Given \\nSubject Domain: Contents- driven algorithmic processing of fuzzy \\nwordmeanings to form dyn amic stereotype representations”, Proc. of \\nthe COLING 1984 , 298-301, 1984  \\n[42] Sharef, N., Shen, Y., “Text Fragment Extraction using Incremental \\nEvolving Fuzzy Grammar Fragments Learning”, Procceedings of the \\n2010 Fuzz -IEEE , WCCI2010, Barcelona , Spain, 3026- 3033, 2010 \\n[43] Sleator D., Temperley D.,   \"Parsing English with a Link Grammar\", \\nCarnegie Mellon University Computer Science technical report CMU -\\nCS-91-196, October 1991.  \\n[44] Taylor, J., Raskin, V., “ Understanding the Unknown: Unattested Input \\nProcessing in Natural Language ”, Procceedings of the 2011 IEEE \\nInternational Conference o n Fuzzy Systems”, Taipei, Taiwan, 94 -101, \\n2011 \\n[45] Toledano, D.T., Gómez, L .H., “ Local refinement of phonetic \\nboundaries: a general framework and its application using different \\ntransition models\", Proc. of the EUROSPEECH -2001, Aalborg, \\nDenmark, 1695- 1698, 2001  \\n[46] Tran,  D., Le, T.V., Wagner, M.,  \"Fuzzy Gaussian mixture models for \\nspeaker recognition\", Proc. of the  ICSLP98, Sidney, Australia , 1998  \\n[47] Van Segbroeck, M.,  Van Hamme, H.,  “Advances in Missing Feature \\nTechniques for Robust Large- Vocabulary Continuous Speech'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='Techniques for Robust Large- Vocabulary Continuous Speech \\nRecog nition”, IEEE Trans. On Audio, Speech, and Language \\nProcessing, Vol 19 (1), 2011 \\n[48] Way, A., Gough, N., “wEBMT: Developing and Validating an Example -\\nBased Machine Translation System Using the World Wide Web”, \\nComputational Linguistics, 29 (3), 2003  \\n[49] Weizenbaum, J., “ELIZA - A Computer Program for the Study of \\nNatural Language Communication between Man and Machine\". \\nCommunications of the ACM 9 (1), 36- 45, 1966 \\n[50] White, E., Mazlack, L., “ Discerning Suicide Notes Causality Using \\nFuzzy Cognitive Maps , Procc eedings of the 2011 IEEE International \\nConference o n Fuzzy Systems”, Taipei, Taiwan, 2940 -2947, 2011 \\n[51] Woods, W.A., et al., \"The lunar sciences natural language information system: Final report\", Cambridge, Massachussets: Bolt Beranek and \\nNewman Inc., 1972  \\n[52] Wu, C -H., Chen,Y -J., “Multi- keyword spotting of telephone speech \\nusing a fuzzy search algorithm and keyword -driven two -level CBSM”,  \\nSpeech Communication, Vol. 33 (3) (2001) pp. 197 -212 \\n[53] Wu, C -H., Yan, G -L., Lin, C -L., “Speech act modeling in a spoken \\ndialog system using a fuzzy fragment -class Markov model”,  Speech \\nCommunication, Vol. 38 (1 -2) (2002) pp. 183 -199 \\n[54] Wu, G -D., Lin, C -T., “ Word boundary detection with mel -scale \\nfrequency bank in noisy environment ”, IEEE Trans. Speech Audio \\nProcessing, vol. 8, pp. 541 – 554, 2000'),\n",
       " Document(metadata={'title': 'A Critical Survey on the use of Fuzzy Sets in Speech and Natural Language Processing', 'year': 2012}, page_content='Processing, vol. 8, pp. 541 – 554, 2000  \\n[55] Yager , R., “ A new Approach to the Summarization of Data ”, \\nInformation Sciences, 28:69 -86, 1982.  \\n[56] Zadeh, L., “Precisiated Natural Language (PNL)”, Artificial Intelligence, \\nAAAI Vol. 25 (3), 2004  \\n[57] Zadeh, L.A.,  “PRUF - A Meaning Representation Language f or Natural \\nLanguages\" , Int.J.Man -Mach.Stud. vol10. 396-460, 1978.  \\n[58] Zadeh, L.A., “A Computational Approach to Fuzzy Quantifiers in \\nNatural Languages” , Comp. Math. with Applic. Vol. 9, 149 -184, 1983  \\n \\n \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='Information Retrieval, 6, 49–73, 2003\\nc/circlecopyrt2003 Kluwer Academic Publishers. Manufactured in The Netherlands.\\nA Memory-Based Approach to Anti-Spam Filtering\\nfor Mailing Lists\\nGEORGIOS SAKKIS gsakis@iit.demokritos.gr\\nInstitute of Informatics and Telecommunications, National Centre for Scientiﬁc Research (NCSR) “Demokritos”,GR-153 10 Ag. Paraskevi, Athens, Greece\\nION ANDROUTSOPOULOS ion@aueb.gr\\nDepartment of Informatics, Athens University of Economics and Business, Patission 76, GR-104 34, Athens,Greece\\nGEORGIOS PALIOURAS paliourg@iit.demokritos.gr\\nVANGELIS KARKALETSIS vangelis@iit.demokritos.grCONSTANTINE D. SPYROPOULOS costass@iit.demokritos.grInstitute of Informatics and Telecommunications, National Centre for Scientiﬁc Research (NCSR) “Demokritos”,GR-153 10 Ag. Paraskevi, Athens, Greece\\nPANAGIOTIS STAMATOPOULOS T.Stamatopoulos@di.uoa.gr\\nDepartment of Informatics, University of Athens, TYPA Buildings, Panepistimiopolis, GR-157 71, Athens, Greece\\nReceived September 17, 2001; Revised August 12, 2002; Accepted October 16, 2002\\nAbstract. This paper presents an extensive empirical evaluation of memory-based learning in the context of'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='anti-spam ﬁltering, a novel cost-sensitive application of text categorization that attempts to identify automaticallyunsolicited commercial messages that ﬂood mailboxes. Focusing on anti-spam ﬁltering for mailing lists, a thoroughinvestigation of the effectiveness of a memory-based anti-spam ﬁlter is performed using a publicly availablecorpus. The investigation includes different attribute and distance-weighting schemes, and studies on the effectof the neighborhood size, the size of the attribute set, and the size of the training corpus. Three different costscenarios are identiﬁed, and suitable cost-sensitive evaluation functions are employed. We conclude that memory-based anti-spam ﬁltering for mailing lists is practically feasible, especially when combined with additional safetynets. Compared to a previously tested Naive Bayes ﬁlter, the memory-based ﬁlter performs on average better,particularly when the misclassiﬁcation cost for non-spam messages is high.\\nKeywords: text categorization, machine learning, unsolicited commercial e-mail, spam\\n1. Introduction\\nThis paper presents a thorough empirical evaluation of memory-based learning in the con-\\ntext of a novel cost-sensitive application, that of ﬁltering unsolicited commercial e-mailmessages.\\nThe increasing popularity and low cost of electronic mail have intrigued direct marketers\\nto ﬂood the mailboxes of thousands of users with unsolicited messages. These messages are50 SAKKIS ET AL.'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='usually referred to as spam or, more formally, Unsolicited Commercial E-mail (UCE), and\\nmay advertise anything, from vacations to get-rich schemes. Spam messages are extremelyannoying to most users, as they waste their time and prolong dial-up connections. Theyalso waste bandwidth, and often expose minors to unsuitable content by advertising porno-graphic sites. A 1998 study found that spam messages constituted approximately 10% ofthe incoming messages to a corporate network (Cranor and LaMacchia 1998). The situationseems to be worsening, and without appropriate counter-measures, spam messages couldeventually undermine the usability of e-mail.\\nThe proposed counter-measures have been either regulatory or technical, with regulatory\\nmeasures having limited effect so far.\\n1Technical measures are based on anti-spam ﬁlters,\\nwhich attempt to discriminate between spam and non-spam, hereafter legitimate , messages.\\nTypical anti-spam ﬁlters currently in the market employ blacklists of known spammers,\\nand handcrafted rules that block messages containing speci ﬁc words or phrases. Blacklists,\\nhowever, are of little use, as spammers often use forged addresses. Handcrafted rules arealso problematic: to be most effective, they need to be tuned to the incoming messages ofparticular users or groups of users, a task requiring time and expertise that has to be repeatedperiodically to account for gradual changes in the characteristics of spam messages (Cranorand Lamacchia 1998).'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='The success of machine learning techniques in text categorization (Sebastiani 2001) has\\nrecently led researchers to explore the applicability of learning algorithms in anti-spamﬁltering.\\n2A supervised learning algorithm is fed with a corpus of messages that have been\\nclassi ﬁed manually as spam or legitimate, and builds a classi ﬁer, which is then used to\\ndetect incoming spam messages. Apart from collecting separately spam and legitimatetraining messages, the learning process is fully automatic, and can be repeated to tailortheﬁlter to the incoming messages of particular users or groups, or to capture changes\\nin the characteristics of spam messages. Anti-spam ﬁltering differs from other electronic\\nmail and news categorization tasks (Lang 1995, Cohen 1996, Payne and Edwards 1997),in that spam messages cover a very wide spectrum of topics, and hence are much lesshomogeneous than other categories that have been considered in the past. Another differenceis that anti-spam ﬁltering is a case of cost-sensitive classi ﬁcation, an area that has not been\\nexplored as intensively in text categorization as in other classi ﬁcation tasks.\\n3The cost of\\naccidentally blocking a legitimate message can be much higher than letting a spam messagepass the ﬁlter, and this cost difference must be taken into account during both training and\\nevaluation.\\nSahami et al. (1998) experimented with an anti-spam ﬁlter based on Naive Bayes (Mitchell'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='Sahami et al. (1998) experimented with an anti-spam ﬁlter based on Naive Bayes (Mitchell\\n1997). In similar anti-spam experiments, Pantel and Lin (1998) found Naive Bayes tooutperform Ripper (Cohen and Singer 1999). Drucker et al. (1999) experimented withRipper, Rocchio ’s classi ﬁer (Rocchio 1971, Joachims 1997), Support Vector Machines\\n(Cristianini and Shawe-Taylor 2000), and boosted decision trees (Quinlan 1993, Schapireand Singer 2000), with results showing that Support Vector Machines and boosted decisiontrees achieve very similar error rates, both outperforming Rocchio ’s classi ﬁer. A direct\\ncomparison of these previous results, however, is impossible, as they are based on differentand not publicly available data sets. Furthermore, the reported ﬁgures can be misleading,\\nsince they are not formulated within a cost-sensitive framework.MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 51\\nResearch on text categorization has bene ﬁted signi ﬁcantly from the existence of publicly\\navailable, manually categorized document collections, like the Reuters corpora, which havebeen used as standard benchmarks.\\n4Producing similar corpora for anti-spam ﬁltering is'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='4Producing similar corpora for anti-spam ﬁltering is\\ncomplicated by privacy issues. Publicizing spam messages does not pose a problem, becausespam messages are distributed blindly to very large numbers of recipients, and, hence, theyare effectively already publicly available; but legitimate e-mail messages cannot usually bereleased without violating the privacy of their recipients and senders. There is, however, atype of anti-spam ﬁltering where researchers can share benchmark corpora without violating\\nprivacy constraints: constructing ﬁlters that will guard against spam messages sent to mailing\\nlists with public archives.\\n5In this case, rather than examining the messages that arrive at\\na user ’s individual mailbox, the anti-spam ﬁlter examines the messages that arrive at the\\nserver of the list, before sending the messages to the subscribers of the list.\\nMailing lists are often targeted by spammers, who either cannot distinguish between\\npersonal addresses and addresses of mailing lists, or deliberately send their messages tolists to reach their subscribers. In either case, the result can be a major waste of bandwidthand storage, especially in mailing lists with large numbers of subscribers. To avoid thiswaste and other abuses, many lists are moderated, i.e., a person is assigned the task ofreading each incoming message before allowing it to be circulated to the subscribers. Listswith intense traf ﬁc, however, can overwhelm their moderators, and in many cases there'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='may be nobody willing to act as a moderator. Hence, a ﬁlter that would automatically detect\\nspam postings to mailing lists, or that would report suspicious postings to a moderator forfurther inspection would be particularly useful.\\nMost mailing lists focus on particular topics. Hence, an anti-spam ﬁlter trained for a\\nparticular mailing list can use features indicating that a message is off-topic (e.g., absenceof particular terminology) as hints that the message is spam. Assuming that the messagesthat most users receive are less topic-speci ﬁc than the messages of a mailing list, it is\\nreasonable to expect that the performance of an anti-spam ﬁlter for a list will be better than\\nthe performance of an anti-spam ﬁlter for a personal mailbox. One cannot, therefore, safely\\ngeneralize conclusions drawn from experimenting with mailing lists to anti-spam ﬁlters\\nfor personal mailboxes, although conclusions of the ﬁrst kind can be seen as preliminary\\nindications of the viability of anti-spam ﬁltering in other settings, and, as already mentioned,\\nanti-spam ﬁltering for mailing lists is valuable in its own right. We also note that in anti-\\nspam ﬁltering approaches that examine only the content of the messages, as in our case, it\\nmakes no difference whether the messages are circulated via a list server or a Usenet-likenewsgroup. Hence the work described here applies equally well to newsgroups.\\nAlong these lines, we have recently introduced Ling-Spam , a publicly available collection'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='Along these lines, we have recently introduced Ling-Spam , a publicly available collection\\nof spam messages and legitimate messages from a mailing list on linguistics, as well assuitable cost-sensitive evaluation measures, which were used to conduct a detailed eval-uation of a Naive Bayes anti-spam ﬁlter (Androutsopoulos et al. 2000a). Continuing that\\nstrand of work, this paper presents a thorough empirical evaluation of a memory-based\\nanti-spam ﬁlter, using Ling-Spam and the same evaluation framework as in our previous\\nexperiments, thus contributing towards standard benchmarks. Memory-based classi ﬁers are\\nparticularly promising for anti-spam ﬁltering, on the grounds that spam messages form a\\nrather incoherent class in terms of topics. Hence, a classi ﬁer that predicts the class of a new52 SAKKIS ET AL.\\nmessage by recalling similar already classi ﬁed messages is likely to perform at least as well\\nas classi ﬁers that build a unique model for each message class. A disadvantage of memory-\\nbased classi ﬁers is that they can be computationally expensive in their classi ﬁcation phase,\\ndue to their “lazy”character. Very ef ﬁcient implementations of memory-based classi ﬁers,\\nhowever, are available that address this issue (Daelemans et al. 1997, 2000). Furthermore,our experiments (Section 6.4) indicate that in many cases memory-based anti-spam ﬁltering\\nis viable with small training sets, which can be handled ef ﬁciently even by less sophisticated'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='is viable with small training sets, which can be handled ef ﬁciently even by less sophisticated\\nimplementations of memory-based classi ﬁers.\\nA preliminary investigation of memory-based anti-spam ﬁltering was presented in a\\nprevious article (Androutsopoulos et al. 2000c), where we experimented with a simplisticversion of the k-Nearest Neighbor algorithm (Mitchell 1997) with promising results. Gomez\\nHidalgo et al. (2000) have reported similar experiments. The work that will be presentedhere is much more detailed, in that it considers the effect of several extensions to thebasic k-Nearest Neighbor algorithm that have not been explored in previous anti-spam\\nexperiments, including different schemes for attribute and distance weighting, as well asthe effect of the neighborhood size, the size of the training corpus, the size of the attributeset, and different cost scenarios. In all cases, we attempt to justify our observations, andthus increase our con ﬁdence that similar behavior is likely to appear in other similar cost-\\nsensitive applications. Overall, our results indicate that memory-based anti-spam ﬁltering\\nfor mailing lists is practically feasible, especially when combined with additional safetynets. Compared to the Naive Bayes ﬁlter, the memory-based ﬁlter performs on average\\nbetter, particularly when the misclassi ﬁcation cost for legitimate messages is high.\\nThe rest of this paper is organized as follows: Section 2 presents our benchmark cor-'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='The rest of this paper is organized as follows: Section 2 presents our benchmark cor-\\npus; Section 3 describes the preprocessing that is applied to the messages to convertthem to training or testing instances; Section 4 discusses the basic memory-based learner;Section 5 introduces the cost-sensitive evaluation measures; Section 6 presents our experi-mental results, investigating separately the effect of each one of the extensions to the basicalgorithm that we have considered; Section 7 concludes and suggests directions for furtherresearch.\\n2. Benchmark corpus\\nThis section describes Ling-Spam, the corpus that was used in our experiments. Ling-\\nSpam is a mixture of spam messages, and legitimate messages sent via the Linguist list, amoderated mailing list about the science and profession of linguistics.\\n6The corpus consists\\nof 2893 messages:\\n•2412 legitimate messages, obtained by randomly downloading digests from the list ’s\\narchives, breaking the digests into their messages, and removing text added by the list ’s\\nserver.\\n•481 spam messages, received by one of the authors. Attachments, HTML tags, and du-\\nplicate spam messages received on the same day have not been included.\\nSpam messages constitute approximately 16% of Ling-Spam, a rate close to those reported\\nby Cranor and LaMacchia (1998), and Sahami et al. (1998). The Linguist messages areMEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 53'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='less topic-speci ﬁc than one might expect. For example, they contain job postings, software\\navailability announcements, and even ﬂame-like responses.\\nThe number of messages in Ling-Spam is small when compared to established bench-\\nmarks for text categorization, such as the Reuters corpora (Section 1). As a partial remedy,we used 10-fold strati ﬁed cross-validation (Kohavi 1995) in all of our experiments, a tech-\\nnique that increases the con ﬁdence of experimental ﬁndings when using small datasets.\\nThat is, Ling-Spam was partitioned in 10 parts, with each part maintaining the same ratioof legitimate and spam messages as in the entire corpus. Each experiment was repeated 10times, each time reserving a different part as the testing corpus and using the remaining 9parts as the training corpus. Performance scores were then averaged over the 10 iterations.\\nTo the best of our knowledge, the only other publicly available collection of spam and\\nlegitimate messages is Spambase (Gomez Hidalgo et al. 2000).\\n7This is a collection of'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='legitimate messages is Spambase (Gomez Hidalgo et al. 2000).\\n7This is a collection of\\n4601 vectors (Section 3), each representing a spam or legitimate message, with spam mes-sages constituting approximately 39% of the total. Each vector contains the values of 58pre-selected attributes, including the category. Spambase is much more restrictive thanLing-Spam, since the original texts are not available. For example, unlike Ling-Spam,with Spambase one cannot experiment with more attributes, different attribute selectionalgorithms, or attributes corresponding to phrases, rather than individual words.\\n3. Message representation and preprocessing\\nFor the purposes of our experiments, each message is converted into a vector /vectorx=/angbracketleftx\\n1,x2,\\nx3,..., xn/angbracketright, where x1,..., xnare the values of attributes X1,..., Xn, as in the vector space\\nmodel (Salton and McGill 1983). All attributes are binary: Xi=1 if some characteristic\\nrepresented by Xiis present in the message; otherwise Xi=0. In our experiments, attributes\\nrepresent words, i.e., each attribute shows if a particular word (e.g., “adult ”) occurs in the\\nmessage. To avoid treating forms of the same word as different attributes, a lemmatizer wasapplied to the corpora to convert each word to its base form (e.g., “was”becomes “be”).\\n8\\nIt is also possible to use attributes corresponding to phrases (e.g., “be over 21 ”) or non-'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='8\\nIt is also possible to use attributes corresponding to phrases (e.g., “be over 21 ”) or non-\\ntextual characteristics (e.g., whether or not the message contains attachments, or whetheror not it was sent on a Sunday). Previous work (Sahami et al. 1998) indicates that usingboth word and phrasal attributes can lead to marginally better results than using only wordattributes. However, a separate, not yet published, strand of our work found no evidencethat using phrasal attributes in anti-spam ﬁltering can lead to consistent improvements. The\\nresults of Sahami et al. (1998) also show that the inclusion of non-textual attributes canbe more bene ﬁcial. Using non-textual attributes, however, requires an additional manual\\npreprocessing stage to devise candidate attributes of this kind (e.g., one may observe that,unlike spam messages, the legitimate messages of a particular mailing list are rarely sentover weekends, and rarely contain attachments, and hence consider these characteristicsas candidate attributes). As we are interested in anti-spam ﬁlters that can be trained fully\\nautomatically, we did not investigate the use of non-textual attributes, though operationalﬁlters may indeed bene ﬁt from attributes of this type.\\nTo reduce the high dimensionality of the instance space, attribute selection was performed.\\nFirst, words occurring in less than 4 messages were discarded, i.e., they were not considered54 SAKKIS ET AL.'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='Table 1 . Attributes with the highest IGscores in the entire Ling-Spam corpus, ordered by decreasing score.\\n1. language 18. market 35. $\\n2. ! 19. advertise 36. mailing3. university 20. get 37. win4. remove 21. business 38. thousand5. free 22. product 39. now6. linguistic 23. just 40. purchase7. your 24. edu 41. earn8. you 25. guarantee 42. best9. click 26. linguistics 43. de\\n10. money 27. internet 44. buy11. sell 28. bulk 45. easy12. english 29. company 46. dollar13. @ 30. % 47. com14. million 31. save 48. every15. our 32. papers 49. hundred16. income 33. conference 50. customer17. today 34. day\\nas candidate attributes. Then, the Information Gain ( IG) of each candidate attribute Xwith\\nrespect to variable C, denoting the category, was computed as in (1) below, and the attributes\\nwith the mhighest IG-scores were selected, with mvarying in our experiments from 50 to\\n700 by 50. The probabilities were estimated from the training corpora using m-estimates\\n(Mitchell 1997).\\nIG(X,C)=/summationdisplay\\nx∈{0,1},c∈{spam,legit}P(X=x,C=c)·log2P(X=x,C=c)\\nP(X=x)·P(C=c)(1)\\nYang and Pedersen (1997) report that it is feasible to remove up to 98% of the candidate\\nattributes using IGor other similar functions, and preserve, or even improve, generalization\\naccuracy. Table 1 shows the attributes with the highest IGscores in the entire Ling-Spam\\ncorpus. They are mostly words that are common in spam messages (e.g., “remove ”,“free”,'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='corpus. They are mostly words that are common in spam messages (e.g., “remove ”,“free”,\\n“your”) and rare in messages about linguistics, or words that are frequent in Linguist\\nmessages (e.g., “language ”,“university ”,“linguistic ”) and uncommon in spam messages.\\nUnlike typical text categorization tasks, punctuation and other special symbols (e.g., “!”,\\n“@”,“%”,“$”) were not discarded during preprocessing, but they were treated as “words ”.\\nIt can be seen from Table 1 that many of these symbols are among the best attributes inLing-Spam, which is not surprising given that these symbols are used much more frequentlyin spam messages than in normal text.MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 55\\n4. Memory-based learning\\nMemory-based, or “instance-based ”, methods do not construct a unique model for each\\ncategory, but simply store the training examples (Aha et al. 1991, Wilson 1997). Testinstances are then classi ﬁed by estimating their similarity to the stored examples. In its\\nsimplest form, memory-based learning treats instances as points in a multi-dimensionalspace de ﬁned by the attributes that have been selected. Classi ﬁcation is usually performed\\nthrough a variant of the basic k-Nearest-Neighbor ( k-NN) algorithm (Cover and Hart 1967),\\nwhich assigns to each test instance the majority class of its kclosest training instances (its\\nk-neighborhood ).\\nVarious metrics can be used to compute the distance between two instances (Giraud-'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='Various metrics can be used to compute the distance between two instances (Giraud-\\nCarrier and Martinez 1995, Wilson and Martinez 1997). With symbolic (nominal) attributes,as in our case, the overlap metric is a common choice. This metric counts the attributes\\nwhere the two instances have different values. Given two instances /vectorx\\ni=/angbracketleftxi1,xi2,..., xin/angbracketright\\nand/vectorxj=/angbracketleftxj1,xj2,..., xjn/angbracketrighttheir overlap distance is:\\nd(/vectorxi,/vectorxj)≡n/summationdisplay\\nr=1δ(xir,xjr) (2)\\nwhere\\nδ(x,y)≡/braceleftbigg0,ifx=y\\n1,otherwise\\nIn our experiments, we used the TiMBL memory-based learning software (Daelemans\\net al. 2000). TiMBL implements the basic k-NN classi ﬁer as above, except that the k-\\nneighborhood is taken to contain allthe training instances at the kclosest distances , rather\\nthan the kclosest instances. As a result, if there is more than one neighbor at some of the k\\nclosest distances, the neighborhood will contain more than kneighbors.\\nWe have also experimented with different attribute-weighting anddistance-weighting\\nschemes. Unlike the basic k-NN classi ﬁer, where all the attributes are treated as equally im-'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='schemes. Unlike the basic k-NN classi ﬁer, where all the attributes are treated as equally im-\\nportant, attribute-weighting extensions assign different importance scores to the attributes,depending on how well they discriminate between the categories, and adjust the distancemetric accordingly (Aha 1992, Wettschereck et al. 1995). Distance weighting takes memory-based learning one step further, by considering neighbors closer to the input instance asmore important, assigning greater voting weight to them (Dudani 1976, Bailey and Jain1978, Wettschereck 1994). This can reduce the sensitivity of the classi ﬁer to the kparame-\\nter, the neighborhood size. Attribute and distance weighting is considered in more detail inSection 6.\\n5. Cost-sensitive classiﬁcation and evaluation\\nIn anti-spam ﬁltering there are two types of possible error: blocking a legitimate message\\n(classifying a legitimate message as spam), and accepting a spam message (classifying aspam message as legitimate). Let L→SandS→Ldenote the two error types, respectively.\\nPrevious work on anti-spam ﬁltering (Sahami et al. 1998; Androutsopoulos et al. 2000a,56 SAKKIS ET AL.\\nTable 2 . Cost matrix used in this paper.\\nClassi ﬁed as legitimate Classi ﬁed as spam\\nLegitimate message c(L→L)=0 c(L→S)=λ\\nSpam message c(S→L)=1 c(S→S)=0\\n2000c) has assumed that L→Sis generally more severe an error than S→L. This is based'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='2000c) has assumed that L→Sis generally more severe an error than S→L. This is based\\non the assumption that most users can tolerate a small percentage of mistakenly admittedspam messages, while they consider losing legitimate messages much more damaging.Invoking a decision-theoretic notion of cost (Lewis 1995), we assume that L→Sisλ\\ntimes more costly than S→L. More precisely, we use the cost matrix of Table 2, where\\nL→LandS→Sdenote the cases where the ﬁlter classi ﬁes correctly a legitimate or spam\\nmessage, respectively. Here cost is intended to re ﬂect the effort that a subscriber of the list,\\nor the moderator if there is one, wastes to recover from the failures of the ﬁlter.\\n9Correctly\\nclassifying a message ( L→LorS→S) is assigned zero cost, since in this case there is\\nno effort to be wasted. Misclassifying a spam message ( S→L) is assigned unary cost, and\\nmisclassifying a legitimate message ( L→S) is taken to be λtimes more costly. The value\\nof the λparameter depends on the usage scenario of the ﬁlter, as will be discussed below;\\nfor example, whether the ﬁlter deletes messages classi ﬁed as spam, or simply ﬂags them as\\nlow-priority.\\nLetWL(/vectorx) and WS(/vectorx) be the degrees of con ﬁdence of the classi ﬁer that instance /vectorxis\\nlegitimate or spam, respectively. For the basic k-NN algorithm of the previous section, a\\nsuitable measure of the con ﬁdence that a test instance belongs in a category (legitimate'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='suitable measure of the con ﬁdence that a test instance belongs in a category (legitimate\\nor spam) is the percentage of training instances in the k-neighborhood that belongs to\\nthat category. We classify a test instance as spam iff the expected cost of classifying it aslegitimate is greater than the expected cost of classifying it as spam, i.e.,\\nW\\nS(/vectorx)·c(S→L)+WL(/vectorx)·c(L→L)>WL(/vectorx)·c(L→S)+WS(/vectorx)·c(S→S)\\nAccording to the cost matrix, this is equivalent to WS(/vectorx)>WL(/vectorx)·λ, i.e., we use the\\nfollowing criterion:\\n/vectorx/mapsto→SiffWS(/vectorx)\\nWL(/vectorx)>λ (3)\\nWL(/vectorx) and WS(/vectorx) can be scaled to the [0,1] interval, so that their sum equals to 1. In this\\ncase, criterion (3) is equivalent to (4), where tis the classi ﬁcation threshold. A message\\nexceeding this threshold is classi ﬁed as spam; otherwise it is classi ﬁed as legitimate.\\n/vectorx/mapsto→Siff WS(/vectorx)>t,with t=λ\\n1+λ,λ=t\\n1−t(4)\\nIfWL(/vectorx) and WS(/vectorx) are accurate estimates of the conditional probabilities P(C=\\nlegit|/vectorX=/vectorx) and P(C=spam |/vectorX=/vectorx), respectively, criteria (3) and (4) achieve opti-\\nmal results (Duda and Hart 1973).MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 57\\nIn the work by Sahami et al. (1998), which considered anti-spam ﬁlters for personal\\nmailboxes, the threshold twas set to 0.999, which corresponds to λ=999, i.e., blocking a\\nlegitimate message was taken to be as bad as letting 999 spam messages pass the ﬁlter. This'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='legitimate message was taken to be as bad as letting 999 spam messages pass the ﬁlter. This\\ncost scenario introduces a very high bias for classifying messages as legitimate, which maybe reasonable when blocked messages are deleted automatically without further process-ing, because most users would consider losing legitimate messages from their mailboxesunacceptable. For the sake of compatibility with previously published work, we includethis cost scenario ( λ=999) in our experiments, though alternative scenarios are possible,\\nespecially in the case of anti-spam ﬁltering for lists, where lower λvalues are reasonable.\\nFor example, an anti-spam ﬁlter may be used as a preprocessor, to reduce the number\\nof spam messages that the moderator of a list has to consider. In this case, the moderatorexamines only messages that pass the ﬁlter. Rather than being deleted, a message blocked\\nby the ﬁlter can be returned to its sender, explaining the reason of the return and asking the\\nsender to repost the message to the moderator for manual approval (see also Hall 1998).The reply of the ﬁlter would not contain verbatim the address of the moderator (e.g., “Send\\nthe message to moderator\\n∗mylist +com, replacing the star with an at-sign and the plus with\\na dot. ”), to prevent the address of the moderator from being harvested by spam robots that\\nprocess the replies they receive. In this scenario, λ=9(t=0.9) seems more reasonable:'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='process the replies they receive. In this scenario, λ=9(t=0.9) seems more reasonable:\\nblocking a legitimate message is penalized mildly more than letting a spam message pass,to account for the fact that recovering from a blocked legitimate message is more costly(counting the senders ’extra work to repost it and their possible frustration) than recovering\\nfrom a spam message that passed the ﬁlter (which requires the moderator to delete the\\nmessage manually).\\nIn a third scenario, the ﬁlter could simply ﬂag messages it suspects to be spam (e.g.,\\nby adding a pre ﬁx like “[spam?] ”to their subjects), without removing them, to help the\\nsubscribers of the list or newsgroup prioritize the processing of their incoming messages.In this case, λ=1(t=0.5) seems reasonable, since none of the two error types is more\\nsigni ﬁcant than the other.\\nApart from the classi ﬁcation threshold, cost must also be taken into account when de ﬁn-\\ning evaluation measures. Cost-sensitive measures have been employed in recent TRECtextﬁltering tasks (Hull and Robertson 2000), and were recently introduced in anti-spam\\nﬁltering (Androutsopoulos et al. 2000a, 2000c). In cost-insensitive classi ﬁcation tasks,\\naccuracy (Acc) and its complementary error rate (Err=1−Acc) are often used. In our\\ncontext:\\nAcc=N\\nL→L+NS→S\\nNL+NSErr=NL→S+NS→L\\nNL+NS,\\nwhere NY→Zis the number of messages in category Ythat the ﬁlter classi ﬁed as Z,NL=\\nNL→L+NL→Sis the total number of legitimate messages to be classi ﬁed, and NS='),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='NL→L+NL→Sis the total number of legitimate messages to be classi ﬁed, and NS=\\nNS→S+NS→Lthe total number of spam messages.\\nAccuracy and error rate assign equal weights to the two error types ( L→SandS→L).\\nTo make these measures sensitive to cost, each legitimate message is treated, for evaluationpurposes, as if it were λmessages. That is, when a legitimate message is blocked, this counts\\nasλerrors; and when it passes the ﬁlter, it counts as λsuccesses. This leads to the following58 SAKKIS ET AL.\\ndeﬁnitions of weighted accuracy (WAcc ) and weighted error rate (WErr =1−WAcc ):\\nWAcc =λ·NL→L+NS→S\\nλ·NL+NSWErr =λ·NL→S+NS→L\\nλ·NL+NS\\nIn terms of cost, the numerator of WErr above is equal to the total cost incurred by using\\ntheﬁlter on the NL+NSmessages, while the denominator is a normalizing factor equal to\\nthe incurred cost of the worst possible ﬁlter, which misclassi ﬁes all the messages. WAcc is\\nsimply the complement of the normalized incurred cost.\\nWhen one of the categories is more frequent than the other, as in our case and especially\\nwhenλ=9 or 999, the values of accuracy, error rate, and their weighted versions are often\\nmisleadingly high. To get a more realistic picture of a classi ﬁer’s performance, it is common\\nto compare its accuracy or error rate to those of a simplistic baseline approach. We considerthe case where no ﬁlter is present as our baseline: legitimate messages are (correctly)'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='never blocked, and spam messages (mistakenly) always pass. The weighted accuracy andweighted error rate of the baseline are:\\nWAcc\\nb=λ·NL\\nλ·NL+NSWErrb=NS\\nλ·NL+NS\\nThetotal cost ratio (TCR) allows the performance of a ﬁlter to be compared easily to that\\nof the baseline:\\nTCR=WErrb\\nWErr=NS\\nλ·NL→S+NS→L\\nGreater TCR values indicate better performance. For TCR<1, not using the ﬁlter is better.\\nIf cost is proportional to wasted effort, TCR expresses how much effort is wasted to delete\\nmanually all spam messages when no ﬁlter is used ( NS), compared to the effort wasted to\\ndelete manually any spam messages that passed the ﬁlter ( NS→L) plus the effort needed to\\nrecover from mistakenly blocked legitimate messages ( λ·NL→S).\\nWe also present our results in terms of recall (R) and precision (P), which in our case are\\ndeﬁned as below:\\nR=NS→S\\nNS→S+NS→LP=NS→S\\nNS→S+NL→S\\nRecall measures the percentage of spam messages that the ﬁlter manages to block (intu-\\nitively, its effectiveness), while precision measures the degree to which the blocked messagesare indeed spam (intuitively, the ﬁlter’s safety). Despite their intuitiveness, comparing dif-\\nferent ﬁlter con ﬁgurations using recall and precision is dif ﬁcult: each ﬁlter con ﬁguration\\nyields a pair of recall and precision results, and without a single combining measure, likeTCR, that incorporates the notion of cost, it is dif ﬁcult to decide which pair is better.\\n10In'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='10In\\nour experiments, the aim was to optimize the ﬁlter’s performance in terms of TCR.MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 59\\n6. Experimental results\\nWe now proceed with the presentation of our experimental results, providing at the same\\ntime more information on the attribute-weighting and distance-weighting extensions to thebasic k-NN classi ﬁer of Section 4 that we have considered.\\nWeﬁrst investigated the impact of attribute weighting , using a weighting scheme based\\non Information Gain ( IG, Section 3). We performed two sets of experiments on Ling-\\nSpam, with and without attribute weighting. In each set of experiments, three different costscenarios were tested, corresponding to three different values of λ, as discussed in Section\\n5. In each scenario, we varied the number of selected attributes from 50 to 700 by 50, eachtime retaining the attributes with the highest IGscores. Furthermore, each experiment was\\nrepeated with three different neighborhood sizes ( k=1,2, and 10); the results suggested\\nthat there was no need to try more values of kto conclude that attribute weighting has a\\npositive effect.\\nIn the second set of experiments, we investigated the effect of distance weighting . Using\\ntheIG-based attribute weighting scheme, various distance-sensitive voting schemes were\\nexamined, for each cost scenario ( λvalue) and number of selected attributes, with the size\\nof the neighborhood ( k) now set to the maximum size of those considered (10). In a third set'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='of the neighborhood ( k) now set to the maximum size of those considered (10). In a third set\\nof experiments, we examined the effect of dimensionality (number of selected attributes)\\nandneighborhood size (k), using the best-performing con ﬁguration in terms of attribute\\nweighting and distance-weighting. Finally, in a fourth set of experiments, we examined theeffect of the training corpus size , for each cost scenario, using the best con ﬁguration of the\\nprevious experiments.\\nIn all of the experiments, 10-fold strati ﬁed cross-validation was employed (Section 2).\\nWAcc was averaged over the 10 iterations, and TCR was computed as WErr\\nbover the\\naverage WErr (Section 5).\\n6.1. Attribute weighting\\nThe basic k-NN classi ﬁer assigns equal importance to all the attributes. In real-world\\napplications, however, irrelevant or redundant attributes are often included in the represen-tation of the instances. This causes the classi ﬁcation accuracy of k-NN to degrade, unless\\nappropriate weights are assigned to the attributes, corresponding to their relevance to theclassi ﬁcation task. The distance metric, that computes the distance between two instances,\\nhas to be adjusted accordingly, to incorporate the attribute weights. Equation (2) becomes:\\nd(/vectorx\\ni,/vectorxj)≡n/summationdisplay\\nr=1wr·δ(xir,xjr), (5)\\nwhere wris the weight assigned to r-th attribute.\\n6.1.1. Attribute-weighting scheme. Information Gain ( IG) was presented in Section 3 as'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='6.1.1. Attribute-weighting scheme. Information Gain ( IG) was presented in Section 3 as\\nour attribute selection function. The same function can be used for attribute weighting. Anequivalent expression of (1) in information theoretic terms is the following:\\nIG(X,C)=H(C)−/summationdisplay\\nx∈{0,1}P(X=x)·H(C|X=x), (6)60 SAKKIS ET AL.\\nwhere H(C)i st h e entropy of variable C, denoting the category. H(C) measures the uncer-\\ntainty on the category of a randomly selected instance, and is de ﬁned as:\\nH(C)=−/summationdisplay\\nc∈{spam,legit}P(C=c)·log2P(C=c). (7)\\nH(C|X=x)i sd e ﬁned similarly, replacing P(C=c) in (7) by P(C=c|X=x).\\nH(C|X=x) measures the uncertainty on the category given the value of attribute X.\\nEquation (6) subtracts from the entropy of the category ( H(C)) the expected value of the\\nentropy when the value of Xis known, averaged over all the possible values of X.IGis\\ntherefore a measure of how much knowing the value of Xreduces the entropy of C. The\\nlarger the reduction, the more useful Xis in predicting C.\\nThe TiMBL software that we used (Section 4) supports both IGand the standard no-\\nweighting scheme (equal weights for all attributes), hereafter EW. We also experimented\\nwith Gain Ratio (Quinlan 1986), an attribute weighting scheme intended to correct the bias\\nofIGtowards attributes with many uniformly distributed values. In our case, however, where\\nall attributes are binary, there is nothing to be gained from using Gain Ratio instead of IG, and'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='all attributes are binary, there is nothing to be gained from using Gain Ratio instead of IG, and\\nthe results that we obtained con ﬁrm this. Recent versions of TiMBL support two additional\\nattribute-weighting measures, namely chi-squared and shared variance. Although we didnot explore these measures thoroughly, the experiments we conducted on some randomlyselected settings showed no signi ﬁcant difference from IGin agreement with Yang and\\nPedersen (1997).\\n6.1.2. Results of the attribute-weighting experiments. Figures 1 and 2 show the TCR of\\n10-NN ( k=10) for λ=1 andλ=999, respectively, for IGandEWand varying numbers\\nFigure 1 .TCR of 10-NN for λ=1 and two attribute-weighting schemes.MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 61\\nFigure 2 .TCR of 10-NN for λ=999 and two attribute-weighting schemes.\\nof attributes. The corresponding curves for λ=9 are similar to those for λ=1, and are\\nomitted to save space.\\nForλ=1 (as well as λ=9), the conclusion is that 10-NN with IGoutperforms 10-NN\\nwith EW(no attribute weighting). The same pattern was observed in the experiments that\\nwe performed with k=1 and k=2. Another interesting pattern is the dependence on\\nthe dimensionality: when using IG, the performance improved continuously as we retained\\nmore attributes; in contrast, with EWthe number of attributes does not seem to affect the\\nperformance.\\nForλ=999 ( ﬁgure 2), the picture is quite different. The distinguishing characteristic of'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='Forλ=999 ( ﬁgure 2), the picture is quite different. The distinguishing characteristic of\\nthis scenario is the unstable behavior of the ﬁlter, which was already noticed in our previous\\nwork with the Naive Bayes classi ﬁer (Androutsopoulos et al. 2000a, 2000c). The reason\\nis that L→Serrors are penalized so heavily, that a single blocked legitimate message\\ncauses the baseline to outperform the memory-based ﬁlter in terms of WAcc , and TCR to\\ndrop below 1. Given this instability, the objective in this scenario is to select a reliable\\nconﬁguration, which maintains TCR constantly above 1, even if that con ﬁguration does not\\nachieve the highest TCR score.\\nBearing the above goal in mind, the most reliable option for 10-NN when λ=999 is to\\nuse no attribute weighting ( EW), since it attains consistently TCR>1. However, its recall\\ndoes not exceed 17%, which is a rather low performance. On the other hand, IGseems to be\\nless reliable, while its recall reaches 47%, blocking almost half of the spam messages in thisdemanding scenario. Unlike 10-NN, 1-NN and 2-NN do not reach the baseline ( TCR<1) in\\nthis scenario for any of the attribute-weighting schemes. The general impression formed bythese results is that high dimensionality and a large k-neighborhood provide more reliably62 SAKKIS ET AL.\\nTCR scores above the baseline, when λ=999. This impression is strengthened by the results\\npresented in the following sections.'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='presented in the following sections.\\n6.1.3. Interpretation of the results of the attribute-weighting experiments. Theﬁrst strik-\\ning observation in ﬁgure 1 is the poor performance of 10-NN when no attribute weighting\\nis used ( EW). This may seem odd, as k-NN has been used successfully in many domains\\nwithout attribute weighting. This phenomenon can be explained by the fact that EWled\\nto large numbers of instances at equal distances in the k-neighborhood. (The reader is re-\\nminded that in our experiments the k-neighborhood comprises all the neighbors at the k\\nclosest distances ; see Section 4.) For example, 10-NN with 700 attributes and EWgave\\nrise to k-neighborhoods of typically 100 –200 instances, while the respective number of\\ninstances for 50 attributes often exceeded 2000! Since the vast majority of messages arelegitimate, it is reasonable that within neighborhoods of hundreds or thousands of in-stances, most of them will also be legitimate. This forces 10-NN to classify almost all of themessages as legitimate, which is why its performance is very close to that of the baseline.A similar explanation can be given to the fact that for λ=999 ( ﬁgure 2), the behavior of\\n10-NN with EWis closer to the behavior of the baseline, compared to the cases where IG\\nis used.\\nThe question that arises is why there are so many instances in the k-neighborhood when\\nEWis used. The answer lies in the representation of the instances and the use of the overlap'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='EWis used. The answer lies in the representation of the instances and the use of the overlap\\ndistance metric (Eq. (2)). As mentioned in Section 4, the metric counts the number ofattributes where the instances have different values. At the same time, the representationresults in sparse instance vectors, i.e., vectors with many zeros, indicating the absenceof attribute-words in the document. Thus, it is frequently the case that many messagesdiffer in the same number of features, but not necessarily the same features. With EW,\\nall these messages are considered equally distant from an incoming new message. Onthe other hand, IGand most attribute-weighting functions avoid this problem: since every\\nattribute weighs differently (Eq. (5)), two instances are equally distant from an incomingmessage practically only when they are identical; and ﬁnding two different messages at\\nthe same distance from a new message becomes further unlikely as the dimensionalityincreases.\\nA second issue to look into is the effect of dimensionality on the two weighting schemes.\\nIn the case of EW, the large neighborhoods that are formed are also responsible for the\\nstability of the curves with different numbers of retained attributes ( ﬁgures 1 and 2): the\\nmajority class (legitimate) prevails most of the times, and the ﬁlter’s behavior is very\\nsimilar to that of the baseline, regardless of the selected attributes. With IG, there is'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='similar to that of the baseline, regardless of the selected attributes. With IG, there is\\nan exponential decrease in the weight of the last retained attribute, as more attributesare added; this can be seen in ﬁgure 3. The overall marginal increase in performance\\n(ﬁgures 1 and 2) as the number of attributes increases, suggests that the weights assigned\\nto the attributes are appropriate. That is, using the IGweights, the classi ﬁer seems to\\nbe taking advantage of even inferior attributes, by assigning them appropriately lowerimportance.\\nThe main conclusion of the discussion above is that IGattribute-weighting has a positive\\neffect. Hence, we use this scheme in the experiments of the following sections.MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 63\\nFigure 3 . Information Gain scores of best word attributes in Ling-Spam.\\n6.2. Distance weighting\\nHaving chosen to use the IGattribute-weighting scheme, we now examine the effect of\\ndistance-weighting. Distance-weighting does not treat all the instances in a k-neighborhood\\nas equally important, but weighs them according to their distance from the incoming in-stance. The advantages of distance weighting, or else weighted voting, in memory-basedmethods have been discussed extensively in the literature (Dudani 1976, MacLeod et al.1987). The main bene ﬁt is that distance weighting reduces the sensitivity of k-NN to the\\nchoice of k.Akvalue that may be suitable for sparsely populated regions may be un-'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='choice of k.Akvalue that may be suitable for sparsely populated regions may be un-\\nsuitable for dense regions, generating in the latter case neighborhoods that contain manyirrelevant instances. Weighted voting undervalues distant neighbors, without ignoring themcompletely, in order to adapt the effective size of the neighborhood to the local distributionof instances.\\n6.2.1. Distance-sensitive voting schemes. Various voting schemes have been proposed\\nfor distance weighting. We experimented with simple voting schemes that use a distance-sensitive function to weigh the vote of each instance in the k-neighborhood. The ﬁrst scheme\\nuses a linear function to weigh the votes:\\nf\\n0(d)=dmax−d,\\nwhere f0(d) is the weight assigned to a neighbor at distance dfrom the incoming instance,\\nanddmaxis the maximum obtainable distance. The maximum distance occurs when two64 SAKKIS ET AL.\\ninstances differ in every attribute; hence, it is equal to the sum of all attribute weights. The\\nother voting schemes use hyperbolic functions, as below:11\\nfn(d)=1\\ndn,n=1,2,3,....\\nIn all of the voting schemes mentioned above, when one or more neighbors are identical to\\nthe incoming instance (i.e., d=0), we classify the incoming instance to the majority class\\nof the identical instances, ignoring all other neighbors.\\nWith distance-weighting, the con ﬁdence level Wc(/vectorx) that the incoming instance /vectorxbelongs\\nto class c, is computed by the following formula:\\nWc(/vectorx)=/summationdisplay'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='to class c, is computed by the following formula:\\nWc(/vectorx)=/summationdisplay\\nifn(d(/vectorx,/vectorxi))·¯δ(c,C(/vectorxi)),\\nwhere ¯δ(x,y)=1−δ(x,y),/vectorxiranges over the instances in the k-neighborhood, and C(/vectorxi)\\nis the class of neighbor /vectorxi. This formula simply weighs the contribution of each neighbor\\nby its distance from the incoming instance. As in the basic k-NN classi ﬁer, the con ﬁdence\\nlevels for the two categories can be scaled to the [0,1] interval, so that their sum equals to1 (Section 5).\\n6.2.2. Results of the distance-weighting experiments. Figure 4 shows the TCR of 10-NN\\nforλ=1. Each curve corresponds to one voting scheme, and there is an additional curve for\\nFigure 4 .TCR of 10-NN for λ=1 and different distance-sensitive voting schemes (using IGfor attribute\\nweighting).MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 65\\nmajority voting (i.e., no distance weighting). The selected value for k(k=10) is the highest\\nof those considered in our experiments, because distance weighting is usually combinedwith a large value for k: one of the principal reasons for employing distance weighting is\\nthe smoothing of the classi ﬁer’s performance for varying k, so that the optimal selection of\\nits value is less crucial. The corresponding ﬁgure for λ=9 is similar to that for λ=1, with\\nrespect to the conclusions drawn, and is therefore omitted. Finally, for λ=999 the curves\\nare almost unin ﬂuenced by the use of distance weighting, and the corresponding ﬁgure is'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='are almost unin ﬂuenced by the use of distance weighting, and the corresponding ﬁgure is\\nmuch like ﬁgure 2.\\nFigure 4 shows clearly the improvement brought by distance weighting. The improvement\\nis greater when distant neighbors are mildly undervalued. TCR clearly improves whenmoving from f\\n1(d)t o f2(d)t o f3(d), where the best results are obtained. However, the\\ntwo highest curves, f2(d) and f3(d), are quite close to each other, suggesting that fn(d)\\nfunctions for n>3 will not lead to further signi ﬁcant improvements in performance. In fact,\\nthe results obtained for f16(d) and f32(d), as well as higher values of nthat are not shown\\ninﬁgure 4 for the sake of readability, show a reduction in performance as distant neighbors\\nget heavily undervalued.\\n6.2.3. Interpretation of the results of the distance-weighting experiments. The fact that\\nperformance improves as close neighbors are slightly overvalued suggests that local clas-siﬁcation is preferable in this task. It is interesting to note that an important contributor to\\nthe success of the distance-sensitive voting schemes that we explored is that when one ormore neighbors are identical to the incoming instance, the incoming instance is classi ﬁed to'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='the majority class of the identical instances, ignoring all the other neighbors; this producesalmost always the correct results. Even when there are no identical neighbors, functions thatprioritize the nearest neighbors reward the local minority class, if its members are closer tothe incoming instance. (The good performance of the IGattribute-weighting scheme helps\\nfurther, by bringing the “right ”instances closer to the incoming instance.)\\nIt should be stressed that mild distance-weighted voting differs from a reduction of the\\nneighborhood size, in that it does not ignore distant neighbors altogether. In other words,reducing the value of kdoes not improve performance. This is also the reason for the reduced\\nperformance obtained by voting schemes that greatly undervalue distant neighbors. As thevalue of nin the f\\nn(d) function becomes very large, the behavior of the method approaches\\nthe effect of using 1-NN, which is a suboptimal choice.\\n6.3. Neighborhood size and dimensionality\\nThe results of the distance-weighting experiments have indicated a clear superiority of\\nvoting schemes that favor mildly close neighbors. The question that arises is whether thevalue of the kparameter, which determines the size of the neighborhood, can still affect\\nthe performance of the classi ﬁer when such voting schemes are employed. We continued\\nour investigation towards that direction, by examining the in ﬂuence of k, in combination'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='our investigation towards that direction, by examining the in ﬂuence of k, in combination\\nwith the dimensionality of the instance space. In all the experiments below, we used IGfor\\nattribute weighting and f\\n3(d)=1/d3for distance weighting, following the conclusions of\\nthe previous experiments.66 SAKKIS ET AL.\\nFigure 5 .TCR ofk-NN for λ=1 and different kvalues (using IGfor attribute weighting and 1 /d3for distance\\nweighting).\\n6.3.1. Results of the neighborhood size and dimensionality experiments. Figures 5 –7\\nshow some representative curves for various k-neighborhood sizes in each one of the cost\\nscenarios ( λ=1,9 and 999), respectively. Although we carried out experiments for every\\nkfrom 1 to 10, we present only some representative curves, for the sake of brevity. The\\ndiscussion below, however, refers to the whole set of results.\\nInﬁgure 5, a general trend involving the dimensionality and the performance of the classi-\\nﬁer is observed: as the number of retained attributes increases, so does TCR. The relationship\\nbetween the size of the neighborhood and the performance of the classi ﬁer is less clear: TCR\\nseems to be improving as kincreases, up to k=8, when it starts to deteriorate slightly. How-\\never, the extensive overlap of the curves does not allow any safe conclusions to be drawn.\\nFigure 6 presents the two best and two worst curves for λ=9. The behavior of the\\nclassi ﬁer is different here, in that the best performance occurs for very small neighborhoods'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='classi ﬁer is different here, in that the best performance occurs for very small neighborhoods\\n(k=2,3). As the neighborhood grows, TCR declines gradually, but steadily. Interestingly\\nenough, 1-NN performs the worst, which can be attributed to the fact that no distanceweighting can be used in this case.\\nFinally, in ﬁgure 7, we observe the same steep ﬂuctuations of TCR as in ﬁgure 2, indicating\\ntransitions from below baseline performance to above, and vice versa. One exception is k-NN\\nfork<4, which is almost always below the baseline. Another interesting phenomenon is\\nthe fact that 4-NN achieves the highest TCR globally for 250 attributes, and yet it is not\\nuseful, as its performance is not steadily above the baseline; in practice, it is not possible topredict accurately the exact dimensionality of such a narrow peak. The general conclusionis that satisfactory performance is obtained reliably using a high dimensionality and a largek-neighborhood.MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 67\\nFigure 6 .TCR ofk-NN for λ=9 and different kvalues (using IGfor attribute weighting and 1 /d3for distance\\nweighting).\\nFigure 7 .TCR ofk-NN for λ=999 and different kvalues (using IGfor attribute weighting and 1 /d3for distance\\nweighting).68 SAKKIS ET AL.\\n6.3.2. Interpretation of the results of the neighborhood size and dimensionality experi-\\nments. The conclusions to be drawn here are less clear than those in the previous experi-'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='ments. The conclusions to be drawn here are less clear than those in the previous experi-\\nments. The optimal value of kdepends heavily on the selected scenario ( λparameter), and\\nalso correlates with the dimensionality in an unintelligible way. The experiments showedthat for a given scenario, there is no clear ranking of k-NN classi ﬁers for various values\\nofk. Furthermore, the effect of the number of retained attributes does not follow a coher-\\nent pattern. For example, k\\n1-NN may be better than k2-NN for 50 –250 retained attributes,\\nk2-NN may outperform k1-NN for 300 –400 attributes, and in a third interval (e.g., 450 –600\\nfeatures) k1-NN may again be better than k2-NN. To some extent this confusing picture\\nis justi ﬁed by the use of the distance-weighting function, which reduces signi ﬁcantly the\\ndependence of the classi ﬁer on the choice of k. It should also be stressed that the difference\\nbetween the best and worst performing con ﬁguration is much smaller in this set of experi-\\nments than in the previous two sets. The performance of the worst classi ﬁer in ﬁgure 5 does\\nnot fall below TCR=5, which is comparable only to the best classi ﬁer of ﬁgure 1, where\\nno distance weighting was used.\\n6.3.3. Best results overall and comparison to a Naive Bayes ﬁlter. It is interesting to\\ntranslate these results into recall and precision, which are perhaps more intuitive than thecombined TCR measure. Table 3 summarizes the best con ﬁgurations witnessed for each'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='scenario. In every case, IGwas used for attribute weighting and f\\n3(d)=1/d3for distance\\nweighting. A recall around 89% and a precision of over 97% attained for λ=1 make a sat-\\nisfactory, if not suf ﬁcient, performance. The gain from moving to the second scenario with\\nλ=9 is, on the other hand, questionable. A small increase of precision by 1.4% is accompa-\\nnied by a nearly ﬁve times greater decrease in recall. However, this might be acceptable, as\\nalmost no legitimate messages are misclassi ﬁed. In the third scenario ( λ=999), the ﬁlter’s\\nsafety (not blocking legitimate messages) becomes the crucial issue, and therefore precisionmust be kept to 100% at any cost. Ignoring abrupt peaks (e.g., the peaks at 250 attributes inﬁgure 7), which do not correspond to stable con ﬁgurations with respect to minor changes\\nto the number of retained attributes, the highest recall is approximately around 60% (withk=7 and 600 attributes); this is notable though far from perfect.\\nThe results presented in Table 3 are directly comparable to the results of our earlier work\\nwith the Naive Bayes classi ﬁer on the same corpus. Table 4 reproduces our earlier best\\nresults, as presented in (Androutsopoulos et al. 2000b). The comparison of the two tablesshows that the memory-based approach compares favourably to the Naive Bayes classi ﬁer.\\nIn terms of TCR, the memory-based classi ﬁer is clearly better for λ=1, and slightly worse'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='In terms of TCR, the memory-based classi ﬁer is clearly better for λ=1, and slightly worse\\nforλ=9 and slightly better again for λ=999. In the strict scenario ( λ=999), it should\\nbe noted that the performance of the Naive Bayes classi ﬁer is very unstable, i.e., the result\\nTable 3 . Best con ﬁgurations per usage scenario and the corresponding performance.\\nλ k Dimensionality Recall (%) Precision (%) TCR\\n1 8 600 88.60 97.39 7.18\\n9 2 700 81.93 98.79 3.64\\n999 7 600 59.91 100 2.49MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 69\\nTable 4 . Best results of the Naive Bayes ﬁlter.\\nλ Dimensionality Recall (%) Precision (%) TCR\\n1 100 82.35 99.02 5.41\\n9 100 77.57 99.45 3.82\\n999 300 63.67 100 2.86\\nshown in Table 4 corresponds to a very narrow peak with respect to the number of retained\\nattributes. Apart from that peak, the Naive Bayes classi ﬁer never exceeded TCR=1 for\\nλ=999. In contrast, as already discussed, there are intervals where the memory-based\\nclassi ﬁer achieves TCR steadily above 1. Examining recall and precision, it is clear that the\\nmemory-based classi ﬁer improves recall for λ=1 andλ=9, at a small cost of precision.\\n6.4. Corpus size\\nHaving examined the basic parameters of the classi ﬁer, we now turn to the size of the\\ntraining corpus, which was kept ﬁxed to 2603 messages (90% of the whole corpus) in'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='training corpus, which was kept ﬁxed to 2603 messages (90% of the whole corpus) in\\nthe experiments presented above. As before, in every ten-fold experiment the corpus wasdivided into ten parts, and a different part was reserved for testing at each iteration. Fromeach of the remaining nine parts, only x% was used for training, with xranging from 10\\nto 100 by 10. For every cost scenario, the best con ﬁguration was employed (as in Table 3),\\nwith IGattribute weighting and f\\n3(d)=1/d3used for distance weighting.\\nFigure 8 .TCR for variable sizes of training corpus (using IGfor attribute weighting and 1 /d3for distance\\nweighting).70 SAKKIS ET AL.\\nFigure 8 presents the resulting learning curves in terms of TCR. In all three scenarios, the\\ndiagram shows a clear trend of improvement as the size of the training corpus increases. Bystudying the corresponding recall and precision measures, we observed that the increase inperformance is primarily due to an increase in recall. For example, the transition from 80to 100% of the training corpus for λ=1 raises recall by nearly 10%, decreasing at the same\\ntime precision by only 1.6%.\\nThe increase of TCR inﬁgure 8 is generally mild, with the exception of a leap from\\n4.5 to 6.5 for λ=1. There is no indication that the learning curves have approached an\\nasymptote for any size of the training set, which suggests that a larger corpus might giveeven better results. This is particularly encouraging for the strict scenario ( λ=999), where'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='there is still a large scope for improvement. It is also notable that, when λ=999, TCR\\nremains above the baseline for all the sizes of the training corpus. In contrast, the NaiveBayes ﬁlter that we examined in previous work (Androutsopoulos et al. 2000a) had reached\\nTCR>1 only for 100% of the training corpus. These ﬁndings indicate that a memory-based\\nanti-spam ﬁlter for mailing lists may be viable in practice, even when absolute precision is\\nrequired.\\n7. Conclusions\\nIn this paper, we presented a thorough empirical evaluation of a memory-based approach to\\nanti-spam ﬁltering for mailing lists. In contrast to anti-spam ﬁlters for personal mailboxes,\\nanti-spam ﬁlters for mailing lists, which apply equally well to Usenet-like newsgroups,\\nare more tractable in terms of evaluation, since publicly available archives can be used asstandard benchmarks without privacy violations. We introduced cost-sensitive evaluationmeasures, along with indicative cost scenarios, discussing how these scenarios relate toadditional safety nets that may be available in mailing lists, such as using the ﬁlter as an\\naid to a moderator. Our experimental results show that the use of a memory-based ﬁlter\\ncan be justi ﬁed, even in the strictest scenario, where the blocking of a legitimate message\\nis practically unacceptable. Furthermore, our memory-based ﬁlter compares favorably to\\nthe probabilistic classi ﬁer that we used in our earlier work on this problem. Overall, then,'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='the probabilistic classi ﬁer that we used in our earlier work on this problem. Overall, then,\\nour work indicates that anti-spam ﬁltering using text categorization techniques is feasible,\\nat least for mailing lists and newsgroups, despite the fact that spam messages cover a verywide spectrum of topics, and hence are much less homogeneous than other categories thathave been considered in the past. Further improvements may be possible by incorporatingnon-textual attributes, though this requires an additional manual preprocessing stage todevise candidate attributes of this kind.\\nThe most important contribution of this article is the exploration of various parameters of\\nthe memory-based method, such as attribute weighting, distance weighting, and neighbor-hood size. Our experiments show that an attribute-weighting scheme based on InformationGain has a positive effect, and that voting schemes that mildly devalue distant neighborsare bene ﬁcial. We have also shown that by using the right attribute-weighting and distance-\\nweighting schemes, the size of the neighborhood of the incoming instances becomes lessimportant. In addition to the parameters of the classi ﬁcation method, we have explored two\\nimportant parameters of the particular classi ﬁcation task: the dimensionality and the sizeMEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 71\\nof the training corpus. Regarding the dimensionality of the task, which is determined by'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='of the training corpus. Regarding the dimensionality of the task, which is determined by\\nthe number of retained attributes after the initial selection, our results show that its effecton classi ﬁcation performance is positive when using attribute-weighting, i.e., the perfor-\\nmance improves as the number of retained attributes increases. Similarly, the performanceimproves as the size of the training corpus increases, which is an indication that a largertraining corpus might lead to even better results.\\nThe experiments presented here have opened a number of interesting research issues,\\nwhich we are currently examining. In the context of the memory-based classi ﬁer, we are\\nexamining non-binary representations of the messages, by taking into account the frequencyof a word within a message. The standard TFIDF weighting, with cosine normalization tocope with variable document length, may be tried as well. Additionally, we would like toexamine other attribute-weighting functions and their relationship to the chosen represen-tation of instances. Weighted voting can also bene ﬁt from functions that do not depend on\\nthe absolute distance from the input instance, but take into account the local properties ofthe neighborhood, as shown by Zavrel (1997). Finally, our main interest is in combiningmemory-based, probabilistic, and other induced classi ﬁers, within a classi ﬁer ensemble'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='framework, such as stacking (Wolpert 1992). Initial results (Sakkis et al. 2001) indicate thatthis can improve anti-spam ﬁltering performance further.\\nAcknowledgments\\nThe authors wish to thank the anonymous reviewers for their constructive comments.\\nNotes\\n1. Consult http://www.cauce.org/, http://spam.abuse.net/, and http://www.junkemail.org/ for further information\\non UCE and related legal issues.\\n2. See http://www.esi.uem.es/ ∼jmgomez/spam/index.html for a collection of resources related to machine learn-\\ning and anti-spam ﬁltering.\\n3. An on-line bibliography on cost-sensitive learning can be found at http://home.ptd.net/ ∼olcay/cost-\\nsensitive.html.\\n4. See http://about.reuters.com/researchandstandards/corpus/.5. An alternative path is to share suitably encrypted mailboxes, which will allow different representation and\\nlearning techniques to be compared, while still maintaining privacy. We have recently started to explore thispath as well (Androutsopoulos et al. 2000b).\\n6. The Linguist list is archived at http://listserv.linguistlist.org/archives/linguist.html.7. Spambase was created by M. Hopkins, E. Reeber G. Forman and J. Suermondt. It is available from http://\\nww.ics.uci.edu/ ∼mlearn/MLRepository.html.\\n8. We used morph , a lemmatizer included in the GATE system. See http://www.dcs.shef.ac.uk/research/groups/\\nnlp/gate/.\\n9. An alternative analysis, e.g., from the view-point of an ISP provider, could also take into account the cost of'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='the bandwidth that is wasted by mistakenly admitting spam messages.\\n10. The F-measure, which is often used in text classi ﬁcation to combine recall and precision (e.g., Riloff and\\nLehnert 1994), cannot be used here, because it is unclear how its weighting factor ( βparameter) relates to the\\ncost ratio of the two error types ( λ) in our experiments.\\n11. f\\n1(d) was proposed by Dudani (1976). f0(d) is a simpli ﬁed version of a similar function also proposed by\\nDudani (1976). fn(d), where n>1, are our own stricter versions, that follow naturally from f1(d).72 SAKKIS ET AL.\\nReferences\\nAha WD (1992) Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. International\\nJournal of Man-Machine Studies, 36:267 –287.\\nAha WD, Kibler D and Albert MK (1991) Instance-based learning algorithms. Machine Learning, 6:37 –66.\\nAndroutsopoulos I, Koutsias J, Chandrinos KV , Paliouras G and Spyropoulos CD (2000a) An evaluation of Naive\\nBayesian anti-spam ﬁltering. In: Proceedings of the Workshop on Machine Learning in the New Information\\nAge, 11th European Conference on Machine Learning (ECML 2000), Barcelona, Spain, pp. 9 –17.\\nAndroutsopoulos I, Koutsias J, Chandrinos KV and Spyropoulos CD (2000b) An experimental comparison of Naive\\nBayesian and keyword-based anti-spam ﬁltering with encrypted personal e-mail messages. In: Proceedings of'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR 2000), Athens, Greece, pp. 160 –167.\\nAndroutsopoulos I, Paliouras G, Karkaletsis V , Sakkis G, Spyropoulos CD and Stamatopoulos P (2000c) Learning\\ntoﬁlter spam e-mail: A comparison of a Naive Bayesian and a memory-based approach. In: Proceedings of the\\nWorkshop on Machine Learning and Textual Information Access, 4th European Conference on Principles andPractice of Knowledge Discovery in Databases (PKDD 2000), Lyon, France, pp. 1 –3.\\nBailey T and Jain AK (1978) A note on distance-weighted k-nearest neighbor rules, IEEE Transactions on Systems,\\nMan, and Cybernetics, 8(4):311 –313.\\nCohen WW (1996) Learning rules that classify e-mail. In: Proceedings of the AAAI Spring Symposium on\\nMachine Learning in Information Access, Palo Alto, US, pp. 18 –25.\\nCohen WW and Singer Y (1999) Context-sensitive learning methods for text categorization. ACM Transactions\\non Information Systems, 17(2):141 –173.\\nCover T and Hart P (1967) Nearest neighbor pattern classi ﬁcation. IEEE Transactions on Information Theory,\\n13:21 –27.\\nCranor LF and LaMacchia BA (1998) Spam!, Communications of ACM, 41(8):74 –83.\\nCristianini N and Shawe-Taylor J (2000) An Introduction to Support Vector Machines and Other Kernel-Based\\nLearning Methods, Cambridge University Press.\\nDaelemans W, Van den Bosch A and Weijters A (1997) IGTREE: Using trees for compression and classi ﬁcation'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='in lazy learning algorithms. Arti ﬁcial Intelligence Review, 11:407 –423.\\nDaelemans W, Zavrel J, van der Sloot K and van den Bosch A (2000) TiMBL: Tilburg Memory Based Learner,\\nversion 3.0, Reference Guide. ILK, Computational Linguistics, Tilburg University. http:/ilk.kub.nl/ ∼ilk/papers.\\nDrucker HD, Wu D and Vapnik V (1999) Support vector machines for spam categorization. IEEE Transactions\\nOn Neural Networks, 10(5):1048 –1054.\\nDuda RO and Hart PE (1973) Bayes decision theory. Chapter 2 in Pattern Classi ﬁcation and Scene Analysis, John\\nWiley, pp. 10 –43.\\nDudani AS (1976) The distance-weighted k-nearest neighbor rule. IEEE Transactions on Systems, Man and\\nCybernetics, 6(4):325 –327.\\nGiraud-Carrier C and Martinez RT (1995) An ef ﬁcient metric for heterogeneous inductive learning applications\\nin the attribute-value language. Intelligent Systems, pp. 341 –350.\\nG´omez Hidalgo JM, Ma ˜na L ´opez M and Puertas Sanz E (2000) Combining text and heuristics for cost-\\nsensitive spam ﬁltering. In: Fourth Computational Natural Language Learning Workshop, CoNLL-2000, Lisbon,\\nPortugal, pp. 99 –102.\\nHall RJ (1998) How to avoid unwanted email. Communications of ACM, 41(3):88 –95.\\nHull D and Robertson S (2000) The TREC-8 ﬁltering track ﬁnal report. In: Proceedings of the Eighth Text Retrieval\\nConference (TREC-8), NIST Special Publication 500-246, pp. 35 –56.\\nJoachims T (1997) A probabilistic analysis of the Rocchio algorithm with TFIDF for text categoriza-'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='Joachims T (1997) A probabilistic analysis of the Rocchio algorithm with TFIDF for text categoriza-\\ntion. In: Proceedings of ICML-97, 14th International Conference on Machine Learning, Nashville, US,pp. 143 –151.\\nKohavi R (1995) A study of cross-validation and bootstrap for accuracy estimation and model selection. In:\\nProceedings of the 14th International Joint Conference on Arti ﬁcial Intelligence (IJCAI-95), Morgan Kaufmann,\\npp. 1137 –1143.\\nLang K (1995) Newsweeder: Learning to ﬁlter netnews. In: Proceedings of the 12th International Conference on\\nMachine Learning, Stanford, CA, pp. 331 –339.MEMORY-BASED APPROACH TO ANTI-SPAM FILTERING 73\\nLewis D (1995) Evaluating and optimizing autonomous text classi ﬁcation systems. In: Proceedings of the 18th\\nAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’95), New York, pp. 246 –254.\\nMacLeod ESJ, Luk A and Titterington DM (1987) A re-examination of the distance-weighted k-nearest neighbor\\nclassi ﬁcation rule. IEEE Transactions on Systems, Man, and Cybernetics, 17(4):689 –696.\\nMitchell TM (1997) Machine Learning. McGraw-Hill.Pantel P and Lin D (1998) SpamCop: A spam classi ﬁcation and organization program. Learning for Text\\nCategorization —Papers from the AAAI Workshop, Madison Wisconsin, pp. 95 –98. AAAI Technical Report\\nWS-98-05.\\nPayne TR and Edwards P (1997) Interface agents that learn: An investigation of learning issues in a mail agent'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='interface. Applied Arti ﬁcial Intelligence, 11(1):1 –32.\\nQuinlan JR (1986) Induction of Decision Trees. Machine learning, 1(1):81 –106.\\nQuinlan JR (1993) C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, California.Rocchio, J. (1971). Relevance feedback information retrieval. In: Salton G, ed., The Smart Retrieval System —\\nExperiments in Automatic Document Processing, Prentice-Hall, Englewood Cliffs, NJ, pp. 313 –323.\\nSahami M, Dumais S, Heckerman D and Horvitz E (1998) A Bayesian approach to ﬁltering junk e-mail. Learning\\nfor Text Categorization —Papers from the AAAI Workshop, Madison Wisconsin, pp. 55 –62. AAAI Technical\\nReport WS-98-05.\\nSakkis G, Androutsopoulos I, Paliouras G, Karkaletsis V , Spyropoulos CD and Stamatopoulos P (2001) Stacking\\nclassi ﬁers for anti-spam ﬁltering of e-mail. In: Proceedings of the Sixth Conference on Empirical Methods in\\nNatural Language Processing (EMNLP 2001) , Carnegie Mellon University, Pittsburgh, PA, USA, pp. 44 –50.\\nSalton G and Buckley C (1988) Term-weighting approaches in automatic text retrieval. Information Processing\\nand Management, 24(3):513 –523.\\nSalton G and McGill MJ (1983) Introduction to Modern Information Retrieval. McGraw-Hill.Schapire RE and Singer Y (2000) BoosTexter: A boosting-based system for text categorization. Machine Learning,\\n39(2/3):135 –168.\\nSebastiani F (2001) Machine Learning in Automated Text Categorization. Revised version of Technical Report'),\n",
       " Document(metadata={'title': 'A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists', 'year': 2003}, page_content='IEI-B4-31-1999, Istituto di Elaborazione dell ’Informazione, Consiglio Nazionale delle Ricerche, Pisa, Italy.\\nWettschereck D (1994) A Study of Distance-Based Machine Learning Algorithms. PhD thesis, Oregon State\\nUniversity.\\nWettschereck D, Aha WD and Mohri T (1995) A Review and Comparative Evaluation of Feature Weighting\\nMethods for Lazy Learning Algorithms, Technical Report AIC-95-012, Washington, DC: Naval ResearchLaboratory, Navy Center for Applied Research in Arti ﬁcial Intelligence.\\nWilson DR (1997) Advances in Instance-Based Learning Algorithms. PhD thesis, Brigham Young University.Wilson DR and Martinez RT (1997) Improved heterogeneous distance functions. Journal of Arti ﬁcial Intelligence\\nResearch, 6(1):1 –34.\\nWolpert D (1992) Stacked generalization. Neural Networks, 5(2):241 –260.\\nYang Y and Pedersen JO (1997) A comparative study on feature selection in text categorization. In: Proceedings\\nof ICML-97, 14th International Conference on Machine Learning, Nashville, US, pp. 412 –420.\\nZavrel J (1997) An empirical re-examination of weighted voting for k-NN. In: Proceedings of the 7th Belgian-\\nDutch Conference on Machine Learning (BENELEARN-97), Tilburg, The Netherlands.'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='1556-603x/14/$31.00©2014ieee  February 2014 | ieee Computational intelligen Ce magazine    31\\nAbstract —There has been a rapid growth in the number of \\ncybercrimes that cause tremendous financial loss to \\norganizations. Recent studies reveal that cybercriminals tend to collaborate or even transact cyber-attack tools via the “dark \\nmarkets” established in online social media. Accordingly, it \\npresents unprecedented opportunities for researchers to tap into these underground cybercriminal communities to develop better insights about collaborative cybercrime activities so as to \\ncombat the ever increasing number of cybercrimes. The main \\ncontribution of this paper is the development of a novel weakly supervised cybercriminal network mining method to facilitate cybercrime forensics. In particular, the proposed method is \\nunderpinned by a probabilistic generative model enhanced by a \\nnovel context-sensitive Gibbs sampling algorithm. Evaluated based on two social media corpora, our experimental results reveal that the proposed method significantly outperforms the Raymond Y.K. Lau  \\nDepartment of Information Systems,  \\nCity University of Hong Kong, Hong Kong SAR\\nYunqing Xia\\nDepartment of Computer Science and Technology,  Tsinghua University, Beijing 100084, CHIn A\\nYunming Ye \\nShenzhen Key Laboratory of Internet Information  Collaboration, Shenzhen graduate School,  Harbin Institute of Technology, Shenzhen 518055, CHIn A\\nDigital Object Identifier 10.1109/MCI.2013.2291689'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='Digital Object Identifier 10.1109/MCI.2013.2291689\\nDate of publication: 14 January 2014A Probabilistic Generative Model \\nfor Mining Cybercriminal Networks \\nfrom Online Social Media\\n© image 100 ltd.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. 32    ieee Computational intelligen Ce magazine | February 2014Latent Dirichlet Allocation (LDA) based method and the \\nSupport Vector Machine (SVM) based method by 5.23% and 16.62% in terms of Area Under the ROC Curve (AUC), \\nrespectively. It also achieves comparable performance as the \\nstate-of-the-art Partially Labeled Dirichlet Allocation (PLDA) method. T o the best of our knowledge, this is the first successful research of applying a probabilistic generative model to mine cybercriminal networks from online social media. \\nI. Introduction \\nccording to Hewlett-Packard’s cybercrime report \\nreleased in 2012, there was a 42% growth in the num-\\nber of cybercrimes when compared to the figure of \\nthe previous year, with organizations experiencing an \\naverage of 102 successful attacks per week.1 The average annu-\\nalized cost of cybercrime also surged to 8.9 million per attacked organization. Existing cyber-security technologies \\nsuch as intrusion prevention systems (IPSs) and anti-malware'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='such as intrusion prevention systems (IPSs) and anti-malware \\nsoftware are not effective enough to protect organizations from various cybercrimes, particularly the distributed denial of ser-vice (DDoS) attacks. One of the reasons is that existing cyber-security solutions are weak in cybercrime forensics and predic-\\ntions. We believe that applying advanced computational \\nintelligence methods to uncover the underground cybercrimi-nal networks is the first step toward comprehensive cybercrime forensics, which contributes to combat the rapidly growing \\ntrend of cybercrimes. \\nIncreasingly more evidences have shown that cybercrimi-\\nnals tend to exchange cyber-attack knowledge, or even transact \\ncyber-attack tools such as Botnets through the dark markets  \\nestablished in online social media (Denning and Denning, \\n2010; Franklin et al., 2007; Goel, 2011). Such a trend offers \\nunprecedented opportunities for cyber-security analysts and \\n1http://www.hpenterprisesecurity.com/ponemon-2012-cost-of-cyber-crime-study-\\nreportsresearchers to tap into the security intelligence embedded in the conversational messages posted to online social media so as to develop better insights about the collaborative cybercrimes \\nand the communities of cybercriminals. By means of auto-'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='and the communities of cybercriminals. By means of auto-\\nmated mining of collaborative cybercriminal networks, the effectiveness and efficiency of cybercrime forensics can be greatly enhanced. Figures 1 and 2 highlight examples of cyber-criminals’ dialogs captured in online social media. Figure 1 \\nshows the hacker group called Anonymous (also known as \\nFawkesSecurity) who performed a live broadcast on T witter and claimed responsibility for the series of DDoS attacks against Hong Kong and Shanghai Banking Corporation (HSBC) in \\nOctober 2012. Figure 2 depicts a dialog about the sales of \\ncyber-attack tools on a popular Internet forum. \\nWhile much research efforts have been devoted to social \\ncommunity mining and social network analysis in the past two decades, little work is performed for the automated discovery \\nand analysis of cybercriminal networks, a special type of social \\nnetwork. Given the tremendous financial losses incurred due to cybercrimes, there is a pressing need to exploit advanced computational intelligence approaches for the development of automated cybercriminal network mining method to facilitate \\ncybercrime forensics. Existing network mining methods \\nmainly utilize manually constructed relationship lexicons (Xia et al., 2013), or manually defined lexico-syntactic patterns (Bao et al., 2008; Li et al., 2006) to uncover the implicit rela-tionships of entities (e.g., companies) from free texts. However,'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='since natural languages are ambiguous and very flexible, pre-\\ndefined lexicons or lexico-syntactic patterns can only identify a limited number of explicit relationships embedded in texts. As a result, the recall achieved by lexicon-based network min-\\ning methods tends to be low.\\nSince concept-level approaches to natural language process-\\ning (Cambria et al., 2013) can better grasp the implicit seman-\\ntics associated with text, they often outperform lexicon-based methods by leveraging on semantic networks or knowledge \\nbases. However, so far concept-level approaches have only been \\napplied to tasks that are loosely related to cybercrimes, e.g., troll filtering (Cambria et al., 2010) and cyberbulling (Dinakar et al., 2012). A possible reason for this might be that it is difficult to mine concept-level knowledge for the cybercrime domain.\\nSupervised machine learning methods do not rely heavily \\non external knowledge and, hence, could represent a viable solution for cybercriminal network mining. However, such methods often require a lot of time and resources to build a dataset that is good enough to effectively train a machine learn-\\ning classifier. Moreover, it is quite difficult to label messages that \\ncapture implicit and hidden cybercriminal relationships even if the \\nannotators are security experts. The following two examples reveal an explicit collaborative and an implicit transactional cyber-\\ncriminal relationships embedded in two messages extracted'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='criminal relationships embedded in two messages extracted \\nfrom online social media. The former is relatively easy to iden-tify by computers using pre-defined relationship indicators such as join captured in a relationship lexicon, whereas the latter is \\nmore difficult for computers to detect automatically.\\nFigure 1  The exchange of knowledge between cybercriminals on Twitter.\\na\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. February 2014 | ieee Computational intelligen Ce magazine    33 ❏Explicit Collaborative Relationship: “Wanna hack remote \\nmachines & turn them to U botnets? Join me in hacker-talk. U’ve got to have fun!”\\n ❏Implicit Transactional Relationship: “T ools’re really cool, but need U support for continuous upgrading; pls. pm me if interested.”\\nThe main contributions of our research reported in this \\npaper are the development and evaluation of a novel weakly \\nsupervised cybercriminal network mining method which can uncover both explicit and implicit relationships among cyber-criminals based on their conversational messages posted to \\nonline social media. T o the best of our knowledge, this is the \\nfirst successful research for developing a probabilistic generative model to mine cybercriminal networks from online social media. In this paper, we focus on mining two types of seman-\\ntics: transactional and collaborative relationships among cybercrim-'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='tics: transactional and collaborative relationships among cybercrim-\\ninals. The basic intuition behind the proposed computational \\nmethod is that a probabilistic generative model is applied to extract multi-word expressions describing two types of cyber-criminal relationships in unlabeled messages. These dynamically \\ndiscovered concept descriptions are then applied to identify \\nboth explicit and implicit cybercriminal relationships embed -\\nded in online messages.\\nOur concept-based cybercriminal relationship classification \\nmethod is more promising than keyword-based methods \\nwhich relies on semantics rather than syntax. Concept-based \\napproaches have already been shown to perform better than word-based methods for tasks such as topic modeling (Rajago-pal et al., 2013), domain adaptation (Xia et al., 2013) and opin-ion mining (Cambria et al., 2013). For example, a keyword-\\nbased method may mistakenly classify the message “just give \\nthe port side a hack with the axe” as a cybercrime message if the keyword “hack” is solely used to identify online hacking activities. In contrast, concept-based methods leverage on \\nsemantic sets to classify messages. For instance, the concept \\n“hacking in the cyberspace” is represented by a set of semanti-cally related terms such as {“computer”, “hack”, “online”, “network”, …,}. Accordingly, our proposed concept-based method will not classify the aforementioned message as a \\ncybercrime related message because the sample message bears'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='cybercrime related message because the sample message bears \\nlittle semantic similarity with the given concept.\\nThe remainder of the paper is organized as follows: Sec-\\ntion\\xa02 provides an overview of existing research related to our study and compares the existing methods with ours; Section 3 \\nillustrates the computational details of the proposed weakly \\nsupervised cybercriminal network mining method; Section 4 describes the evaluation procedures and discusses our experi-mental results; finally, Section 5 offers concluding remarks and describes future directions of our research work. \\n2. Related Work \\nThe work closest to ours is the application of a probabilistic latent semantic analysis (pLSA) model to mine latent topics describing cybercrimes from blog messages (Tsai and Chan, \\n2007). However, only a qualitative evaluation about the quality of the extracted cybercrime concepts was performed. More-\\nover, the pLSA model suffers from the so-called problem of over-fitting and the extraordinary computational costs of learn-ing a large number of model parameters (e.g., learning model \\nparameters per document) (Blei et al., 2003; Rosen-Zvi et al., \\n2010). Our proposed method not only discovers latent topics (concepts) about cybercrimes from online social media, but it also leverages these latent topics to uncover a cybercriminal network. Du and Yang (2011) applied social network analysis \\n(SNA) method to construct cyber-attack graphs based on the'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='(SNA) method to construct cyber-attack graphs based on the \\nsource and destination IP addresses of cyber-attacks extracted from the Internet. Our proposed approach can tap into online social media and utilize high-level features (e.g., concepts \\nembedded in conversational messages) to uncover the collabor-\\native patterns of cybercriminals. \\nHu et al. (2009) applied dynamic social network analysis \\nmethods to identify facilitators of co-offending relationships in a large-scale narcotics network consisting of individuals and \\nvehicles. Wu and Banzhaf (2010) examined various computa-\\ntional intelligence methods (e.g., artificial neural networks, fuzzy systems, evolutionary computation methods, artificial immune systems, and swarm intelligence) for intrusion detec-tion using low-level network-based features. Ting et al. (2010) \\napplied the Apriori association rule mining method to linkage \\nidentification which attempted to identify the tightly linked genes and bound them together to form building blocks to alleviate the disruptiveness induced by crossover operations. Abbass et al. (2011) proposed the computational red team \\n(CRT) method that leverages the relationships between both \\nblue team and red team of agents to improve agents’ decision making processes. Martino and Sperduti (2010) evaluated two classes of methods, namely neural networks and kernel meth-\\nods for modeling entities and their relationships in the forms of \\ntrees and graphs.'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='ods for modeling entities and their relationships in the forms of \\ntrees and graphs.\\nThe CoMiner system applied Natural Language Processing \\n(NLP) techniques and predefined lexico-syntactic patterns to identify competitive relationships and competitive domains \\namong companies (Bao et al., 2008; Li et al., 2006). In particular, \\nthe Point-wise Mutual Information (PMI) measure was applied to estimate the strength of a competitive relationship between two companies. Xia et al. (2013) employed a predefined relation-ship lexicon and shallow NLP techniques to develop the CoNet \\nsystem for the discovery of potential business relationships from \\nFigure 2  A transactional dialog between cybercriminals at www.\\nhafeezcentre.pk.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. 34    ieee Computational intelligen Ce magazine | February 2014online financial news articles. T wo different types of business rela-\\ntionships such as cooperative and competitive relationships were identified according to a set of pre-defined relationship indicators \\ncaptured in a relationship lexicon. Since the CoNet system \\nmainly relied on a limited set of seeding relationship indicators to identify business relationships, the recall of such a system may be low. Our research differs from the aforementioned studies in that we examine the mining of cybercriminal networks instead \\nof business networks.'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='of business networks.\\n3. A Methodology of Collaborative  \\nCybercriminal Network Discovery \\nThe basic intuition behind the proposed cybercriminal network \\ndiscovery method is that latent concepts describing specific types of cybercriminal relationships (e.g., transacting cyber-attack tools) \\nare extracted by a probabilistic generative model to bootstrap the \\nperformance of cybercriminal relationship identification. Figure 3 illustrates the main steps of the proposed cybercriminal network mining methodology. First, conversational messages \\nUS i that refer \\nto at least two users are extracted from a collection of unlabeled documents (e.g., online messages posted by hackers). In addition, generic seeding relationship indicators are applied to label a set of \\nmessages \\nLS i describing transactional activities, or collaborative \\ncyber-attack activities among cybercriminals.\\nThe entities (i.e., individuals or groups of cybercriminals) \\nbeing referred to within the conversational messages are identi-\\nfied using an extended named entity recognition (NER) mod-ule of GATE (Maynard et al., 2001). An initial list of well-known cybercriminals is provided by cyber-security experts to enrich the ordinary entity dictionary of GATE. In addition, the \\nuser identities associated with these messages are extracted'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='user identities associated with these messages are extracted \\nbased on the publicly available user profiles on online social media. The extracted messages are then fed into an LDA-based (Blei et al., 2003; Rosen-Zvi et al., 2010; Steyvers et al., 2004) \\ntopic modeling module to extract relevant concepts (i.e., the \\ntopics describing various cybercriminal relationships) to allevi-ate the low-recall problem of a purely lexicon-based relation-ship identification approach. In particular, we developed a novel context-sensitive (CS) Gibbs sampling algorithm to implement \\nthe LDA-based probabilistic generative model.\\nMessage Extraction\\nMessage ExtractionILM-Based\\nRelationship\\nInferenceLatent Topic Modeling\\nCS Gibbs\\nSampling\\nOnline Social\\nMediaSeeding\\nRelationship\\nIndicatorsMessage\\nLabelingUnlabeled Message\\nLabeled MessageLatent Concepts\\nLaplacian\\nSemantic\\nInferenceTransactional\\nand\\nCollaborative\\nConcept\\nLabelingUS1\\nUS2\\n...\\nUSn\\nTraining Setd1\\nd2\\n...\\ndN\\nTest Setd1\\nd2\\n...\\ndMLS1Topic1\\nTopic2\\n...\\nTopick\\nTopicTran1\\nTopicTran2\\n...\\nSelected ConceptsTopicTranNTopicCol1\\nTopicCol2\\n...\\nTopicColMLS2\\n...\\nLSn\\nTest MessageTS1\\nTS2\\n...\\nTSm\\nLabeled Cybercriminal\\nRelationshipsTS1\\nTS2\\n...\\nTSmTran\\nColl\\n...\\nColl\\nFigure 3  A probabilistic generative model for collaborative cybercriminal network mining.'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='...\\nColl\\nFigure 3  A probabilistic generative model for collaborative cybercriminal network mining.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. February 2014 | ieee Computational intelligen Ce magazine    35By applying a Laplacian-based semantic ranking method \\n(Song et al., 2009), the messages with relationship labels LS i \\nare used to infer the semantic labels (e.g., transactional or collabor-\\native) of the mined latent concepts. Then, these labeled con-\\ncepts are applied to determine the implicit cybercriminal rela-\\ntionship embedded in an arbitrary message TS i that refers to at \\nleast two parties (i.e., cybercriminals). More specifically, we develop a novel inferential language modeling (ILM) method \\nto infer the hidden cybercriminal relationship in a message. \\nFinally, based on the identified cybercriminal relationship between each pair of users, a cybercriminal network is gener-ated for a given period. The proposed computational method-ology is general enough to support the discovery of any types \\nof cybercrime related relationships if the corresponding sets of \\nseeding relationship indicators are provided. Since our method-ology requires a small set of seeding relationship indicators as input, it is a weakly supervised relationship mining method. \\nHowever, the distinct advantage of the proposed method is that'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='However, the distinct advantage of the proposed method is that \\nthere is no need for the expensive manual labeling of a large number of training messages.\\n3.1. Learning Latent Concepts\\nFor LDA-based latent concept learning, each conversational message \\ndDd of an unlabeled social media corpus D is con-\\nsidered to be characterized by a multinomial distribution ,i \\nwhich is in turn controlled by a hyper-parameter, a Dirichlet priori \\na (Blei et al., 2003). Z represents the set of latent con-\\ncepts characterizing .D A latent concept zZid (e.g., a type of \\ncybercriminal relationship) is selected according to the multino-mial distribution \\n.i Given the concept ,zi a term tk is then gen-\\nerated according to the multinomial distribution ,z controlled \\nby another hyper-parameter, a Dirichlet priori .b Our ultimate \\ngoal is to infer the conditional probability Prtzki z= ^h  which \\nrepresents a latent concept about a specific type of cybercriminal relationship. However, directly computing the multinomial distri-\\nbution \\nz or i is computationally very expensive. Accordingly, \\nGibbs sampling (Geman and Geman, 1984), a Markov Chain \\nMonte Carlo algorithm, is extended to estimate the multinomial \\ndistributions of the LDA model. For Gibbs sampling, a Markov \\nchain is established by repeatedly drawing a latent topic for each observable term, based on its conditional probability over other variables (Steyvers et al., 2004). The approximations \\nz and i of'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='z and i of \\nthe multinomial distributions z and i are defined as follows.\\n NZN\\nnpZD\\nnZnpZD\\ni\\naa=++\\ndll/ (1)\\n NN\\nVmn\\nmnVZ\\nmVVZ\\nzb\\nb=++\\ndll/. (2)\\nThe term NmnVZ represents a count matrix that captures the \\nnumber of times that term tmk= is assigned to the latent con-\\ncept , zni= excluding the current word position. The terms m\\nand n represent the indices of the count matrix .NmnVZ The term \\nV is the set of vocabulary that is used to compose the collec-\\ntion ,D and the set Z represents the set of latent concepts characterizing the collection .D The count matrix NnpZD \\nrecords the number of times that the latent concept zni= is \\nassigned to a document , dpi= excluding the current docu-\\nment under consideration. The terms n and p are the indices \\nof the count matrix .NnpZD In Eq. 2, the term NmnVZb+ ^h  \\nNVmnVZ\\nmVb+dll `j/  is used to estimate the probability of \\nthe term tk given the latent concept ,zi whereas NnpZDa+ ^h\\nNZnpZD\\nnZa+dll `j/  is applied to approximate the probability \\nof zi given the document di in Eq. 1.\\nOur previous success in applying association rule mining to \\nimprove both the quality of the mined latent aspects and the \\ntime of convergence of an expectation-maximization (EM) \\nalgorithm for a probabilistic latent semantic indexing (pLSI) model (Song et al., 2012) motivates us to explore context-sen-sitive text mining to extend the Gibbs sampler for LDA-based latent topic modeling. This is indeed one of the main research'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='contributions of our research work. The details of our novel \\ncontext-sensitive text mining (Lau et al., 2008) enhanced Gibbs sampling algorithm CS-GibbsSampling are depicted in Figure\\xa04. The algorithm begins with the initialization of sev-\\neral control variables and the count matrices \\nNmnVZ and NnpZD \\nwhich are the basis to estimate i and .z Then, step 6 of the \\nCS-GibbsSampling algorithm invokes context-sensitive text \\nmining to discover term association knowledge (i.e. informa-\\ntion flows) to guide term-to-topic assignment in the count \\nmatrix .NmnVZ In context-sensitive text mining (Lau et al., \\n2008), a virtual text window of win~ terms is composed, and it \\nmoves from left to right one term each time in each docu-ment. The co-occurrence information among terms within \\neach virtual text window is collected and analyzed to produce \\nterm associations of the form \\n,, ,, tt tt k 12 1 k\"f - ^h  where tk is \\na term of the corpus D (Lau et al., 2008). More specifically, \\nsteps 7 to 11 of the algorithm states that if the terms appearing in the left hand side of a context-sensitive term association \\n(i.e., information flow) are assigned to a topic (column) of \\n,NmnVZ the term that appears in the right-hand side of the asso-\\nciation should also be assigned to the same topic (column) \\nbecause terms ,, , tt t 12 1 kf - ^h  logically implies .tk When com-\\npared to the original Gibbs sampling algorithm which ran -\\ndomly assigns terms to topics (Geman and Geman, 1984), our'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='domly assigns terms to topics (Geman and Geman, 1984), our \\nproposed algorithm can better leverage contextual knowledge \\n(i.e., information flows) extracted from a domain-specific cor-pus to make a more informed term-to-topic assignment.\\nThe main loop of the CS-GibbsSampling algorithm is con-\\ntrolled by testing change of perplexity and the maximum num-\\nber of iterations (step 12). If the change of perplexity in recent \\ntwo iterations \\np is less than or equal to the threshold g or the \\nnumber of iterations I reaches the maximum Max,I the Gibbs \\nsampling algorithm will be terminated. Basically, steps 13 to 30 of the algorithm illustrates the iteration process of updating the \\ncount matrices \\nNmnVZ and .NnpZD The proposed Gibbs sampling \\nalgorithm computes the entropy of a term tm to filter noisy \\nterms (steps 31 to 35). These noisy terms are associated with \\nmany topics, and are unlikely to be representative terms to \\nuniquely describe a topic. The entropy of a term is defined as \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. 36    ieee Computational intelligen Ce magazine | February 2014follows: entrop ya ss , tz tZ mi miZ\\n1==^^ ` hh j /  where the \\nassignment function ass,ztim^h  returns 1 if the term tm is \\nassigned to the topic . zZid  The computational complexity \\nof the context-sensitive Gibbs sampling algorithm is character-'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='of the context-sensitive Gibbs sampling algorithm is character-\\nized by , OI ZV D2$$ $ ^h  where I is the number of Gibbs \\niterations; V is the vocabulary set of a corpus ,D and Z is \\nthe pre-defined number of latent concepts. The additional computational cost of our CS-GibbsSampling algorithm when \\ncompared to the classical Gibbs Sampling is characterized by V2 which is devoted to perform context-sensitive text min-\\ning over a corpus. Our previous empirical results have shown \\nthat the text mining process is relatively efficient even for a \\nmedium to large textual corpus (Lau et al., 2008, 2009).\\nDetermining the optimal number of latent concepts of an \\nLDA-based model is always a challenge. We adopted a perplexity-\\nbased approach to estimate a reasonable number of latent con-cepts characterizing an unlabeled training corpus \\nD and filter  \\nout noisy topics (Steyvers et al., 2004). More specifically, the  Algorithm CS-GibbsSampling(D, a, b, g, d, l, ~win, MaxI)\\nInputs: an unlabeled corpus D, hyper-parameters a and b controlling Dirichlet prior distributions, termination\\nthreshold g, entropy threshold d for noisy term remo val,thenumber oflatent topics l, thetextwindow size ~win\\nforconte xt-sensitiv etextmining, maximum numbero fiterations MaxI\\nMain Procedur e: \\n1 NoisyTerms # z, I # 0\\n2 p # g + 1  //initialize theperple xitychange variable p\\n3 ]Zb # l     // speciﬁed no.oftopics\\n4initialize count matrix Nnp  withrandom non-ne gativeintegers'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='4initialize count matrix Nnp  withrandom non-ne gativeintegers\\n5initialize count matrix Nmn  withrandom non-n egativeintegers\\n6applyconte xt-sensiti vetextmining to D using ~win toobtain ruleset R of theform (t1, t2, ..., tk-1)  $ tk\\n7 foreach column n in Nmn do       // adjust each column n of countm atrix Nmn according to R\\n8    ]   if (t1, t2, ..., tk-1)  $ tk ! R and min({c1n, c2n, ..., c(k-1)n}) > 0\\n9    ]  ]    Ckn # min( {c1n, c2n, ..., c(k-1)n})   // Ckn isanelement in Nmn\\n10  ]   end\\n11 end\\n12 while p > g and  I < MaxI do\\n13    ]  for each di ! D do    \\n14    ]  ]  for each ti ! di do\\n15    ]  ]  ]  if ti g Noisy Terms   // skipping noisy terms\\n16    ]  ]  ]  ]  m # row (Nmn, ti)   //obtaint ermindex in Nmn\\n17    ]  ]  ]  ]  n # col (Nmn, ti)   // retriev ecurrent topic assignment\\n18    ]  ]  ]  ]  cmn # cmn - 1   // decrement counti n Nmn\\n19    ]  ]  ]  ]  p # doc (di)   //obtain document index in Nnp\\n20    ]  ]  ]  ]  cnp # cnp - 1   //decrementc ount in Nnp\\n21    ]  ]  ]  ]  for k = 1 to  ]Zb do\\n22    ]  ]  ]  ]  \\n23    ]  ]  ]  ]  end\\n24    ]  ]  ]  ]  samplet oobtain z using thenewdistrib ution Pr(zk]ti, di)\\n25    ]  ]  ]  ]  n # row (z)   //obtain topic indexfrom Nnp\\n26    ]  ]  ]  ]  cnp # cnp + 1   //incrementc ount in Nnp\\n27    ]  ]  ]  ]  cmn # cmn + 1   // incrementc ount in Nmn\\n28    ]  ]  ]  end\\n29    ]  ]  end\\n30    ]  end\\n31    ]  for each row m in Nmn do\\n32    ]  ]  if entr opy(tm) > d'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content=\"29    ]  ]  end\\n30    ]  end\\n31    ]  for each row m in Nmn do\\n32    ]  ]  if entr opy(tm) > d\\n33    ]  ]  ]  No isyTerms # N oisayT erms , tm   //skippingn oisyterms inGibbs sampling\\n34    ]  ]  end\\n35    ]  end\\n36    ]  compute theperple xity perp ofaheld-out sample Dtest using thetraine dmodel\\n37    ]  compute theperple xity change p between therecent twoiterations\\n38    ]  I # I + 1\\n39 end\\n40 return Nmn, NnpZD\\nVZ\\nVZ\\nVZ\\nVZ\\nVZ\\nVZ\\nVZ\\nVZ ZDZD\\nZD\\nZD\\nZDVZVZVZ\\nPr(zk]ti, di) #  · ZDNnp + a\\nZDRn'!Z Nn'p + ]ZbaVZNmn + b\\nVZRm'!V Nm'n + ]Vbb\\nFigure 4  Context-sensitive gibbs sampling.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. February 2014 | ieee Computational intelligen Ce magazine    37context-sensitive Gibbs sampling algorithm is invoked with dif-\\nferent number of concepts .l Then, we select the smallest number \\nof concepts that achieved a good (i.e., small) perplexity score. We extended the open source Core LingPipe API\\n2 to implement the \\nnovel context-sensitive Gibbs sampling algorithm that operation-alizes the aforementioned probabilistic generative model.\\n3.2. Laplacian Scoring for Latent Concept Labeling\\nSince the latent concepts discovered by LDA-based latent semantic mining are unlabeled, we must extend the classical LDA method such that we can infer the semantic label (e.g., \\ncollaborative or transactional) of a mined latent concept. Song\"),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='collaborative or transactional) of a mined latent concept. Song \\net al. (2009) have applied Laplacian scoring, which is originally developed for feature selection, to latent topic selection. The intuition is that if a candidate latent concept is semantically close to some messages characterized by a specific semantic \\nlabel, the latent concept is likely to have such a semantic label \\nas well. A message with relationship label \\nLS i was extracted by \\napplying some generic seeding relationship indicators to an unlabeled corpus \\n.D For example, by applying the seeding \\nindicator “sales” that usually signifies a transactional cybercrimi-nal relationship, the following message \\nLS i is labeled based on \\nour social media corpus: “@iqbal hacking tools 4 sales; U’d pm me if like purchasing.” \\nSince we focus on collaborative and transactional relation-\\nships in this paper, two different Laplacian rankings are con-structed to identify two sets of latent concepts with the respec-tive semantics. Let \\n,{ collaborative, transactional } Lr,krd  \\ndenotes the Laplacian score of the kth concept for the seman-\\ntic label .r Moreover, let d ,ik represents the ith message corre-\\nsponding to the kth concept. Based on the outputs from the \\nLaplacian scoring method (Song et al., 2009), we can then select the highly ranked and consistent latent concepts \\ndom zLird ^h to represent the specific type of cybercriminal \\nrelationship .r For each relationship type ,r an aggregated con-'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='relationship .r For each relationship type ,r an aggregated con-\\ncept {d m: } o Cz Lj r ji r d== ^h  is then constructed to infer \\nthe relationship label of an arbitrary message .TS i\\n3.3. Semantic Inference of Cybercriminal Relationships\\nAfter Laplacian-based latent concept labeling (Song et al., 2009), the aggregated concepts \\nCtran (i.e., sets of transactional \\nconcepts ) and Ccoll (i.e., sets of collaborative concepts) are \\napplied to infer the relationship label of an arbitrary message \\ndT Si=  that refers to at least two cybercriminals. The basic \\nintuition is that if the contents of the message d are more likely \\nto generate the transactional (collaborative) concept, the mes-sage will be considered to describe a transactional (collabora-\\ntive) cybercriminal relationship. We develop a novel inferential \\nlanguage modeling method to estimate the probability of \\nd \\ngenerating a specific cybercriminal relationship label.\\n (| )( |) rC Mr tM PP jd id\\ntCij=\\n!%  (3)\\n2http://alias-i.com/lingpipe/web/download.html (| )( )( |) (| ) rtMr tM rt M PP P 1 INFM L id id iD mm=- +  (4)\\n (| )( )( |) (| ) rt Mr tM rt M PP P 1 INFM LT M id id iR cc=- +  (5)\\n (| )( )( |) Pr Pr Pr tanh tM tt tM TM ML\\n()iR ji jd\\ntt R ji\" $ =\\n\"!cm/ . (6)\\nSemantic-based language modeling (| ) PrCMjd is applied to \\nestimate the probability that the message dT Si=  generates the \\naggregated concept ,Cj where Md is a document language \\nmodel (Ponte and Croft, 1998). However, to cope with the chal-'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='model (Ponte and Croft, 1998). However, to cope with the chal-\\nlenge that the mined concepts are incomplete, each term tCij! \\nshould be smoothed with respect to the entire cybercrime mes-\\nsage corpus D by means of the maximum likelihood collection \\nlanguage model MD in Eq. 4. In this paper, we develop a novel \\ninferential language model (| ) Pr tM INF id that consists of the maxi-\\nmum likelihood estimation (| ) Pr tM ML id of the term ti with \\nrespect to ,d and the context-sensitive text mining (Lau et al., \\n2008) based smoothing (| ). Pr tM TM iR  The maximum likelihood \\ndocument language model (| )f req(,)/|| Pr tM td d ML id i =  is defined \\nbased on the frequency of ti appearing in the message .d The \\nJelinek-Mercer smoothing parameters .035 m=  and .031 c=  \\nare empirically established based on a subset of our cybercrime \\nmessage corpus. Previous research also shows that the Jelinek-Mercer smoothing parameter usually falls in the range of \\n[. ,. ] 01 07 (Zhai and Lafferty, 2004).\\nFinally, context-sensitive text mining based language model \\nMR is defined according to Eq. 6. The set of term associations \\n(i.e., information flows) R is mined by applying the context-\\nsensitive text mining method (Lau et al., 2008) to the unla-beled cybercrime corpus \\nD. A context-sensitive term associa-\\ntion of the form ttji\" is applied to text mining based \\nlanguage modeling (| ) Pr tM TM iR  to estimate the probability \\nthat the message generates a term tj which is contextually'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='that the message generates a term tj which is contextually \\nassociated with a relationship indicator ti captured in .Cj In \\nparticular, the strength of term association ()Pr ttji\" is \\nderived based on the context-sensitive text mining method. Pragmatically, we only consider the top \\n3|= term associa-\\ntions for each relationship indicator ti captured in .R Since \\nthe inference that d generating tj which implies the relation-\\nship indicator ti involves uncertainty, the maximum likelihood \\nestimation of (| ) Pt M ML jd is discounted by a factor () . Pr ttji\"  \\nThe hyperbolic tangent function ensures that (| ) Pr tM TM iR is a \\nvalid probability. The main differences between the proposed language model and other exiting inference-based language \\nmodels (Nie et al., 2006) are that we apply context-sensitive \\ntext mining to extract term associations and we use a summa-tion instead of multiplication operator to combine the proba-bilities of deduced terms to smoothen the maximum likeli-hood document language model.\\nFor a binary classification, if \\n(( |) Pr Pr CMtran d-\\n(| )) CM > coll rel d~ or (( |) (| )) Pr Pr CM CM > coll tran rel dd ~ -  is estab-\\nlished, the message d is classified to have the same cyber-\\ncriminal relationship label of the aggregated concept Cj \\nwhich is generated with a higher probability according to the proposed inferential language model. Otherwise, the'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='Authorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. 38    ieee Computational intelligen Ce magazine | February 2014cybercriminal relationship is undefined. The classification \\nthreshold .E02 8 rel~=-  was empirically established based \\non a subset of our cybercrime message corpus. Alternatively, the proposed inferential language model can perform a rank-\\ning-based classification of the relationship labels of messages. \\nIn that case, messages are ranked according to the their prob-abilities of having a specific relationship label. Accordingly, a binary classification threshold is not needed.\\nAfter relationship classification, a frequency count \\nRel( ,)vvjx y \\nfor a specific type of relationship {transactional, j!\\ncollaborative } is developed for each pair of cybercriminals \\n(, ). vvxy These frequency values are then subject to a linear nor-\\nmalization Rel (RelR el )/(Rel Rel) norm in maxm in =- -  to \\ndevelop the final relationship scores for all the pairs. If a pair (, ) vvxy \\nhas both a transactional relationship and a collaborative relationship at the same time (i.e., \\nRel( ,)vv 0> tran xy  and Rel( ,)vv 0> coll xy ), our \\nsystem simply assigns the more specific transactional relationship to the pair. Finally, a cybercriminal network is composed based on the \\nidentified relationships among all the valid pairs pertaining to a \\nspecific period.\\n4. System Evaluation'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='specific period.\\n4. System Evaluation\\nT o the best of our knowledge, a benchmark data set for the evaluation of cybercriminal network mining algorithms is not yet available at the time of this writing. T o evaluate the effec-\\ntiveness of the proposed cybercriminal network mining \\nmethod, we first needed to retrieve cybercrime related mes-sages from online social media. For the construction of our evaluation corpora, we made use of two kinds of social media sources, namely micro blogs and online forums. We accessed to \\nthe largest micro blog service, T witter, and a dozen of online \\nforums (e.g., hacktalk.net, blackhatworld.com, pastebin.com, etc.) to develop two cybercrime related corpora. We manually identified a list of 35 well-known cybercriminals as the seeding \\nusers. Then, we used these seeding users as the starting points to \\nperform breadth-first crawling to retrieve the messages posted by other suspects of cybercrimes. As for T witter, we retrieved the relevant tweets via a publicly available API called T opsy.\\n3 \\n3http://topsy.com/For instance, for the well-known cybercriminal group Anony-mous (also known as FawkesSecurity on T witter) who claimed to be responsible for a series of cyber-attacks against HSBC in \\nOctober 2012, our crawler first identified all the followers of'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='October 2012, our crawler first identified all the followers of \\nthis account, and then invoked an internal filter to extract all cybercrime related tweets from different followers or friends. For online forums, our crawler identified the related user accounts that appeared in the same thread of messages or \\ndirectly embedded in the message contents. A total of 28,114 \\ncybercrime related messages covering the period from January 2009 to December 2012 were retrieved from T witter in Janu-ary 2013. In addition, a total of 25,695 cybercrime related mes-\\nsages of the same period were retrieved from various Internet \\nforums. T o distinguish cybercrime related messages from ordi-nary online chatting, our internal filter simply utilized a list of 21 common cybercrime keywords to determine the nature of each conversational message. These keywords were provided by \\na group of six cyber-security experts who were the employees \\nof a cyber-security consulting firm.\\nFor each cybercrime message corpus, a subset of messages \\nwith at least two cybercriminals mentioned in each message was manually inspected and annotated by a group of three cybersecu-\\nrity experts so as to determine the specific cybercriminal relation-\\nship captured in the message. For the experiments reported in this paper, we only focus on two types of cybercriminal relationships namely transactional relationship and collaborative relationship. A \\ntransactional relationship refers to buying or selling cyber-attack'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='transactional relationship refers to buying or selling cyber-attack \\ntools between two parties, whereas a collaborative relationship \\nsimply implies the sharing of information or tools between cybercriminals and it does not involve any monetary exchange between the two parties. Only if all three experts agreed on a spe-\\ncific type of relationship captured in a message, would that mes-\\nsage be annotated with the corresponding relationship label. The average inter-rater agreement of six annotators as measured by Cohen’s Kappa is \\n.075 K=  which indicates a relatively consis-\\ntent and reliable expert judgment for the construction of our evaluation corpora. The details of our cybercrime message cor-pora applied to our experiments are given in Table 1. Common performance evaluation measures such as Precision (P), Recall (R), F-measure (F), and Accuracy (A) were applied to our experi-\\nments. Moreover, the Receiver Operating Characteristic (ROC) \\ncurve (Hand and Till, 2001) was also adopted to assess the perfor-mance of all systems such that the results are independent of any particular classification threshold value chosen.\\nApart from the context-sensitive LDA (CSLDA) experi-\\nmental system that is underpinned by context-sensitive Gibbs sampling for latent cybercriminal relationship mining, we also implemented several baseline systems to perform a compara-tive evaluation. Other probabilistic generative models such as Partially Labeled Dirichlet Allocation (Ramage et al., 2011)'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='and Latent Dirichlet Allocation (Blei et al., 2003) were also \\nadopted as baseline systems. For both of these baseline systems, a classical Gibbs sampler (Geman and Geman, 1984) were employed. PLDA requires human assigned tags for documents. \\nUnfortunately, unlike Web pages, these tags are not normally Table 1  Details of the cybercrime corpora and latent topics.\\nTwiTTerOnline \\nFOrums TOTal\\n# CyberCrime messA ges 28,114 25,695 53,809\\n# CyberCrime senTenCes 87,153 131,045 218,198\\n# messA ges wiTh T wo users 5,112 3,294 8,406\\n# Anno TATed messA ges 4,283 2,830 7,113\\n# CollAbor ATive messA ges 1,045 972 2,017\\n# TrAnsACTionAl messA ges 314 677 991\\n# Ambiguous messA ges 2,924 1,181 4,105\\n# sele CTed T opiCs 65 70 –\\n# TrAnsACTionAl C onCepTs 5 7 –\\n# CollAbor ATive C onCepTs 11 11 –\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. February 2014 | ieee Computational intelligen Ce magazine    39available for cybercrime related \\nmessages. Given the large num-ber of un-tagged messages of \\nour corpora, we employed a \\nsemi-automated method for tag generation. Firstly, six annota-tors were responsible for tag-ging around 10% of our mes-\\nsages. Secondly, the cosine \\nsimilarities between tagged messages and un-tagged mes-sages were computed. Finally, an \\nun-tagged message was assigned'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='un-tagged message was assigned \\nthe tag of its most similar tagged message if the cosine similarity score was above a predefined threshold (i.e., 0.5). For the \\nremaining messages without any \\ntag assigned, we annotated them with the generic tag “cyber-crime”. Similar to the approach adopted by (Ramage et al., 2011), we specified that each tag was associated with 5 topics \\nfor the PLDA baseline system.\\nAnother baseline systems (SEED) employed 11 seeding trans-\\nactional indicators, 16 seeding collaborative indicators, and the \\ncorresponding synonyms (top 3 synonyms of each seeding indi-cator) extracted from WordNet (Fellbaum, 1998) to identify \\ncybercriminal relationships from messages. These relationship \\nindicators were also used by the experimental system to infer the labels of mined latent concepts. The seeding relationship indica-tors and the cybercrime corpora were stemmed using the same \\nPorter stemming algorithm (Porter, 1980). The SEED baseline \\nsystem simply uses a transactional (collaborative) strength measure \\ntran () |TranInd || CollIn d| |TranInd || CollIn d| CP i=- + ^^ hh\\ncoll() |CollInd ||TranIn d| |TranInd ||CollInd | CP i=- + ^^ ^h hh\\nto determine the relationship label of a test message ,CP i where \\nTranInd  and CollIn d are the sets of transactional and collabora-\\ntive indicators found in the message. For example, if \\ntran () (coll( )) CP CP >> tran coll ii~~  is true, the message is con-\\nsidered to be transactional (collaborative). The threshold'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='sidered to be transactional (collaborative). The threshold \\n() tran coll ~~  was empirically established for our experiments. In \\naddition, classical supervised machine learning classifiers such as Support Vector machine (SVM) with a RBF kernel\\n4, and Condi-\\ntional Random Fields (CRF)5 were also used. Stop word \\nremoval, case transformation, and stemming were applied to the cybercrime corpora before they were processed by the experi-\\nmental and the baseline systems. For the SVM and CRF baseline \\nsystems, word-based features and TFIDF term weighting were applied. In addition, part-of-speech, number of seeding transac-tional indicators, number of seeding collaborative indicators, and lexical features such as sentence length and lexical diversity were \\napplied to the baseline systems. For the CRF baseline system, \\n4http://www.csie.ntu.edu.tw/ cjlin/libsvm/\\n5http://crfpp.googlecode.com/svn/trunk/doc/index.htmlcontextual information such as words preceding and after seed-\\ning relationship indicators or user names, was also used. A stan-dard three-fold cross validation was applied to our experiments.\\nFor the experimental system CSLDA, concepts representing \\ntransactional and collaborative cybercriminal relationships were first acquired via LDA-based latent topic modeling. The num-ber of latent concepts \\n||Z was estimated according to the per-\\nplexity measure (Steyvers et al., 2004). Following the empirical finding of Griffiths and Steyvers (Steyvers et al., 2004), the'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='hyper-parameters \\na and b of the context-sensitive Gibbs sam-\\npling algorithm was set to /||Z50  and .,01 respectively. T wo \\nindependent Gibbs samples were used and the first three hun-\\ndreds Gibbs samples produced by our algorithm were ignored \\nto ensure that the burn-in period had been by-passed. The max-\\nimum loop control of Gibbs sampling Max, 1000 I=  was set \\nto ensure a proper convergence. The statistics of latent concepts \\nlearning and Laplacian concept labeling of the experimental system are summarized in the second half of Table 1. For con-\\ntext-sensitive Gibbs sampling, the virtual text window size \\nwin~ \\nand the information flow quality threshold IF~ for context-\\nsensitive term associations extraction (Lau et al., 2008) were \\nempirically established based on the T witter corpus. We tried \\ndifferent combinations of win~ and IF~ as shown in Figure 5 \\nwhile fixing the values of other system parameters. We found \\nthat ,6 win~= and .015 IF~=  led to the best F-measure, and \\nthen we applied these parameter values to the experiments based on the forum corpus as well. Based on the selection parameter \\n.,015 IF~=  there were ,1512 and ,1306 context-\\nsensitive term associations extracted from the T witter and the forum corpora, respectively.\\nIt should be noted that our empirical parameter setting \\nmethod may not be able to identify the global optimum for \\n,win~ ,IF~ ,rel~ and other parameters. A more sophisticated'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content=',win~ ,IF~ ,rel~ and other parameters. A more sophisticated \\nparameter tuning method will only further improve the perfor-mance of our proposed computational method reported in this \\n0.65\\n0.64\\n0.63\\n0.62\\n~IF~Win0.61\\n0.6\\n0.59\\n0.500.45 0.40 0.35 0.30 0.25 0.200.15 0.100.05 0.01222018161412108642Average F-Score\\nFigure 5  empirical parameter setting for context-sensitive term associations.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. 40    ieee Computational intelligen Ce magazine | February 2014paper. Applying soft-computing methods such as genetic algo-\\nrithms (Goldberg, 1989; Lau et al., 2006) to bootstrap the per-formance of our probabilistic generative model for cybercrimi-nal network mining will be left as part of our future work.\\nThe results achieved by the experimental and the baseline sys-\\ntems are summarized in Table\\xa02.\\xa0The column with label “Thr.” refers to the classification threshold applied to a specific system to produce the corresponding performance scores.  Table\\xa0 2 shows \\nthat the CSLDA experimental system outperforms most baseline \\nsystems for both transactional and collaborative cybercriminal \\nrelationship classification. In terms of F-measure, the experimental system significantly outperforms the LDA baseline system by \\n.% ((). ,. tp 4822 3620 1< =  of paired one-tail t -test) and \\n.% ((). ,. tp 5032 3780 1< =  of paired one-tail t -test) for transac-'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='.% ((). ,. tp 5032 3780 1< =  of paired one-tail t -test) for transac-\\ntional and collaborative cybercriminal relationship classification, respectively. Moreover, the experimental system significantly  \\noutperforms the best supervised classifier (i.e., SVM) by \\n.% ((). ,. tp 1055 24 87 01< =  of paired one-tail t-test) and \\n.% ((). ,. tp 9802 4120 1< =  of paired one-tail t-test) for transac-\\ntional and collaborative cybercriminal relationship classification, respectively. The CSLDA system performs slightly better than the PLDA system for transactional relationship classification, and it achieves comparable performance as the PLDA system for collab-\\norative relationship classification. Through weakly supervised \\nmining of an unlabeled corpus, the CSLDA system produces concept descriptions corresponding to two types of cybercriminal relationships; these concepts are then applied to bootstrap the per-\\nformance of cybercriminal relationship classification. Table 3 \\nhighlights two representative concepts (i.e., relationship descrip-tions) mined by the CSLDA method and two concepts mined by Table 2  The comparative relationship classification performance of various systems.\\nCOrpus sysT em Transa CTiOnal rela TiOnship COllab OraTive rela TiOnship\\nThr. r p F a Thr. r p F a\\nTwiTTer Csld A 0.2e-8 0.589 0.670 0.627 0.949 0.2e-8 0.636 0.687 0.661 0.841\\npldA 0.2e-8 0.580 0.662 0.618 0.947 0.2e-8 0.640 0.690 0.664 0.842\\nldA 0.2e-8 0.557 0.632 0.592 0.944 0.2e-8 0.599 0.646 0.622 0.822'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='ldA 0.2e-8 0.557 0.632 0.592 0.944 0.2e-8 0.599 0.646 0.622 0.822\\nsvm 0.025 0.532 0.599 0.563 0.940 0.020 0.575 0.618 0.596 0.810\\nCrF 0.005 0.506 0.562 0.533 0.935 0.008 0.564 0.600 0.581 0.802\\nseed 0.010 0.296 0.596 0.396 0.934 0.015 0.326 0.575 0.416 0.777\\nonline Forums Csld A 0.2e-8 0.567 0.663 0.611 0.828 0.2e-8 0.618 0.665 0.641 0.762\\npldA 0.2e-8 0.563 0.664 0.609 0.827 0.2e-8 0.616 0.663 0.639 0.760\\nldA 0.2e-8 0.548 0.637 0.589 0.817 0.2e-8 0.598 0.638 0.617 0.746\\nsvm 0.025 0.518 0.602 0.557 0.803 0.020 0.577 0.602 0.589 0.724\\nCrF 0.005 0.510 0.591 0.547 0.798 0.008 0.557 0.585 0.570 0.712\\nseed 0.010 0.288 0.598 0.389 0.783 0.015 0.303 0.595 0.402 0.690\\nAver Age Csld A – 0.578 0.667 0.619 0.888 – 0.627 0.676 0.651 0.801\\npldA – 0.571 0.663 0.614 0.887 – 0.628 0.677 0.651 0.801\\nldA – 0.553 0.635 0.591 0.881 – 0.598 0.642 0.620 0.784\\nsvm – 0.525 0.600 0.560 0.871 – 0.576 0.610 0.593 0.767\\nCrF – 0.508 0.576 0.540 0.867 – 0.560 0.592 0.576 0.757\\nseed – 0.292 0.597 0.392 0.859 – 0.315 0.585 0.409 0.733\\nTable 3  Examples of mined concepts for cybercriminal relationship classification.\\nCslD a lDa\\nTerm TransaCTi Onal Term COllabO raTive Term TransaCTi Onal Term COllabO raTive \\nPrtzki^ ^ hh Prtzki^ ^ hh Prtzki^ ^ hh Prtzki^ ^ hh\\nsell 0.137 ChAT 0.283 buy 0.116 CollAbor ATe 0.165\\nmoney 0.106 join 0.267 ACCoun T 0.101 work 0.142\\nACCoun T 0.102 CollAbor ATe 0.229 inTeres T 0.087 ChAT 0.128\\nTrAnsF er 0.095 joinT 0.164 sell 0.075 join 0.103'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='TrAnsF er 0.095 joinT 0.164 sell 0.075 join 0.103\\nbuy 0.091 leArn 0.103 send 0.069 downlo Ad 0.098\\nbill 0.083 ACquire 0.098 money 0.062 Ask 0.087\\nCrediT 0.075 downlo Ad 0.071 exChAnge 0.055 TAlk 0.082\\nseTTle 0.073 exChAnge 0.052 pAy 0.048 direCT 0.077\\nbid 0.065 AdviCe 0.033 bill 0.041 leArn 0.063\\npAy 0.052 help 0.026 Ask 0.032 Fun 0.054\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. February 2014 | ieee Computational intelligen Ce magazine    41the LDA method, respectively. For brevity reason, only the top 10 \\nterms for each concept are shown in Table 3.\\nIn addition, Figures 6 and 7 show the ROC curves of vari-\\nous systems for transactional and collaborative relationship classification under the T witter corpus and the online forums corpus, respectively. These ROC curves were plotted using SPSS with a confidence level of \\n.. p 05<  The area under a \\nROC curve (AUC), that is, the probability of a classifier cor-rectly identifies a true-positive case, was also used for a com-parative evaluation. Table 4 summarizes the AUC values achieved by the experimental and baseline systems. Again, it is \\nshown that the CSLDA experimental system performs better \\nthan most of the baseline systems do. In terms of average AUC values, the experimental system significantly outperforms the LDA baseline system by \\n.% ((). ,. tp 5232 3810 1< =  of paired'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='.% ((). ,. tp 5232 3810 1< =  of paired \\none-tail t-test) and the best supervised classifier (i.e., SVM) by \\n.% ((). ,. tp 1662 26 25 01< =  of paired one-tail t -test), respec-\\ntively. The main reason of such a remarkable performance improvement is that the CSLDA method can leverage on a large number of unlabeled messages to learn domain-specific concepts about cybercriminal relationships, and hence to \\nenhance cybercriminal relationship classification.The CSLDA system performs slightly better than the state-\\nof-the-art PLDA system although the improvement is not statis-tically significant. However, the distinct advantage of the CSLDA \\nsystem over the PLDA system is that only a small number of seeding relationship indicators are required to guide latent topic modeling and concept-based cybercriminal relationship classifi-cation. In contrast, the PLDA system requires a manually assigned \\ntag for each document to guide topic modeling. Unfortunately, \\nthese human assigned tags are not readily available for cyber-crime related messages; this makes it extremely difficult to apply the PLDA method to mine cybercriminal networks.\\nAccording to our experimental results, we can conclude that'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='According to our experimental results, we can conclude that \\nthe proposed weakly supervised concept-based cybercriminal relationship classification method significantly outperforms most of the baseline methods. It also achieves comparable performance as the state-of-the-art PLDA method. The proposed system is more effective than the SEED baseline system that only utilizes a \\nlimited number of pre-defined relationship keywords. Moreover, \\nthe proposed CSLDA system significantly outperforms the super-vised machine learning classifiers such as SVM and CRF . Surpris-ingly, the state-of-the-art classifier CRF did not perform well in \\nour experiments. A possible reason is that the number of labeled \\nROC Curve for Transactional Relationship\\nFalse Positive RateTrue Positive Rate1.0\\n1.00.8\\n0.80.6\\n0.60.2\\n0.20.4\\n0.40.0\\n0.0RandomSEEDCRFSVMLDAPLDACSLDA\\nROC Curve for Collaborative Relationship\\nFalse Positive RateTrue Positive Rate1.0\\n1.00.8\\n0.80.6\\n0.60.2\\n0.20.4\\n0.40.0\\n0.0RandomSEEDCRFSVMLDAPLDACSLDA\\nFigure 6  The roC curve based on the Twitter corpus.ROC Curve for Transactional Relationship\\nFalse Positive RateTrue Positive Rate1.0\\n1.00.8\\n0.80.6\\n0.60.2\\n0.20.4\\n0.40.0\\n0.0RandomSEEDCRFSVMLDAPLDACSLDA\\nROC Curve for Collaborative Relationship\\nFalse Positive RateTrue Positive Rate1.0\\n1.00.8\\n0.80.6\\n0.60.2\\n0.20.4\\n0.40.0\\n0.0RandomSEEDCRFSVMLDAPLDACSLDA\\nFigure 7  The roC curve based on the forum corpus.'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='0.20.4\\n0.40.0\\n0.0RandomSEEDCRFSVMLDAPLDACSLDA\\nFigure 7  The roC curve based on the forum corpus.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. 42    ieee Computational intelligen Ce magazine | February 2014training examples is not sufficient to effectively train the CRF \\nclassifier to learn discriminative label sequences. Since it is extremely costly and time-consuming to label a large number of \\ncybercriminal relationships, it may not be practical to apply super-\\nvised machine learning classifiers to cybercriminal relationship mining from online social media. With the absence of a large number of labeled cybercrime messages, the proposed CSLDA method is just able to leverage unlabeled cybercrime messages to learn latent concepts which are semantically rich representations of different types of cybercriminal relationships. Our CSLDA \\nmethod outperforms the classical LDA method because the pro-\\nposed context-sensitive Gibbs sampling algorithm can discover higher quality latent concepts (as shown in Table 3) when com-pared to that produced by the standard Gibbs sampler. These high-quality latent concepts can then be applied to bootstrap \\ncybercriminal relationship classification.\\nFigure 8 shows a sample segment of the cybercriminal net-\\nwork mined based on our cybercrime corpora. The cybercrim-\\ninal network is plotted using the open source graph display program called Pajek.'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='inal network is plotted using the open source graph display program called Pajek.\\n6 Each circle represents a cybercriminal or \\ncybercriminal group (e.g., the Anonymous group) and a square box represents an attack incident (e.g., “Bank of America”) \\nwhich is most likely associated with the corresponding cyber-\\ncriminal. We employed a PMI-based method to estimate the strength of association between a cybercriminal and an attack mentioned in social media messages. However, the computa-tional details about cybercrime forensics will not be covered in \\nthis paper. Dash lines between cybercriminals represent collab-\\norative cybercriminal relationships (e.g., Anonymous and \\nnullcrew ), whereas solid lines \\nbetween cybercriminals indicate \\ntransactional relationships (e.g., \\nugnazi and r00tw0rm). The strength of a cybercriminal rela-tionship is shown along an edge which is labeled with the type of \\nrelationship. This network seg-\\nment was verified as a correct sub-network by our cyber-secu-rity experts. According to our experts’ qualitative feedback, this \\nkind of automatically generated \\ncybercriminal network can con-siderably enhance cyber-security forensics because a large amount of intuitive and high-quality \\ncyber-security intelligence can \\nbe generated instantly with min-imal human intervention.\\n5. Conclusions\\nLatest cyber-security studies show that there is a rapid growth in the number of cyber-\\ncrimes which cause tremendous'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='crimes which cause tremendous \\nfinancial losses to click-and-mortar organizations in recent \\nyears. The main contribution of the research work reported in this paper is the design of a novel, weakly supervised cyber-criminal network mining method that is underpinned by a \\n6http://vlado.fmf.uni-lj.si/pub/networks/pajek/Table 4  The comparative AUC values obtained  \\nby various systems.\\nCOrpus sysT emTransaCTi Onal \\nrela TiOnshipCOllabO raTive \\nrela TiOnship\\nauC sTD. e rr. auC sTD. e rr.\\nTwiTTer Csld A 0.848 0.013 0.857 0.014\\npldA 0.839 0.014 0.860 0.017\\nldA 0.811 0.015 0.821 0.016\\nsvm 0.703 0.017 0.754 0.018\\nCrF 0.685 0.022 0.742 0.019\\nseed 0.541 0.022 0.552 0.020\\nonline\\nForumsCsld A 0.823 0.015 0.835 0.018\\npldA 0.819 0.017 0.828 0.021\\nldA 0.775 0.021 0.789 0.017\\nsvm 0.695 0.022 0.734 0.024\\nCrF 0.551 0.012 0.603 0.021\\nseed 0.531 0.013 0.511 0.018\\nAver Age Csld A 0.836 0.014 0.846 0.016\\npldA 0.829 0.016 0.844 0.019\\nldA 0.793 0.018 0.805 0.017\\nsvm 0.699 0.020 0.744 0.021\\nCrF 0.618 0.017 0.673 0.020\\nseed 0.536 0.018 0.532 0.019\\nLaunched 0.205Attacked CitiBank\\nLaunched 0.221Attacked Sonyteampoison\\nlulzsecLaunched 0.182Attacked Bank of America\\nanonymousLaunched 0.174Attacked Petraeus Mistress\\nAttached FED\\nLaunched 0.244\\nLaunched 0.209Attacked Westboro\\nBaptist Church\\nugnazi\\nLaunched 0.155Attacked\\nNASAr00tw0rmTransact 0.438\\ninj3ct0rLaunched 0.217\\nAttacked Exploit HubLaunched 0.142\\nAttacked Sony Mobilenullcrew\\nLaunched 0.107'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='Attacked Exploit HubLaunched 0.142\\nAttacked Sony Mobilenullcrew\\nLaunched 0.107\\nAttacked Stratfor Global Intelligenceantisec-operationLaunched 0.195Collaborate 0.513Collaborate 0.589Collaborate 0.406\\nCollaborate 0.617Collaborate 0.425Collaborate 0.411\\nCollaborate 0.453\\nFigure 8  A sample segment of the mined cybercriminal network.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply. February 2014 | ieee Computational intelligen Ce magazine    43context-sensitive text mining enhanced probabilistic genera-\\ntive model. The proposed computational algorithm can effec-tively extract semantically rich representations of latent con-\\ncepts describing transactional and collaborative relationships \\namong cybercriminals based on publicly accessible messages posted to online social media. These latent concepts are then applied to bootstrap the performance of inferential language modeling-based relationship classification in texts. Based on \\ntwo textual corpora extracted from online social media, our \\nexperimental results reveal that the proposed weakly super-vised cybercriminal network mining method significantly outperforms the classical unsupervised LDA method by \\n5.23% and the well-known supervised SVM classifier by'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='5.23% and the well-known supervised SVM classifier by \\n16.62% in terms of AUC, respectively. In addition, the pro-posed method achieves comparable performance as the state-of-the-art PLDA method. However, the distinct advantage of the proposed method over the PLDA method is that manu-\\nally tagged messages are not required for cybercrime concept \\nlearning, hence making it feasible to apply the proposed method to cybercriminal network mining. \\nFuture work will examine soft-computing methods such \\nas genetic algorithms to search for optimal or near-optimal \\nsystem parameter values to further enhance the effectiveness \\nof the proposed method. The issue of evolutionary cyber-criminal networks will also be examined. In particular, a dynamic topic model will be examined to mine evolving concepts about cybercriminal relationships. A larger scale of \\nempirical experiment with more corpora extracted from \\nonline social media will be performed to further evaluate both the effectiveness and the scalability of the proposed computational method. Finally, the analysis and forensics of \\ncyber-attack behavior based on the mined cybercriminal net-\\nwork will be carried out. \\nAcknowledgment\\nThe work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. \\nCityU 145712), SRG grants from City University of Hong'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='CityU 145712), SRG grants from City University of Hong \\nKong (Project No. 7003002, 7004120, 7008138), and a grant from the Shenzhen Municipal Science and T echnology R&D Funding—Basic Research Program (Project No.  JCYJ20130401145617281). Xia’s research was supported in \\npart by Natural Science Foundation of China (Project No. \\n61272233). Y e’s research was supported in part by Natural Science Foundation of China (Project No. 61272538), and Shenzhen Strategic Emerging Industries Program (Project \\nNo. JCYJ20120613135329670).\\nReferences\\n[1] H. Abbass , A. Bender , S. Gaidow , and P.  Whitbread , “Computational red teaming: \\nPast, present and future ,” IEEE Comput. Intell. Mag. , vol. 6, no. 1 , pp. 30 –42, 2011 .\\n[2] S. Bao, R. Li, Y. Yu, and Y.  Cao, “Competitor mining with the web,” IEEE Trans. \\nKnowl. Data Eng. , vol. 20 , no. 10 , pp. 1297 –1310, 2008 .\\n[3] D. M.  Blei, A. Y. Ng, and M. I. Jordan , “Latent dirichlet allocation ,” J. Mach. Learn. \\nRes., vol. 3,  pp. 993 –1022,  Mar.  2003 .\\n[4] E. Cambria , B. Schuller , Y. Xia, and C.  Havasi , “New avenues in opinion mining and \\nsentiment analysis ,” IEEE Intell. Syst. , vol. 28, no. 2 , pp. 15 –21, 2013 .[5] E. Cambria , T. Mazzocco , and A.  Hussain , “Application of multi-dimensional scaling \\nand artificial neural networks for biologically inspired opinion mining ,” Biol. Insp. Cogn. \\nArch., vol. 4,  pp. 41 –53, Apr.  2013 .'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='Arch., vol. 4,  pp. 41 –53, Apr.  2013 .\\n[6] E. Cambria , P. Chandra , A. Sharma , and A.  Hussain , “Do not feel the trolls ,” in Proc. \\nInt. Semantic Web Conf. , Shanghai , China , 2010 .\\n[7] K. Dinakar , B. Jones, C. Havasi , H. Lieberman , and R.  Picard , “Common sense rea -\\nsoning for detection, prevention, and mitigation of cyberbullying ,” ACM Trans. Interact. \\nIntell. Syst. , vol. 2, no. 3 , pp. 29 –31, 2012 .\\n[8] P. J. Denning  and D. E.  Denning , “The profession of IT: Discussing cyber attack ,” \\nCommun. ACM , vol. 53, no. 9 , pp. 29 –31, 2010 .\\n[9] H. Du and S. J. Yang, “Discovering collaborative cyber attack patterns using social \\nnetwork analysis ,” in Proc. 4th Int. Conf. Social Computing, Behavioral-Cultural Modeling \\nPrediction , 2011 , vol. 6589 , pp. 129 –136.\\n[10] C. Fellbaum , Wordnet: A electronic lexical database . Cambridge , MA : MIT Press , 1998 .\\n[11] J. Franklin , A. Perrig , V. Paxson , and S. Savage , “An inquiry into the nature and \\ncauses of the wealth of internet miscreants ,” in Proc. ACM Conf. Computer Communications \\nSecurity , Alexandria , VA, Oct.  28-31, 2007 , pp. 375 –388.\\n[12] S. Geman  and D. Geman , “Stochastic relaxation, Gibbs distributions, and the \\nBayesian relation of images ,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 6,  no. 6, pp. \\n721–741, 1984.\\n[13] S. Goel, “Cyberwarfare: Connecting the dots in cyber intelligence ,” Commun. ACM , \\nvol. 54,  no. 8 , pp. 132 –140, 2011 .'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='vol. 54,  no. 8 , pp. 132 –140, 2011 .\\n[14] D. Goldberg , Genetic Algorithms in Search, Optimisation and Machine Learning . Reading , \\nMA: Addison-Wesley , 1989 .\\n[15] D. J. Hand  and R. J. Till, “A simple generalisation of the area under the ROC curve \\nfor multiple class classification problems ,” Mach. Learn. , vol. 45,  no. 2 , pp. 171 –186, 2001.\\n[16] D. Hu, S. Kaza, and H.  Chen , “Identifying significant facilitators of dark network \\nevolution ,” J. Amer. Soc. Inform. Sci. Technol. , vol. 60,  no. 4 , pp. 655 –665, 2009 .\\n[17] R. Y. K. Lau, P. Bruza , and D. Song, “Towards a belief revision based adaptive and con -\\ntext-sensitive information retrieval system ,” ACM Trans. Inform. Syst. , vol. 26,  no. 2 , 2008.\\n[18] R. Y. K. Lau, D. Song, Y. Li, C. H. Cheung , and J. X. Hao, “Towards a fuzzy domain \\nontology extraction method for adaptive e-learning ,” IEEE Trans. Knowl. Data Eng. , vol. \\n21, no. 6 , pp. 800 –813, 2009 .\\n[19] R. Y. K. Lau, M. Tang, O. Wong , S. Milliner , and Y. Chen , “An evolutionary learning \\napproach for adaptive negotiation agents ,” Int. J. Intell. Syst. , vol. 21,  no. 1, pp. 41 –72, 2006.\\n[20] R. Li, S. Bao, J. Wang , Y. Yu, and Y.  Cao, “Cominer: An effective algorithm for \\nmining competitors from the Web ,” Data Mining, in Proc. Int. Conf. Data Mining , 2006, \\npp. 948 –952.\\n[21] G. Martino  and A.  Sperduti , “Mining structured data ,” IEEE Comput. Intell. Mag. , \\nvol. 5,  no. 1 , pp. 42 –49, 2010 .'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='vol. 5,  no. 1 , pp. 42 –49, 2010 .\\n[22] D. Maynard , V. Tablan , C. Ursu, H. Cunningham , and Y.  Wilks , “Named entity \\nrecognition from diverse text types ,” in Proc. Conf. Recent Advances Natural Language Pro -\\ncessing , 2001 .\\n[23] J.-Y. Nie, G. Cao, and J.  Bai, “Inferential language models for information retrieval ,” \\nACM Trans. Asian Lang. Inf. Process. , vol. 5,  no. 4 , pp. 296 –322, 2006 .\\n[24] J. M. Ponte  and W. B.  Croft , “A language modeling approach to information retriev -\\nal,” in Proc. 21st Annu. Int. ACM SIGIR Conf. Research Development Information Retrieval , \\nMelbourne , Australia , 1998 , pp. 275 –281.\\n[25] M. Porter , “An algorithm for suffix stripping ,” Program , vol. 14,  no. 3 , pp. 130 –137, 1980.\\n[26] D. Rajagopal , D. Olsher , E. Cambria , and K.  Kwok , “Commonsense-based topic \\nmodeling ,” in Proc. ACM Int. Conf. Knowledge Discovery Data Mining , Chicago , IL, 2013 .\\n[27] D. Ramage , C. D.  Manning , and S. T. Dumais , “Partially labeled topic models for \\ninterpretable text mining ,” in Proc. 17th ACM SIGKDD Int. Conf. Knowledge Discovery \\nData Mining , San Diego , CA, 2011 , pp. 457 –465.\\n[28] M. Rosen-Zvi , C. Chemudugunta , T. L.  Griffiths , P. Smyth , and M.  Steyvers , \\n“Learning author-topic models from text corpora,” ACM Trans. Inform. Syst. , vol. 28,  no. \\n1, pp. 1 –38, Article 4 , 2010 .\\n[29] D. Song, Q. Huang , P. D. Bruza , and R. Y. K. Lau, “An aspect query language model'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='[29] D. Song, Q. Huang , P. D. Bruza , and R. Y. K. Lau, “An aspect query language model \\nbased on query decomposition and high-order contextual term association ,” Comput. In -\\ntell., vol. 28,  no. 1 , pp. 1 –23, 2012 .\\n[30] Y. Song, S. Pan, S. Liu, M. X. Zhou , and W.  Qian, “Topic and keyword re-ranking \\nfor LDA-based topic modeling ,” in Proc. 18th ACM Conf. Information Knowledge Manage -\\nment, 2009 , pp. 1757 –1760.\\n[31] M. Steyvers , P. Smyth , M. Rosen-Zvi , and T. L.  Griffiths , “Probabilistic author-topic \\nmodels for information discovery ,” in Proc. 10th ACM SIGKDD Int. Conf. Knowledge \\nDiscovery Data Mining , Seattle , Washington , 2004 , pp. 306 –315.\\n[32] C.-K.  Ting, W.-M.  Zeng , and T.-C.  Lin, “Linkage discovery through data mining \\n[research frontier] ,” IEEE Comput. Intell. Mag. , vol. 5,  no. 1 , pp. 10 –13, 2010 .\\n[33] F. S. Tsai and K. L. Chan , “Detecting cyber security threats in weblogs using proba -\\nbilistic models ,” in Proc. Pacific Asia Workshop Intelligence Security Informatics (Lecture Notes \\nin Computer Science), 2007 , vol. 4430 , pp. 46 –57.\\n[34] S. X. Wu and W.  Banzhaf , “The use of computational intelligence in intrusion detec -\\ntion systems: A review ,” Appl. Soft Comput. , vol. 10,  no. 1 , pp. 1 –35, 2010 .\\n[35] Y. Xia, W. Su, R. Y. K. Lau, and Y.  Liu, “Discovering latent commercial networks \\nfrom online financial news articles ,” Enterprise Inform. Syst. , vol. 7,  no. 3 , pp. 303 –331, 2013.'),\n",
       " Document(metadata={'title': 'A Probabilistic Generative Model for Mining Cybercriminal Networks from Online Social Media', 'year': 2014}, page_content='[36] R. Xia, C. Zong , X. Hu, and E.  Cambria , “Feature ensemble plus sample selection: \\nA comprehensive approach to domain adaptation for sentiment classification ,” IEEE Intell. \\nSyst., vol. 28, no. 3 , pp. 10 –18, 2013 .\\n[37] C. Zhai and J.  Lafferty , “A study of smoothing methods for language models applied to \\ninformation retrieval ,” ACM Trans. Inform. Syst. , vol. 22,  no. 2 , pp. 179 –214, 2004.\\n \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='546 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 7, NO. 6, DECEMBER 2003\\nA Semantically Guided and Domain-Independent\\nEvolutionary Model for Knowledge Discovery\\nFrom Texts\\nJohn Atkinson-Abutridy, Chris Mellish, and Stuart Aitken\\nAbstract— Wepresentanovelevolutionarymodelforknowledge\\ndiscovery from texts (KDTs), which deals with issues concerningshallowtext representation andprocessingfor miningpurposesinan integrated way. Its aims is to look for novel and interesting ex-planatory knowledge across text documents. The approach usesnaturallanguagetechnologyandgeneticalgorithmstoproduceex-planatory novel hypotheses. The proposed approach is interdisci-\\nplinary, involving concepts not only from evolutionary algorithms\\nbut also from many kinds of text mining methods. Accordingly,new kinds of genetic operations suitable for text mining are pro-posed. The principles behind the representation and a new pro-posal for using multiobjective evaluation at the semantic level aredescribed.Somepromisingresultsandtheirassessmentbyhumanexperts are also discussed which indicate the plausibility of themodel for effective KDT.\\nIndex Terms— Data mining, genetic algorithms (GAs), knowl-\\nedge discovery from texts (KDTs).\\nI. MOTIVATION\\nKNOWLEDGEdiscoveryfromtexts(KDTs)involvesdis-\\ncoveringinterestingunseenpatternsintextdatabases[1],\\n[2]. Establishing which kind of knowledge should be acquiredand how its novelty and interestingness should be evaluated is'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='still a matter of research. The metrics commonly used come\\nfromdata mining (DM) or knowledge discovery in databases\\n(KDDs) techniques [3]–[5] and were developed for structureddatabases.However,theycannotbeimmediatelyappliedtotext\\ndata.\\nDespite the large amount of research over the last few years,\\nonly few research efforts worldwide have realized the need forhigh-level representations (i.e., not just keywords), for taking\\nadvantage of linguistic knowledge and for the specific purpose\\nof producing and assessing the unseen knowledge. The rest ofthe effort has concentrated on doing text mining from an in-formation retrieval (IR) perspective and so both representation(keyword based) and data analysis are restricted.\\nThe most sophisticated approaches to text mining or KDT\\nare characterized by an intensive use of external electronic re-sources including ontologies, thesauri, etc., which highly re-stricts the application of the unseen patterns to be discovered\\nManuscriptreceivedAugust30,2002;revisedMay8,2003andJuly25,2003.\\nJ. Atkinson-Abutridy and S. Aitken are with the School of Informatics,\\nUniversity of Edinburgh, Edinburgh, EH1 1HN, U.K. (e-mail: atkinson@\\ndai.ed.ac.uk; stuart@aiai.ed.ac.uk).\\nC. Mellish is with the University of Aberdeen, Aberdeen AB24 3UE, U.K.\\n(e-mail: cmellish@csd.ab.dn.ac.uk).\\nDigital Object Identifier 10.1109/TEVC.2003.819262andtheirdomainindependence.Inaddition,thesystemssopro-\\nducedhavefewmetrics(ornoneatall)whichallowthemtoes-'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='ducedhavefewmetrics(ornoneatall)whichallowthemtoes-\\ntablish whether the patterns are interesting and novel.\\nTheseissuessuggestthatanumberofimportantfeaturesmust\\nbe dealt with in order to come up with effective KDT.\\n• Simpleandeasy-to-interpretdeeperrepresentationfortext\\ndocuments:becauseofefficiencyandcomplexity,fullrep-\\nresentation for natural language text is not usually pos-sible.Ontheotherhand,keyword-basedtextmininglackskey information and it is insufficient for providing infor-\\nmative and meaningful relationships.\\n• Global discovery by “linking” knowledge from different\\nsources(documents)withoutrestrictiontosomesetofpre-defined patterns.\\n• Explanatory knowledge rather than numerical associa-\\ntions: traditional associations from KDDs do not help aperson to understand the origin of a relationship.\\n• Assessment of the quality of the unseen knowledge in\\nterms of novelty, interestingness, etc., so as to prove its\\neffectiveness.\\nIn this context, genetic algorithms (GAs) have several\\npromisingadvantagesovertheusuallearning/analysismethodsemployed in KDT: the ability to perform global search (tradi-tional approaches deal with predefined patterns and restricted\\nscope), the exploration of solutions in parallel, the robustness\\nto cope with noisy and missing data (something critical indealingwithtextinformationaspartialtextanalysistechniquesmay lead to imprecise outcome data), and the ability to assessthe goodness of the solutions as they are produced.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='Inthispaper,weproposeanewmodelforKDTwhichbrings\\ntogether the benefits of shallow text processing and GAs to\\nproduce effective novel knowledge. In particular, the approachintegrates information extraction (IE) technology and a multi-objective GA. It aims at extracting key underlying linguisticknowledge from text documents (i.e., rhetorical and semantic\\ninformation) and then hypothesizing and assessing interesting\\nandunseenexplanatoryknowledge.UnlikeotherapproachestoKDT, we do not use additional electronic resources or domainknowledge beyond the text database. Therefore, the model is\\ndomain-independenteventhoughitisgenre-basedsoastotake\\nadvantage of the documents’ structure.\\nThe task we are addressing can be stated as follows: assume\\nwe have been given a corpus of documents in some domain,for instance, biology, and we have been asked to discoverinteresting and unseen relationships between concepts such as\\n1089-778X/03$17.00 © 2003 IEEE\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. ATKINSON-ABUTRIDY et al.: SEMANTICALLY GUIDED AND DOMAIN-INDEPENDENT EVOLUTIONARY MODEL 547\\nvaccine ax-1 and allergic reactions. Rather than discovering\\nnumericalassociationsbetweenthemorperformingsomeotherkindofanalysis(e.g.,co-occurrences,trends,support,etc.),we\\nwould like the model to discover novelexplanatory hypotheses'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='would like the model to discover novelexplanatory hypotheses\\nacrossthewholecorpusbycombiningunitsoflocalknowledgefrom the different documents. The approach is capable ofproducing that explanatory pattern in terms of premises andconsequences in rules, which look like:\\nIf the goal is to develop the vaccine ax-1 ... and im-\\nmunogicaltechniques...havebeenused...Then,itislikelythat as a result, the vaccine … has an effect ... on the al-\\nlergic reactions …\\nThe above kind of hypothesis involves putting together\\nknowledge at the semantic and rhetorical level, from differentsources and connecting it in such a way that the GA considers\\ninteresting, novel and/or worth exploring by using a set of key\\nevaluation criteria.\\nBecause of the difficulties in handling text data, their repre-\\nsentation and the problem of producing and assessing this sortof new knowledge, our model aims at developing specialized\\ntechniques and strategies to deal with text-based discovery in a\\ngeneric way.\\nThe rest of the paper is organized as follows: Section II de-\\nscribesthemainapproachestoKDTandhowtheyhavetackled\\nthe different issues. Section III presents our approach to KDT\\nusing evolutionary techniques, along with issues regardingrepresentation,hypothesesdiscovery,andautomaticevaluation.Section IV describes the experiments carried out with expert\\nhumans to assess the outcome along with the analysis of the\\nresults obtained. Finally, the conclusions and further researchare highlighted.\\nII. R\\nELATEDWORK'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='results obtained. Finally, the conclusions and further researchare highlighted.\\nII. R\\nELATEDWORK\\nKDT is an emerging technology for analyzing large collec-\\ntions of unstructured documents for the purposes of extractinginterestingandnovelknowledge(i.e.,relations,associations).Itcan be seen as a leap from DM or KDD [3], [4], [6].\\nTraditional approaches to KDT share many characteristics\\nwithclassicalDMbuttheyalsodifferinmanyways:manyclas-sicalDMalgorithms[4],[7]areirrelevantorillsuitedfortextualapplicationsastheyrelyonthestructuringofdataandtheavail-\\nability of large amounts of structured information [1], [2], [8].\\nManyKDTtechniquesinherittraditionalDMmethodsandkey-word-based representation, which are insufficient to cope withthe rich information contained in natural-language text. In ad-\\ndition, it is still unclear how to rate the novelty and/or interest-\\ningness of the knowledge discovered from texts.\\nSome people suggest that inadequacy and failure to report\\nnovelresultsislikelybecauseoftheconfusionbetweenfinding/\\naccessing information in texts (i.e., using IR and data analysis\\ntechniques)andtextmining:thegoalofinformationaccessistohelp users find documents that satisfy their information needs,whereasKDTaimsatdiscoveringorderivingnovelinformation\\nfromtexts,findingpatternsacrossthedocuments[9].Here,two'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='fromtexts,findingpatternsacrossthedocuments[9].Here,two\\nmain approaches can be distinguished: those based on bag-of-words(BOWs)representations,andthosebasedonmorestruc-tured representations and, therefore, the data analysis.A. BOWs Based Approaches\\nSomeoftheearlyworkonTMcamefromtheIRcommunity,\\nhence, the assumption of textrepresented as a BOWs, and thentobeprocessedviaclassicalDMmethods[1],[8].Sincethereisadditional information beyond these keywords and issues suchas their order do not matter in a BOW approach, it will usuallybe referred to as nonstructured representation.\\nOnce the initial information (i.e., terms, keywords) has been\\nextracted,KDDoperationscanbecarriedouttodiscoverunseenpatterns. Representative methods in this context have includedregular associations [1], [4],concept hierarchies [10],full text\\nmining[8],clustering , andself-organizing maps [11].\\nMostoftheseapproachesworkinaverylimitedwaybecause\\nthey rely on surface information extracted from the texts, andonitsstatisticalanalysis.Asaconsequence,keyunderlyinglin-guistic information is lost. The systems may be able to detectrelationsorassociationsbetweenitems,buttheycannotprovideany description of what those relations are. At this stage, it istheuser’sresponsibilitytolookforthedocumentsinvolvedwiththose concepts and relations to find the answers. Thus, the re-lations are just a “clue” that there is something interesting butwhich needs to be manually verified.\\nB. High-Level Representation Approaches'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='B. High-Level Representation Approaches\\nAnothermainstreaminKDTinvolvesusingmorestructured\\nor higher level representations to perform deeper analysis so\\nto discover more sophisticated novel/interesting knowledge.\\nAlthough, in general, the different approaches have beenconcerned with either performing exploratory analysis forhypothesis formation, or finding new connections/relations\\nbetween previously analyzed natural language knowledge,\\nit has also involved using term-level knowledge for otherpurposes than just statistical analysis [12].\\nSomeearlyresearchbySwansononthetitlesofarticlesstored\\nin MEDLINE used an augmented low-level representation (the\\nwords in the titles) and exploratory data analysis to discoverhidden connections [13] leading to very promising and inter-esting results in terms of answering questions for which the\\nanswer was not currently known. He showed how chains of\\ncausal implication within the medical literature (in the titles ofthedocuments)canleadtohypothesesforcausesofrarediseases,some of whichhave received scientificsupporting evidence.\\nOtherapproachesusingIEwhichinheritedsomeofSwanson’s\\nideas to derive new patterns from a combination of text frag-ments,havealsobeensuccessful.Essentially,IEisanatural-lan-guage(NL)technologywhichanalyzesaninputNLdocumentin\\nashallowwaybyusingdefinedpatternsalongwithmechanisms'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='ashallowwaybyusingdefinedpatternsalongwithmechanisms\\nto resolve implicit discourse-level information (i.e., anaphora,coreference,etc.)tomatchimportantinformationfromthetexts.As a result, an IE task produces an intermediate representation\\ncalled“templates,”inwhichinformationrelevanthasbeenrec-\\nognized,forexample:names,events,entities,etc.,orhigh-levellinguistic entities: noun phrases, etc.\\nUsing IE techniques and electronic linguistic resources,\\nHearst[2],[7],[14]proposesadomain-independentmethodfor\\nthe automatic discovery of WordNet-style lexicosemantic rela-tions by searching for corresponding lexicosyntactical patternsin unrestricted text collections. This technique is meant to be\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 548 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 7, NO. 6, DECEMBER 2003\\nusefulasanautomatedorsemiautomatedaidforlexicographers\\nand builders of domain-dependent knowledge bases. Also,it does not require an additional knowledge base or specific\\ninterpretation procedures in order to propose new instances of\\nWordNetrelations[15].Oncethebasicrelations(i.e.,hyponyms,hypernyms, etc.) are obtained, they are used to find commonlinks with other “similar” concepts in WordNet [15] and so to\\ndiscover new semantic links [2], [14]. However, there are tasks'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='discover new semantic links [2], [14]. However, there are tasks\\nwhich need to performed by hand such as deciding on alexicalrelation that is of interest (i.e., hyponym) and a list of wordpairs from WordNet this relation is known to hold between.\\nOneofthemainadvantagesofthismethodisitslowcostfor\\naugmenting the structure of WordNet and its simplicity of re-lations. However, it also has some drawbacks including its de-pendenceon thestructureofageneral-purposeontology which\\npreventsitfromreasoningaboutspecificterminology/concepts,\\nthe restricted set of defined semantic relations (i.e., only rela-tions contained in WordNet are dealt with), its dependence onWordNet’s terms (i.e., what if there are specific terms in some\\ndomain?),thekindofinferenceenabled(i.e.,itisonlypossible\\ntoproducedirectlinks;whataboutifwewishtorelatedifferenttermsforwhichtheredoesnotexist(apparently)somerelation-ship?), etc.\\nA natural further important step would be using knowledge\\nbasesuchasWordNettosupporttextinferencetoextractrelevant,unstated information from the text. Harabagiu and Moldovan[16] address this issue by using WordNet as a commonsense\\nknowledge base and designing relation-driven inference mech-\\nanismswhichlookforcommonsemanticpathsinordertodrawconclusions.Oneoutstandingfeatureoftheirmethodisthatfromthesegeneratedinferences,itiseasytoaskforunknownrelations\\nbetweenconcepts. Thishas beenprovedtobeextremelyuseful'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='betweenconcepts. Thishas beenprovedtobeextremelyuseful\\nin the context of question-answering systems. However, theapproachlackstheassessmentofbothplausibilityandgoodness:to what extent are the facts we are discovering interesting and\\nnovel?Althoughthemethodexhibitsunderstandingcapabilities,\\nthecommonsensefactsdiscoveredhavenotbeen demonstratedto be novel and interesting from a KDD viewpoint.\\nMooney and colleagues [17], [18] have also attempted to\\nbringtogethergeneralontologies,IEtechnology,andtraditional\\nmachine learning methods to mine interesting patterns. Unlikeprevious approaches, Mooney deals with a different kind ofknowledge, e.g., prediction rules. In addition, an explicit mea-\\nsure of novelty of the mined rules is proposed by establishing\\nsemanticdistancesbetweenrules’antecedentsandconsequentsusing the underlying organization of WordNet. Novelty is thendefined as the average (semantic) distance between the words\\nin a rule’s antecedent andconsequent. A key problem with this\\nis that the method depends highly on WordNet’s organizationand idiosyncratic features. As a consequence, since a lot ofinformation extracted from the documents are not included in\\nWordNet the predicted rules will lead to misleading decisions\\non their novelty.\\nC. GAs in KDT\\nEvolutionary computation has already been used in general\\nDM [19]–[23]. For KDT, however, the application of GAs is a'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='DM [19]–[23]. For KDT, however, the application of GAs is a\\nrecent research topic in which, with some isolated exceptionssuch as [24] on text classification and, more recently, [25] on\\nsemanticrelationextraction,nootherresearcheffortsareunderway. Some of the drawbacks in traditional GA-based knowl-\\nedge discovery which make necessary to encourage the design\\nofspecific-purposeoperationstodealwithtextdataincludethefollowing.\\n• Data are traditionally based on a binary representation in\\nwhichdiscreteinformationisassumed(evenincontinuousdata,rangerepresentationsarepossible)andsotheopera-tionsinvolve“modifying”bitswithoutconcernforanyun-\\nderlyingsemantics.Indealingwithtextdata,representing\\nthe linguistic knowledge is an important issue in whichtraditionalbinarycodingisinsufficient,andsonewrepre-\\nsentation schemes should be investigated.\\n• Whilethereisagreementaboutrepresentingthehypotheses\\ninarule-likeform(i.e.,antecedent-consequent),mostofthetasks addressed concern classification or prediction. This\\nassumes that a sort of “database” (i.e., training examples\\norarealdatarepository)isavailableandtheinformationisprecise, discrete, and complete. In dealing with informa-tion/knowledge extractedfrom texts,thisis notplausible.\\n• Operations involving generalization/specialization which\\nare a core issue in knowledge discovery, deal with modi-fications in such a way that changing a couple of bits can\\nturn ahypothesis into amore general/specificone. Intext'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='turn ahypothesis into amore general/specificone. Intext\\nmining such information cannot be dealt with in this waybecause of the data’s nature and complexity.\\n• Another common assumption is the ability to compute\\nthe prediction level of the rules in terms of their predic-\\ntiveaccuracywhichisdefinedasa confidencefactor [21].\\nHowever,forKDTpurposestherulerepresentationisnotrestrictedtoaprediction-likeconsequent(actually,itisim-\\npossible to extract this kind of information from texts) so\\nthe “consequent” is a general-purpose element involvingrhetorical, semantic and syntactic information. In addi-\\ntion, in order to compute the accuracy one would need to\\nhavesomeconceptorganizationtotelluswhetheronere-lation “covers” another one.\\nA recent work in KDT using GA [25] proposes a system to\\nautomatically evolve patterns for relation extraction from the\\nWebtextusinggeneticprogramming(GP)whichhasalsopoten-tialapplicationtoextendor toupdateathesaurus orasemanticnetwork. The method is capable of finding patterns expressing\\nmeronym or hyponym relations, and then have them evaluated\\nusing WordNet. GP is used to automatically learn feasible pat-terns/segments for extraction of these semantic relations.\\nTheresultsdescribedarepromisingintermsofdetectingso-\\nphisticated patterns. However, the work is at an early stage andthe system built is only able to find patterns which match spe-cific kinds of semantic relationships. In addition, there is no'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='proof that there are interesting and novel learned patterns from\\na user viewpoint.\\nIII. A H\\nIGH-LEVELEVOLUTIONARY MODEL FOR KDT\\nWe develop a semantically guided model for evolutionary\\nKDT which is domain-independent but genre-based. Unlikeprevious approaches to KDT, our approach does not rely\\non external resources or descriptions, hence, its domain in-\\ndependence. Instead, it performs the discovery only using\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. ATKINSON-ABUTRIDY et al.: SEMANTICALLY GUIDED AND DOMAIN-INDEPENDENT EVOLUTIONARY MODEL 549\\nFig. 1. GA-based KDT.\\ninformation from the original corpus of text documents and\\nfrom the training data generated from them. In addition, anumber of strategies have been developed for automaticallyevaluating the quality of the hypotheses. This is an important\\ncontribution on a topic which has been neglected in most of\\nKDT research over the last years.\\nWe have adopted GAs as central to our approach to KDT.\\nHowever, forproper GA-based KDT thereare importantissues\\nto be addressedincluding representation andguided operations\\ntoensurethattheproducedoffspringaresemanticallycoherent.\\nIn order to deal with these issues and produce an effective\\nKDT process, our working model has been divided into two'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='KDT process, our working model has been divided into two\\nphasesasseeninFig.1.Thefirstphaseisthepreprocessingstepaimed to produce both training information for further evalua-tionandtheinitialpopulationoftheGA.Thesecondphasecon-\\nstitutestheknowledgediscoveryitself,inparticular,thisaimsat\\nproducing and evaluating explanatory unseen hypotheses.\\nThewholeprocessingstartsbyperformingtheIEtaskwhich\\napplies extraction patterns and then generates a rule-like rep-\\nresentation for each document of the specific domain corpus.\\nAfterprocessing\\ndocumentstheextractionstagewillproduce\\nrules, each one representing the document’s content in terms\\nof its conditions and conclusions. Once generated, these rules,\\nalong with other training data, become the “model” which willguide the GA-based discovery.\\nInordertogenerateaninitialsetofhypotheses,aninitialpop-\\nulation (small size\\n) is created by building random hy-\\npotheses from the initial rules, that is, hypotheses containingpredicate and rhetorical information from the rules are con-structed. The GA then runs for a number of generations until\\na fixed number of generations is achieved. At the end, a small\\nset of the best\\nhypotheses is obtained ( is a user-defined\\nparameter, usually fixed to 5% of the population).\\nThe description of the model is organized as follows.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='The description of the model is organized as follows.\\nSection III-A presents the main features of the text prepro-cessing phase and how the representation for the hypotheses isgenerated. In addition, training tasks which generate the initial\\nknowledge (semantic and rhetorical information) to feed the\\ndiscovery are described. Section III-B describes constrainedgenetic operations to enable the hypotheses discovery, andproposes different evaluation metrics to assess the plausibility\\nof the discoveredhypotheses in a multiobjective context.\\nA. Text Preprocessing and Training\\nThe preprocessing phase has two main goals: to extract im-\\nportant information from the texts and to use that informationto generate both training data and the initial population for the\\nGA.\\n1) Preprocessing: An underlying principle in our research\\nistobeabletomakegooduseofthestructureofthedocuments\\nfor the discovery process. It is well-known that processing fulldocumentshasinherentcomplexities[26],sowehaverestrictedourscopesomewhattoconsiderascientificgenreinvolvingsci-\\nentific/technical abstracts. These have a well-defined structure\\nto “summarize” what the author states in the full document.In addition, the style of technical documents avoids many con-cept-level ambiguities.\\nIn general, it is suggested that an abstract in some given do-'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='In general, it is suggested that an abstract in some given do-\\nmainfollowsa“macro-structure”(i.e.,genre-dependentrhetor-ical structure), which is used by its author to state the back-ground information,methods, achievements, conclusions,etc.\\nUnlikepatternsextractedforusualIEpurposessuchasin[2],\\n[7],[27],and[28],thismacrostructureanditsrolesaredomain-independentbutgenre-based,soitisrelativelyeasytotranslateit into different contexts.\\nAs an example, suppose that we are given the following ab-\\nstract,whereboldsequencesofwordsindicatethemarkerstrig-gering the IE patterns.\\nFrom such a structure, important constituents can be identi-\\nfied.\\n•Rhetorical Roles (discourse-level knowledge): These in-\\ndicateimportantplaceswheretheauthormakessome“as-\\nsertions” abouthis/her work (i.e.,the author is statingthe\\ngoals, used methods, achieved conclusions, etc.). In theexample above, the roles are represented by goal, object,method, and conclusion.\\n•Predicate Relations: These are represented by actions\\n(predicate and arguments) which are directly connectedto the role being identified and state a relation whichholds between a set of terms (words which are part of a\\nsentence), a predicate and the role which they are linked\\nto. Thus, for the example, they are as follows:\\nprovide (“the basic information …”)\\nanalyze (“long-term trends …”)study (“lands plot using …”)\\nimprove (“soil … improved after …”).'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='analyze (“long-term trends …”)study (“lands plot using …”)\\nimprove (“soil … improved after …”).\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 550 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 7, NO. 6, DECEMBER 2003\\n•Causal Relation(s): Although there are no explicit causal\\nrelations in the above example, we can hypothesize asimple rule of the form.\\nIFthe current goals are G1,G2, ... and the\\nmeans/methods used M1,M2, ... (and any otherconstraint/feature),\\nTHENitistruethatwecanachieve\\nthe conclusions C1,C2, ...\\nInordertoextractthisinitialkeyinformationfromthetexts,an\\nIEmodulewasbuilt.Essentially,ittakesasetoftextdocuments,has them tagged through a previously trained part-of-speech\\n(POS) tagger, and produces an intermediate representation for\\nevery document (i.e., template, in an IE sense), which is thenconverted into a general rule. A set of hand-crafted domain-in-dependent extraction patterns were written and coded. For this\\npurpose,eachpatternconstructsanoutputrepresentationwhich\\ninvolves the two-level linguistic knowledge: the rhetorical roleandtheactionrepresentedbythepredicaterelationanditsargu-ments (partial sentences). Forexample, an extractionpattern to\\nrecognizesomespecificrhetoricalrole(i.e.,goal)anditscontents\\ncan be specified by hand as follows:\\ncon el proposito de @ACTION/inf OBJECT:\\ngoal (ACTION [OBJECT])\\n/* In order to ACTION (OBJECT) ... */'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='goal (ACTION [OBJECT])\\n/* In order to ACTION (OBJECT) ... */\\nwheretheleft-hand-sideexpressionstatesthepatterntobeiden-\\ntifiedalongwiththesyntacticcategories(i.e.,nouns,verbs,etc.)\\nandtheright-handside(following“:”),thecorrespondingaction(predicate relation) tobe produced.Specifically, thisextractionpatternindicatesthatthegoalwillbeanaction(alongwithatag\\nfor an infinitive verb) followed by some object (i.e., ACTION\\nandOBJECTputtogetherrepresentanunderlyingverbalphrasebeenrecognized),anditisthentranslatedintothecorrespondingpredicate structure (i.e., the predicate action and the whole se-\\nquence of terms which represents its argument).\\nFor example, a typical rule containing the information ex-\\ntractedbythesystemforonedocumentlookslikethefollowing:\\nrule ( ‘Document 1’,\\n[% Antecedent List\\ngoal (\\nprovide ( ‘basic information about ...’\\n)),\\ngoal (\\nanalyze ( ‘long-term trends of the ...’\\n)),method (\\nstudy ( ‘land’s plots...’ ))\\n],[% Consequent List\\nconclusion (\\nshow ( ‘soils have improved after ...’ ))\\n])\\nIt is important to note that, at this point, we are not making\\nany inferences, but rather representing the facts already stated\\nbytheauthor.Eachdocumentwillhaveitsownrulerepresenta-\\ntion (initial local knowledge) containing the relations and rolesidentified for that document.2) Training Information: In addition to the templates ex-\\ntracted so far, there is important training data that can be cap-turedfromthecorpusofdocumentsitselfandfromthesemantic'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='informationcontainedintherules.Thiscanguidethediscovery\\nprocess in making further similarity judgments and assessingthe plausibility of the produced hypotheses.\\n•Training Information From the Corpus: It has been sug-\\ngested that huge amounts of texts represent a valuable\\nsource of semantic knowledge. In particular, in latent se-\\nmantic analysis (LSA) [29]–[31] it is claimed that thisknowledge is at the word level. That is, LSA considersonly patterns of word usage: syntax or rhetorical struc-\\nture arenot takeninto account.Sinceweare dealingwith\\nmore structured information from the document, it hasbeen necessary to complement LSA with additional lin-guistic knowledge. In particular, following work by [32]\\nand [33] on LSA incorporating structure, we have de-\\nsigned a semistructured LSA representation for text datain which we represent predicate information (i.e., verbs)\\nandarguments(i.e.,setofterms)separatelyoncetheyhave\\nbeen properly extracted in the IE phase.\\nWe propose a simple strategy for representing the\\nmeaningofthepredicateswitharguments.Then,asimple\\nmethod is developed to measure the similarity between\\nthese units.\\nGivenapredicate\\nanditsargument ,thevectorsrep-\\nresentingthemeaningforbothofthemcanbedirectlyex-\\ntractedfromthetraininginformationprovidedbytheLSA\\nanalysis.Representingtheargumentinvolvessummingupallthevectorsrepresentingthetermsoftheargumentand\\nthenaveragingthem,asisusuallyperformedinsemistruc-'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='thenaveragingthem,asisusuallyperformedinsemistruc-\\ntured LSA [34]. Once this is done, the meaning vector ofthe predicate and the argument is obtained by computingthesumofthetwovectorsasusedin[32].Ifthereismore\\nthan one argument, then the final vector of the argument\\nis just the sum of the individual arguments’ vectors.\\nNext,inmakingfurthersemanticsimilarityjudgements\\nbetweentwopredicates\\nand ,wetaketheir\\ncorrespondingpreviouslycalculatedmeaningvectorsand\\nthen the similarity is determined by how close these twovectorsare.Wecanevaluatethisbycomputingthe cosine\\nbetweenthesevectorswhichgivesusaclosenessmeasure\\nbetween\\n1(completeunrelatedness)and1(completere-\\nlatedness) [31].\\nAccordingly, the semantic similarity (SemSim) be-\\ntween two hypotheses and, can be effectively\\ncalculated with the simple procedure.\\nPROCEDURE SemSim\\nIN:List of predicates with arguments\\nof and (,)\\nOUT:SemanticSimilarity (between\\nand )\\nRETURN SemanticSimilarity\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. ATKINSON-ABUTRIDY et al.: SEMANTICALLY GUIDED AND DOMAIN-INDEPENDENT EVOLUTIONARY MODEL 551\\nwhere ComputeCompoundVector ( ) computes the re-\\nsulting centroid vector of summing up the the predicate(and arguments)\\nof the corresponding hypothesis\\n( ).Notethatrhetoricalrolesarenot\\ntakenintoaccountingettingthemeaningvectorsorthesim-\\nilaritymeasuresbecausetheLSAtraininginformationmay'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='ilaritymeasuresbecausetheLSAtraininginformationmay\\nnotcontainthetermsrepresentingtheroles.Therolesareconsidered inother ways asthe hypothesesareevaluated.\\n•Training Information From the Rules: Training informa-\\ntionfromthetextsisnotsufficientasitonlyconveysdata\\natawordsemanticslevel.Weclaimthatbothbasicknowl-edgeatarhetorical,semanticlevel,andco-occurrencein-formation can be effectively computed to feed the dis-\\ncovery and to guide the GA.\\nAccordingly,weperformtwokindsoftasks:creatingtheinitial\\npopulation and computing training informationfrom therules.\\n1)Creating the initial population of hypotheses:\\nOnce the initial rules have been produced, their com-\\nponents(rhetoricalroles,predicaterelations,etc.)areiso-\\nlatedandbecomeaseparate“database.”Thisinformation\\nisusedbothtobuildtheinitialhypothesesandtofeedthefurthergeneticoperations(i.e.,mutationofroleswillneedto randomly pick a role from this database).\\nThe basic components for building the initial hy-\\npotheses are randomly picked and used to producehypotheses which contain combinations of elementsfrom the database. An important consequence of this\\nsampling is that even if the rules extracted from the texts\\nare incomplete or miss information (i.e., rules withoutconclusions, antecedents without goals, etc.), we ensurethatthediscoveryprocessdealswithruleswhichinclude\\nthe basic information. Whether it is consistent or not\\ndoes not matter because the learning step will establishwhich ones “make sense” and which ones do not.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='2)Computing training information in which two kinds of\\ntraining data are obtained:\\n–Computing correlations between rhetorical roles and\\npredicate relations: The connection between rhetor-\\nical information and the predicate action performed\\nconstituteskeyinformationforproducingcoherenthy-\\npotheses.Forexample,is,insomedomain,the goalof\\nsome hypothesis likely to be associated with the con-\\nstruction ofsomecomponent?Inahealthcontext,this\\nconnection would be less likely than having “ finding\\na new medicine for ...” as a goal.\\nInordertoaddressthisissue,weadoptedaBayesian\\napproach,whereweobtaintheconditionalprobability\\nofsomepredicate\\ngivensomeattachedrhetoricalrole\\n, namely .\\n–Computing co-occurrences of rhetorical information:\\nOne could think of a hypothesis as an abstract having\\ntextparagraphswhicharesemanticallyrelatedtoeachother. Consequently, the meaning of the scientific ev-idence stated in the abstract may subtly change if the\\norder of the facts is altered.\\nThis suggests that in generating valid hypotheses\\nthere will be rule structures which are more or lessdesirable than others. For instance, if every rule con-\\ntains a “goal” as the first rhetorical role, and the GAhasgeneratedahypothesisstartingwithsome“conclu-\\nsion” or “method,” it will be penalized and, therefore,\\nitisveryunlikely forthat tosurviveinthenextgener-ation.Since the order mattersin terms of affecting therule’smeaning,wecanthinkofthe\\nrolesofarule,as\\na sequence of tags: such that precedes'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='rolesofarule,as\\na sequence of tags: such that precedes\\n, so we generate, from the rules, the conditional\\nprobabilities , for every role ,. The\\nprobability that precedes will be used in evalu-\\nating new hypotheses.\\nB. Hypothesis Discovery and Evaluation\\nOur approach to KDT is strongly guided by semantic and\\nrhetorical information, and consequently there are some soft\\nconstraints to be met before producing the offspring so as tokeep them coherent.\\nThe GA will start from a initial population, which in this\\ncase, is a set of semirandom hypotheses built up from the pre-\\nprocessing phase. Next, constrained GA operations are appliedand the hypotheses are evaluated. In order for every individualtohaveafitnessassigned,weuseaevolutionarymultiobjective\\noptimizationstrategybasedonthestrengthParetoevolutionary\\nalgorithm(SPEA)[35]inawaywhichallowsincrementalcon-structionofaPareto-optimalsetandusesasteady-statestrategyfor the population update.\\nForsemanticconstraints,judgmentsofsimilaritybetweenhy-\\npotheses or components of hypotheses (i.e., predicates, argu-ments,etc.)arecarriedoutusingtheLSAtrainingdataandpred-icate-levelinformation previously discussed inSection III-A2.\\n1) Hypothesis Discovery: Using the semantic measure\\naboveandadditionalconstraintsdiscussedlateron,weproposenew operations to allow guided discovery such that unrelatednew knowledge is avoided, as follows.\\n•Selection: selects a small number of the best parent hy-\\npotheses of every generation ( generation gap ) according'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='potheses of every generation ( generation gap ) according\\nto their Pareto-based fitness.\\n•Crossover: a simple recombination of both hypotheses’\\nconditions and conclusions takes place, where two indi-\\nviduals swap their conditions to produce new offspring(the conclusions remain).\\nUnder normal circumstances, crossover works on\\nrandom parents and positions where their parts should be\\nexchanged. However, in our case this operation must berestricted to preserve semantic coherence. We use softsemantic constraints to define the following two kind of\\nrecombinations.\\n1)Swanson’sCrossover: BasedonSwanson’shypoth-\\nesis [13], [36], [37], we propose a recombination\\noperation as follows. If there is a hypothesis (AB)\\nsuchthat“\\nIFATHENB”andanotherone(BC)such\\nthat “IFBTHENC,” (B’ being something semanti-\\ncallysimilartoB)thenanewinterestinghypothesis\\n“IFATHENC” can be inferred, only if the conclu-\\nsions of AB have high semantic similarity (i.e., viaLSA) with the conditions of hypothesis BC.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 552 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 7, NO. 6, DECEMBER 2003\\nFig. 2. Swanson’s crossover.\\nFig. 3. Default semantic crossover.\\nThesimilarityiscomputedfromthepredicatere-\\nlationsandtheirargumentsusingLSA,irrespectiveof the rhetorical roles, as the roles of B’ and B are\\ndisjoint.\\nIn practical terms, this means that if two hy-'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='disjoint.\\nIn practical terms, this means that if two hy-\\npothesesmeetthesimilarityrequirements,theywillalwaysswaptheirmaterialatthepointbetweenan-\\ntecedent and consequent (see Fig. 2).\\n2)Default Semantic Crossover: If the previous tran-\\nsitivity does not apply, then the recombination isperformed as long as both hypotheses as a whole\\nhave high semantic similarity, which is defined in\\nadvance by providing minimum thresholds. Eventhough the offspring may not be so interesting, theconstraint ensures that the hypotheses will at least\\nhave some semantic coherence (see Fig. 3).\\n•Mutation: aims to make small random changes on hy-\\npotheses to explore new possibilities in the search space.\\nAs in recombination, we have dealt with this operation inaconstrainedway,soweproposethreekindsofmutationsto dealwith the followinghypotheses’ different objects.\\n1)Role Mutation: One rhetorical role (including its\\ncontents: relations and arguments) is selected andrandomlyreplacedbyarandomonefromtheinitial\\nrole database.\\n2)Predicate Mutation: One inner predicate and argu-\\nment is selected and randomly replaced by anotherfrom the initial predicate databases.\\n3)Argument Mutation: Since we have no information\\nabout arguments’ semantic types, we choose a newargument by the following guided procedure:\\nLet\\nbe the set of the possible predicate re-\\nlations (predicates with arguments) from the pre-processing set, the current predicate and ar-\\ngument, and , the LSA semantic'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='gument, and , the LSA semantic\\nsimilarity[29]betweenarguments and.Then\\na) select candidate predicate relations\\n,\\n;\\nb) select random argument\\n.•Population Update: We use a nongenerational GA in\\nwhich some individuals are replaced by the new off-spring in order to preserve the hypotheses’ good material\\nfrom one generation to another and so to encourage\\nthe improvement of the population’s quality. We use asteady-state strategy in which each individual from asmall number of the worst hypotheses is replaced by an\\nindividual from the offspring only if the latter are better\\nthan the former.\\n2) Evaluation: In single steady-state GAs, establishing\\nwhetheroneindividualisbetterthanotherisstraightforwardasit only involves computing each hypothesis’ fitness indepen-\\ndently and then comparing them.\\nSinceeachindividualinourmodelhastobeassessedbydif-\\nferent criteria, usual methods for evaluating fitness are not ap-propriate.Instead,thecriteriaofonesolutionmustbetradedoff\\nagainst those of other solutions. Taking into account the whole\\nset of criteria, we say that one hypothesis is a better solutioncomparedwithanotherhypothesisiftheformerisbetterthanthelatterwithrespecttoatleastoneobjectivevalueandtheformer\\nis not worse than the latter with respect to any objective value.\\nThiskindofcomparisonrelationisusuallyreferredtoas domi-\\nnanceandithasbeenakeyissueinevolutionarymultiobjective\\noptimization (EMOO), which is the strategywe have taken.\\nWe propose EMOO-based evaluation methods (metrics) to'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='We propose EMOO-based evaluation methods (metrics) to\\nassessthehypotheses’fitnessinadomain-independentwayand,unlike other approaches, without using any external source ofdomain knowledge.\\nThe different metrics are represented by multiple criteria by\\nwhich the hypotheses are assessed. On one side, they considersemanticandpragmaticmeasuresinordertoensurethatthehy-potheses being produced from text data are coherent and plau-\\nsible. On the other hand, there are criteria for more subjective\\nmetrics such as interestingness, novelty, etc., based on the hy-potheses’contentsandonthetrainingdata.Thehypothesessat-isfying ideal criteria will be those which are produced in the\\nprocessoftradingoffbetweencompetingsolutions’objectives.\\nIn order to establish evaluation criteria, we have taken into\\naccount different issues concerning plausibility (Is the hypoth-esis semantically sound?, Are the GA operations producing\\nsomething coherent in the current hypothesis?), and quality\\nitself (How is the hypothesis supported from the initial textdocuments?, How interesting is it?). Accordingly, we have de-finedeightevaluationmethodstoassessthehypotheses(i.e.,in\\ntermsofParetodominance,itwill produceaeight-dimensional\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. ATKINSON-ABUTRIDY et al.: SEMANTICALLY GUIDED AND DOMAIN-INDEPENDENT EVOLUTIONARY MODEL 553'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='vector of objective functions) given by: relevance, structure,\\ncohesion, interestingness, coherence, coverage, simplicity,plausibility of origin .\\nThe current hypothesis to be assessed will be denoted as\\n,\\nand the training rules as . Evaluation methods (criteria) by\\nwhich the hypotheses are assessed and the questions they are\\ntrying to address are as follows.\\n•Relevance (How important is the hypothesis to the target\\nquestion? ): measures the semantic closeness between the\\nhypothesis’ predicates (relations and arguments) and thetarget concepts. Relevance is then computed from com-\\npound vectors obtained in the LSA analysis.\\nThe semantic representation for the hypothesis’ pred-\\nicates should show a similarity to the target concepts\\n.FollowingworkbyKintschon pred-\\nication[33],weproposeanadaptationofhisalgorithmto\\ncompute the overall relevance of the hypothesis.\\nThe strength is determined by how\\nclosely related two target concepts (i.e., pair of relevant\\nuser-defined terms for which an hypothesis that explains\\nitsunseenrelationshipislookedfor)aretobothpredicate\\nand argument\\nterm term\\nwherethestrengthfunction [33]mustbechoseninsuch\\na way that only if term is close to\\nbothand. Otherwise, it has the value 0.\\nIn order to compute the similarity function for both\\nterms,wejusttaketheaverageof forbothterms.So,the\\noverall relevance will be given by\\nrelevance\\nterm term\\nwhere denotes the length of hypothesis (i.e.,\\nnumber of predicate actions).'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='relevance\\nterm term\\nwhere denotes the length of hypothesis (i.e.,\\nnumber of predicate actions).\\n•Structure (How good is the structure of the rhetorical\\nroles?): measures how much of the rules’ structure is ex-\\nhibited in the current hypothesis. Since we have previouspreprocessinginformationregardingbi-gramsofroles,the\\nstructureiscomputedbyfollowingaMarkovchain[26]as\\nfollows:\\nStructure\\nwhere represents the th role of the hypothesis ,\\ndenotes the conditional probability that\\nrole immediately precedes , denotes the\\nprobability that no role precedes (beginning of the\\nstructure).\\n•Cohesion (How likely is a predicate action to be associ-\\natedwithsomespecificrhetoricalrole? ):measuresthede-\\ngree of “connection” between rhetorical information andpredicate actions. The issue here ishow likely (accordingto the rules) some predicate relation\\nin the current hy-\\npothesisistobeassociatedwithrole .Formally, cohesion\\nfor hypothesis is expressed as\\ncohesion\\nwhere states the conditional probability of\\nthe predicate given the rhetorical role .\\n•Interestingness (How interesting is the hypothesis in\\nterms of its antecedent and consequent? ): Unlike other\\napproaches to measure “interestingness” which use an\\nexternal resource (e.g., WordNet) and rely on its organ-ization [17], [38] we propose a different view wherethe criterion can be evaluated from the semistructured\\ninformation provided by the LSA analysis. Accordingly,\\nthe measure for hypothesis\\nis defined as a degree of'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='the measure for hypothesis\\nis defined as a degree of\\nunexpectedness as follows:\\ninterestingness ( )=\\nDissimilarity between Antecedent\\nand Consequent\\n= 1 -SemSim (Antecedent ( ), Conse-\\nquent ( )).\\nThatis,thelowerthesimilarity,themoreinterestingthe\\nhypothesisislikelytobe.Otherwise,itmeansthehypoth-\\nesisinvolvesacorrelationbetweenitsantecedentandcon-sequent which may be an uninteresting known commonfact [39].\\nWe propose the application of “interestingness” for\\nevery hypothesis to be delayed until some condition ismet (otherwise, anyrandom incoherenthypotheses mightbe regarded to as “interesting”). In particular, we have\\nset an experimental threshold in such a way that the\\ncriterion is applied when the average fitness produced bythe modified SPEA algorithm for the individuals of thepopulation exceeds that threshold.\\n•Coherence: This metrics addresses the question whether\\ntheelementsofthecurrenthypothesisrelatetoeachotherin a semantically coherentway. Unlike rules producedbyDMtechniquesinwhichtheorderoftheconditionsisnot\\nan issue, the hypotheses produced in our model rely on\\npairs of adjacent elements which should be semanticallysound, a property which has long been dealt with in thelinguistic domain, in the context of text coherence [40],\\n[41].\\nAswehavesemanticinformationprovidedbytheLSA\\nanalysiswhichiscomplementedwithrhetoricalandpred-icate-level knowledge, we developed a simple method to\\nmeasurecoherence,followingworkby[41]onmeasuring\\ntext coherence.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='measurecoherence,followingworkby[41]onmeasuring\\ntext coherence.\\nSemanticcoherenceiscalculatedbyconsideringtheav-\\nerage semantic similarity between consecutive elements\\nofthehypothesis.However,notethatthisclosenessisonly\\ncomputedonthesemantic informationthatthepredicatesand their arguments convey(i.e., not theroles) as the role\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 554 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 7, NO. 6, DECEMBER 2003\\nstructure has been considered in a previous criterion. Ac-\\ncordingly, the criterion can be expressed as follows:\\nCoherence\\nwhere( ) denotes the number of adjacent pairs.\\n•Coverage: The coverage metric tries to address the\\nquestion of how much the hypothesis is supported by the\\nmodel (i.e., rules representing documents and semantic\\ninformation).\\nCoverage of a hypothesis has usually been measured\\nin KDD approaches by considering some structuring in\\ndata(i.e.,discreteattributes)whichisnotpresentintextualinformation. Besides, most of the KDD approaches haveassumed the use of linguistic or conceptual resources to\\nmeasure the degree of coverage of the hypotheses (i.e.,\\nmatch against databases, positive examples).\\nInordertodealwiththecriterioninthecontextofKDT,\\nwe say that a hypothesis\\ncovers an extracted rule\\nonly if the predicates of are roughly (or exactly, in the\\nbest case) contained in .\\nFormally, the rules covered are defined as\\n:'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='best case) contained in .\\nFormally, the rules covered are defined as\\n:\\nwhere representstheLSA-basedsim-\\nilarity between hypothesis predicate and rule predi-\\ncate, threshold denotes a minimum fixed user-defined\\nvalue, denotes the whole set of rules, rep-\\nresentsthelistofpredicateswithargumentsof ,and\\nrepresents a predicate (with arguments) contained in .\\nOnce the set of rules covered is computed, the criterioncan finally be computed as\\nCoverage\\nwhere and denotethesize\\nofthesetofrulescoveredby ,andthesizeoftheinitial\\nset of extracted rules, respectively.\\n•Simplicity (How simple is the hypothesis? ): Shorter\\nand/or easy-to-interpret hypotheses are preferred. Sincethe criterion has to be maximized, the evaluation is given\\nby\\nwhere denotes the maximum user-defined\\nnumber of elements for any hypothesis.•Plausibility of Origin (How plausible is the hypothesis\\nproducedbySwanson’sevidence? ):Ifthecurrenthypoth-\\nesiswasanoffspringfromparentswhichwererecombined\\nby a Swanson’s transitivity-like operator, then the higher\\nthe semantic similarity between one parent’s consequentand the other parent’s antecedent, the more precise is theevidence,andconsequentlyworthexploringasanovelhy-\\npothesis.Ifnobetterhypothesisisfoundsofar,thecurrent\\nsimilarity is inherited from one generation to the next.\\nAccordingly,thecriterion forahypothesis\\nissimply\\ngiven by the equationshown at thebottom of the page.\\nNotethatsincewearedealingwithamultiobjectiveproblem,'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='Notethatsincewearedealingwithamultiobjectiveproblem,\\nthere is no simple way to get independent fitness values as thefitness involves a set of objective functions to be assessed forevery individual. Therefore, the computation is performed by\\ncomparing objectives of one individual with others in terms\\nofPareto dominance [42], [43] in which nondominated solu-\\ntions (Pareto individuals) are searched for in every generation.In other words, the computation of the fitnesses of the individ-\\nuals depends on the current population.\\nWetookasimpleapproachinwhichanapproximationtothe\\nParetooptimalsetisincrementallybuiltastheGAgoeson.Thebasicideaistodeterminewhetherasolutionisbetterthanother\\nin global terms, that is, a child is better if this is a candidate to\\nbecome part of the Pareto set. Specifically, this means that thefollowing condition must should be met: there is no hypothesis\\nin the whole population which dominates this child .\\nWhile this enables the child to get into the Pareto set, it also\\nmakes it possible for other members of the set to be dominatedby the new one. In this case, the Pareto set is updated by re-moving any dominated element to become part of the popula-\\ntion, and the child is added to the Pareto set.\\nSince our model is based on a multicriteria approach, we\\nhave to face three important issues in order to assess every hy-pothesis’fitness:Paretodominance,fitnessassignment,andthe\\ndiversity problem [44], [43]. Despite an important number of'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content=\"diversity problem [44], [43]. Despite an important number of\\nstate-of-the-artmethodstohandletheseissues[43],onlyasmallnumberofthemhasfocusedontheprobleminanintegratedandrepresentation-independentway.Inparticular,Zitzler[35],[45]\\nproposesaninterestingmethod,SPEAwhichusesamixture of\\nestablished methods and new techniques in order to find mul-tiple Pareto-optimal solutions in parallel, and at the same timeto keep the population as diverse as possible.\\nThe number of nondominated hypotheses is a user-defined\\nvalue. If at some generation we get more than that, the SPEAalgorithmperformsaclusteranalysisbyusingtheobjectivevec-torsofeachsolutionandthenreducesthenumberofParetosolu-\\ntions leaving the most representative ones (i.e., the individuals\\nwhich are the center of each cluster). This clustering is donewithout losing the characteristics of the Pareto-optimal set.\\nInordertoassignscalarfitnessvaluestoindividuals,thePareto\\ndominanceisused.Here,thefitnessoftheParetomembersand\\nPlausibility\\nifwas created froma Swanson's crossover\\nifisin the original population oris aresultof another operation\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. ATKINSON-ABUTRIDY et al.: SEMANTICALLY GUIDED AND DOMAIN-INDEPENDENT EVOLUTIONARY MODEL 555\\nthe population is computed differently. For a Pareto member,\"),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='the population is computed differently. For a Pareto member,\\nits fitness (strength) is proportional to the number of individ-ualsdominatedbythismember.Foranon-Paretomember(i.e.,\\nmemberofthepopulation),thefitnessiscomputedasthesumof\\nthe strengthof alltheindividualsthat dominatethisnon-Paretoindividual.Asaconsequence,asbothkindoffitnessarerelated,lowerfitnessvaluesarepreferredbecausethismeansthatfitin-\\ndividual will cover less fit solutions.\\nOntheotherhand,theoriginalSPEAalgorithmusesanelitist\\nGA, so we have adapted it in order to allow incremental up-dating of the Pareto-optimal set which along with our steady-\\nstate replacement method allows us to improve Pareto-optimal\\nindividuals.\\nIV. E\\nXPERIMENTS AND RESULTS\\nA prototypeto evaluatethe underlying modelhas been built.\\nThe IE task has been implemented as a set of modules whose\\nmain outcome is the set of rules extracted from the documents.\\nIn addition, an intermediate training module is responsible forgenerating information from the LSA analysis and from therulesjustproduced.Allthisinformationexpressedinafact-like\\nform, feeds a Prologsystem in which the GA-based discovery\\nhasbeenimplemented.Theinitialrulesarerepresentedbyfactscontaininglistsofrelationsbothforantecedentandconsequent.\\nForthepurposeoftheexperiments,thecorpusofdocuments\\nhasbeenobtainedfromthe AGRISdatabaseforagriculturaland'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='hasbeenobtainedfromthe AGRISdatabaseforagriculturaland\\nfood science (http://www.fao.org) in the Spanish language. Weselected this kind of corpus as it has been properly cleaned-upand builds upon a scientific area which we do not have any\\nknowledge about so to avoid any possible bias and to make\\nthe results more realistic. A set of 1000 documents was ex-tracted from which one third were used for setting parametersand making generaladjustments,and the restwereused for theGA itself in the evaluation stage.\\nNext, we tried to provide answers to three basic questions\\nconcerning our original aims.\\n1) How well does the GA for KDT behave?\\n2) How good are the hypotheses produced according to\\nhuman experts in terms of text mining’s ultimate goals:\\ninterestingness, novelty and usefulness, and how signifi-\\ncant are the results?\\n3) Howmuchdoestheknowledgeprovideextrainformation\\nto better understand the relationship between the input\\nconcepts?\\nInordertoaddresstheseissues,weusedamethodologycon-\\nsisting of two phases: the system evaluation and the experts’assessment.\\n1)System Evaluation: This aims at investigating the be-\\nhavior and theresults produced by theGA (question 1).\\nWe set the GA by generating an initial population of\\n100 semirandom hypotheses. In addition, we defined themain global parameters as follows.\\n•Mutation probability: 0.2. This is relatively higher\\nthan usual because of the soft constraints.\\n•Cross-over probability: 0.8.\\n•MaximumsizeofParetoset: 5%.Thisisthenumber'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='•Cross-over probability: 0.8.\\n•MaximumsizeofParetoset: 5%.Thisisthenumber\\nof solutions in the Pareto-optimal set.\\n•Population size: 100 initial hypotheses.TABLE I\\nANALYSIS OF THE BEHAVIOR OF THE GA UNDERDIFFERENT PARAMETERS\\n•Size of training rule set: 700 rules.\\n•Targetterms: Thesequestiontermsarerequiredfor\\nthe relevance measurements (Section III-B). How-ever,these items need to be changed in everyrun.\\nWe ran five versions of the GA with the same configura-tion of parameters but different pairs of terms to addressthequestforexplanatorynovelhypotheses. Theeffectofsomeoftheparametersontheperformanceofthemethod\\nwasevaluated.Forexample,TableIshowspartofthesen-\\nsitivityanalysiscarriedoutfortheGAwithdifferentmu-tation (\\n) and crossover ( ) parameters values (with\\n, the finally used values).\\nTest parameter values were established for 20 runs of\\ntheGA,eachupto1000generations,withainitialpopu-lation of 100 hypotheses. Here, different probabilities ofmutation(\\n)andcrossover( )aretestedforeveryrun\\n(), and the resulting average fitness (AvgF) of all the\\nsolutions of the last-generation population, its standarddeviation (StdDev), and the minimum (MinF) and max-imum values of fitness (MaxF) are drawn.\\nThe parameters were systematically tested with steps\\nof approximately 5% (starting from 0.025) for\\n, and\\n10% (starting from 0.50) for . Thus, the final range\\nforis from 0.025 to 0.2, whereas for this is from'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='foris from 0.025 to 0.2, whereas for this is from\\n0.50 to 0.80. Thus, the table shows the different settingsinvolving moving through the range for\\nand fixing\\na value for . For example, the first five runs consider\\nsetting fixed and testing with different values\\nof.\\nNote that because of constraints in the genetic opera-\\ntors,smallvariationsinthevalueofthe parameterdo\\nnot have significanteffect on the best obtained fitness.\\nInaddition,asthevalueofthe parameterincreases,\\nthere is a tendency for the minimum fitness to decrease.However,notethatbecauseofthemultiobjectivenatureofthe model, having fitness values of zero between runs 11\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 556 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 7, NO. 6, DECEMBER 2003\\nand20doesnotnecessarilyimplythatthereisnochanges\\nin the best solutions.\\nBecause of the nature of our model, which involves\\nhumanexpertswhoassessthefinaloutcome,thebehaviorof the GA across different runs and parameters settings\\nis not determining. Hence, the expert participation in the\\nprocess is crucial to establish the degree of prediction ofthe model according to human performance.\\nThe different results obtained from running the GA as\\nused for our experiment are shown in the form of a rep-resentative behavior in Fig. 4, where the number of gen-erations is placed against the average objective value foreach of the eight criteria.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='The results are drawn from GA runs for 1000 genera-\\ntions forfive differentpairsof targetconcepts, andeverygraphicrepresentstheaveragemaximumvaluesforeverycriterionforeachofthefiveruns.Keepinmindthatmax-imum objective function values are looked for.\\nSomeinterestingfactscanbenoted.Almostallthecri-\\nteria seem to stabilize after (roughly) generation 700 for\\nall the runs, that is, no further improvement beyond this\\npointisachievedandsothismaygiveusanapproximateindication of the limits of the objective function values.Considering that each run of the model has been per-formedwithdifferenttargetconcepts,theselimitsappeartobeaplausiblefactortobetakenintoaccountforfurtherexperimentsintermsofupperandlowerobjectivevalues.However, having this level of stabilization does not en-surethatthemethodachievesthebestobjectivevaluesasthisdependsonthedominancedecisionsmadeintheop-timization stage.\\nAnother aspect worth highlighting is that despite a\\nsteady-state strategy being used by the model to producesolutions, the individual evaluation criteria behave inunstable ways to accommodate solutions which had to\\nbe removed or added. As a consequence, it is not neces-\\nsarily the case that all the criteria have to monotonicallyincrease.\\nIn order to see this behavior, look at the results for the\\ndifferent criteria for the same period of time, betweengenerations 200 and 300 for run 4. For an average hy-pothesis, the quality of coherence ,cohesion,simplicity ,\\nandstructure getsworse,'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='andstructure getsworse,\\n1whereasthisimprovesfor cov-\\nerage,interestingness ,andRelevance ,andhassomevari-\\nations for Plausibility .\\n2)ExpertAssessment: thisaimsatassessingthequality(and\\ntherefore, effectiveness) of the discovered knowledge ondifferent criteria by human domain experts (questions 2and 3).\\nFor this, we designed an experiment in which 20 ex-\\nperts were involved.\\n• Eachexpertassessed fivehypothesesselectedfrom\\ntheParetosetinthelast-generationpopulationfromdifferent runs.\\n• Weaskedtheexpertstoassessthehypothesesfrom\\none (worst) to five (best) in terms of the following\\n1“Worse” means the quality from the single criterion point of view. When\\ntrading off with other criteria, low objective values may not be undesirable as\\nlong as they trade off correctly with other objectives.criteria:interestingness(INT),novelty(NOV),use-\\nfulness (USE), and sensibleness (SEN).\\nSince we are also trying to show that the hy-\\npotheses produced by the model may providefurther explanations to help one better understand\\nthe implicit and complex relationship between\\nthe target terms, the experts were also asked toevaluate a fifth criterion: “additional information”(ADD) (does the hypothesis contribute additionalinformationtohelponeunderstandtherelationshipbetween the target concepts?).\\n• Thesamplesetconsistedof25hypothesesproduced\\nfrom the different runs.\\n• Eachhypothesiswasrandomlyassessedbyfourex-\\nperts in order to get an overall opinion.\\n• Each hypothesis was semiautomatically converted'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='perts in order to get an overall opinion.\\n• Each hypothesis was semiautomatically converted\\ninto an abstract-like form in order for the expert tounderstand it.\\nIn order to select worthwhile terms for the experiment, we\\naskedonedomainexperttofilterpairsoftargettermspreviouslyrelated according to traditional clustering analysis. The pairswhich finally deserved attention are shown in Table II.\\nOncethesystemhypotheseswereproduced,theexpertswere\\nasked to score them according to the five subjective criteria.Next,wecalculatedthescores foreverycriterion asseen intheoverall results in Fig. 5.\\nThe assessment of individual criteria shows that some hy-\\npothesesdid wellwith scoresabovetheaverage(3).This isthecaseforhypotheses11,16,and19intermsofINT(hypotheses\\n7,17,23arejustattheaverage),hypotheses14and19interms\\nof SEN (hypotheses 3, 11, and 17 are just at the average), hy-potheses1,5,11,17,and19intermsofUSE(hypotheses3,10,and 15 are just at the average), and hypotheses 24 in terms ofNOV (hypotheses 11, 19, and 23 are just at the average). Notealso that the assessment seems to be consistent for individualhypotheses across the criteria: hypothesis 19 is well above theaverageforalmostallthecriteria(exceptforNOV),hypothesis18 always received a score below 2 (25%) except for ADD inwhichthisisslightlyhigher.Similarsituationscanbeobservedfor hypotheses 2, 21, etc.\\nNote also that the assessment for ADD (\\n) may depend on how much information contained in one'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='Note also that the assessment for ADD (\\n) may depend on how much information contained in one\\nhypothesisisrelevanttothetargetterms,butasdominancecon-ditionsmustbemet,the relevance valuesdonotalwaysachieve\\nhigh values. Regardless of the runs, relevance objective values\\ndid not exceed 30% (Fig. 4).\\nIn general, although the general results are not extremely\\ngood,theylookpromisinginthatthisisaverydemanding,real\\nhuman evaluation of discovered discovery.\\nTheseresultsandtheevaluationproducedbythemodelwere\\nusedtomeasurethecorrelationbetweenthescoresofthehuman\\nsubjects and the system’s model evaluation. Since both the ex-\\npert and the system’s model evaluated the results consideringseveralcriteria,wefirstperformedanormalizationaimedatpro-ducinga single“quality”valuefor eachhypothesis as follows.\\n•For the expert assessment: the scores of the different cri-\\nteriaforeveryhypothesisareaveraged.Thiswillproducevalues between 1 and 5, with 5 being the best.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. ATKINSON-ABUTRIDY et al.: SEMANTICALLY GUIDED AND DOMAIN-INDEPENDENT EVOLUTIONARY MODEL 557\\nFig. 4. GA evaluation for individual objective values (criteria).\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 558 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 7, NO. 6, DECEMBER 2003\\nTABLE II'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='TABLE II\\nPAIRS OFSELECTED USER-DEFINEDTARGETTERMS\\nFig. 5. Experts’ assessment of hypotheses.\\n•For the model evaluation: for every hypothesis, both the\\nobjective values and its fitness are considered as follows:asthevaluesshouldshowthefactthatthehigher,thebest,we subtract the fitness from 1 (the lower the fitness, the\\nbetter) for that hypothesis, and then we add this to the\\naveragevalueof theobjectivesvalues forthishypothesis.Notethatthiswillproducevaluesbetween0and2,with2\\nbeing the best.\\nWe then calculated the pair of values for every hypothesis\\nand obtained a (Spearman) correlation\\n(\\n, , ). From this result, we see that\\nthecorrelationshowsagoodlevelof predictioncomparedwith\\nhumans.Thisindicatesthatforsuchacomplextask(knowledgediscovery), the model’s behavior is not too different from theexperts’.\\nNote that in Mooney’s experiment using simple discovered\\nrules [38],alower human-system correlation of\\nwas\\nobtained. Considering also that the human subjects were not\\ndomain experts as in our case, our results are encouraging as\\ntheseinvolveamoredemandingprocesswhichrequiresfurthercomprehension of both the hypothesis itself and the workingdomain. In addition, our model was able to do it better without\\nanyexternal linguistic resource as in Mooney’s experiments.Since the quality of the hypotheses is measured by using all\\nthe criteria, it does not evaluate particular objectives (in isola-tion)usedabovebecauseoftheeffectofthetradeoffsintheopti-'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='mizationphase.However,thecorrelationoftheseobjectivesand\\nparticular expert’scriteria show some facts worth highlighting.\\n• There is a very good correlation\\n(\\n, , ) forinterestingness between\\nthe system and the experts. This suggests that the systemhas a good notion of what the interesting hypotheses are,compared with human judgment. Note that the model is\\nabletoachievethiswithoutusinganyexternalontological\\nresource.\\n• There is no correlation between the system’s simplicity\\nandanyoftheexpert’scriteria:thissuggeststhatthesim-\\nplicity of a hypothesis may not be a determining factorfor the real novelty or interestingness as assessed by theexperts.\\n• Thereisacorrelation\\n( ,,\\n) between the system’s coherence and the ex-\\nperts’ usefulness: as the coherence measures a hypoth-esis’ semantic features, the correlation suggests that for\\nthe expert, the hypothesis’ comprehensibility and read-\\nabilitymaybeakeyissueindeterminingitsdegreeofuse-fulness.\\n• Thesystem’s plausibility issomewhatcorrelatedwiththe\\nexpert’scriterionADD(\\n, ,,\\n) and sensibleness ( , ,\\n, ): the correlation with ADD suggests\\nthattheexpertsmaynotbeconsideringtheexplicitexpla-\\nnation of a hypothesis for the relation between the targetterms, but the degree of the hypothesis’ plausibility.\\nInordertoshowwhatthefinalhypotheseslooklikeandhow\\nthegoodcharacteristicsandlessdesirablefeaturesasaboveare\\nexhibited, we picked the two best hypotheses as assessed bythe experts (out of 25 best hypotheses) considering the average'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='valueofthefivescoresassignedbytheuser.Aswedonothave\\ndomain knowledge to analyze the content of these selected hy-potheses,somegeneraldescriptionsofthepredicates’argumentareprovidedtogiveaflavoroftheknowledgeinvolved(seeused\\ntarget concepts for each run in Table II).\\n• Hypothesis65ofrun4:thisisrepresentedbythefollowing\\nrule(numericalvaluesrepresentinternalidentifiersforthe\\narguments and their semantic vectors):\\nIF goal (perform (19311))\\nand goal (analyze (20811))THEN establish (111)\\nand has a criteria vector [0.92,0.09,0.50,0.005,0.7,0.00,\\n0.30,0.25] (the vector’s elements represent the values for\\nthe criteria relevance, structure, coherence, cohesion, in-terestingness, plausibility, coverage, and simplicity) andobtained an average expert’s assessment of 3.74. In NL\\ntext, this can roughly be interpreted as follows:\\nIF work aims at performing the genetic\\ngrouping of populations..\\nAND to analyze the vertical integra-tion\\nfor elaborating Pinus timber…\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. ATKINSON-ABUTRIDY et al.: SEMANTICALLY GUIDED AND DOMAIN-INDEPENDENT EVOLUTIONARY MODEL 559\\nTHEN the best agricultural use for\\nland\\nlots of organic agriculture must be\\nestablished….\\nThe hypothesis appears to be more relevant and co-\\nherent than the others ( ). However, this\\nis not complete in terms of cause-effect. For instance, the\\nmethods are missing.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='is not complete in terms of cause-effect. For instance, the\\nmethods are missing.\\n• Hypothesis88ofrun3:thisisrepresentedbythefollowing\\nrule:\\nIF goal (show (11511))\\nand method (use(25511))THEN effect (1931,1932)\\nand has a criteria vector [0.29,0.18,0.41,0.030,0.28,0.99,\\n0.30,0.50]andobtainedanaverageexpert’sassessmentof3.20.InNLtext,thiscanroughlybeinterpretedasfollows:\\nIF the goal is to show that the forest\\nrestoration ..\\nAND the method is based on the use of\\nmicro-environments for capturingfarm mice..\\nTHEN digestibity \"in vitro\" should\\nhave\\nan effect on the bigalta cuttings..\\nThis hypothesis looks more complete (goal, methods,\\netc.) but is less relevant than the previous hypothesis de-spite its close coherence. Note also that the plausibility is\\nmuch higher than for hypothesis 65, but the other criteria\\nseemed to be a key factor for the experts.\\nIn addition, there is also qualitative evidence that there were\\nother subjective factors which influenced some hypotheses’low scores, which was extracted from the experts’ overallcomments. In particular the following.\\n• The expertswereselectedfrom differentcountries.There\\nare certain domain concepts or processes which arecommon in some regions but not in others due to diverse\\nreasons including the specificity of local knowledge, dif-\\nferent experience, etc. This tended to produce misleadingassessments. For instance, some experts assessed certain\\nhypotheses as uninteresting because they barely knew\\nanything about them.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='hypotheses as uninteresting because they barely knew\\nanything about them.\\n• Providing the hypothesis in a abstract-like form is not\\nalways clear for some experts and they had problems\\nunderstanding the underlying idea. This suggests that\\nrepresentingthehypothesesinareadablewayshouldbeamatteroffurtherresearchasithasstronginfluenceontheresults.\\n• Itwasnotedthatforsometexts,therewas(apparently)no\\nconsistent connection between topics of the paragraphs.However, it is claimed that these hypotheses still con-\\ntain information worth exploring. A likely explanation is\\nthatingeneral,relatednessbetweenconditions(i.e.,para-graphs) is regarded as a semantic similarity task, specifi-callyevaluatedbycriteriasuchas coherence .However,aswearemeasuringLSA-basedcoherence,therelationisex-\\npressed as acontextualclosenessmeasure. Therefore,the“predictions” made by LSA for the hypotheses are incor-\\nrect despite the fact that for the domain experts, the para-\\ngraphs containing unseen connections.\\n• Sometexts(hypotheses)arenotproperlyassessedasthey\\nwerenotconsideredgoodhypothesesbecausetheymissed\\nsome key elements. For example, a hypothesis may be\\ncomposed of only the goal and the results, but additionalelements are needed to figure out a cause-effect relation\\n(i.e., methods).\\nWhiletheIEtaskmayaffecttheinformationcontained\\nin the hypotheses, the fact that there may be missing orincomplete facts in them may be due to two factors: the\\ntraining data extracted from the corpus and the GA it-'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='training data extracted from the corpus and the GA it-\\nself. If some associations between roles or predicates arenot significantly represented in the corpus, these will notbe captured in the training outcome. Hence, the final hy-\\npotheses will fail to express the kind of underlying asso-\\nciations of interest for the experts.\\nV. C\\nONCLUSION AND FURTHERWORK\\nUnlike traditional approachesto KDT,in this paper, wecon-\\ntributeaninnovativewayofcombiningadditionallinguisticin-\\nformationandevolutionarylearningtechniquesinordertopro-ducenovelhypotheses,whichinvolveexplanatoryandeffective\\nnovel knowledge.\\nWe also introduced a unique approach for evaluation which\\ndeals with semantic and DM issues in a high-level way. In thiscontext, the proposed representation for hypotheses suggests\\nthatperformingshallowanalysisofthedocumentsandthencap-\\nturing key rhetorical information may be a good level of pro-cessing which constitutes a tradeoff between completely deepandkeyword-basedanalysisof textdocuments.Inaddition,the\\nresults suggest that the performance of the model in terms of\\nthe correlation with human judgments are slighty better thanapproachesusingexternalresourcesasin[39].Inparticularcri-\\nteria, the model shows a good correlation between the system\\nevaluation and the expert assessment of the hypotheses.\\nThe model deals with the hypothesis production and evalua-\\ntioninaverypromisingwaywhichisshownintheoverallresults\\nobtained from the experts evaluation and the individual scores'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='obtained from the experts evaluation and the individual scores\\nforeachhypothesis.However,itisimportanttonotethatunliketheexpertswhohavealotof experience,preconceivedconceptmodels, and complex knowledge in their areas, the system has\\ndonerelativelywellonlyexploringthecorpusoftechnicaldoc-\\numents and the implicit connections contained in it.\\nAlthoughfurtherresearchindifferenttechnicaldomainsmay\\nbe needed, the domain-independent criteria used in the proto-\\ntypesuggestthattherearenodomain-specificissueswhichpre-ventthehypothesesfromhavinggoodquality.Nevertheless,thisdoes not seem to represent a big obstacle because preliminary\\nexperiments in including deeper coherence evaluations may be\\nproducing more fair connections specially between the condi-tions of the hypotheses.\\nFromanevolutionaryKDTviewpoint,thecorrelationsandthe\\nqualityofthefinalhypothesesshowthattheGAoperationsandthesystem’sevaluationoftheindividualsmaybeeffectivepredic-tionsof really usefulnovelknowledgefromauserperspective.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 560 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 7, NO. 6, DECEMBER 2003\\nREFERENCES\\n[1] R. Feldman, “Knowledge management: A text mining approach,” pre-\\nsentedatthe 2ndInt.Conf.PracticalAspectsofKnowledgeManagement\\n(PAKM98), Basel, Switzerland, Oct. 1998.\\n[2] M.Hearst,“Untanglingtextdatamining,”presentedatthe 37thAnnual'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='[2] M.Hearst,“Untanglingtextdatamining,”presentedatthe 37thAnnual\\nMeetingoftheACL .(InvitedPaper)Univ.Maryland,CollegePark,MD,\\nJune 1999.\\n[3] C. Aggarwal and P. Yu, “Data mining techniquesfor associations,clus-\\ntering and classification,” in Proc. 3rd Pacific-Asia Conf., PAKDD-99 ,\\nBeijing, China, Apr. 1999, pp. 13–23.\\n[4] U. Fayyad, G. Piatesky-Shapiro, and P. Smith, “From data mining to\\nknowledge discovery: An overview,” in Advances in Knowledge Dis-\\ncoveryandDataMining . Cambridge,MA:MITPress,1996,pp.1–36.\\n[5] J. Han and M. Kamber, Data Mining: Concepts and Techniques . San\\nMateo, CA: Morgan-Kanfmann, 2001.\\n[6] Y.YaoandC.Bultz,“Oninformation-theoreticmeasuresofattribueim-\\nportance,” in Proc. 3rd Pacific-Asia Conf. PAKDD-99 , Beijing, China,\\nApr. 1999, pp. 133–137.\\n[7] M.Hearst,“Textminingtools:Instrumentsforscientificdiscovery,”pre-\\nsentedatthe IMATextMiningWorkshop ,Minneapolis,MN,Apr.2000.\\n[8] M.RajmanandR.Besancon,“Textmining:Knowledgeextractionfrom\\nunstructured textual data,” presented at the 6th Conf. Int. Federation of\\nClassification Societies (IFCS-98), Rome, Italy, July 1998.\\n[9] M. Hearst, “Text data mining: Issues. Techniques and the relation to\\ninformationaccess,” Univ.California atBerkeley, Berkeley, CA, 1998.\\n[10] R. Feldman, “Text mining at the term level,” in Lecture Notes in Arti-\\nficialIntelligence . Berlin,Germany:Springer-Verlag,Sept.1998,vol.\\n1510, pp. 65–73.\\n[11] D. Merkl, “Text data mining,” in Handbook of Natural Language Pro-'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='1510, pp. 65–73.\\n[11] D. Merkl, “Text data mining,” in Handbook of Natural Language Pro-\\ncessing,R.Dale,H.Moisl,andH.Somers,Eds. Switzerland:Dekker,\\n2000, pp. 800–820.\\n[12] C. Muller, “Acquisition et structuration des connaissances en corpus:\\nElements methodologiques,” INRIA-Lorraine, Nancy, France, Rapport\\nde Recherche 3198, 1997.\\n[13] D.Swanson,“Migraineandmagnesium:Elevenneglectedconnections,”\\nPerspectives Biol. Med. , no. 31, pp. 526–557, 1988.\\n[14] M. Hearst, “Distinguishing between web data mining and information\\naccess,” presented at the Panel on Web Data Mining KDD97, Newport\\nBeach, CA, Aug. 1997.\\n[15] C. Fellbaum, WordNet: An Electronic Lexical Database . Cambridge,\\nMA: MIT Press, 1998.\\n[16] S.HarabagiuandD.Moldovan,“Knowledgeprocessingonanextended\\nwordnet,” in WordNet: An Electronic Lexical Database . Cambridge,\\nMA: MIT Press, 1998, pp. 379–403.\\n[17] U. Nahm and R. Mooney, “A mutually beneficial integration of data\\nminingandinformationextraction,”presentedatthe 17thNationalConf.\\nAI, Austin, TX, July 2000.\\n[18] , “Using information extraction to aid the discovery of prediction\\nrules from text,” presented at the 6th Int. Conf. Knowledge Discovery\\nand Data Mining (KDD-2000) Workshop on Text Mining , Boston, MA,\\nAug. 2000.\\n[19] N. Radcliff and P. Surry, “Co-Operation Through Hierarchical Com-\\npetition in Genetic Data Mining,” Univ. Edinburgh, Edinburg, U.K.,\\nEPCC-TR94-09, 1994.\\n[20] A.Freitas,“Ageneticalgorithmforgeneralizedruleinduction,”in Proc.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='EPCC-TR94-09, 1994.\\n[20] A.Freitas,“Ageneticalgorithmforgeneralizedruleinduction,”in Proc.\\n3rd On-Line World Conf. Soft Computing , 1999, pp. 340–353.\\n[21] ,“Asurveyofevolutionaryalgorithmsfordataminingandknowl-\\nedge discovery,” in Advances in Evolutionary Computation .N e w\\nYork: Springer-Verlag, 2002.\\n[22] E. Noda, A. Freitas, and H. Lopes, “Discovering interesting prediction\\nrules with a genetic algorithm,” in Proc. Congress Evolutionary Com-\\nputation (CEC-99) , Washington, DC, July 1999, pp. 1322–1329.\\n[23] G. Williams, “Evolutionary hot spots data mining,” in Proc. 3rd Pa-\\ncific-Asia Conf. PAKDD-99 ,Beijing, China, Apr. 1999,pp. 184–193.\\n[24] B. Masand, “Optimizing confidence of text classification via evolution\\nof symbolic expressions,” in Advances in Genetic Programming .\\nCambridge, MA: MIT Press, 1994.\\n[25] A. Bergstron, P. Jaksetic, and P. Nordin, “Acquiring textual relations\\nautomaticallyonthewebusinggeneticprogramming,”in Proc.EuroGP\\n2000, Edinburgh, U.K., Apr. 2000, pp. 237–246.\\n[26] C. Manning and H. Schutze, Foundations of Statistical Natural Lan-\\nguage Processing . Cambridge, MA: MIT Press, 1999.\\n[27] C. Jacquemin and E. Tzoukermann, “NLP for term variant extraction:\\nSynergy between morphology, Lexicon, and syntax,” in Natural Lan-\\nguage Information Retrieval . Norwell, MA: Kluwer, 1999.\\n[28] S. Basu, R. Mooney, K. Pasupuleti, and J. Ghosh, “Evaluating the nov-\\nelty of text-mined rules using lexical knowledge,” presented at the 7th'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='elty of text-mined rules using lexical knowledge,” presented at the 7th\\nInt. Conf. Knowledge Discovery and Mining (KDD-2001), San Fran-\\ncisco, CA, Aug. 2001.\\n[29] M. Berry and M. Browne, Understanding Search Engines: Mathemat-\\nical Modeling and Text Retrieval , 2001, SIAM Book Series: Software,\\nEnvironments, and Tools.[30] E. Kintsch and G. S. D. Steinhart, “Developing summarization skills\\nthrough the use of LSA-based feedback,” Interactive Learn. Environ.:\\nAn Int. J., vol. 8, no. 2, pp. 87–109, 2000.\\n[31] T.Landauer,P.Foltz,andD.Laham,“Anintroductiontolatentsemantic\\nanalysis,” Discourse Processes , vol. 10, no. 25,pp. 259–284, 1998.\\n[32] P. Wiemer-Hastings, “Adding syntactic information to LSA,” in Proc.\\n22nd Annu. Conf. CongnitiveScience Society , 2000,pp. 989–993.\\n[33] W.Kintsch,“Predication,” Congn.Sci. ,vol.25,no.2,pp.173–202,2001.\\n[34] P. Wiemer-Hastings and I. Zipitria, “Rules for syntax, vectors for se-\\nmantics,” presented at the 23rd Annu. Conf. Cognitive Science Society ,\\nEdinburgh, U.K., 2001.\\n[35] E. Zitzler and L. Thiele, “An evolutionary algorithm for multiobjective\\noptimization: The strength pareto approach,” Swiss Fed. Inst. Technol.\\n(ETH), Zurich, Switzerland, 43, 1998.\\n[36] R. Finn, “Program uncovers hidden connections in the literature,” The\\nScientist, vol. 10, May 1998.\\n[37] D.Swanson,“Onthefragmentationofknowledge,theconnectionexplo-\\nsion,andassemblingotherpeople’sideas,”in Annu.MeetingAmerican\\nSociety Information Science Technology , vol. 27, Feb. 2001.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='Society Information Science Technology , vol. 27, Feb. 2001.\\n[38] S.Basu,R.Mooney,K.Pasupuleti,andJ.Ghosh,“Usinglexicalknowl-\\nedge to evaluate the novelty of rules mined from text,” presented at the\\nNAACL 2001 Workshop on WordNet and Other Lexical Resources: Ap-\\nplications, Extensions and Customizations , Pittsburg,PA,June 2001.\\n[39] U. Nahm and R. Mooney, “Text mining with information extraction,”\\npresented at the AAAI 2002 Spring Symp. Mining Answers from Texts\\nand Knowledge Bases , Stanford, CA, 2002.\\n[40] T. V. Dijk and W. Kintsch, Strategies of Discourse Comprehension .\\nNew York: Academic, 1983.\\n[41] P.Foltz,W.Kintsch,and T.Landauer,“Themeasurementoftextualco-\\nherencewithlatentsemanticanalysis,” DiscourseProcesses ,vol.25,no.\\n2, pp. 259–284, 1998.\\n[42] K. Deb, “Multi-objective genetic algorithms: Problem difficulties and\\nconstructionoftestproblems,” Evol.Comput. ,vol.7,no.3,pp.205–230,\\n1999.\\n[43] C.Coello,“AnupdatedsurveyofGA-basedmultiobjectiveoptimization\\ntechniques,” ACM Comput.Surveys ,vol. 32,no. 2,pp. 109–143,2000.\\n[44] K. Deb, Multi-Objective Optimization Using Evolutionary Algo-\\nrithms. New York: Wiley, 2001.\\n[45] E.ZitzlerandL.Thiele,“Multiobjectiveoptimizationusingevolutionary\\nalgorithms: A comparative case study,” Parallel Prob. Solving Nature ,\\npp. 292–301, Sept. 1998.\\nJohn Atkinson-Abutridy received the B.Eng. and\\nM.Eng.degreesininformaticsfromUniversidadTec-\\nnica Federico Santa Maria, Chile, in 1990 and 1994,'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='M.Eng.degreesininformaticsfromUniversidadTec-\\nnica Federico Santa Maria, Chile, in 1990 and 1994,\\nrespectively,andthePh.D.degreeinartificialintelli-\\ngence from the University of Edinburgh, Edinburgh,\\nU.K., in 2003\\nHe is an Assistant Professor in the Departa-\\nmento de Informatica, Universidad de Concepcion,\\nConcepcion, Chile. His research interests include\\nnatural-language processing, knowledge discovery\\nfrom texts, artificial intelligence, and evolutionary\\ncomputation.\\nChris Mellish received the Ph.D. degree in artificial\\nintelligencefrom theUniversityof Edinburgh,Edin-\\nburgh, U.K., in 1980.\\nFrom 2001 to 2003, he held the Chair in Natural\\nLanguageProcessinginEdinburgh.HeholdsaChair\\nin the Department of Computing Science at the Uni-\\nversity of Aberdeen, Aberdeen, U.K.He hasworked\\nmainly in the fields of natural language processing\\nandlogicprogramming.Heisespeciallyinterestedin\\nnaturallanguagegeneration.Hehascoauthoredtext-bookson Prologand NaturalLanguageProcessing .\\nStuart Aitken received the Ph.D. degree from the\\nFacultyofEngineering,UniversityofGlasgow,Scot-\\nland, U.K., in 1990.\\nSince then he has worked in artificial intelligence\\nand human-computer interaction.His research spans\\ntheareasofontology,bioinformatics,intelligenttoolsforknowledgeacquisition, and machine learning.\\nDr. Aitken is currently a Member of the Artificial\\nIntelligenceApplicationsInstitute,TheUniversityof\\nEdinburgh, Edinburgh, U.K.'),\n",
       " Document(metadata={'title': 'A Semantically Guided and Domain-Independent Evolutionary Model for Knowledge Discovery From Texts', 'year': 2003}, page_content='IntelligenceApplicationsInstitute,TheUniversityof\\nEdinburgh, Edinburgh, U.K.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='1\\nA Survey of Knowledge-Enhanced Text Generation\\nAccepted to ACM Computing Survey (CUSR) WENHAO YU, University of Notre Dame, USA\\nCHENGUANG ZHU, Microsoft Research, USA\\nZAITANG LI, The Chinese University of Hong Kong, China\\nZHITING HU, University of California at San Diego, USA\\nQINGYUN WANG, University of Illinois at Urbana-Champaign, USA\\nHENG JI, University of Illinois at Urbana-Champaign, USA\\nMENG JIANG, University of Notre Dame, USA\\nThe goal of text-to-text generation is to make machines express like a human in many applications such as\\nconversation, summarization, and translation. It is one of the most important yet challenging tasks in natural\\nlanguage processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal\\nby learning to map input text to output text. However, the input text alone often provides limited knowledge\\nto generate the desired output, so the performance of text generation is still far from satisfaction in many\\nreal-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge\\nembedded in the input text and (ii) external knowledge from outside sources such as knowledge base and\\nknowledge graph into the text generation system. This research topic is known as knowledge-enhanced text\\ngeneration . In this survey, we present a comprehensive review of the research on this topic over the past five'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='years. The main content includes two parts: (i) general methods and architectures for integrating knowledge\\ninto text generation; (ii) specific techniques and applications according to different forms of knowledge data.\\nThis survey can have broad audiences, researchers and practitioners, in academia and industry.\\nCCS Concepts: •General and reference →Surveys and overviews ;•Computing methodologies →\\nNatural language processing ;Neural networks .\\nAdditional Key Words and Phrases: Natural language generation, Knowledge-enhanced Methods\\n§§Some useful materials related to this survey:\\n•A tutorial entitled “Knowledge-enriched Natural Language Generation”, at EMNLP 2021.\\nTutorial abstract, slides, videos can be found at https://kenlg-tutorial.github.io.\\n•A tutorial entitled “Knowledge-augmented Methods for NLP”, to appear at ACL 2022.\\n•A Github repository with a more complete collection of papers and codes can be found at\\nhttps://github.com/wyu97/KENLG-Reading. It will be frequently updated with new papers.\\nAuthors’ addresses: Wenhao Yu, wyu1@nd.edu, University of Notre Dame, Notre Dame, Indiana, USA, 46556; Chenguang Zhu,\\nchezhu@microsoft.com, Microsoft Research, Redmond, Washington, USA, 98052; Zaitang Li, 1155107739@link.cuhk.edu.hk,\\nThe Chinese University of Hong Kong, Hong Kong, China, 999077; Zhiting Hu, zhitinghu@gmail.com, University of\\nCalifornia at San Diego, San Diego, California, USA, 92092; Qingyun Wang, qingyun4@illinois.edu, University of Illinois at'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Urbana-Champaign, Urbana, Illinois, USA, 61801; Heng Ji, hengji@illinois.edu, University of Illinois at Urbana-Champaign,\\nUrbana, Illinois, USA, 61801; Meng Jiang, mjiang2@nd.edu, University of Notre Dame, Notre Dame, Indiana, USA, 46556.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n©2022 Association for Computing Machinery.\\n0360-0300/2022/1-ART1 $15.00\\nhttps://doi.org/10.1145/3512467\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.arXiv:2010.04389v4  [cs.CL]  22 Jan 20221:2 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\n1 INTRODUCTION\\nText generation, which is often formally referred as natural language generation (NLG), is one of\\nthe most important yet challenging tasks in natural language processing (NLP) [ 37]. NLG aims at\\nproducing understandable text in human language from linguistic or non-linguistic data in a variety'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='producing understandable text in human language from linguistic or non-linguistic data in a variety\\nof forms such as textual data, numerical data, image data, structured knowledge bases, and knowl-\\nedge graphs. Among these, text-to-text generation is one of the most important applications and\\nthus often shortly referred as “text generation”. Researchers have developed numerous technologies\\nfor this task in a wide range of applications [ 38,55,125]. Text generation takes text (e.g., a sequence,\\nkeywords) as input, processes the input text into semantic representations, and generates desired\\noutput text. For example, machine translation generates text in a different language based on the\\nsource text; summarization generates an abridged version of the source text to include salient\\ninformation; question answering (QA) generates textual answers to given questions; dialogue\\nsystem supports chatbots to communicate with humans with generated responses.\\nWith the recent resurgence of deep learning technologies [ 66], deep neural NLG models have\\nachieved remarkable performance in enabling machines to understand and generate natural lan-\\nguage. A basic definition of the text generation task is to generate an expected output sequence from\\na given input sequence , called sequence-to-sequence (Seq2Seq). The Seq2Seq task and model were\\nfirst introduced in 2014 [ 117]. It maps an input text to an output text under encoder-decoder schemes.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='The encoder maps the input sequence to a fixed-sized vector, and the decoder maps the vector to the\\ntarget sequence. Since then, developing NLG systems has rapidly become a hot topic. Various text\\ngeneration models have been proposed under deep neural encoder-decoder architectures. Popular\\narchitectures include recurrent neural network (RNN) encoder-decoder [ 117], convolutional neural\\nnetwork (CNN) encoder-decoder [39], and Transformer encoder-decoder [122].\\nNevertheless, the input text alone contains limited knowledge to support neural generation\\nmodels to produce the desired output. Meanwhile, the aforementioned methods generally suffer\\nfrom an inability to well comprehend language, employ memory to retain and recall knowledge,\\nand reason over complex concepts and relational paths; as indicated by their name, they involve\\nencoding an input sequence, providing limited reasoning by transforming their hidden state given\\nthe input, and then decoding to an output. Therefore, the performance of generation is still far\\nfrom satisfaction in many real-world scenarios. For example, in dialogue systems, conditioning on\\nonly the input text, a text generation system often produces trivial or non-committal responses of\\nfrequent words or phrases in the corpus [ 139,159], such as “Me too. ” or“Oh my god!” given the input\\ntext “My skin is so dry. ” These mundane responses lack meaningful content, in contrast to human'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='text “My skin is so dry. ” These mundane responses lack meaningful content, in contrast to human\\nresponses rich in knowledge. In comparison, humans are constantly acquiring, understanding, and\\nstoring knowledge from broader sources so that they can be employed to understand the current\\nsituation in communicating, reading, and writing. For example, in conversations, people often first\\nselect concepts from related topics (e.g., sports, food), then organize those topics into understandable\\ncontent to respond; for summarization, people tend to write summaries containing keywords used\\nin the input document and perform necessary modifications to ensure grammatical correctness and\\nfluency; in question answering (QA), people use commonsense orprofessional knowledge pertained\\nto the question to infer the answer. Therefore, it is often the case that knowledge beyond the input\\nsequence is required to produce informative output text.\\n1.1 What is Knowledge-enhanced Text Generation?\\nIn general, knowledge is the familiarity, awareness, or understanding that coalesces around a\\nparticular subject. In NLG systems, knowledge is an awareness and understanding of the input text\\nand its surrounding context. These knowledge sources can be categorized into internal knowledge\\nand external knowledge (see Figure 1). Internal knowledge creation takes place within the input\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:3'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='ConceptNetInputKeyword… …TopicGeneration modelOutputInternal knowledgeKnowledge extractionKnowledge sourceInputGeneration modelOutput… …Path on KG……Relevantdoc.External knowledgeKnowledge acquisition\\nFig. 1. We divide different knowledge sources into internal knowledge and external knowledge. Internal\\nknowledge creation takes place within the input text(s), while external knowledge acquisition occurs when\\nknowledge is provided from outside sources (e.g., Wikipedia, ConceptNet [115]).\\ntext(s), including but not limited to keyword, topic, linguistic features, and internal graph structure.\\nExternal knowledge acquisition occurs when knowledge is provided from outside sources, including\\nbut not limited to knowledge base, external knowledge graph, and grounded text. These sources\\nprovide information (e.g., commonsense triples, topic words, reviews, background documents)\\nthat can be used as knowledge through various neural representation learning methods, and then\\napplied to enhance the process of text generation. In addition, knowledge introduces interpretability\\nfor models with explicit semantics. This research direction of incorporating knowledge into text\\ngeneration is named as knowledge-enhanced text generation .\\nProblem 1 (Knowledge-enhanced Text Generation). Given a text generation problem where\\nthe system is given an input sequence 𝑋, and aims to generate an output sequence 𝑌. Assume we'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='the system is given an input sequence 𝑋, and aims to generate an output sequence 𝑌. Assume we\\nalso have access to additional knowledge denoted as 𝐾. Knowledge-enhanced text generation aims\\nto incorporate the knowledge 𝐾to enhance the generation of 𝑌given𝑋, through leveraging the\\ndependencies among the input text, knowledge, and output text.\\nMany existing knowledge-enhanced text generation systems have demonstrated promising\\nperformance on generating informative, logical, and coherent texts. In dialogue systems, a topic-\\naware Seq2Seq model helped understand the semantic meaning of an input sequence and generate\\na more informative response such as “Then hydrate and moisturize your skin. ” to the aforementioned\\nexample input “My skin is so dry. ” In summarization, knowledge graph produced a structured\\nsummary and highlight the proximity of relevant concepts, when complex events related with the\\nsame entity may span multiple sentences. A knowledge graph enhanced Seq2Seq model generated\\nsummaries that were able to correctly answer 10% more topically related questions [ 54]. In question\\nanswering (QA) systems, facts stored in knowledge bases completed missing information in the\\nquestion and elaborate details to facilitate answer generation [ 30,48]. In story generation, using\\ncommonsense knowledge acquired from knowledge graph facilitated understanding of the storyline\\nand better narrate following plots step by step, so each step could be reflected as a link on the'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='and better narrate following plots step by step, so each step could be reflected as a link on the\\nknowledge graph and the whole story would be a path [46].\\n1.2 Why a Survey of Knowledge-enhanced Text Generation?\\nRecent years have witnessed a surge of interests in developing methods for incorporating knowledge\\nin NLG beyond input text. However, there is a lack of comprehensive survey of this research topic.\\nRelated surveys have laid the foundation of discussing this topic. For example, Garbacea et al. [ 37]\\nand Gatt et al. [ 38] reviewed model architectures for core NLG tasks but did not discuss knowledge-\\nenhanced methods. Ji et al. [ 58] presented a review on knowledge graph techniques which could\\nbe used for enhancing NLG. Wang et al. [ 125] summarized how to represent structural knowledge\\nsuch as knowledge base and knowledge graph for reading comprehension and retrieval.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:4 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nSection 2General methodsSection 3-4Knowledge sourcesSection 6Future directionsSection 2.1-2.2Knowledge-enhanced model architecturesSection 2.3Knowledge-enhanced learning and inferenceSection 3Internal knowledgeSection 4External knowledgeAttention mechanismGraph neural networkPointer networkMemory networkKnowledge-enhanced Text Generation'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='TopicKeywordLinguistic featuresInternal graph structureConstraint-driven learningPosterior regularizationMulti-task learningPlug and play methodsKnowledge baseKnowledge graphUnstructured textUse knowledge for VL tasksLearn with broader sourcesLearn with limited resourcesLearn in a continuous waySection 5Benchmark and toolkit\\nFig. 2. Categorization of information sources and methods for knowledge-enhanced text generation. Knowl-\\nedge can be learnt from various information sources, and then integrated into the generation process.\\nTo the best of our knowledge, this is the first survey that presents a comprehensive review of\\nknowledge-enhanced text generation. It aims to provide NLG researchers a synthesis and pointer\\nto related research. Our survey includes a detailed discussion about how NLG can benefit from\\nrecent progress in deep learning and artificial intelligence, including technologies such as graph\\nneural network, reinforcement learning, and neural topic modeling.\\n1.3 What are the Challenges in Knowledge-enhanced Text Generation?\\nTo start with, we note that the first challenge in knowledge-enhanced NLG is to obtain useful related\\nknowledge from diverse sources. There has been a rising line of work that discovers knowledge\\nfrom topic, keyword, knowledge base, knowledge graph and knowledge grounded text. The second\\nchallenge is how to effectively understand andleverage the acquired knowledge to facilitate text'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='challenge is how to effectively understand andleverage the acquired knowledge to facilitate text\\ngeneration. Multiple methods have been explored to improve the encoder-decoder architecture\\n(e.g., attention mechanism, copy and pointing mechanism).\\nBased on the first challenge, the main content of our survey is divided into two parts: (1) gen-\\neral methods of integrating knowledge into text generation (Section 2); (2) specific methods and\\napplications according to different sources of knowledge enhancement (Sections 3–4). More con-\\ncretely, since knowledge can be obtained from different sources, we first divide existing knowledge\\nenhanced text generation work into two categories: internal knowledge enhanced and external\\nknowledge enhanced text generation. The division of internal and external knowledge is widely\\nadopted by management science [ 88], which can be analogous with knowledge enhanced text\\ngeneration. Based on the second challenge, we categorize recent knowledge-enhanced text gen-\\neration methods evolved from how knowledge is extracted and incorporated into the process of\\ntext generation in each section (named as M1, M2, and etc). Furthermore, we review methods for a\\nvariety of natural language generation applications in each section to help practitioners choose,\\nlearn, and use the methods. In total, we discuss seven mainstream applications presented in more\\nthan 80 papers that were published or released in or after the year of 2016.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='than 80 papers that were published or released in or after the year of 2016.\\nAs shown in Figure 2, the remainder of this survey is organized as follows. Section 2 presents basic\\nNLG models and general methods of integrating knowledge into text generation. Sections 3 reviews\\ninternal knowledge-enhanced NLG methods and applications. The internal knowledge is obtained\\nfrom topic, keyword, linguistic features and internal graph structures. Sections 4 reviews external\\nknowledge-enhanced NLG methods and applications. The external knowledge sources include\\nknowledge bases, knowledge graphs, and grounded text. Section 5 presents knowledge-enhanced\\nNLG benchmarks. Section 6 discusses future work and concludes the survey.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:5\\n2 GENERAL METHODS OF INTEGRATING KNOWLEDGE INTO NLG\\n2.1 The Basic Text Generation Models\\nEarly encoder-decoder frameworks are often based on recurrent neural network (RNN) such as RNN-\\nSeq2Seq [ 117]. Convolutional neural network (CNN) based encoder-decoder [ 39] and Transformer\\nencoder-decoder [ 122] have been increasingly widely used. From a probabilistic perspective, the\\nencoder-decoder frameworks learn the conditional distribution over a variable length sequence\\nconditioned on yet another variable length sequence:\\n𝑃(𝑌|𝑋)=𝑃(𝑦1,···,𝑦𝑚|𝑥1,···,𝑥𝑛)=𝑚Ö\\n𝑡=1𝑝(𝑦𝑡|𝑋,𝑦 1,···,𝑦𝑡−1). (1)'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='𝑃(𝑌|𝑋)=𝑃(𝑦1,···,𝑦𝑚|𝑥1,···,𝑥𝑛)=𝑚Ö\\n𝑡=1𝑝(𝑦𝑡|𝑋,𝑦 1,···,𝑦𝑡−1). (1)\\nEncoder. The encoder learns to encode a variable length sequence into a fixed length vector\\nrepresentation. RNN encoder reads the input sentence 𝑋sequentially . CNN encoder performs con-\\nvolutional operations on a word and its surrounding word(s) in a sequential window. Transformer\\nencoder eschews recurrence and instead relying entirely on the self-attention mechanism to draw\\nglobal dependencies between different tokens in the input 𝑋. We denote them uniformly as:\\n(h1,h2,···,h𝑛)=Encoder(e(𝑥1),e(𝑥2),···,e(𝑥𝑛)), (2)\\nwhere e(𝑥𝑖)is the word embedding of word 𝑥𝑖,h𝑖is the contextualized hidden representation of 𝑥𝑖.\\nDecoder. The decoder is to decode a given fixed length vector representation into a variable\\nlength sequence [ 117]. Specially, the decoder generates an output sequence one token at each time\\nstep. At each step the model is auto-regressive, consuming the previously generated tokens as\\nadditional input when generating the next token. Formally, the decoding function is represented as:\\ns𝑡=Decoder(s𝑡−1,e(𝑦𝑡−1)), (3)\\n𝑝(𝑦𝑡|𝑦𝑡−1,𝑦𝑡−2,···,𝑦1)=Readout(s𝑡), (4)\\nwhere Readout(·)is a nonlinear multi-layered function that outputs the probability of 𝑦𝑡.\\nOptimization. A generation process is regarded as a sequential multi-label classification problem.\\nIt can be directly optimized by the negative log likelihood (NLL) loss. Therefore, the objective of a'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='text generation model via maximum likelihood estimation (MLE) is formulated as:\\nL𝑁𝐿𝐿(𝜃)=−log𝑝𝜃(𝑌|𝑋)=−𝑚∑︁\\n𝑡=1log(𝑝𝜃(𝑦𝑡|𝑦<𝑡,𝑋)). (5)\\n2.2 Knowledge-enhanced Model Architectures\\nThe most popular idea of incorporating knowledge is designing specialized architectures of text\\ngeneration models that can reflect the particular type of knowledge. In the context of neural net-\\nworks, several general neural architectures are widely used and customized to bake the knowledge\\nabout the problems being tackled into the models.\\n2.2.1 Attention Mechanism .It is useful to capture the weight of each time step in both encoder\\nand decoder [ 3]. During the decoding phase, the context vector c𝑡is added, so the hidden state s𝑡is:\\ns𝑡=Decoder(s𝑡−1,e(𝑦𝑡−1),c𝑡). (6)\\nUnlike Eq.(3), here the probability is conditioned on the distinct context vector c𝑡for target word 𝑦𝑡,\\nandc𝑡depends on a sequence of hidden states H={h𝑖}𝑛\\n𝑖=1that were mapped from input sequence.\\nIn RNN-Seq2Seq decoder, the c𝑡is computed as a weighted sum of {h𝑖}𝑛\\n𝑖=1:\\nc𝑡=𝑛∑︁\\n𝑖=1𝛼𝑡𝑖h𝑖,where𝛼𝑡𝑖=exp(𝜂(s𝑡−1,h𝑖))Í𝑛\\n𝑘=1exp(𝜂(s𝑡−1,h𝑘)), (7)\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:6 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nTable 1. NLG methods that incorporates knowledge attention ( §2.2.1) and knowledge mode ( §2.2.2).\\nTopic Keyword Knowledge base Knowledge graph Grounded text'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Topic Keyword Knowledge base Knowledge graph Grounded text\\nKnowledge-related attention [134, 139, 152] [69, 70, 73] [34, 48] [46, 54, 151, 159] [9, 87]\\nKnowledge-related mode [139] [70] [48] [57, 151, 159] [87, 107]\\nKnowledge-related memory [34, 158] - [82, 135] [144] [61]\\nwhere𝜂(·)is parametrized as a multi-layer perception to compute a soft alignment. 𝜂(·)enables the\\ngradient of loss function to be backpropagated. There are six alternatives for the 𝜂(·)function (see\\nTable 2 in [ 37]). The probability 𝛼𝑡𝑖reflects the importance of the hidden state of input sequence in\\npresence of the previous hidden state s𝑡−1for deciding the next hidden state.\\nIn Transformer decoder, on top of the two sub-layers in the encoder, the decoder inserts a third\\nsub-layer, which performs multi-head attention over the output of the encoder stack H. Efficient\\nimplementations of the transformer use the cached history matrix S𝑡to generate next token. To\\ncompare with RNN-Seq2Seq, we summarize the Transformer decoder using recurrent notation:\\nS𝑡=Transformer-Decoder (S𝑡−1,e(𝑦𝑡−1),H), (8)\\nwhere S𝑡=[(K(1)\\n𝑡,V(1)\\n𝑡),···,(K(𝑙)\\n𝑡,V(𝑙)\\n𝑡)], where(K(𝑖)\\n𝑡,V(𝑖)\\n𝑡)corresponds to the key-value pairs\\nfrom the𝑖-th layer generated at all time-steps from 0to𝑡. Instead of noting a specific name, we\\nwill use Encoder (·) and Decoder (·) to represent encoder and decoder in the following sections.\\nKnowledge-related attention. Attention mechanism has been widely used to incorporate knowl-'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Knowledge-related attention. Attention mechanism has been widely used to incorporate knowl-\\nedge representation in recent knowledge-enhanced NLG work. The general idea is to learn a\\nknowledge-aware context vector (denoted as ec𝑡) by combining both hidden context vector ( c𝑡)\\nand knowledge context vector (denoted as c𝐾\\n𝑡) into decoder update, such as ec𝑡=𝑓𝑚𝑙𝑝(c𝑡⊕c𝐾\\n𝑡).\\nThe knowledge context vector ( c𝐾\\n𝑡) calculates attentions over knowledge representations (e.g.,\\ntopic vectors, node vectors in knowledge graph). Table 1 summarizes a variety of knowledge\\nattentions, including keyword attention [ 69,70,73], topic attention [ 79,134,139,152], knowledge\\nbase attention [ 34,48], knowledge graph attention [ 54,63,151], and grounded text attention [ 9,87].\\n2.2.2 Copy and Pointing Mechanisms .CopyNet and Pointer-generator (PG) are used to choose\\nsubsequences in the input sequence and put them at proper places in the output sequence.\\nCopyNet and PG have a differentiable network architecture [ 43]. They can be easily trained\\nin an end-to-end manner. In CopyNet and PG, the probability of generating a target token is a\\ncombination of the probabilities of two modes, generate-mode and copy-mode. First, they represent\\nunique tokens in the global vocabulary Vand the vocabulary of source sequence V𝑋. They build\\nan extended vocabulary Vext=V∪V𝑋∪{unk}. The difference between CopyNet and PG is the\\nway to calculate distribution over the extended vocabulary. CopyNet calculates the distribution by'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='way to calculate distribution over the extended vocabulary. CopyNet calculates the distribution by\\n𝑝(𝑦𝑡)=𝑝𝑔(𝑦𝑡)+𝑝𝑐(𝑦𝑡), (9)\\nwhere𝑝𝑔(·|·)and𝑝𝑐(·|·)stand for the probability of generate-mode and copy-mode. Differently, PG\\nexplicitly calculates a switch probability 𝑝𝑚between generate-mode and copy-mode. It recycles the\\nattention distribution to serve as the copy distribution. The distribution over Vextis calculated by\\n𝑝(𝑦𝑡)=𝑝𝑚(g)·𝑝𝑔(𝑦𝑡)+(1−𝑝𝑚(g))·𝑝𝑐(𝑦𝑡), (10)\\nwhere𝑝𝑚(g)indicates the probability of choosing generate-mode, which is obtained by a nonlinear\\nmulti-layered (MLP) function. Importantly, CopyNet and pointer-generator network have been\\nused as the base module for a lot of knowledge-enhanced NLG work.\\nKnowledge-related mode. A knowledge-related mode chooses subsequences in the obtained\\nknowledge and puts them at proper places in the output sequence. It helps NLG models to generate\\nwords that are not included in the global vocabulary ( V) and input sequence ( V𝑋). For example,\\nby adding the model of knowledge base, the extended vocabulary ( V𝑒𝑥𝑡) adds entities and relations\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:7\\nfrom the knowledge base, i.e., V𝑒𝑥𝑡=V+V𝑋+V𝐾𝐵. The probability of generating a target token\\nis a combination of the probabilities of three modes: generate-mode, copy-mode and knowledge\\nbase-mode. Therefore, knowledge-related mode is not only capable of regular generation of words'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='base-mode. Therefore, knowledge-related mode is not only capable of regular generation of words\\nbut also operation of producing appropriate subsequences in knowledge sources. Table 1 summa-\\nrizes different kinds of knowledge-related modes such as topic mode [ 139], keyword mode [ 70],\\nknowledge base mode [48], knowledge graph mode [151, 159], and background mode [87, 107].\\n2.2.3 Memory Network .Memory networks (MemNNs) are recurrent attention models over\\na possibly large external memory [ 116]. They write external memories into several embedding\\nmatrices, and use query (generally speaking, the input sequence 𝑋) vectors to read memories\\nrepeatedly. This approach encodes long dialog history and memorize external information.\\nGiven an input set {𝑚1,···,𝑚𝑖}to be stored in memory. The memories of MemNN are repre-\\nsented by a set of trainable embedding matrices C={C1,···,C𝐾+1}, where each C𝑘maps tokens\\nto vectors, and a query (i.e., input sequence) vector h𝑘\\n𝑋is used as a reading head. The model loops\\nover𝐾hops and it computes the attention weights at hop 𝑘for each memory 𝑚𝑖using:\\np𝑘\\n𝑖=softmax((h𝑘\\n𝑋)⊤C𝑘\\n𝑖), (11)\\nwhere C𝑘\\n𝑖=C𝑘(𝑚𝑖)is the memory content in 𝑖-th position, i.e., mapping 𝑚𝑖into a memory vector.\\nHere, p𝑘is a soft memory selector that decides the memory relevance with respect to the query\\nvector h𝑘\\n𝑋. Then, the model reads out the memory o𝑘by the weighted sum over C𝑘+1,\\no𝑘=∑︁\\n𝑖p𝑘\\n𝑖C𝑘+1\\n𝑖. (12)\\nThen, the query vector is updated for the next hop by using h𝑘+1\\n𝑋=h𝑘'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='o𝑘=∑︁\\n𝑖p𝑘\\n𝑖C𝑘+1\\n𝑖. (12)\\nThen, the query vector is updated for the next hop by using h𝑘+1\\n𝑋=h𝑘\\n𝑋+o𝑘. The result from the\\nencoding step is the memory vector o𝐾and becomes the input for the decoding step.\\nKnowledge-related memory. Memory augmented encoder-decoder framework has achieved\\npromising progress for many NLG tasks. For example, MemNNs are widely used for encoding\\ndialogue history in task-oriented dialogue systems [ 106,135]. Such frameworks enable a decoder\\nto retrieve information from a memory during generation. Recent work explored to model external\\nknowledge with memory network such as knowledge base [82, 144] and topic [34, 158].\\n2.2.4 Graph Network .Graph network captures the dependence of graphs via message pass-\\ning between the nodes of graphs. Graph neural networks (GNNs) [ 138] and graph-to-sequence\\n(Graph2Seq) [ 6] potentiate to bridge up the gap between graph representation learning and text\\ngeneration. Knowledge graph, dependency graph, and other graph structures can be integrated\\ninto text generation through various GNN algorithms. Here we denote a graph as G=(U,E),\\nwhereUis the set of entity nodes and Eis the set of (typed) edges. Modern GNNs typically follow\\na neighborhood aggregation approach, which iteratively updates the representation of a node by\\naggregating information from its neighboring nodes and edges. After 𝑘iterations of aggregation, a\\nnode representation captures the structural information within its 𝑘-hop neighborhood. Formally,'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='node representation captures the structural information within its 𝑘-hop neighborhood. Formally,\\nthe𝑘-th layer of a node 𝑢∈U is:\\nu(𝑘)=Combine 𝑘(u(𝑘−1),Aggregate 𝑘(\\x08\\n(u(𝑘−1)\\n𝑖,e(𝑘−1)\\n𝑖𝑗,u(𝑘−1)\\n𝑗):∀(𝑢𝑖,𝑒𝑖𝑗,𝑢𝑗)∈N(𝑢)\\t\\n)),(13)\\nwhereN(𝑢)={(𝑢𝑖,𝑒𝑖𝑗,𝑢𝑗)∈E|𝑢𝑖=𝑢or𝑢𝑗=𝑢}denotes the set of edges containing node 𝑢,u(𝑘)\\nande(𝑘)\\n𝑖𝑗are feature vectors of a node 𝑢and the edge between 𝑢𝑖and𝑢𝑗at the𝑘-th iteration/layer.\\nThe choice of Aggregate(·)andCombine(·)in GNNs is crucial. A number of architectures for\\nAggregate(·)have been proposed in different GNN works such as GAT [ 123]. Meanwhile, the\\nAggregate(·)function used in labeled graphs (e.g., a knowledge graph) is often taken as those\\nGNNs for modeling relational graphs [ 108]. To obtain the representation of graph G(denoted as\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:8 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nh𝐺), the Readout(·)function (either a simple permutation invariant function or sophisticated\\ngraph-level pooling function) pools node features from the final iteration 𝐾,\\nh𝐺=Readout(\\x08\\nu(𝐾):𝑢∈U\\t\\n). (14)\\nApplications. Graph network has been commonly used in integrating knowledge in graph struc-\\nture such as knowledge graph and dependency graph. Graph attention network [ 123] can be\\ncombined with sequence attention and jointly optimized [ 151,159]. We will introduce different'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='combined with sequence attention and jointly optimized [ 151,159]. We will introduce different\\ngraph structure knowledge in subsequent sections such as knowledge graph (Section 4.2), depen-\\ndency graph (Section 3.3.2-3.3.3), and open knowledge graph (OpenKG) (Section 3.4).\\n2.2.5 Pre-trained Language Models .Pre-trained language models (PLMs) aims to learn uni-\\nversal language representation by conducting self-supervised training on large-scale unlabeled\\ncorpora. Recently, substantial PLMs such as BERT [ 25] and T5 [ 104] have achieved remarkable\\nperformance in various NLP downstream tasks. However, these PLMs suffer from two issues when\\nperforming on knowledge-intensive tasks. First, these models struggle to grasp structured world\\nknowledge, such as concepts and relations, which are very important in language understanding.\\nFor example, BERT cannot deliver great performance on many commonsense reasoning and QA\\ntasks, in which many of the concepts are directly linked on commonsense knowledge graphs [ 146].\\nSecond, due to the domain discrepancy between pre-training and fine-tuning, these models do not\\nperform well on domain-specific tasks. For example, BERT can not give full play to its value when\\ndealing with electronic medical record analysis task in the medical field [78].\\nRecently, a lot of efforts have been made on investigating how to integrate knowledge into\\nPLMs [ 45,78,80,140,146,161]. Specifically, we will introduce some PLMs designed for NLG'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='PLMs [ 45,78,80,140,146,161]. Specifically, we will introduce some PLMs designed for NLG\\ntasks. Overall, these approaches can be grouped into two categories: The first one is to explicitly\\ninject entity representation into PLMs, where the representations is pre-computed from external\\nsources [ 80,155]. For example, KG-BART encoded the graph structure of KGs with knowledge\\nembedding algorithms like TransE [ 11], and then took the informative entity embeddings as\\nauxiliary input [ 80]. However, the method of explicitly injecting entity representation into PLMs\\nhas been argued that the embedding vectors of words in text and entities in KG are obtained in\\nseparate ways, making their vector-space inconsistent [ 78]. The second one is to implicitly modeling\\nknowledge information into PLMs by performing knowledge-related tasks, such as concept order\\nrecovering [ 161], entity category prediction [ 146]. For example, CALM proposed a novel contrastive\\nobjective for packing more commonsense knowledge into the parameters, and jointly pre-trained\\nboth generative and contrastive objectives for enhancing commonsense NLG tasks [161].\\n2.3 Knowledge-enhanced Learning and Inference\\nBesides specialized model architectures, one common way of injecting knowledge to generation\\nmodels is through the supervised knowledge learning. For example, one can encode knowledge into\\nthe objective function that guides the model training to acquire desired model behaviors [ 27,61].'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='the objective function that guides the model training to acquire desired model behaviors [ 27,61].\\nSuch approaches enjoy the flexibility of integrating diverse types of knowledge by expressing them\\nas certain forms of objectives. In general, knowledge-enhanced learning is agnostic to the model\\narchitecture, and can be combined with the aforementioned architectures.\\n2.3.1 Learning with knowledge-related tasks .One could devise learning tasks informed by\\nthe knowledge so that the model is trained to acquire the knowledge information.\\nKnowledge as target .The methods can be mainly divided into two categories as shown in Figure\\n3. The first category of knowledge-related tasks creates learning targets based on the knowledge,\\nand the model is trained to recover the targets. These tasks can be combined as auxiliary tasks with\\nthe text generation task, resulting in a multi-task learning setting. For example, knowledge loss is\\ndefined as the cross entropy between the predicted and true knowledge sentences, and it is combined\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:9\\nConceptNetInputGeneration modelOutputGeneration loss\\nKnowledge lossLabelsLabelsInputGeneration modelOutputGeneration lossLabelscreate\\nkeyword… …\\nConceptNet\\nkeyword… …\\nFig. 3. Incorporating knowledge into text generation by treating knowledge as the target. The first category'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='of methods (left) combine knowledge-related tasks as auxiliary into the text generation task, resulting in\\namulti-task learning setting. The second category of methods (right) create weakly-supervised labels from\\nknowledge, enforcing the relevancy between the knowledge and the target sequence.\\nwith the standard conversation generation loss to enhance grounded conversation [ 27,61]. Similar\\ntasks include keyword extraction loss [ 70], template re-ranking loss [ 13,129], link prediction loss\\non knowledge graph [ 57], path reasoning loss [ 81], mode loss [ 137,159], bag-of-word (BOW)\\nloss [ 74,143], etc. The second category of methods directly derive the text generation targets\\nfrom the knowledge, and use those (typically noisy) targets as supervisions in the standard text\\ngeneration task. The approach is called weakly-supervised learning. Weakly-supervised learning\\nenforces the relevancy between the knowledge and the target sequence. For example, in the problem\\nof aspect based summarization, the work [ 118] automatically creates target summaries based on\\nexternal knowledge bases, which are used to train the summarization model in a supervised manner.\\nKnowledge as condition .The second way of devising knowledge-related tasks is to augment\\nthe text generation task by conditioning the generation on the knowledge. That is, the goal is\\nto learn a function 𝑝𝜃(𝑌|𝑋,𝐾), where𝑋is the input sequence, 𝑌is the target text and 𝐾is the'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='to learn a function 𝑝𝜃(𝑌|𝑋,𝐾), where𝑋is the input sequence, 𝑌is the target text and 𝐾is the\\nknowledge. Generally, the knowledge 𝐾is first given externally (e.g., style, emotion) or retrieved\\nfrom external resources (e.g., facts from knowledge base, a document from Wikipedia) or extracted\\nfrom the given input text (e.g., keywords, topic words). Second, a conditional text generation model\\nis used to incorporate knowledge and generate target output sequence. In practice, knowledge is\\noften remedied by soft enforcing algorithms such as attention mechanism [ 3] and copy/pointing\\nmechanism [ 43,109]. Regarding knowledge as condition is widely used in knowledge-enhanced\\ntext generation. For examples, work has been done in making personalized dialogue response by\\ntaking account of persona [ 154] and emotion [ 158], controlling various aspects of the response\\nsuch as politeness [ 96], grounding the responses in external source of knowledge [ 27,42,159] and\\ngenerating topic-coherent sequence [ 119,143]. Besides, using variational autoencoder (VAE) to\\nenforce the generation process conditioned on knowledge is one popular approach to unsupervised\\nNLG. By manipulating latent space for certain attributes, such as topic [ 132] and style [ 50], the\\noutput sequence can be generated with desired attributes without supervising with parallel data.\\n2.3.2 Learning with knowledge constraints .Instead of creating training objectives in stan-'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='2.3.2 Learning with knowledge constraints .Instead of creating training objectives in stan-\\ndalone tasks that encapsulate knowledge, another paradigm of knowledge-enhanced learning is to\\ntreat the knowledge as the constraints to regularize the text generation training objective.\\nThe posterior regularization (PR) framework was proposed to restrict the space of the model\\nposterior on unlabeled data as a way to guide the model towards desired behavior [ 35,164]. PR\\nhas been used as a principled framework to impose knowledge constraints on probabilistic models\\n(including deep networks) in general [ 51,153]. PR augments any regular training objective L(𝜃)\\n(e.g., negative log-likelihood, as in Eq. (5)) with a constraint term to encode relevant knowledge.\\nFormally, denote the constraint function as 𝑓(𝑋,𝑌)∈Rsuch that a higher 𝑓(𝑋,𝑌)value indicates\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:10 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\na better generated sequence 𝑌that incorporates the knowledge. PR introduces an auxiliary distri-\\nbution𝑞(𝑌|𝑋), and imposes the constraint on 𝑞by encouraging a large expected 𝑓(𝑋,𝑌)value:\\nE𝑞[𝑓(𝑋,𝑌)]. Meanwhile, the model 𝑝𝜃is encouraged to stay close to 𝑞through a KL divergence\\nterm. The learning problem is thus a constrained optimization:\\nmax\\n𝜃,𝑞L(𝜃)−KL(𝑞(𝑌|𝑋)||𝑝𝜃(𝑌|𝑋))+𝜉 (15)\\n𝑠.𝑡.E𝑞[𝑓(𝑋,𝑌)]>𝜉, (16)'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='max\\n𝜃,𝑞L(𝜃)−KL(𝑞(𝑌|𝑋)||𝑝𝜃(𝑌|𝑋))+𝜉 (15)\\n𝑠.𝑡.E𝑞[𝑓(𝑋,𝑌)]>𝜉, (16)\\nwhere𝜉is the slack variable. The PR framework is also related to other constraint-driven learning\\nmethods [14, 83]. We refer readers to [35] for more discussions.\\n2.3.3 Inference with knowledge constraints .Pre-trained language models leverage large\\namounts of unannotated data with a simple log-likelihood training objective. Controlling lan-\\nguage generation by particular knowledge in a pre-trained model is difficult if we do not modify\\nthe model architecture to allow for external input knowledge or fine-tuning with specific data [ 24].\\nPlug and play language model (PPLM) opened up a new way to control language generation with\\nparticular knowledge during inference. At every generation step during inference, the PPLM shifts\\nthe history matrix in the direction of the sum of two gradients: one toward higher log-likelihood\\nof the attribute 𝑎under the conditional attribute model 𝑝(𝑎|𝑌)and the other toward higher log-\\nlikelihood of the unmodified pre-trained generation model 𝑝(𝑌|𝑋)(e.g., GPT). Specifically, the\\nattribute model 𝑝(𝑎|𝑌)makes gradient based updates to ΔS𝑡as follows:\\nΔS𝑡←ΔS𝑡+∇ΔS𝑡log𝑝(𝑎|S𝑡+ΔS𝑡)\\n||∇ΔS𝑡log𝑝(𝑎|S𝑡+ΔS𝑡)||𝛾, (17)\\nwhere𝛾is the scaling coefficient for the normalization term; ΔS𝑡is update of history matrix\\nS𝑡(see Eq.(8)) and initialized as zero. The update step is repeated multiple times. Subsequently,\\na forward pass through the generation model is performed to obtain the updated eS𝑡+1aseS𝑡+1='),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='a forward pass through the generation model is performed to obtain the updated eS𝑡+1aseS𝑡+1=\\nDecoder((S𝑡+ΔS𝑡),e(𝑦𝑡),H). The perturbed eS𝑡+1is then used to generate a new logit vector. PPLMs\\nis efficient and flexible to combine differentiable attribute models to steer text generation [102].\\n3 NLG ENHANCED BY INTERNAL KNOWLEDGE\\n3.1 NLG Enhanced by Topic\\nTopic, which can be considered as a representative or compressed form of text, has been often\\nused to maintain the semantic coherence and guide the NLG process. Topic modeling is a powerful\\ntool for finding the high-level content of a document collection in the form of latent topics [ 10]. A\\nclassical topic model, Latent Dirichlet allocation (LDA), has been widely used for inferring a low\\ndimensional representation that captures latent semantics of words and documents [ 10]. In LDA,\\neach topic is defined as a distribution over words and each document as a mixture distribution\\nover topics. LDA generates words in the documents from topic distribution of document and\\nword distribution of topic. Recent advances of neural techniques open a new way of learning low\\ndimensional representations of words from the tasks of word prediction and context prediction,\\nmaking neural topic models become a popular choice of finding latent topics from text [12, 47].\\nNext, we introduce popular NLG applications enhanced by topics:\\n•Dialogue system. A vanilla Seq2Seq often generates trivial or non-committal sentences of'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='•Dialogue system. A vanilla Seq2Seq often generates trivial or non-committal sentences of\\nfrequent words or phrases in the corpus [ 139]. For example, a chatbot may say “I do not know” ,\\n“I see” too often. Though these off-topic responses are safe to reply to many queries, they are\\nboring with very little information. Such responses may quickly lead the conversation to an\\nend, severely hurting user experience. Thus, on-topic response generation is highly needed.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:11\\nSequence decoderOutput textMixture distributionLDAInput textSequence encoderCNN topic encoderLatent topicLatent textSequence decoderOutput textInput textSequence encoderTopic words encoderLatent topicLatent textTopic words\\nSequence decoderOutput textInput textSequence encoderNeural topic encoderLatent topicLatent text(M1) Leverage topic words from generative topic models(M2) Jointly optimize generation model and CNN topic model(M3) Enhance NLG by neural topic models with variational inference\\nFig. 4. Three typical methodologies for incorporating topics into NLG. Detailed designs are not included.\\n•Machine translation. Though the input and output languages are different (e.g., translating\\nEnglish to Chinese), the contents are the same, and globally, under the same topic. Therefore,\\ntopic can serve as an auxiliary guidance to preserve the semantics information of input text'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='topic can serve as an auxiliary guidance to preserve the semantics information of input text\\nin one language into the output text in the other language.\\n•Paraphrase. Topic information helps understand the potential meaning and determine the\\nsemantic range to a certain extent. Naturally, paraphrases concern the same topic, which can\\nserve as an auxiliary guidance to promote the preservation of source semantic.\\nAs shown in Figure 4, we summarize topic-enhanced NLG methods into three methodologies:\\n(M1) leverage topic words from generative topic models; (M2) jointly optimize generation model\\nand CNN topic model; (M3) enhance NLG by neural topic models with variational inference.\\n3.1.1 M1: Leverage Topic Words from Generative Topic Models .Topics help understand\\nthe semantic meaning of sentences and determine the semantic spectrum to a certain extent. To\\nenhanced text generation, an effective solution is to first discover topics using generative topic\\nmodels (e.g., LDA), and then incorporate the topics representations into neural generation models,\\nas illustrated in Figure 4(a). In existing work, there are two mainstream methods to represent topics\\nobtained from generative topic models. The first way is to use the generated topic distributions for\\neach word (i.e., word distributions over topics) in the input sequence [ 94,152]. The second way is to\\nassign a specific topic to the input sequence, then picks the top- 𝑘words with the highest probabilities'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='under the topic, and use word embeddings (e.g., GloVe) to represent topic words [ 79,139]. Explicitly\\nmaking use of topic words can bring stronger guidance than topic distributions, but the guidance\\nmay deviate from the target output sequence when some generated topic words are irrelevant.\\nZhang et al. proposed the first work of using a topic-informed Seq2Seq model by concatenating the\\ntopic distributions with encoder and decoder hidden states [ 152]. Xing et al. designed a topic-aware\\nSeq2Seq model in order to use topic words as prior knowledge to help dialogue generation [139].\\n3.1.2 M2: Jointly Optimize Generation Model and CNN Topic Model .The LDA models\\nwere separated from the training process of neural generation model and were not able to adapt\\nto the diversity of dependencies between input and output sequences. Therefore, the idea of\\naddressing this issue is to use neural topic models. Convolutional neural networks (CNN) were used\\nto learn latent topic representations through iterative convolution and pooling operations. There\\nare growing interests of using the CNNs to map latent topics implicitly into topic vectors that can be\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:12 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nTable 2. Natural language generation methods that incorporate topic knowledge in text generation. Since'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='most of the methods are tested on different tasks and datasets, we only compare the performance between\\n“w/o topic” setting and “with topic” setting. For evaluation metrics, PPL is short for perplexity (lower is better);\\nB-4 is short for BLEU-4 (higher is better); R-L is short for ROUGE-L (higher is better).\\nTask Method Ref. Cat.Framework components Effect of topic modeling\\nSeq. Enc/Dec Topic model Dataset w/o topic with topic\\nDialogue\\nsystemTp-S2S [139] M1 RNN Seq2Seq LDA topics Baidu Tieba (PPL) 147.0 (PPL) 134.6\\nPEE [143] M3 RNN Seq2Seq Neural topics PersonaChat (B-4) 2.98 (B-4) 3.56\\nMachine\\ntranslationTp-NMT [152] M1 RNN Seq2Seq LDA topics NIST (B-4) 34.76 (B-4) 35.91\\nBLT-NMT [134] M2 RNN Seq2Seq CNN topics NIST (B-4) 38.97 (B-4) 40.10\\nSummari\\n-zationTp-CS2S [94] M1 CNN Seq2Seq LDA topics XSum (R-L) 25.23 (R-L) 25.75\\nTGVAE [132] M3 RNN with VAE Neural topics Gigawords (R-L) 32.13 (R-L) 33.02\\nVHTM [33] M3 RNN with VAE Neural topics CNN/DM (R-L) 36.73 (R-L) 37.18\\nParaphraseTGLM [36] M2 RNN Seq2Seq CNNs topics Yahoo! Ans (PPL) 99.13 (PPL) 88.69\\nPTA [79] M1 RNN Seq2Seq LDA topics Quora (B-4) 28.76 (B-4) 31.75\\nused to enhance text generation tasks [ 36,134]. Empirical analyses showed that convolution-based\\ntopic extractors could outperform LDA-based topic models for multiple applications (e.g., dialogue\\nsystem, text summarization, machine translation). However, theoretical analysis was missing to'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='system, text summarization, machine translation). However, theoretical analysis was missing to\\nensure the quality of the topics captured by the convolutions. And their interpretability is not as\\nsatisfactory as the LDA-based topic models.\\n3.1.3 M3: Enhance NLG by Neural Topic Models with Variational Inference .Neural topic\\nmodels can be trained efficiently by backpropagation [ 12]. In neural topic models, Dirichlet distri-\\nbutions can be employed as the prior to generate the parameters of the multinomial distribution 𝜃𝑑\\nfor each document [ 89]. The generative process of LDA is represented as: (1) 𝜃𝑑∼Dirichlet(𝛼);\\n(2)𝑡𝑖∼Multinomial(𝜃𝑑); (3)𝑤𝑖∼Multinomial(𝛽𝑡𝑖), where𝑑denotes the bag-of-words representa-\\ntion of a document, 𝑡𝑖represents the topic assignment for word 𝑤𝑖, and𝛽𝑡𝑖represents the topic\\ndistribution over words given topic assignment 𝑡𝑖. However, a directed generative model comes up\\nagainst the problem of establishing low variance gradient estimators. Miao et al. parameterized\\nthe multinomial distributions with neural networks and jointly learned the model parameters\\nvia variational inference [ 89]. They created neural structures for constructing topic distributions\\nconditioned on a draw from a multivariate Gaussian distribution, represented as 𝜃𝑑∼G(𝜇0,𝜎2\\n0),\\nwhere G(𝜇0,𝜎2\\n0)is composed of a neural network conditioned on an isotropic Gaussian N(𝜇0,𝜎2\\n0).\\nTaking a Gaussian prior distribution makes re-parameterization feasible to build an unbiased and'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Taking a Gaussian prior distribution makes re-parameterization feasible to build an unbiased and\\nlow-variance gradient estimator for the variational distribution [ 26]. Without conjugacy prior,\\nthe updates of the parameters are derived directly and easily from the variational lower bound.\\nFormally, a variational lower bound for the document log-likelihood is:\\nJ𝑡𝑜𝑝𝑖𝑐=E𝑞(𝜃|𝑑)[log𝑝(𝑑|𝛽,𝜃)]−KL(𝑞(𝜃|𝑑)||𝑝(𝜃|𝜇0,𝜎2\\n0)), (18)\\nwhere𝑞(𝜃|𝑑)is the variational distribution approximating the true posterior 𝑝(𝜃|𝑑). Its lower\\nbound is estimate by sampling 𝜃from𝑞(𝜃|𝑑)=G(𝜃|𝜇(𝑑),𝜎2(𝑑)).\\nIn order to combine neural topic model and neural generation model, the idea is to use the\\nVariational Auto-Encoder (VAE) [ 26]. It adopts autoregressive networks (e.g., LSTM) both as the\\nencoder and decoder. VAE can learn latent codes 𝑧of texts by reconstructing texts with its decoder.\\nIt assumes that the generation process is controlled by codes in a continuous latent space. This\\nkind of VAE implementation considers sequential information of texts that can model the linguistic\\nstructure of texts. Wang et al. proposed topic guided variational autoencoder (TGVAE), to draw\\nlatent code𝑧from a topic-dependent Gaussian Mixture Prior in order to incorporate the topical\\nknowledge into latent variables [ 132]. The topic-dependent Gaussian Mixture Model (GMM) is\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:13\\ndefined as:𝑝(𝑧|𝛽,𝑡)=Í𝑇'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='defined as:𝑝(𝑧|𝛽,𝑡)=Í𝑇\\n𝑖=1𝑡𝑖N(𝜇(𝛽𝑖),𝜎2(𝛽𝑖)), where𝑇is the number of topics, 𝜇(𝑑)and𝜎2(𝑑)are\\nfunctions implemented by MLP. TGVAE uses bag-of-words as input and embeds an input document\\ninto a topic vector. The topic vector is then used to reconstruct the bag-of-words input, and the\\nlearned topic distribution over words is used to model a topic-dependent prior to generate an output\\nsequence𝑌from conditioned on an input sequence 𝑋. Therefore, to maximize the log-likelihood\\nlog𝑝(𝑌,𝑑|𝑋), a variational objective function is constructed as:\\nJ𝑠𝑒𝑞2𝑠𝑒𝑞=E𝑞(𝑧|𝑋)[log𝑝(𝑌|𝑋,𝑧)]−E𝑞(𝜃|𝑑)[KL(𝑞(𝑧|𝑋)||𝑝(𝑧|𝛽,𝜃))], (19)\\nwhere𝑞(𝑧|𝑋)is variational distributions for 𝑧. The combined object function is given by:\\nJ=J𝑡𝑜𝑝𝑖𝑐+J𝑠𝑒𝑞2𝑠𝑒𝑞. (20)\\n3.1.4 Discussion and Analysis of Different Methods .ForM1, topic models (e.g., LDA) has a\\nstrict probabilistic explanation since the semantic representations of both words and documents\\nare combined into a unified framework. Besides, topic models can be easily used and integrated\\ninto generation frameworks. For example, topic words can be represented as word embeddings;\\ntopic embeddings can be integrated into the decoding phase through topic attention. However,\\nLDA models are separated from the training process of generation, so they cannot adapt to the\\ndiversity of dependencies between input and output sequences.\\nForM2, it is an end-to-end neural framework that simultaneously learns latent topic represen-'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='ForM2, it is an end-to-end neural framework that simultaneously learns latent topic represen-\\ntations and generates output sequences. Convolutional neural networks (CNN) are often used to\\ngenerate the latent topics through iterative convolution and pooling operations. However, theoreti-\\ncal analysis is missing to ensure the quality of the topics captured by the convolutions. And their\\ninterpretability is not as good as the LDA-based topic models.\\nForM3, neural topic models combine the advantages of neural networks and probabilistic\\ntopic models. They enable back propagation for joint optimization, contributing to more coherent\\ntopics, and can be scaled to large data sets. Generally, neural topic models can provide better topic\\ncoherence than LDAs [ 12,132,143]. However, neural variational approaches share a same drawback\\nthat topic distribution is assumed to be an isotropic Gaussian, which makes them incapable of\\nmodeling topic correlations. Existing neural topic models assume that the documents should be i.i.d.\\nto adopt VAE, while they are commonly correlated. The correlations are critical for topic modeling.\\n3.2 NLG Enhanced by Keywords\\nKeyword (aka., key phrase, key term) is often referred as a sequence of one or more words, providing\\na compact representation of the content of a document. The mainstream methods of keyword\\nacquisition for documents can be divided into two categories [ 112]: keyword assignment and'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='acquisition for documents can be divided into two categories [ 112]: keyword assignment and\\nkeyword extraction. Keyword assignment means that keywords are chosen from a controlled\\nvocabulary of terms or predefined taxonomy. Keyword extraction selects the most representative\\nwords explicitly presented in the document, which is independent from any vocabulary. Keyword\\nextraction techniques (e.g., TF-IDF, TextRank, PMI) have been widely used over decades. Many NLG\\ntasks can benefit from incorporating such a condensed form of essential content in a document to\\nmaintain the semantic coherence and guide the generation process.\\nNext, we introduce popular NLG applications enhanced by keywords:\\n•Dialogue system. Keywords help enlighten and drive the generated responses to be infor-\\nmative and avoid generating universally relevant replies which carry little semantics. Besides,\\nrecent work introduced personalized information into the generation of dialogue to help\\ndeliver better dialogue response such as emotion [71, 114, 158], and persona [154, 157].\\n•Summarization. Vanilla Seq2Seq models often suffer when the generation process is hard\\nto control and often misses salient information [ 69]. Making use of keywords as explicit\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:14 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nTable 3. Natural language generation methods that incorporate keyword in text generation.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Table 3. Natural language generation methods that incorporate keyword in text generation.\\n(a) (M1) Descriptions and quantitative comparisons between three methods for emotional dialogue systems.\\nTask Method Ref. Assignment methodExperiments on NLPCC dataset\\nBLEU D-1/D-2 Emotion w/s\\nDialogue\\nsystemSeq2Seq [3] Seq2Seq attention without using keywords 1.50 0.38/1.20 33.5/37.1\\nE-SCBA [71] MLP classifier to 7 emotions (categories) 1.69 0.54/4.84 72.0/51.2\\nEmoChat [158] E-SCBA + two memory modules for decoding 1.68 0.90/7.35 76.5/58.0\\nEmoDS [114] MLP classifier after decoding (discriminator) 1.73 1.13/8.67 81.0/68.7\\n(b) (M2) As most methods are tested on different tasks and datasets, we only compare the performance\\nbetween “w/o keyword” setting and “with keyword” setting. Besides, HM is short for human evaluation.\\nTask Method Ref.Extraction Keyword Effect of keyword\\nmethod labels Dataset w/o keyword with keyword\\nSummari-\\nzationKIGN [69] TextRank UnsupervisedCNN/DM (R-2) 15.66 (R-2) 17.12\\nGigaword (R-2) 23.61 (R-2) 23.93\\nComGen [73] PMI and TFIDF Unsupervised Tencent (HM) 5.77 (HM) 7.19\\nKGAS [70] BiLSTM-Softmax w(𝑋)∩w(𝑌)Gigaword (R-2) 23.61 (R-2) 25.06\\nQuestion\\ngenerationSelector [22] BiLSTM-Softmax w(𝑋)∩w(𝑌)SQuAD (B-4) 14.72 (B-4) 15.87\\nPrior [133] BiLSTM-Softmax w(𝑋)∩w(𝑌)SQuAD (B-4) 14.72 (B-4) 15.34\\nguidance can provide significant clues of the main points about the document [ 69,70]. It is'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='guidance can provide significant clues of the main points about the document [ 69,70]. It is\\ncloser to the way that humans write summaries: make sentences to contain the keywords, and\\nthen perform necessary modifications to ensure the fluency and grammatically correctness.\\n•Question generation. It aims to generate questions from a given answer and its relevant\\ncontext. Given an answer and its associated context, it is possible to raise multiple questions\\nwith different focuses on the context and various means of expression.\\nResearchers have developed a great line of keyword-enhanced NLG methods. These methods can\\nbe categorized into two methodologies: (M1) Incorporate keyword assignment into text generation;\\n(M2) Incorporate keyword extraction into text generation.\\n3.2.1 M1: Incorporate Keyword Assignment into Text Generation .When assigning a key-\\nword to an input document, the set of possible keywords is bounded by a pre-defined vocabu-\\nlary [ 112]. The keyword assignment is typically implemented by a classifier that maps the input\\ndocument to a word in the pre-defined vocabulary [ 23,71,114,158]. Unfortunately, some NLG sce-\\nnarios do not hold an appropriate pre-defined vocabulary, so keyword assignment cannot be widely\\nused to enhance NLG tasks. One applicable scenario is to use a pre-determined domain specific\\nvocabulary to maintain relevance between the input and the output sequence [ 23]. Another scenario'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='vocabulary to maintain relevance between the input and the output sequence [ 23]. Another scenario\\nis to generate dialogue with specific attributes such as persona [113, 143], emotion [71, 114, 158].\\nM1.1: Adding assigned keyword into the decoder .A straightforward method of keyword\\nassignment is to assign the words from pre-defined vocabulary and use them as the keywords [ 113,\\n143]. Sometimes, the input sequence does not have an explicit keyword, but we can find one from\\nthe pre-defined vocabulary. For example, a dialogue utterance “If you had stopped him that day,\\nthings would have been different. ” expresses sadness but it does not have the word “sad.” To address\\nthis issue, Li et al. propose a method to predict an emotion category by fitting the sum of hidden\\nstates from encoder into a classifier [ 71]. Then, the response will be generated with the guidance\\nof the emotion category. In order to dynamically track how much the emotion is expressed in the\\ngenerated sequence, Zhou et al. propose a memory module to capture the emotion dynamics during\\ndecoding [ 158]. Each category is initialized with an emotion state vector before the decoding phase\\nstarts. At each step, the emotion state decays by a certain amount. Once the decoding process is\\ncompleted, the emotion state decays to zero, indicating that the emotion is completely expressed.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:15'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='M1.2: Assigning keyword for generated sequence .As mentioned in [ 114], explicitly incorpo-\\nrating emotional keywords suffers from expressing a certain emotion overwhelmingly. Instead,\\nSong et al. propose to increase the intensity of the emotional experiences not by using emotional\\nwords explicitly, but by implicitly combining neutral words in distinct ways on emotion [ 114].\\nSpecifically, they use an emotion classifier to build a sentence-level emotion discriminator, which\\nhelps to recognize the responses that express a certain emotion but not explicitly contain too many\\nliteral emotional words. The discriminator is connected to the end of the decoder.\\n3.2.2 M2: Incorporate Keyword Extraction into Text Generation .Keyword extraction se-\\nlects salient words from input documents [ 112]. Recent work has used statistical keyword extraction\\ntechniques (e.g., PMI [ 73], TextRank [ 69]), and neural-based keyword extraction techniques (e.g.,\\nBiLSTM [ 70]). The process of incorporating extracted keywords into generation is much like the\\nprocess discussed in Section 3.2.1. It takes keywords as an additional input into decoder. Recent\\nwork improves encoding phase by adding another sequence encoder to represent keywords [ 69,70].\\nThen, the contextualized keywords representation is fed into the decoder together with input\\nsequence representation. To advance the keyword extraction, Li et al. propose to use multi-task'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='sequence representation. To advance the keyword extraction, Li et al. propose to use multi-task\\nlearning for training a keyword extractor network and generating summaries [ 22,70]. Because both\\nsummarization and keyword extraction aim to select important information from input document,\\nthese two tasks can benefit from sharing parameters to improve the capacity of capturing the gist\\nof the input text. In practice, they take overlapping words between the input document and the\\nground-truth summary as keywords, and adopt a BiLSTM-Softmax as keyword extractor. Similar\\nidea has also been used in question generation tasks [ 22,133]. They use overlapping words between\\nthe input answer context and the ground-truth question as keywords.\\n3.2.3 Discussion and Analysis of Different Methods .\\nPros and cons. ForM1, the primary advantage of keyword assignment is that the quality of\\nkeywords is guaranteed, because irrelevant keywords are not included in the pre-defined vocabulary.\\nAnother advantage is that even if two semantically similar documents do not have common words,\\nthey can still be assigned with the same keyword. However, there are mainly two drawbacks. On\\none hand, it is expensive to create and maintain dictionaries in new domains. So, the dictionaries\\nmight not be available. On the other hand, potential keywords occurring in the document would be\\nunfortunately ignored if they were not in the vocabulary. Therefore, keyword assignment is suitable'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='unfortunately ignored if they were not in the vocabulary. Therefore, keyword assignment is suitable\\nfor the task that requires specific categories of keywords to guide the generated sentences with\\nthese key information. For example, dialogue systems generate responses with specific attitudes.\\nForM2, keyword extraction selects the most representative words explicitly presented in the\\ndocument, which is independent from any vocabulary. It is easy to use but has two drawbacks. First,\\nit cannot guarantee consistency because similar documents may still be represented by different\\nkeywords if they do not share the same set of words. Second, when an input document does not\\nhave a proper representative word, and unfortunately, the keyword extractor selects an irrelevant\\nword from the document as a keyword, this wrong guidance will mislead the generation. Therefore,\\nkeyword extraction is suitable for the task that the output sequence needs to keep important\\ninformation in the input sequence such as document summarization and paraphrase.\\nQuantitative analysis. Table 3 summarizes tasks and datasets used in keyword-enhanced NLG\\nwork. Comparing with keyword-enhanced methods (E-SCBA [ 71]) and the basic Seq2Seq attention\\nmodel, keyword-enhanced methods can greatly improve both generation quality (evaluated by\\nBLEU) and emotional expression (evaluated by emotion-w and emotion-s) on the NLPCC dataset.\\nBesides, as shown in Table 3(a), EmoDS [ 114] achieved the best performance among three M1'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Besides, as shown in Table 3(a), EmoDS [ 114] achieved the best performance among three M1\\nmethods, which indicates taking keyword assignment as a discriminant task can make better\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:16 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nimprovement than assigning keyword before the sentence decoding. For M2 methods, since most\\nmethods were evaluated on different tasks, we can only compare the performance between “without\\nusing keyword” and “using keyword”. As shown in Table 3(b), leveraging extracted keywords from\\ninput sequence into Seq2Seq model can improve the generation quality on summarization and\\nquestion generation tasks. Comparing with KGAS [ 70] and KIGN [ 69], we can observe using\\nBiLSTM-Softmax to extract keyword (a supervised manner by using overlapping words between 𝑋\\nand𝑌as labels) can make better performance than using TextRank (an unsupervised manner).\\n3.3 NLG Enhanced by Linguistic Features\\nFeature enriched encoder means that the encoder not only reads the input sequence, but also\\nincorporates auxiliary hand-crafted features [ 110,149,160]. Linguistic features are the most common\\nhand-crafted features, such as part-of-speech (POS) tags, dependency parsing, and semantic parsing.\\n3.3.1 POS tags and NER tags .Part-of-speech tagging (POS) assigns token tags to indicate the'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='3.3.1 POS tags and NER tags .Part-of-speech tagging (POS) assigns token tags to indicate the\\ntoken’s grammatical categories and part of speech such as noun (N) ,verb (V) ,adjective (A) . Named-\\nentity recognition (NER) classifies named entities mentioned in unstructured text into pre-defined\\ncategories such as person (P) ,location (L) ,organization (O) . CoreNLP is the most common used\\ntool [ 84]. In spite of homonymy and word formation processes, the same surface word form may\\nbe shared between several word types. Incorporating NER tags and POS tags can detect named\\nentities and understand input sequence better, hence, further improve NLG [28, 93, 160].\\n3.3.2 Syntactic dependency graph .Syntactic dependency graph is a directed acyclic graph\\nrepresenting syntactic relations between words [ 4]. For example, in the sentence “The monkey eats\\na banana”, “monkey” is the subject of the predicate “eats”, and “banana” is the object. Enhancing\\nsequence representations by utilizing dependency information captures source long-distance depen-\\ndency constraints and parent-child relation for different words [ 1,4,15]. In NLG tasks, dependency\\ninformation is often modeled in three different ways as follows: (i) linearized representation: lin-\\nearize dependency graph and then use sequence model to obtain syntax-aware representation [ 1];\\n(ii) path-based representation: calculate attention weights based on the linear distance between a'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='(ii) path-based representation: calculate attention weights based on the linear distance between a\\nword and the aligned center position, i.e., the greater distance a word to the center position on the\\ndependency graph is, the smaller contribution of the word to the context vector is [ 15]; and (iii)\\ngraph-based representation: use GNNs to aggregate information from dependency relations [4].\\n3.3.3 Semantic dependency graph .Semantic dependency graph represents predicate-argument\\nrelations between content words in a sentence and have various semantic representation schemes\\n(e.g., DM) based on different annotation systems. Nodes in a semantic dependency graph are\\nextracted by semantic role labeling (SRL) or dependency parsing, and connected by different\\nintra-semantic and inter-semantic relations [ 98]. Since semantic dependency graph introduces a\\nhigher level of information abstraction that captures commonalities between different realizations\\nof the same underlying predicate-argument structures, it has been widely used to improve text\\ngeneration [ 59,75,98]. Jin et al. propose a semantic dependency guided summarization model [ 59].\\nThey incorporate the semantic dependency graph and the input text by stacking encoders to guide\\nsummary generation process. The stacked encoders consist of a sequence encoder and a graph\\nencoder, in which the sentence encoder first reads the input text through stacked multi-head'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='encoder, in which the sentence encoder first reads the input text through stacked multi-head\\nself-attention, and then the graph encoder captures semantic relationships and incorporates the\\nsemantic graph structure into the contextual-level representation.\\n3.4 NLG Enhanced by Open Knowledge Graphs\\nFor those KGs (e.g., ConceptNet) constructed based on data beyond the input text, we refer them as\\nexternal KGs . On the contrary, an internal KG is defined as a KG constructed solely based on the\\ninput text. In this section, we will discuss incorporating internal KG to help NLG [30, 54].\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:17\\nInternal KG plays an important role in understanding the input sequence especially when it is of\\ngreat length. By constructing an internal KG intermediary, redundant information can be merged or\\ndiscarded, producing a substantially compressed form to represent the input document [ 30]. Besides,\\nrepresentations on KGs can produce a structured summary and highlight the proximity of relevant\\nconcepts, when complex events related with the same entity may span multiple sentences [ 54]. One\\nof the mainstream methods of constructing an internal KG is using open information extraction\\n(OpenIE). Unlike traditional information extraction (IE) methods, OpenIE is not limited to a small\\nset of target entities and relations known in advance, but rather extracts all types of entities and'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='relations found in input text [ 95]. In this way, OpenIE facilitates the domain independent discovery\\nof relations extracted from text and scales to large heterogeneous corpora.\\nAfter obtaining an internal KG, the next step is to learn the representation of the internal KG and\\nintegrate it into the generation model. For example, Zhu et al. use a graph attention network (GAT)\\nto obtain the representation of each node, and fuse that into a transformer-based encoder-decoder\\narchitecture via attention [ 163]. Their method generates abstractive summaries with higher factual\\ncorrectness. Huang et al. extend by first encoding each paragraph as a sub-KG using GAT, and then\\nconnecting all sub-KGs with a Bi-LSTM [ 54]. This process models topic transitions and recurrences,\\nwhich enables the identification of notable content, thus benefiting summarization.\\n4 NLG ENHANCED BY EXTERNAL KNOWLEDGE\\n4.1 NLG Enhanced by Knowledge Base\\nOne of the biggest challenges in NLG is to discover the dependencies of elements within a sequence\\nand/or across input and output sequences. The dependencies are actually various types of knowledge\\nsuch as commonsense, factual events, and semantic relationship. Knowledge base (KB) is a popular\\ntechnology that collects, stores, and manages large-scale information for knowledge-based systems\\nlike search engines. It has a great number of triples composed of subjects, predicates, and objects.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='People also call them “facts” or “factual triplets”. Recently, researchers have been designing methods\\nto use KB as external knowledge for learning the dependencies easier, faster, and better.\\nNext, we introduce popular NLG applications enhanced by knowledge base:\\n•Question answering. It is often difficult to generate proper answers only based on a given\\nquestion. This is because, depending on what the question is looking for, a good answer may\\nhave different forms. It may completes the question precisely with the missing information.\\nIt may elaborate details of some part of the question. It may need reasoning and inference\\nbased on some facts and/or commonsense. So, only incorporating input question into neural\\ngeneration models often fails the task due to the lack of commonsense/factual knowledge [ 8].\\nRelated structured information of commonsense and facts can be retrieved from KBs.\\n•Dialogue system. The needs of KB in generating conversations or dialogues are relevant with\\nQA but differ from two aspects. First, a conversation or dialogue could be open discussions\\nwhen started by an open topic like “ Do you have any recommendations? ” Second, responding\\nan utterance in a certain step needs to recall previous contexts to determine involved entities.\\nKB will play an important role to recognize dependencies in the long-range contexts.\\nTo handle different kinds of relationships between KB and input/output sequences, these methods'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='To handle different kinds of relationships between KB and input/output sequences, these methods\\ncan be categorized into two methodologies which is shown in Figure 5: (M1) design supervised\\ntasks around KB for joint optimization; (M2) enhance incorporation by selecting KB or facts.\\n4.1.1 M1: Design Supervised Tasks around KB for Joint Optimization .Knowledge bases\\n(KBs) that acquire, store, and represent factual knowledge can be used to enhance text generation.\\nHowever, designing effective incorporation to achieve a desired enhancement is challenging because\\na vanilla Seq2Seq often fails to represent discrete isolated concepts though they perform well to\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:18 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nJet Li was born in Singapore. Heis now a Singaporeancitizen.Top-3triplesSequence encoderTriple encoderSequence DecoderOutput text\\n(a) M1: Retrieve relevant triples, use them for generation Input textDo    you   know where  was  Jet_Li fromsubjectpredicateobject 1Jet_LigenderMale2Jet_Liprofessionactor3Jet_LinationalitySingapore4Jet_LibirthplaceBeijing5………Jet Li was born in Beijing. Heis now a Singaporeancitizen.Top-3triplesSequence encoderTriple encoderSequence DecoderOutput text'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='source conceptsKBlocate triples(b) M2: Use KL tomeasure the proximity between prior and posteriorInput textDo    you   know where  was  Jet_Li fromsubjectpredicateobject 1Jet_LigenderMale2Jet_LinationalitySingapore3Jet_LibirthplaceBeijing4Jet_Liprofessionactor5………\\n𝑝(𝑘|𝑌)123124\\n𝑝(𝑘|𝑋)KL div losslocate triplesKB\\nsource conceptsKBlocate triples\\nFig. 5. The left figure demonstrates retrieving relevant triples, then using them for generation; the right figure\\ndemonstrate using KL to measure the proximity between prior and posterior distribution.\\nlearn smooth shared patterns (e.g., language diversity). To fully utilize the knowledge bases, the\\nidea is to jointly train neural models on multiple tasks. For example, the target task is answer\\nsequence generation, and additional tasks include question understanding and fact retrieval in\\nthe KB. Knowledge can be shared across a unified encoder-decoder framework design. Typically,\\nquestion understanding and fact retrieval are relevant and useful tasks, because a question could\\nbe parsed to match (e.g., string matching, entity linking, named entity recognition) its subject and\\npredicate with the components of a fact triple in KB, and the answer is the object of the triple.\\nKBCopy was the first work to generate responses using factual knowledge bases [ 29]. During the\\ngeneration, KBCopy is able to copy words from the KBs. However, the directly copying relevant'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='generation, KBCopy is able to copy words from the KBs. However, the directly copying relevant\\nwords from KBs is extremely challenging. CoreQA used both copying and retrieving mechanisms to\\ngenerate answer sequences with an end-to-end fashion [ 48]. Specifically, it had a retrieval module to\\nunderstand the question and find related facts from the KB. Then, the question and all retrieved facts\\nare transformed into latent representations by two separate encoders. During the decoding phase,\\nthe integrated representations are fed into the decoder by performing a joint attention on both\\ninput sequence and retrieved facts. Figure 5(a) demonstrates a general pipeline that first retrieves\\nrelevant triples from KBs, then leverages the top-ranked triples into the generation process.\\n4.1.2 M2: Enhance Incorporation by Selecting KB or Facts in KB .Ideally, the relevance of\\nthe facts is satisfactory with the input and output sequence dependencies, however, it is not always\\ntrue in real cases. Lian et al. addressed the issue of selecting relevant facts from KBs based on\\nretrieval models (e.g. semantic similarity) might not effectively achieve appropriate knowledge\\nselection [ 74]. The reason is that different kinds of selected knowledge facts can be used to generate\\ndiverse responses for the same input utterance. Given a specific utterance and response pair, the\\nposterior distribution over knowledge base from both the utterance and the response may provide'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='posterior distribution over knowledge base from both the utterance and the response may provide\\nextra guidance on knowledge selection. The challenge lies in the discrepancy between the prior\\nand posterior distributions. Specifically, the model learns to select effective knowledge only based\\non the prior distribution, so it is hard to obtain the correct posterior distribution during inference.\\nTo tackle this issue, the work of Lian et al. [ 74] and Wu et al. [ 137] (shown in Figure 5(b))\\napproximated the posterior distribution using the prior distribution in order to select appropriate\\nknowledge even without posterior information. They introduced an auxiliary loss, called Kullback-\\nLeibler divergence loss (KLDivLoss), to measure the proximity between the prior distribution and\\nthe posterior distribution. The KLDivLoss is defined as follows:\\nLKLDiv(𝜃)=𝑁∑︁\\n𝑖=1𝑝(𝑘=𝑘𝑖|𝑋,𝑌)log𝑝(𝑘=𝑘𝑖|𝑋,𝑌)\\n𝑝(𝑘=𝑘𝑖|𝑋), (21)\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:19\\nTable 4. M2-based methods can retrieve more precise triples, and further improve the generation performance.\\nMethod Cat. Ref.Chinese Weibo (large) [137] Chinese Weibo (small) [136]\\nEntity score Generation score Entity score Generation score\\nMatch Recall BLEU-2 Dist-2 Match Recall BLEU-2 Dist-2\\nGenDS M1 [165] 0.97 0.37 3.42 4.27 0.75 0.26 2.09 1.66\\nCCM M1 [159] 1.09 0.37 4.75 4.87 0.99 0.28 3.26 2.59\\nConKADI M2 [136] - - - - 1.48 0.38 5.06 23.93'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='CCM M1 [159] 1.09 0.37 4.75 4.87 0.99 0.28 3.26 2.59\\nConKADI M2 [136] - - - - 1.48 0.38 5.06 23.93\\nTaFact M2 [137] 1.81 0.47 5.07 23.56 - - - -\\nwhere𝑁is the number of retrieved facts. When minimizing KLDivLoss, the posterior distribution\\n𝑝(𝑘|𝑋,𝑌)can be regarded as labels to apply the prior distribution 𝑝(𝑘|𝑋)for approximating\\n𝑝(𝑘|𝑋,𝑌). Finally, the total loss is written as the sum of the KLDivLoss and NLL (generation) loss.\\n4.1.3 Discussion and Analysis of Different Methods .The relevance between triples in KBs\\nand input sequences plays a central role in discovering knowledge for sequence generation. Meth-\\nods in M1typically follows the process that parses input sequence, retrieves relevant facts, and\\nsubsequently, a knowledge-aware output can be generated based on the input sequence and previ-\\nously retrieved facts. Even though the improvement by modeling KB with memory network [ 82],\\nexisting KG-enhanced methods still suffer from effectively selecting precise triples.\\nMethods of M2improve the selection of facts, in which the ground-truth responses used as the\\nposterior context knowledge to supervise the training of the prior fact probability distribution. Wu\\net al. used exact match and recall to measure whether the retrieved triples is used to generate the\\ntarget outputs [ 136]. Table 4 shows the entity recall scores of M1-based methods and M2-based\\nmethods reported in [ 136,137]. We observe that compared to M1-based methods, M2-based methods'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='methods reported in [ 136,137]. We observe that compared to M1-based methods, M2-based methods\\ncan greatly improve the accuracy of triple retrieval, as well as the generation quality.\\nThere are still remaining challenges in KB-enhanced methods. One is that retrieved facts may\\ncontain noisy information, making the generation unstable [ 61]. This problem is extremely harmful\\nin NLG tasks, e.g., KB-based question answering and task-oriented dialogue system, since the\\ninformation in KB is usually the expected entities in the response.\\n4.2 NLG Enhanced by Knowledge Graph\\nKnowledge graph (KG), as a type of structured human knowledge, has attracted great attention\\nfrom both academia and industry. A KG is a structured representation of facts (a.k.a. knowledge\\ntriplets) consisting of entities∗, relations, and semantic descriptions [ 58]. The terms of “knowledge\\nbase” and “knowledge graph” can be interchangeably used, but they do not have to be synonymous.\\nThe knowledge graph is organized as a graph, so the connections between entities are first-class\\ncitizens in it. In the KG, people can easily traverse links to discover how entities are interconnected\\nto express certain knowledge. Recent advances in artificial intelligence research have demonstrated\\nthe effectiveness of using KGs in various applications like recommendation systems [127].\\nNext, we introduce popular NLG applications that have been enhanced by knowledge graph:'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Next, we introduce popular NLG applications that have been enhanced by knowledge graph:\\n•Commonsense reasoning. It aims to empower machines to capture the human common-\\nsense from KG during generation. The methods exploit both structural and semantic informa-\\ntion of the commonsense KG and perform reasoning over multi-hop relational paths, in order\\nto augment the limited information with chains of evidence for commonsense reasoning.\\nPopular tasks in commonsense reasoning generation include abductive reasoning (e.g., the\\n𝛼NLG task) [ 7,57], counterfactual reasoning [ 56,57], and entity description generation [ 21].\\n•Dialogue system. It frequently makes use of KG for the semantics in linked entities and\\nrelations [ 97,121,151,159]. A dialogue may shift focus from one entity to another, breaking\\n∗For brevity, we use “entities” to denote both entities (e.g., prince) and concepts (e.g., musician) throughout the paper.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:20 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\none discourse into several segments, which can be represented as a linked path connecting\\nthe entities and their relations.\\n•Creative writing. This task can be found in both scientific and story-telling domains. Scien-\\ntific writing aims to explain natural processes and phenomena step by step, so each step can'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='tific writing aims to explain natural processes and phenomena step by step, so each step can\\nbe reflected as a link on KG and the whole explanation is a path [ 63,130]. In story generation,\\nthe implicit knowledge in KG can facilitate the understanding of storyline and better predict\\nwhat will happen in the next plot [45, 46, 80].\\nCompared with separate, independent knowledge triplets, knowledge graph provides compre-\\nhensive and rich entity features and relations for models to overcome the influence of the data\\ndistribution and enhance its robustness. Therefore, node embedding and relational path have played\\nimportant roles in various text generation tasks. The corresponding techniques are knowledge\\ngraph embedding (KGE) [ 131] and path-based knowledge graph reasoning [ 17]. Furthermore, it\\nhas been possible to encode multi-hop and high-order relations in KGs using the emerging graph\\nneural network (GNN) [138] and graph-to-sequence (Graph2Seq) frameworks [6].\\nDefinition 4.1 (Knowledge graph (KG)). A knowledge graph (KG) is a directed and multi-relational\\ngraph composed of entities and relations which are regarded as nodes and different types of edges.\\nFormally, a KG is defined as G=(U,E,R), whereUis the set of entity nodes and E⊆U×R×U\\nis the set of typed edges between nodes in Uwith a certain relation in the relation schema R.\\nThen given the input/output sequences in the text generation task, a subgraph of the KG which'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Then given the input/output sequences in the text generation task, a subgraph of the KG which\\nis associated with the sequences can be defined as below.\\nDefinition 4.2 (Sequence-associated K-hop subgraph). A sequence-associated K-hop subgraph\\nis defined asG𝑠𝑢𝑏=(U𝑠𝑢𝑏,E𝑠𝑢𝑏,R), whereU𝑠𝑢𝑏is the union of the set of entity nodes mapped\\nthrough an entity linking function𝜓:U×X→U 𝑠𝑢𝑏andtheir neighbors within K-hops. Similarly,\\nE𝑠𝑢𝑏⊆U𝑠𝑢𝑏×R×U𝑠𝑢𝑏is the set of typed edges between nodes in U𝑠𝑢𝑏.\\nSequence-associated subgraph provides a graphical form of the task data (i.e., sequences) and\\nthus enables the integration of KGs and the sequences into graph algorithms.\\nMany methods have been proposed to learn the relationship between KG semantics and in-\\nput/output sequences. They can be categorized into four methodologies as shown in Figure 6: (M1)\\nincorporate knowledge graph embeddings into language generation; (M2) transfer knowledge into\\nlanguage model with triplet information; (M3) perform reasoning over knowledge graph via path\\nfinding strategies; and (M4) improve the graph embeddings with graph neural networks.\\n4.2.1 M1: Incorporate Knowledge Graph Embeddings into Language Generation .Knowl-\\nedge graph embedding (KGE) techniques learn node embedding from a KG [ 131]. KGE aims to\\ncapture the semantic relatedness between entity nodes from their connectivity information (i.e.,\\ndifferent types of relations) in the KG. The primary idea is to represent entities and relations in a'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='low-dimensional vector space R𝑑, where𝑑≪|U∪R| , to reduce data dimensionality while preserv-\\ning the inherent structure of the KG. TransE [ 11] is the most widely used KGE technique. In TransE,\\ngiven a KG edge(𝑢𝑖,𝑟,𝑢𝑗), the relation is seen as a translation vector rso that the embedded entities\\nu𝑖and u𝑗can be connected with low translation error, namely u𝑖+r≈u𝑗. For example, we have\\n−−−−−→𝑇𝑜𝑘𝑦𝑜+−−−−−−−−−−−−→𝐼𝑠𝐶𝑎𝑝𝑡𝑖𝑐𝑎𝑙𝑂𝑓≈−−−−→𝐽𝑎𝑝𝑎𝑛 for the knowledge edge (Tokyo,IsCapticalOf ,Japan). As shown\\nin Figure 6(a), a common strategy of incorporating KGE into NLG is to concatenate the original\\nword representations ( x) with the corresponding entity representations ( u) from KGE [151, 159].\\n4.2.2 M2: Transfer Knowledge into Language Model with Knowledge Triplet Informa-\\ntion .The vector spaces of entity embeddings (from KGE) and word embeddings (from pre-trained\\nlanguage models) are usually inconsistent [ 80]. Beyond a simple concatenation, recent methods\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:21\\nfuturechattalkdreamPath 1Path 2futurechattalkdream\\n: GNN aggregationYeah, it ’s not a dreamto have a talkwith robot!Input textInput subgraphSequence encoderGNN encoderSequence DecoderOutput text\\nChatbasedon knowledge  is    the   futuresource conceptsKGlocate subgraphYeah, it ’s not a dreamto have a talkwith robot!Input textInput pathsSequence encoderPath encoderSequence DecoderOutput text'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Chatbasedon knowledge  is    the   futuresource conceptsKGlocate subgraph(M3) Performing path reasoning on KG(M4) Aggregating sub-KG via GNNChatbasedon knowledge  is    the   futureInput text+++++++KG embeddingLanguage generation modelPretrained through TransEidentityvector1chat[-0.1, 0.2, 0.5]2talk[0.3, 0.4, -0.2]3future[0.6, 0.1, 0.1]4based[-0.4, 0.4, 0.2]entity emb.word emb.KG-enhanced language modelParisAtLocationEiffel tower<s>   Eiffel   tower   is      at    Paris Eiffel  tower     is      at   Paris   </s>Pretrained language modeltraining with all KG triples(M1) Incorporate KGE into language generation(M2) Transfer knowledge into pretrained LM\\nFig. 6. Four typical methodologies for incorporating KG semantics into text generation.\\nhave explored to fine-tune the language models directly on knowledge graph triplets. Guan et al.\\ntransformed the commonsense triplets (in ConceptNet and ATOMIC) into readable sentences using\\ntemplates, as illustrated in Figure 6(b). And then the language model (e.g., GPT-2) is fine-tuned on\\nthe transformed sentences to learn the commonsense knowledge to improve text generation.\\n4.2.3 M3: Perform Reasoning over Knowledge Graph via Path Finding Strategies .KGE\\nlearns node representations from one-hop relations through a certain semantic relatedness (e.g.\\nTransE). However, Xiong et al. argued that an intelligent machine is supposed to be able to conduct'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='TransE). However, Xiong et al. argued that an intelligent machine is supposed to be able to conduct\\nexplicit reasoning over relational paths to make multiple inter-related decisions rather than merely\\nembedding entities in the KGs [ 141]. Take the QA task an example. The machine performs reasoning\\nover KGs to handle complex queries that do not have an obvious answer, infer potential answer-\\nrelated entities, and generate the corresponding answer. So, the challenge lies in identifying a subset\\nof desired entities and mentioning them properly in a response [ 91]. Because the connected entities\\nusually follow natural conceptual threads, they help generate reasonable and logical answers to\\nkeep conversations engaging and meaningful. As shown in Figure 6(c), path-based methods explore\\nvarious patterns of connections among entity nodes such as meta-paths and meta-graphs. Then\\nthey learn from walkable paths on KGs to provide auxiliary guidance for the generation process.\\nThe path finding based methods can be mainly divided into two categories: (1) path ranking based\\nmethods and (2) reinforcement learning (RL) based path finding methods.\\nM3.1: Path routing and ranking .Path ranking algorithm (PRA) emerges as a promising\\nmethod for learning and inferring paths on large KGs [ 65]. PRA uses random walks to perform\\nmultiple bounded depth-first search processes to find relational paths. Coupled with elastic-net'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='multiple bounded depth-first search processes to find relational paths. Coupled with elastic-net\\nbased learning [ 166], PRA picks plausible paths and prunes non-ideal, albeit factually correct\\nKG paths. For example, Tuan et al. proposed a neural conversation model with PRA on dynamic\\nknowledge graphs [ 121]. In the decoding phase, it selected an output from two networks, a general\\nGRU decoder network and a PRA based multi-hop reasoning network, at each time step. Bauer et\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:22 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nal. ranked and filtered paths to ensure both the information quality and variety via a 3-step scoring\\nstrategy: initial node scoring, cumulative node scoring, and path selection [ 5]. Ji et al. heuristically\\npruned the noisy edges between entity nodes and proposed a path routing algorithm to propagate\\nthe edge probability along multi-hop paths to the entity nodes [56].\\nM3.2: Reinforcement learning based path finding .Reinforcement learning (RL) based meth-\\nods make an agent to perform reasoning to find a path in a continuous space. These methods\\nincorporate various criteria in their reward functions of path finding, making the path finding\\nprocess flexible. Xiong et al. proposed DeepPath, the first work that employed Markov decision\\nprocess (MDP) and used RL based approaches to find paths in KGs [ 141]. Leveraging RL based'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='process (MDP) and used RL based approaches to find paths in KGs [ 141]. Leveraging RL based\\npath finding for NLG tasks typically consists of two stages [ 81,97]. First, they take a sequence\\nas input, retrieve a starting node 𝑢0onG, then perform multi-hop graph reasoning, and finally\\narrive at a target node 𝑢𝑘that incorporates the knowledge for output sequence generation. Second,\\nthey represent the sequence 𝑋and selected path Φ𝑘(𝑢0,𝑢𝑘)through two separate encoders. They\\ndecode a sequence with multi-source attentions on the input sequence and selected path. Path-based\\nknowledge graph reasoning converts the graph structure of a KG into a linear path structure that\\ncan be easily represented by sequence encoders (e.g, RNN) [ 30,97,121]. For example, Niu et al.\\nencoded selected path and input sequence with two separate RNNs and generated sequence with\\na general attention-based RNN decoder [ 97]. To enhance the RL process, Xu et al. proposed six\\nreward functions for training an agent in the reinforcement learning process. For example, the\\nfunctions looked for accurate arrival at the target node as well as the shortest path between the\\nstart and target node, i.e., minimize the length of the selected path Φ𝑘(𝑢0,𝑢𝑘)[142].\\n4.2.4 M4: Improve the Graph Embeddings with Graph Neural Networks .The contexts\\nsurrounding relevant entities on KGs play an important role in understanding the entities and'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='surrounding relevant entities on KGs play an important role in understanding the entities and\\ngenerating proper text about their interactions [ 46,63]. For example, in scientific writing, it is\\nimportant to consider the neighboring nodes of relevant concepts on a taxonomy and/or the\\nglobal context of a scientific knowledge graph [ 63]. However, neither KGE nor relational path\\ncould fully represent such information. Graph-based representations aim at aggregating the con-\\ntext/neighboring information on graph data; and recent advances of GNN models demonstrate a\\npromising advancement in graph-based representation learning [ 138]. In order to improve text\\ngeneration, graph-to-sequence (Graph2Seq) models encode the structural information of the KG in\\na neural encoder-decoder architecture [ 6]. Since then, GNNs have been playing an important role\\nin improving the NLG models. They have been applied to both encoding anddecoding phases.\\nLearning KG-aware input text representation with GNNs (Encoding). For encoding phase,\\na general process of leveraging GNNs for incorporating KG is to augment semantics of a word\\nin the input text by combining with the vector of the corresponding entity node vector to the\\nword on the KG [ 46,54,150,151,159]. A pre-defined entity linking function 𝜓:U×X→U 𝑠𝑢𝑏\\nmaps words in the input sequence to entity nodes on the KG. Given an input sequence, all the\\nlinked entities and their neighbors within 𝐾-hops compose a sequence-associated K-hop subgraph'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='linked entities and their neighbors within 𝐾-hops compose a sequence-associated K-hop subgraph\\nG𝑠𝑢𝑏(formally defined in Definition 4.2). For each entity node in G𝑠𝑢𝑏, it uses the KG structure as\\nwell as entity and edge features (e.g., semantic description if available) to learn a representation\\nvector u. Specifically, a GNN model follows a neighborhood aggregation approach that iteratively\\nupdates the representation of a node by aggregating information from its neighboring nodes and\\nedges. After 𝑘iterations of aggregation, the node representation captures the structural information\\nwithin its𝑘-hop neighborhood. Formally, the 𝑘-th layer of a node 𝑢∈U𝑠𝑢𝑏is:\\nu(𝑘)=Combine 𝑘(u(𝑘−1),Aggregate 𝑘(\\x08\\n(u(𝑘−1)\\n𝑖,e(𝑘−1)\\n𝑖𝑗,u(𝑘−1)\\n𝑗):∀(𝑢𝑖,𝑒𝑖𝑗,𝑢𝑗)∈N(𝑢)\\t\\n)).(22)\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:23\\nTable 5. Tasks, datasets and KG sources used in different KG-enhanced papers. We also compared the\\nperformance of different models before and after incorporating KG into the generation process, in which “w/o\\nKG” performance comes from the best baseline method; “with KG” comes from the KG-enhanced method.\\nTasks Methods Ref. Cat.Dataset Information Effect of KG KG\\nName #Instance w/o KG with KG ΔBLEU source\\nCommon-\\nsense\\nreasoningKG-BART [80] M4 CommonGen 77,449 28.60 30.90 +2.30 ConceptNet\\nCE-PR [56] M3 ComVE 30,000 15.70 17.10 +1.60 ConceptNet\\nGRF [57] M4𝛼NLG-ART 60,709 9.62 11.62 +2.00 ConceptNet'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='GRF [57] M4𝛼NLG-ART 60,709 9.62 11.62 +2.00 ConceptNet\\nMGCN [21] M3 EntDesc 110,814 24.90 30.00 +4.30 Self-built KG\\nStory\\ngenerationIE+MSA [46] M4 ROCStories98,1628.25 9.36 +1.11 ConceptNet\\nGRF [57] M4 (split-1) 10.40 11.00 +0.60 ConceptNet\\nKEPM [45] M2ROCStories98,162 14.10 14.30 +0.20ConceptNet\\n(split-2) & ATOMIC\\nMRG [156] M3 VisualStory 50,000 3.18 3.23 +0.05 ConceptNet\\nScientific\\nwritingGraphWriter [63] M4 AGENDA 40,000 12.20 14.30 +1.90 Self-built KG\\nPaperRobot [130] M4 PaperWriting 27,001 9.20 13.00 +3.80 Self-built KG\\nDialogue\\nsystemConceptFlow [151] M4 Reddit-10M 3,384K 1.62 2.46 +0.84 ConceptNet\\nAKGCM [81] M3 EMNLP dialog 43,192 32.45 30.84 -1.61 Self-built KG\\nAKGCM [81] M3 ICLR dialog 21,569 6.74 6.94 +0.20 Self-built KG\\nQuestion\\nansweringMHPGM [5] M3 NarrativeQA 46,765 19.79 21.07 +1.28 Self-built KG\\nTable 6. Qualitative comparison between different KG-enhanced methods.\\nMethods Ref.Method category Multi-hop info. Multi-hop path Auxiliary (knowledge\\nM1 M2 M3 M4 aggregation reasoning related) task(s)\\nTHOTH [92] ✓ ×× ×\\nCCM [159] ✓×, one-hop× ×\\nKEPM [45] ✓×× ×\\nAKGCM [81] ✓× ✓, Markov decision ✓, Path selection\\nIE+MSA [46] ✓✓, by GNN× ×\\nConceptFlow [151] ✓✓, by GNN× ×\\nCE-PR [56] ✓× ✓, Path routing ✓, Concept selection\\nGRF [57] ✓✓, by GNN ✓, Path scoring ✓, Link prediction\\nThe sub-graph representation h𝑠𝑢𝑏𝐺 is learned thorough a Readout(·)function from all entity node\\nrepresentations (i.e., h𝑠𝑢𝑏𝐺=Readout(\\x08\\nu(𝑘),𝑢∈U𝑠𝑢𝑏\\t\\n). Zhou et al. was the first to design such'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='representations (i.e., h𝑠𝑢𝑏𝐺=Readout(\\x08\\nu(𝑘),𝑢∈U𝑠𝑢𝑏\\t\\n). Zhou et al. was the first to design such\\na knowledge graph interpreter to enrich the context representations with neighbouring concepts\\non ConceptNet using graph attention network (GAT) [159].\\nDynamically attending KG representation (Decoding). The sequence decoder uses attention\\nmechanism to find useful semantics from the representation of KG as well as the hidden state of\\nthe input text, where the KG’s representation is usually generated by GNNs. Specially, the hidden\\nstate is augmented by subgraph representation h𝑠𝑢𝑏𝐺, i.e., s0=h𝑛⊕h𝑠𝑢𝑏𝐺 [6]. Then, the decoder\\nattentively reads the retrieved subgraph to obtain a graph-aware context vector. Then it uses the\\nvector to update the decoding state [ 46,57,80,151,159]. It adaptively chooses a generic word or\\nan entity from the retrieved subgraph to generate output words. Because graph-level attention\\nalone might overlook fine-grained knowledge edge information, some recent methods adopted the\\nhierarchical graph attention mechanism [ 46,80,159]. It attentively read the retrieved subgraph\\nG𝑠𝑢𝑏and then attentively read all knowledge edges E𝑠𝑢𝑏involved inG𝑠𝑢𝑏. Ji et al. added a relevance\\nscore that reflected the relevancy of the knowledge edge according to the decoding state [57].\\n4.2.5 Discussion and Analysis of the Methodologies and Methods .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='4.2.5 Discussion and Analysis of the Methodologies and Methods .\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:24 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nPros and cons. Knowledge graph embedding ( M1) was the earliest attempt to embed com-\\nponents of a KG including entities and relations into continuous vector spaces and use them to\\nimprove text generation. Those entity and relation embeddings can simply be used to enrich\\ninput text representations (e.g., concatenating embeddings), bridging connections between entity\\nwords linked from input text in latent space. Because the graph projection and text generation are\\nperformed as two separate steps, the embedding vectors from knowledge graph and the hidden\\nstates from input text were in two different vector spaces. The model would have to learn to bridge\\nthe gap, which might make a negative impact on the performance of text generation.\\nFine tuning pre-trained language models on the KG triplets ( M2) can eliminate the gap between\\nthe two vector spaces. Nevertheless, M1 and M2 share two drawbacks. First, they only preserve\\ninformation of direct (one-hop) relations in a KG, such as pair-wise proximity in M1 and KG triplet\\nin M2, but ignore the indirect (multi-hop) relations of concepts. The indirect relations may provide\\nplausible evidence of complex reasoning for some text generation tasks. Second, from the time'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='plausible evidence of complex reasoning for some text generation tasks. Second, from the time\\nKGs were encoded in M1 or M2 methods, the generation models would no longer be able to access\\nthe KGs but their continuous representations. Then the models could not support reasoning like\\ncommonsense KG reasoning for downstream tasks. Due to these two reasons, M1 and M2 were\\noften used to create basic KG representations upon which the KG path reasoning (M3) and GNNs\\n(M4) could further enrich the hidden states [151, 159].\\nThe path finding methods of KG reasoning ( M3) perform multi-hop walks on the KGs beyond\\none-hop relations. It enables reasoning that is needed in many text generation scenarios such as\\ncommonsense reasoning and conversational question answering. At the same time, it provides\\nbetter interpretability for the entire generation process, because the path selected by the KG\\nreasoning algorithm will be explicitly used for generation. However, the selected paths might not\\nbe able to capture the full contexts of the reasoning process due to the limit of number. Besides,\\nreinforcement-learning based path finding uses heuristic rewards to drive the policy search, making\\nthe model sensitive to noises and adversarial examples.\\nThe algorithms of GNN and Graph2Seq ( M4) can effectively aggregate semantic and structural\\ninformation from multi-hop neighborhoods on KGs, compared to M3 that considers multi-hop'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='information from multi-hop neighborhoods on KGs, compared to M3 that considers multi-hop\\npaths. Therefore, the wide range of relevant information can be directly embedded into the en-\\ncoder/decoder hidden states. Meanwhile, M4 enables back propagation for jointly optimizing text\\nencoder and graph encoder. Furthermore, the attention mechanism that has been applied in GNN\\nand Graph2Seq (e.g., graph attention) can explain the model’s output at some extent, though the\\nmulti-hop paths from M3 has better interpretability.\\nM3 and M4 are able to use multi-hop relational information, compared to M1 and M2. However,\\nthey have two weak points. First, they have higher complexity than M1 and M2. In M3, the action\\nspace of path finding algorithms can be very large due to the large size and sparsity of the knowledge\\ngraph. In M4, the decoder has to attentively read both input sequence and knowledge graph. Second,\\nthe subgraphs retrieved by M3 and M4 might provide low coverage of useful concepts for generating\\nthe output. For example, people use ConceptNet, a widely used commonsense KG, to retrieve the\\nsubgraph on three generative commonsense reasoning tasks. The task datasets are ComVE [ 57],\\n𝛼-NLG [ 7], and ROCSories [ 46]. We found 25.1% / 24.2% / 21.1% of concepts in the output could\\nbe found on ConceptNet, but only 11.4% / 8.1% / 5.7% of concepts in the output can be found on\\nthe retrieved 2-hop sequence-associated subgraph, respectively. It means that a large portion of'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='the retrieved 2-hop sequence-associated subgraph, respectively. It means that a large portion of\\nrelevant concepts on the KG are not utilized in the generation process.\\nQuantitative analysis. Table 5 summarizes tasks, datasets, and KG sources used in existing\\nKG-enhanced works. Three important things should be mentioned. First, all the datasets in the table\\nare public, and we include their links in Table 11. CommonGen [ 76], ComVE [ 124] and𝛼-NLG [ 7]\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:25\\nB: I like country music. It is the most listened to rush hour radio genre.3 relevant docsSequence encoderDocument encoderSequence DecoderOutput text\\n(a) M1: Retrieve relevant documents, use them for generation Input textB: It made $279,167,575 at the box office.Background docSequence encoderDocument encoderSequence DecoderOutput text\\n(b) M2: Read background document and generate outputInput text'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='(b) M2: Read background document and generate outputInput text\\n(1) In 2009, country music was the most lis-tenedto rush hourradio genrein the US.(2) Country is a musical genre that origin-atedin the southern US in the early 1920s.(3) George Glenn Jones was an American musician, singer and songwriter.A dialogue between A and BA: Do you know George Glenn Jones?B: Yes, he was a famous American        singer and songwriter.A: Cool! You sure know some stuff about country music!Retrieve from WikipediaBackground: ... but if you like ben stiller, go see“meet the fockers”. Dustin’s antics will favorite character was jack (the older one), because he was so serious but always plotting and putting up a front. I think it was $279,167,575 awards ASCAP film and television music awards 2005 top box office films MTV… (~250 words)A dialogue between A and BA: That name is so ridiculous but funny. B: First off, the writers did not miss a single opportunity to play off ofthe name “focker”. A: Yeah, I heard it was a pretty successfulmovie overall.A background-based conversion (BBC)\\nFig. 7. The left figure demonstrates retrieving relevant documents, then using them for generation; the right\\nfigure demonstrate reading background document to conduct conversions.\\nhave a public leaderboard for competition. Second, for KG sources, we observe that eight (57.1%)\\npapers use ConceptNet as external resource, while six (42.9%) papers constructed their own KGs'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='papers use ConceptNet as external resource, while six (42.9%) papers constructed their own KGs\\nfrom domain-specific corpus. For example, Koncel et al. created a scientific knowledge graph by\\napplying the SciIE tool (science domain information extraction) [ 63]. Besides, Zhao et al. compared\\nthe performance of models between using ConceptNet and using a self-built KG, and found the\\nmodel with self-built KG could work better on story generation and review generation tasks [ 156].\\nThird, we observed that KG-enhanced NLG methods made the largest improvement on generative\\ncommonsense reasoning tasks, in which the average improvement is +2.55% in terms of ΔBLEU,\\nwhile the average improvement on all different tasks is +1.32%.\\nQualitative analysis. Table 6 compares different KG-enhanced methods from three dimensions:\\nmulti-hop information aggregation, multi-hop path reasoning, and auxiliary knowledge graph\\nrelated tasks. M3 is commonly used for multi-hop path reasoning and M4 is used for multi-hop\\ninformation aggregation, except that CCM [ 159] only aggregates one-hop neighbors. Besides, the\\nauxiliary KG-related tasks are often used to further help the model learn knowledge from the KG.\\nFor example, ablation studies in [ 56,57,81] show that the tasks of path selection, concept selection\\nand link prediction can further boost the generation performance. GRF [ 57] learns these three\\nabilities at the same time. It achieves the state-of-art performance on three generation tasks.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='abilities at the same time. It achieves the state-of-art performance on three generation tasks.\\n4.3 NLG enhanced by Grounded Text\\nKnowledge grounded text refers to textual information that can provide additional knowledge\\nrelevant to the input sequence. The textual information may not be found in training corpora\\nor structured databases, but can be obtained from massive textual data from online resources.\\nThese online resources include encyclopedia (e.g., Wikipedia), social media (e.g., Twitter), shopping\\nwebsites (e.g., Amazon reviews). Knowledge grounded text plays an important role in understanding\\nthe input sequence and its surrounding contexts. For example, Wikipedia articles may offer textual\\nexplanations or background information for the input text. Amazon reviews may contain necessary\\ndescriptions and reviews needed to answer a product-related question. Tweets may contain people’s\\ncomments and summaries towards an event. Therefore, knowledge grounded text is often taken as\\nan important external knowledge source to help with a variety of NLG applications.\\nNext, we introduce popular NLG applications enhanced by knowledge grounded text:\\n•Dialogue system. Building a fully data-driven dialogue system is difficult since most of\\nthe universal knowledge is not presented in the training corpora [ 42]. The lack of universal\\nknowledge considerably limits the appeal of fully data-driven generation methods, as they'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='knowledge considerably limits the appeal of fully data-driven generation methods, as they\\nare bounded to respond evasively or defectively and seldom include meaningfully factual\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:26 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\ncontents. To infuse the response with factual information, an intelligent machine is expected\\nto obtain necessary background information to produce appropriate response.\\n•Summarization. Seq2Seq models that purely depend on the input text tend to “lose control”\\nsometimes. For example, 3% of summaries contain less than three words, and 4% of summaries\\nrepeat a word for more than 99 times as mentioned in [ 13]. Furthermore, Seq2Seq models\\nusually focus on copying source words in their exact order, which is often sub-optimal in\\nabstractive summarization. Therefore, leveraging summaries of documents similar as the\\ninput document as templates can provide reference for the summarization process [ 13,129].\\n•Question answering (QA). It is often difficult to generate proper answers only based on\\nthe given question. For example, without knowing any information of an Amazon product, it\\nis hard to deliver satisfactory answer to the user questions such as “Does the laptop have a\\nlong battery life?” or“Is this refrigerator frost-free?” So, the product description and customer'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='long battery life?” or“Is this refrigerator frost-free?” So, the product description and customer\\nreviews can be used as a reference for answering product-related questions [9, 16].\\nTo handle different kinds of relationships between grounded text and input/output sequences,\\nthese methods can be categorized into two methodologies as shown in Figure 7: (M1) guiding gener-\\nation with retrieved information; (M2) modeling background knowledge into response generation.\\n4.3.1 M1: Guiding Generation with Retrieved Information .Because knowledge grounded\\ntext is not presented in the training corpora, an idea is to retrieve relevant textual information (e.g.,\\na review, a relevant document, a summary template) from external sources based on the input text\\nand to incorporate the retrieved grounded text into the generation process. This process is similar\\nto designing knowledge acquisition and incorporation of KBs and KGs in text generation tasks.\\nThe difference is that ground text is unstructured and noisy. So, researchers design knowledge\\nselection and incorporation methods to address the challenges. Based on the number of stages,\\nwe further divide related methods into two categories: retrieve-then-generate (also known as\\nretrieval-augmented generation, short as RAG, in many existing papers [ 64,68,99]) methods\\n(2-stage methods) and retrieve, rerank and rewrite methods (3-stage methods).\\nM1.1: Retrieval-augmented generation (RAG) .RAG follows a two-stage process: retrieval'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='M1.1: Retrieval-augmented generation (RAG) .RAG follows a two-stage process: retrieval\\nand generation. Specially, as shown in Figure 7(a), a retriever 𝑝(𝑍|𝑋)first returns (usually top-K\\ntruncated) distributions over text passages given a query 𝑋, and then a generator 𝑝(𝑦𝑖|𝑋,𝑍,𝑦 1:𝑖−1)\\ngenerates a current token based on a context of the previous tokens 𝑦1:𝑖−1, the original input 𝑋\\nand a retrieved passage 𝑍. Methods for retrieving fact or review snippets are various, including\\nmatching from a collection of raw text entries indexed by named entities [ 42]; scoring relevant\\ndocuments within a large collection by statistical approaches such as BM25 [ 27], or neural-based\\nretrieval approaches such as dense paragraph retrieval (DPR) [ 68]. For training the retriever and\\ngenerator, most of existing work has jointly optimized these two components, without any direct\\nsupervision on what document should be retrieve [ 64,68]. However, by asking human experts to\\nlabel what document should be retrieved and adding the retrieval loss (resulting in a multi-task\\nlearning setting), the generation performance can be greatly improved [ 27,61], though the labelling\\nprocess is an extremely time-consuming and labor-intensive task.\\nGhazvininejad et al. proposed a knowledge grounded neural conversation model (KGNCM),\\nwhich is the first work to retrieve review snippets from Foursquare and Twitter. Then it incorporates'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='the snippets into dialogue response generation [ 42]. It uses an end-to-end memory network [ 116]\\nto generate responses based on the selected review snippets. Lewis et al. introduced a general\\nretrieval-augmented generation (RAG) framework by leveraging a pre-trained neural retriever and\\ngenerator. It can be easily fine-tuned on downstream tasks, and it has demonstrated state-of-the-\\nart performance on various knowledge intensive NLG tasks [ 68]. Recently, the fusion-in-decoder\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:27\\nTable 7. Tasks, datasets and evidence sources used in retrieve-then-generate (M1) papers. We also include\\ntheir document(d)/sentence(s) retrieval space and the number of retrieved document(d)/sentence(s).\\nEvidenceTasks Methods Ref.Dataset Information Retrieval # Retri-\\nsources Name #Instance space (d/s) eved d/s\\nWikipediaDialogue\\nsystemMemNet [27] Wizard of\\nWikipedia (WoW)22,311 5.4M/93M7\\nSKT [61] 7\\nQuestion\\nansweringRAG [68] MS-MARCO 267,287 21M/- 10\\nBART+DPR [99]ELI5 274,7413.2M/- -\\nRT+C-REALM [64] 3.2M/- 7\\nArgument\\ngenerationH&W [53]ChangeMyView 287,1525M/- 10\\nCANDELA [52] 5M/- 10\\nOnline platform\\n(e.g., Amazon)Dialogue (for\\nbusiness)AT2T [62] Amazon books 937,032 -/131K 10\\nKGNCM [42] Foursquare 1M -/1.1M 10\\nGigawordsSummari-\\nzationR3Sum [13]Gigawords 3.8M-/3.8M 30\\nBiSET [129] -/3.8M 30\\nTable 8. Qualitative comparison between different grounded text enhanced methods.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Table 8. Qualitative comparison between different grounded text enhanced methods.\\nMethods Ref.Method categoryRetrieval supervisionRetriever Number\\nM1.1 M1.2 M2 pre-training of stages\\nMemNet [27] ✓ ✓, Human annotated labels × 2\\nSKT [61] ✓ ✓, Human annotated labels × 2\\nR3Sum [13] ✓ ✓, Pseudo labels × 3, with rerank\\nBiSET [129] ✓ ✓, Pseudo labels × 3, with rerank\\nRefNet [87] ✓× × 1, no retrieval\\nGLKS [107] ✓× × 1, no retrieval\\nRAG [68] ✓ × ✓, DPR 2\\nKilt [99] ✓ × ✓, DPR 2\\nRT+C-REALM [64] ✓ × ✓, REALM 2\\nmethods (i.e., the decoder performs attention over the concatenation of the resulting representations\\nof all retrieved passages [ 72,145]) could even outperform RAG as reported in KILT benchmark [ 99].\\nM1.2: Retrieve, rerank and rewrite ( 𝑅3).Different from RAG, a 𝑅3-based method is expected\\nto retrieve a most precise reference document that can be directly used for rewriting/editing. 𝑅3-\\nbased method has proved successful in a number of NLG tasks such as machine translation [ 44],\\nand summarization [ 13,129]. In summarization, Seq2Seq models that purely depend on the input\\ndocument to generate summaries tend to deteriorate with the accumulation of word generation, e.g.,\\nthey generate irrelevant and repeated words frequently [13, 129]. Template-based summarization\\nassume the golden summaries of the similar sentences (i.e., templates) can provide a reference point\\nto guide the input sentence summarization process [ 13,129]. These templates are often called soft'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='to guide the input sentence summarization process [ 13,129]. These templates are often called soft\\ntemplates in order to distinguish from the traditional rule-based templates. Soft template-based\\nsummarization typically follows a three-step design: retrieve, rerank, and rewrite. The step of\\nretrieval aims to return a few candidate templates from a summary collection. The reranking\\nidentifies the best template from the retrieved candidates. And the rewriting leverages both the\\nsource document and template to generate more faithful and informative summaries.\\nDifference between RAG and 𝑅3.Compared with 𝑅3-based methods, RAG-based have several\\ndifferences, including less of emphasis on lightly editing a retrieved item, but on aggregating\\ncontent from several pieces of retrieved content, as well as learning latent retrieval, and retrieving\\nevidence documents rather than related training pairs.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:28 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\n4.4 M2: Modeling Background Knowledge into Response Generation\\nBackground document, with more global and comprehensive knowledge, has been often used for\\ngenerating informative responses and ensuring a conversation to not deviate from its topic. Keeping\\na conversation grounded on a background document is referred as background based conversation'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='a conversation grounded on a background document is referred as background based conversation\\n(BBC) [ 9,90]. Background knowledge plays an important role in human-human conversations.\\nFor example, when talking about a movie, people often recall important points (e.g., a scene or\\nreview about the movie) and appropriately mention them in the conversation context. Therefore, an\\nintelligent NLG model is expected to find an appropriate background snippet and generate response\\nbased on the snippet. As shown in Figure 7(b), the task of BBC is often compared with machine\\nreading comprehension (MRC), in which a span is extracted from the background document as a\\nresponse to a question [ 105]. However, since BBC needs to generate natural and fluent responses,\\nthe challenge lies in not only locating the right semantic units in the background, but also referring\\nto the right background information at the right time in the right place during the decoding phase.\\nAs MRC models tie together multiple text segments to provide a unified and factual answer,\\nmany BBC models use the same idea to connect different pieces of information and find the\\nappropriate background knowledge based on which the next response is to be generated [ 87,101].\\nFor instance, Qin et al. proposed an end-to-end conversation model that jointly learned response\\ngeneration together with on-demand machine reading [ 101]. The MRC models can effectively'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='generation together with on-demand machine reading [ 101]. The MRC models can effectively\\nencode the input utterance by treating it as a question in a typical QA task (e.g., SQuAD [ 105]) and\\nencode the background document as the context. Then, they took the utterance-aware background\\nrepresentation as input into decoding phase.\\n4.4.1 Discussion and Analysis of Different Methods .\\nPros and cons. For M1, guiding generation with retrieved information explicitly exposes the\\nrole of world knowledge by asking the model to decide what knowledge to retrieve and use\\nduring language generation. Since retrieval-augmented generation (RAG) captures knowledge in a\\ninterpretable and modular way, it is often used for knowledge-intensive tasks such as long-form QA\\nand argument generation. However, a knowledge retriever is expected to retrieve documents from\\na large-scale corpus, e.g., the entire Wikipedia, which causes significant computational challenge.\\nBesides, one input often requires retrieved text whose amount is much larger than the input itself\\n(as indicated in Table 7), leading to serious information overwhelming for the generation model.\\nFor M2, background based conversations (BBCs) avoid generating generic responses in a dialogue\\nsystem and are able to generate more informative responses by exploring related background\\ninformation. However, existing methods still cannot solve inherent problems effectively, such as'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='information. However, existing methods still cannot solve inherent problems effectively, such as\\ntending to break a complete semantic unit and generate shorter responses [87].\\nQualitative analysis. Table 7 summarizes tasks, datasets and evidence sources used in existing\\ngrounded text enhanced work. Three important things should be mentioned. First, all the datasets in\\nthe table are public, and we include their links in Table 11. Second, Wikipedia is the most commonly\\nused evidence source since it is the largest free online encyclopedia. Besides, some online platforms\\ncontain plenty of product-related textural information, e.g., product reviews on Amazon, which\\nare often used to build up task/goal oriented dialogue systems for business purpose. Third, the\\nretrieval space of candidate documents are usually larger than 1 million and only 7-10 documents\\nare selected. So, the process of retrieving relevant documents is challenging.\\nTable 8 compares different grounded text enhanced methods from three dimensions: retrieval\\nsupervision, pre-training of the retriever, and number of stages. First, as mentioned above, retrieving\\nrelevant documents from a large candidate set is a challenging task. To improve the retrieval\\naccuracy, four (57.1%) papers added the retrieval supervision either by human annotated labels\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:29'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='or pseudo labels, resulting in a multi-task learning setting. Besides, three (42.9%) papers used pre-\\ntrained language models to produce document representation for better retrieval. Though existing\\nwork has greatly improved the retrieval accuracy, the performance is still far from satisfactory in\\nmany text generation tasks [ 64,68]. How to learn mutually enhancement between retrieval and\\ngeneration is still a promising direction in the grounded text enhanced text generation systems.\\n5 BENCHMARK, TOOLKIT AND LEADERBOARD PERFORMANCE\\nThe development of general evaluation benchmarks for text generation helps to promote the\\ndevelopment of research in related fields. Existing text generation benchmarks did not specially\\nfocus on choosing the tasks and datasets that have been widely used for knowledge-enhanced\\ntext generation. Therefore, we re-screened from the existing four text generation benchmarks, i.e.,\\nGLGE [ 77], GEM [ 40], KilT [ 99], GENIE [ 60], and determined 9 benchmark datasets for evaluating\\nknowledge-enhanced NLG methods. Here is our criteria for selection:\\n•We only consider benchmark datasets that have open-access downloading link.\\n•We focus on diverse text generation tasks, involving various applications.\\n•We select at most three benchmark datasets for each text generation task.\\n•We include a mix of internal and external knowledge focused datasets.\\n•We prefer multi-reference datasets for robust automatic evaluation.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='•We prefer multi-reference datasets for robust automatic evaluation.\\nBased on the benchmark selection criteria, we finalize 9 knowledge-centric tasks that covers\\nvarious NLG tasks, including commonsense reasoning, text summarization, question generation,\\ngenerative question answering, and dialogue. The data statistics is shown in Table 9. Descriptions\\nand dataset links are listed as follows:\\n•Wizard of Wikipedia (WOW): It is an open-domain dialogue dataset, where two speakers\\nconduct an open-ended conversion that is directly grounded with knowledge retrieved from\\nWikipedia. (Data link: https://parl.ai/projects/wizard_of_wikipedia/)\\n•CommonGen: It is a generative commonsense reasoning dataset. Given a set of common\\nconcepts, the task is to generate a coherent sentence describing an everyday scenario using\\nthese concepts. (Data link: https://inklab.usc.edu/CommonGen/)\\n•𝛼NLG-ART: It is a generative commonsense reasoning dataset. Given the incomplete obser-\\nvations about the world, the task it to generate a valid hypothesis about the likely explanations\\nto partially observable past and future. (Data link: http://abductivecommonsense.xyz/)\\n•ComVE: It is a generative commonsense reasoning dataset. The task is to generate an\\nexplanation given a counterfactual statement for sense-making. (Data link: https://github.\\ncom/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation\\n•ELI5: It is a dataset for long-form question answering. The task is to produce explana-'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='•ELI5: It is a dataset for long-form question answering. The task is to produce explana-\\ntory multi-sentence answers for diverse questions. Web search results are used as evidence\\ndocuments to answer questions. (Data link: https://facebookresearch.github.io/ELI5/)\\n•SQuAD: It is a dataset for answer-aware question generation. The task is to generate a\\nquestion asks towards the given answer span based on a given text passage or document.\\n(Data link: https://github.com/magic282/NQG)\\n•CNN/DailyMail (CNN/DM): It is a dataset for summarization. Given a news aticles, the goal\\nis to produce a summary that represents the most important or relevant information within\\nthe original content. (Data link: https://www.tensorflow.org/datasets/catalog/cnn_dailymail)\\n•Gigaword: It is a dataset for summarization. Similar with CNN/DM, the goal is to generate a\\nheadline for a news article. (Data link: https://www.tensorflow.org/datasets/catalog/gigaword)\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:30 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nTable 9. We choose 9 knowledge-enhanced NLG benchmark datasets. These datasets have been included in\\nfour existing general NLG benchmarks (i.e., GLGE [ 77], GEM [ 40], Kilt [ 99], GENIE [ 60]) or in SemEval tasks.\\nTasks Ref.Dataset Information Leader In which NLG Papers including\\nName #Train #Dev. #Test board benchmark this dataset\\nDialogue\\nsystem[27]Wizard of'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Name #Train #Dev. #Test board benchmark this dataset\\nDialogue\\nsystem[27]Wizard of\\nWikipedia18,430 1,948 1,933 ✓∗Kilt [27, 61, 74]\\n[154] PersonaChat 122,499 14,602 14,056× GLGE [27, 74]\\nQuestion\\nanswering[31] ELI5 272,634 1,507 600 ✓‡Kilt [64, 99]\\nQuestion\\ngeneration[105] SQuAD 75,722 10,570 11,877× GLGE [19, 22, 133]\\nCommonsense\\nreasoning[76] CommonGen 67,389 4,018 6,042 ✓§GEM [32, 80, 126]\\n[7]𝛼NLG-ART 50,481 7,252 2,976 ✓⁄pilcrowGENIE [7, 57]\\n[124] ComVE 25,596 1,428 2,976 ✓∥SemEval [56, 57]\\nSummarization[109] CNN/DM 287,226 13,368 11,490 ✓∗∗GLGE [33, 41, 163]\\n[109] Gigaword 3.8M 189K 1,951 ✓††GLGE [13, 59, 69]\\n•PersonaChat: It is an open-domain dialogue dataset. It presents the task of making chit-\\nchat more engaging by conditioning on profile information. (Data link: https://github.com/\\nfacebookresearch/ParlAI/tree/master/projects/personachat)\\n6 DISCUSSION ON FUTURE DIRECTIONS\\nMany efforts have been conducted to tackle the problem of knowledge-enhanced text generation\\nand its related applications. To advance the field, there remains several open problems and future\\ndirections. Designing more effective ways to represent knowledge and integrate them into the\\ngeneration process is still the most important trend in knowledge-enhanced NLG systems. From a\\nbroader perspective, we provide three directions that make focusing such efforts worthwhile now:\\n(i) incorporating knowledge into visual-language generation tasks, (ii) learning knowledge from'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='(i) incorporating knowledge into visual-language generation tasks, (ii) learning knowledge from\\nbroader sources, especially pre-trained language models, (iii) learning knowledge from limited\\nresources, (iv) learning knowledge in a continuous way.\\n6.1 Incorporate Knowledge into Visual-Language Generation Tasks\\nBeyond text-to-text generation tasks, recent years have witnessed a growing interest in visual-\\nlanguage (VL) generation tasks, such as describing visual scenes [ 49], and answering visual-related\\nquestions [ 85]. Although success has been achieved in recent years on VL generation tasks, there\\nis still room for improvement due to the fact that image-based factual descriptions are often not\\nenough to generate high-quality captions or answers [ 162]. External knowledge can be added in\\norder to generate attractive image/video captions. We observed some pioneer work has attempted\\nto utilize external knowledge to enhance the image/video captioning tasks. For example, Tran et\\nal. proposed to detect a diverse set of visual concepts and generate captions by using an external\\nknowledge base (i.e., Freebase), in recognizing a broad range of entities such as celebrities and\\nlandmarks [ 120]. Zhou et al. used a commonsense knowledge graph (i.e., ConceptNet), to infer a\\n∗https://parl.ai/projects/wizard_of_wikipedia\\n†https://nikitacs16.github.io/holl-e-website/\\n‡https://facebookresearch.github.io/ELI5/\\n§https://inklab.usc.edu/CommonGen/leaderboard.html'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='‡https://facebookresearch.github.io/ELI5/\\n§https://inklab.usc.edu/CommonGen/leaderboard.html\\n⁄pilcrowhttps://leaderboard.allenai.org/genie-anlg/submissions/public\\n∥https://competitions.codalab.org/competitions/21080#results\\n∗∗https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail\\n††https://paperswithcode.com/sota/text-summarization-on-gigaword\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:31\\nset of terms directly or indirectly related to the words that describe the objects found in the scene\\nby the object recognition module [ 162]. In addition, Mao et al . [85] proposed a neuro-symbolic\\nlearner for improving visual-language generation tasks (e.g., visual question answering).\\nHowever, existing approaches for knowledge-enhanced visual-language generation tasks still\\nhave a lot of space for exploration. Some promising directions for future work include using other\\nknowledge sources, such as retrieving image/text to help solve open-domain visual question answer-\\ning and image/video captioning tasks; bringing structured knowledge for providing justifications\\nfor the captions that they produce, tailoring captions to different audiences and contexts, etc.\\n6.2 Learning Knowledge from Broader Sources\\nMore research efforts should be spent on learning to discover knowledge more broadly and combine\\nmultiple forms of knowledge from different sources to improve the generation process. More'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='multiple forms of knowledge from different sources to improve the generation process. More\\nknowledge sources can be but not limited to network structure, dictionary and table. For examples,\\nYu et al. [ 147] and An et al. [ 2] augmented the task of scientific papers intention detection and\\nsummarization by introducing the citation graph; Yu et al. augmented the rare word representations\\nby retrieving their descriptions from Wiktionary and feed them as additional input to a pre-\\ntrained language model [ 148]. Besides, structured knowledge and unstructured knowledge can\\nplay a complementary role in enhancing text generation. To improve knowledge richness, Fu et al.\\ncombined both structured (knowledge base) and unstructured knowledge (grounded text) [34].\\nLeveraging Knowledge from Pre-trained Language Models. Pre-trained language models can learn a\\nsubstantial amount of in-depth knowledge from data without any access to an external memory, as a\\nparameterized implicit knowledge base [ 68,104]. However, as mentioned in [ 45], directly fine-tuning\\npre-trained language generation models on the story generation task still suffers from insufficient\\nknowledge by representing the input text thorough a pre-trained encoder, leading to repetition,\\nlogic conflicts, and lack of long-range coherence in the generated output sequence. Therefore,\\ndiscovering knowledge from pre-trained language models can be more flexible, such as knowledge'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='discovering knowledge from pre-trained language models can be more flexible, such as knowledge\\ndistillation, data augmentation, and using pre-trained models as external knowledge [ 100]. More\\nefficient methods of obtaining knowledge from pre-trained language models are expected.\\n6.3 Learning Knowledge from Limited Resources\\nMost of current NLG research conduct on extensively labelled data to favor model training. However,\\nthis is in contrast to many real-world application scenarios, where only a few shots of examples are\\navailable for new domains. Limited data resources lead to limited knowledge that can be learnt in new\\ndomains. For examples, learning topical information of a dialogue occurring under a new domain\\nis difficult since the topic may be rarely discussed before; constructing a syntactic dependency\\ngraph of a sequence in a low-resource language is hard since many linguistic features are of great\\nuniqueness. Besides, external knowledge bases are often incomplete and insufficient to cover full\\nentities and relationships due to the human costs of collecting domain-specific knowledge triples.\\nTherefore, quick domain adaptation is an essential task in text generation tasks. One potential route\\ntowards addressing these issues is meta-learning, which in the context of NLG means a generation\\nmodel develops a broad set of skills and pattern recognition abilities at training time, and quickly'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='adapt to a new task given very few examples without retraining the model from scratch. Recently,\\nthere has been raising interests in both academia and industry to investigate meta-learning in\\ndifferent NLG tasks. Thus, it is a promising research direction to build efficient meta-learning\\nalgorithms that only need a few task-specific fine-tuning to learn the new task quickly. And for\\nknowledge-enhanced text generation, it is of crucial importance to adapt the model quickly on new\\ndomains with limited new knowledge (e.g., only a few knowledge triples).\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:32 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\n6.4 Learning Knowledge in a Continuous Way\\nA machine learning is expected to learn continuously, accumulate the knowledge learned in\\nprevious tasks, and use it to assist future learning. This research direction is referred as lifelong\\nlearning [ 20]. In the process, the intelligent machine becomes more and more knowledgeable\\nand effective at learning new knowledge. To make an analogy, humans continuously acquire\\nnew knowledge and constantly update the knowledge system in the brain. However, existing\\nknowledge-enhanced text generation systems usually do not keep updating knowledge in real time\\n(e.g., knowledge graph expansion). A meaningful exploration of was discussed in [ 86]. They built a'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='(e.g., knowledge graph expansion). A meaningful exploration of was discussed in [ 86]. They built a\\ngeneral knowledge learning engine for chatbots to enable them to continuously and interactively\\nlearn new knowledge during conversations. Therefore, it is a promising research direction to\\ncontinuously update knowledge obtained from various information sources, empowering intelligent\\nmachines with incoming knowledge and improving the performance on new text generation tasks.\\n7 CONCLUSIONS\\nIn this survey, we present a comprehensive review of current representative research efforts and\\ntrends on knowledge-enhanced text generation, and expect it can facilitate future research. To\\nsummarize, this survey aims to answer two questions that commonly appears in knowledge-\\nenhanced text generation: how to acquire knowledge andhow to incorporate knowledge to facilitate\\ntext generation . Base on knowledge acquisition, the main content of our survey is divided into\\nthree sections according to different sources of knowledge enhancement. Based on knowledge\\nincorporation, we first present general methods of incorporating knowledge into text generation and\\nfurther discuss a number of specific ideas and technical solutions that incorporate the knowledge to\\nenhance the text generation systems in each section. Besides, we review a variety of text generation\\napplications in each section to help practitioners learn to choose and employ the methods.\\nACKNOWLEDGEMENTS'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='ACKNOWLEDGEMENTS\\nWe thank all anonymous reviewers for valuable comments. We also appreciate the suggestions from\\nreaders of the pre-print version. We thank Dr. Michael Zeng (Microsoft) and Dr. Nazneen Rajani\\n(Saleforce) for their constructive comments and suggestions. Wenhao Yu and Dr. Meng Jiang’s\\nresearch is supported by National Science Foundation grants IIS-1849816, CCF-1901059, and IIS-\\n2119531. Qingyun Wang and Dr. Heng Ji’s research is based upon work supported by Agriculture\\nand Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178\\nfrom the USDA National Institute of Food and Agriculture, U.S. DARPA SemaFor Program No.\\nHR001120C0123, DARPA AIDA Program No. FA8750-18-2-0014, and DARPA KAIROS Program No.\\nFA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should\\nnot be interpreted as necessarily representing the official policies, either expressed or implied, of\\nDARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute\\nreprints for governmental purposes notwithstanding any copy-right annotation therein.\\nREFERENCES\\n[1] Roee Aharoni and Yoav Goldberg. 2017. Towards String-To-Tree Neural Machine Translation. In Annual Meeting of\\nthe Association for Computational Linguistics (ACL) .\\n[2]Chenxin An, Ming Zhong, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2021. Enhancing scientific'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='papers summarization with citation graph. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) .\\n[3]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to\\nalign and translate. In International Conference for Learning Representation (ICLR) .\\n[4]Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’an. 2017. Graph Convolutional Encoders\\nfor Syntax-aware Neural Machine Translation. In Empirical Methods in Natural Language Processing (EMNLP) .\\n[5]Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for Generative Multi-Hop Question Answering\\nTasks. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:33\\n[6]Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-Sequence Learning using Gated Graph Neural\\nNetworks. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[7]Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug\\nDowney, Scott Wen-tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In International Conference for\\nLearning Representation (ICLR) .\\n[8]Bin Bi, Chen Wu, Ming Yan, Wei Wang, Jiangnan Xia, and Chenliang Li. 2019. Incorporating External Knowledge'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='into Machine Reading for Generative Question Answering. In Conference on Empirical Methods in Natural Language\\nProcessing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) .\\n[9]Bin Bi, Chen Wu, Ming Yan, Wei Wang, Jiangnan Xia, and Chenliang Li. 2020. Generating Well-Formed Answers by\\nMachine Reading with Stochastic Selector Networks. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[10] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. In Journal of Machine Learning\\nResearch (JMLR) .\\n[11] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating\\nembeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (NeurIPS) .\\n[12] Ziqiang Cao, Sujian Li, Yang Liu, Wenjie Li, and Heng Ji. 2015. A novel neural topic model and its supervised extension.\\nInAAAI Conference on Artificial Intelligence (AAAI) .\\n[13] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2018. Retrieve, rerank and rewrite: Soft template based neural\\nsummarization. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[14] Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. Guiding semi-supervision with constraint-driven learning. In\\nannual meeting of the association of computational linguistics (ACL) .\\n[15] Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, and Tiejun Zhao. 2018. Syntax-directed attention for neural'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='machine translation. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[16] Shiqian Chen, Chenliang Li, Feng Ji, Wei Zhou, and Haiqing Chen. 2019. Driven answer generation for product-related\\nquestions in e-commerce. In International Conference on Web Search and Data Mining (WSDM) .\\n[17] Xiaojun Chen, Shengbin Jia, and Yang Xiang. 2020. A review: Knowledge reasoning over knowledge graph. In Expert\\nSystems with Applications .\\n[18] Xiuyi Chen, Fandong Meng, Peng Li, Feilong Chen, Shuang Xu, Bo Xu, and Jie Zhou. 2020. Bridging the Gap between\\nPrior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation. In Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) .\\n[19] Yu Chen, Lingfei Wu, and Mohammed J Zaki. 2020. Reinforcement learning based graph-to-sequence model for\\nnatural question generation. In International Conference of Learning Representation (ICLR).\\n[20] Zhiyuan Chen and Bing Liu. 2018. Lifelong machine learning. In Synthesis Lectures on Artificial Intelligence and\\nMachine Learning . Morgan & Claypool Publishers.\\n[21] Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, and Luo Si. 2020. Entdesc: Entity\\ndescription generation by exploringknowledge graph. In Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP) .\\n[22] Jaemin Cho, Minjoon Seo, and Hannaneh Hajishirzi. 2019. Mixture Content Selection for Diverse Sequence Generation.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='InConference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[23] Sajal Choudhary, Prerna Srivastava, Lyle Ungar, and Joao Sedoc. 2017. Domain aware neural dialog system. In arXiv\\npreprint arXiv:1708.00897 .\\n[24] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne\\nLiu. 2020. Plug and play language models: a simple approach to controlled text generation. In International Conference\\nfor Learning Representation (ICLR) .\\n[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding. In Conference of the North American Chapter of the Association for\\nComputational Linguistics (NAACL) .\\n[26] P Kingma Diederik, Max Welling, et al .2014. Auto-encoding variational bayes. In Proceedings of the International\\nConference on Learning Representations (ICLR) .\\n[27] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of wikipedia:\\nKnowledge-powered conversational agents. In International Conference for Learning Representation (ICLR) .\\n[28] Xiangyu Dong, Wenhao Yu, Chenguang Zhu, and Meng Jiang. 2021. Injecting Entity Types into Entity-Guided Text\\nGeneration. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[29] Mihail Eric and Christopher D Manning. 2017. A Copy-Augmented Sequence-to-Sequence Architecture Gives Good'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Performance on Task-Oriented Dialogue. In Conference of the European Chapter of the Association for Computational\\nLinguistics (EACL) .\\n[30] Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. 2019. Using Local Knowledge Graph Construction to\\nScale Seq2Seq Models to Multi-Document Inputs. In Conference on Empirical Methods in Natural Language Processing\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:34 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nand International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) .\\n[31] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long Form\\nQuestion Answering. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[32] Zhihao Fan, Yeyun Gong, Zhongyu Wei, Siyuan Wang, Yameng Huang, Jian Jiao, Xuan-Jing Huang, Nan Duan,\\nand Ruofei Zhang. 2020. An Enhanced Knowledge Injection Model for Commonsense Generation. In International\\nConference on Computational Linguistics (COLING) .\\n[33] Xiyan Fu, Jun Wang, Jinghan Zhang, Jinmao Wei, and Zhenglu Yang. 2020. Document Summarization with VHTM:\\nVariational Hierarchical Topic-Aware Mechanism. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[34] Yao Fu and Yansong Feng. 2018. Natural answer generation with heterogeneous memory. In Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics (NAACL) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='American Chapter of the Association for Computational Linguistics (NAACL) .\\n[35] Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar, et al .2010. Posterior regularization for structured latent variable\\nmodels. In Journal of Machine Learning Research (JMLR) .\\n[36] Ce Gao and Jiangtao Ren. 2019. A topic-driven model for learning to generate diverse sentences. In Neurocomputing .\\n[37] Cristina Garbacea and Qiaozhu Mei. 2020. Neural Language Generation: Formulation, Methods, and Evaluation. In\\narXiv preprint arXiv:2007.15780 .\\n[38] Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks,\\napplications and evaluation. In Journal of Artificial Intelligence Research (JAIR) .\\n[39] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. 2017. Convolutional sequence to\\nsequence learning. In International Conference on Machine Learning (ICML) .\\n[40] Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo,\\nAntoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D Dhole, et al .2021. The\\ngem benchmark: Natural language generation, its evaluation and metrics. In Annual Meeting of the Association for\\nComputational Linguistics (ACL) .\\n[41] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-Up Abstractive Summarization. In\\nConference on Empirical Methods in Natural Language Processing (EMNLP) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[42] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley.\\n2018. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[43] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating Copying Mechanism in Sequence-to-\\nSequence Learning. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[44] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2018. Search engine guided neural machine translation. In\\nProceedings of the AAAI Conference on Artificial Intelligence (AAAI) .\\n[45] Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. 2020. A knowledge-enhanced pretraining\\nmodel for commonsense story generation. In Transactions of the Association for Computational Linguistics (TACL) .\\n[46] Jian Guan, Yansen Wang, and Minlie Huang. 2019. Story ending generation with incremental encoding and common-\\nsense knowledge. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[47] Dandan Guo, Bo Chen, Ruiying Lu, and Mingyuan Zhou. 2020. Recurrent Hierarchical Topic-Guided RNN for\\nLanguage Generation. In Proceedings of the 37th International Conference on Machine Learning (ICML) .\\n[48] Shizhu He, Cao Liu, Kang Liu, and Jun Zhao. 2017. Generating natural answers by incorporating copying and'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='retrieving mechanisms in sequence-to-sequence learning. In Annual Meeting of the Association for Computational\\nLinguistics (ACL) .\\n[49] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. 2019. A comprehensive survey of deep\\nlearning for image captioning. In ACM Computing Surveys (CSUR) .\\n[50] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. 2017. Toward controlled generation\\nof text. In International Conference on Machine Learning (ICML) .\\n[51] Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Xiaodan Liang, Lianhui Qin, Haoye Dong, and Eric Xing. 2018. Deep\\nGenerative Models with Learnable Knowledge Constraints. In Advances in Neural Information Processing Systems .\\n[52] Xinyu Hua, Zhe Hu, and Lu Wang. 2019. Argument Generation with Retrieval, Planning, and Realization. In Annual\\nMeeting of the Association for Computational Linguistics (ACL) .\\n[53] Xinyu Hua and Lu Wang. 2018. Neural Argument Generation Augmented with Externally Retrieved Evidence. In\\nAnnual Meeting of the Association for Computational Linguistics (ACL) .\\n[54] Luyang Huang, Lingfei Wu, and Lu Wang. 2020. Knowledge Graph-Augmented Abstractive Summarization with\\nSemantic-Driven Cloze Reward. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[55] Touseef Iqbal and Shaima Qureshi. 2020. The Survey: Text Generation Models in Deep Learning.. In Journal of King\\nSaud University-Computer and Information Sciences . Elsevier.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Saud University-Computer and Information Sciences . Elsevier.\\n[56] Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, and Minlie Huang. 2020. Generating Commonsense Explanation by\\nExtracting Bridge Concepts from Reasoning Paths. In Conference of the Asia-Pacific Chapter of the Association for\\nComputational Linguistics and International Joint Conference on Natural Language (AACL-IJCNLP) .\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:35\\n[57] Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, and Minlie Huang. 2020. Language Generation with\\nMulti-Hop Reasoning on Commonsense Knowledge Graph. In Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP) .\\n[58] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S Yu. 2020. A survey on knowledge graphs:\\nRepresentation, acquisition and applications. In arXiv preprint arXiv:2002.00388 .\\n[59] Hanqi Jin, Tianming Wang, and Xiaojun Wan. 2020. SemSUM: Semantic Dependency Guided Neural Abstractive\\nSummarization. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[60] Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A Smith, and\\nDaniel S Weld. 2021. Genie: A leaderboard for human-in-the-loop evaluation of text generation. In arXiv preprint\\narXiv:2101.06561 .\\n[61] Byeongchang Kim, Jaewoo Ahn, and Gunhee Kim. 2020. Sequential Latent Knowledge Selection for Knowledge-'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Grounded Dialogue. In International Conference for Learning Representation (ICLR) .\\n[62] Jihyeok Kim, Seungtaek Choi, Reinald Kim Amplayo, and Seung-won Hwang. 2020. Retrieval-Augmented Controllable\\nReview Generation. In International Conference on Computational Linguistics (COLING) .\\n[63] Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation\\nfrom Knowledge Graphs with Graph Transformers. In Conference of the North American Chapter of the Association for\\nComputational Linguistics (NAACL) .\\n[64] Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to Progress in Long-form Question Answering. In\\nConference of the North American Chapter of the Association for Computational Linguistics (NAACL) .\\n[65] Ni Lao, Tom Mitchell, and William W Cohen. 2011. Random walk inference and learning in a large scale knowledge\\nbase. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[66] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. In Nature . Nature Publishing Group.\\n[67] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\\nTranslation, and Comprehension. In Annual Meeting of the Association for Computational Linguistics (ACL) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='[68] Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler,\\nMike Lewis, Wen-tau Yih, Tim Rocktäschel, et al .2020. Retrieval-augmented generation for knowledge-intensive nlp\\ntasks. In Advances in Neural Information Processing Systems (NeurIPS) .\\n[69] Chenliang Li, Weiran Xu, Si Li, and Sheng Gao. 2018. Guiding generation for abstractive text summarization based\\non key information guide network. In Conference of the North American Chapter of the Association for Computational\\nLinguistics (NAACL) .\\n[70] Haoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong, and Xiaodong He. 2020. Keywords-guided abstractive\\nsentence summarization. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[71] Jingyuan Li and Xiao Sun. 2018. A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional\\nConversation Generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[72] Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, and Junping Du. 2021. Leveraging Graph to Improve\\nAbstractive Multi-Document Summarization. In Annual Meeting of Association for Computational Linguistics (ACL) .\\n[73] Wei Li, Jingjing Xu, Yancheng He, ShengLi Yan, Yunfang Wu, and Xu Sun. 2019. Coherent Comments Generation for\\nChinese articles with a Graph-to-Sequence Model. In Annual Meeting of Association Computational Linguistics (ACL) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='[74] Rongzhong Lian, Min Xie, Fan Wang, Jinhua Peng, and Hua Wu. 2019. Learning to select knowledge for response\\ngeneration in dialog systems. In International Joint Conference on Artificial Intelligence (IJCAI) .\\n[75] Kexin Liao, Logan Lebanoff, and Fei Liu. 2018. Abstract Meaning Representation for Multi-Document Summarization.\\nInInternational Conference on Computational Linguistics (COLING) .\\n[76] Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020.\\nCommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning. In Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP-Findings) .\\n[77] Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun Shou, Ming Gong,\\net al.2021. Glge: A new general language generation evaluation benchmark. In Annual Meeting of the Association for\\nComputational Linguistics (ACL) .\\n[78] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-BERT: Enabling\\nLanguage Representation with Knowledge Graph.. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[79] Yuanxin Liu, Zheng Lin, Fenglin Liu, Qinyun Dai, and Weiping Wang. 2019. Generating Paraphrase with Topic as\\nPrior Knowledge. In International Conference on Information and Knowledge Management (CIKM) .\\n[80] Ye Liu, Yao Wan, Lifang He, Hao Peng, and Philip S Yu. 2021. KG-BART: Knowledge Graph-Augmented BART for'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Generative Commonsense Reasoning. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[81] Zhibin Liu, Zheng-Yu Niu, Hua Wu, and Haifeng Wang. 2019. Knowledge aware conversation generation with\\nreasoning on augmented graph. In Conference on Empirical Methods in Natural Language Processing and International\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:36 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) .\\n[82] Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2018. Mem2Seq: Effectively Incorporating Knowledge Bases\\ninto End-to-End Task-Oriented Dialog Systems. In Annual Meeting of Association for Computational Linguistics (ACL) .\\n[83] Gideon S Mann and Andrew McCallum. 2007. Simple, robust, scalable semi-supervised learning via expectation\\nregularization. In International Conference on Machine Learning (ICML) .\\n[84] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. 2014.\\nThe Stanford CoreNLP natural language processing toolkit. In Annual Meeting of the Association for Computational\\nLinguistics: System Demonstration (ACL) .\\n[85] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. 2019. The neuro-symbolic concept\\nlearner: Interpreting scenes, words, and sentences from natural supervision. International Conference for Learning\\nRepresentation (ICLR) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Representation (ICLR) .\\n[86] Sahisnu Mazumder, Nianzu Ma, and Bing Liu. 2018. Towards a continuous knowledge learning engine for chatbots.\\nInarXiv preprint arXiv:1802.06024 .\\n[87] Chuan Meng, Pengjie Ren, Zhumin Chen, Christof Monz, Jun Ma, and Maarten de Rijke. 2020. RefNet: A reference-\\naware network for background based conversation. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[88] Tanya Menon and Jeffrey Pfeffer. 2003. Valuing internal vs. external knowledge: Explaining the preference for\\noutsiders. In Management science .\\n[89] Yishu Miao, Edward Grefenstette, and Phil Blunsom. 2017. Discovering discrete latent topics with neural variational\\ninference. In International Conference on Machine Learning (ICML) .\\n[90] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M Khapra. 2018. Towards Exploiting Background\\nKnowledge for Building Conversation Systems. In Empirical Methods in Natural Language Processing (EMNLP) .\\n[91] Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019. Opendialkg: Explainable conversational\\nreasoning with attention-based walks over knowledge graphs. In Annual Meeting of the Association for Computational\\nLinguistics (ACL) .\\n[92] Diego Moussallem, Tommaso Soru, and Axel-Cyrille Ngonga Ngomo. 2019. THOTH: Neural Translation and\\nEnrichment of Knowledge Graphs. In International Semantic Web Conference (ISWC) .\\n[93] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive Text Summarization using'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Sequence-to-sequence RNNs and Beyond. In Conference on Computational Natural Language Learning (SIGNLL) .\\n[94] Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don’t Give Me the Details, Just the Summary! Topic-Aware\\nConvolutional Neural Networks for Extreme Summarization. In Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP) .\\n[95] Christina Niklaus, Matthias Cetto, André Freitas, and Siegfried Handschuh. 2018. A Survey on Open Information\\nExtraction. In International Conference on Computational Linguistics (COLING) .\\n[96] Tong Niu and Mohit Bansal. 2018. Polite dialogue generation without parallel data. In Transactions of the Association\\nfor Computational Linguistics (TACL) .\\n[97] Zheng-Yu Niu, Hua Wu, Haifeng Wang, et al .2019. Knowledge Aware Conversation Generation with Explainable Rea-\\nsoning over Augmented Graphs. In Conference on Empirical Methods in Natural Language Processing and International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) .\\n[98] Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, and Min-Yen Kan. 2020. Semantic Graphs for Generating\\nDeep Questions. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[99] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine\\nJernite, Vassilis Plachouras, Tim Rocktäschel, et al .2021. KILT: a benchmark for knowledge intensive language tasks.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='InConference of the North American Chapter of the Association for Computational Linguistics (NAACL) .\\n[100] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.\\n2019. Language Models as Knowledge Bases?. In Conference on Empirical Methods in Natural Language Processing and\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) .\\n[101] Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin Choi, and Jianfeng Gao. 2019.\\nConversing by Reading: Contentful Neural Conversation with On-demand Machine Reading. In Annual Meeting of\\nthe Association for Computational Linguistics (ACL) .\\n[102] Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena Hwang, Ronan Le Bras, Antoine Bosselut, and\\nYejin Choi. 2020. Backpropagation-based Decoding for Unsupervised Counterfactual and Abductive Reasoning. In\\nConference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[103] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are\\nunsupervised multitask learners. In OpenAI Blog .\\n[104] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. In Journal of\\nMachine Learning Research (JMLR) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Machine Learning Research (JMLR) .\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:37\\n[105] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine\\nComprehension of Text. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[106] Revanth Gangi Reddy, Danish Contractor, Dinesh Raghu, and Sachindra Joshi. 2019. Multi-Level Memory for Task\\nOriented Dialogs. In the North American Chapter of the Association for Computational Linguistics (NAACL) .\\n[107] Pengjie Ren, Zhumin Chen, Christof Monz, Jun Ma, and Maarten de Rijke. 2020. Thinking Globally, Acting Locally:\\nDistantly Supervised Global-to-Local Knowledge Selection for Background Based Conversation. In AAAI Conference\\non Artificial Intelligence (AAAI) .\\n[108] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018.\\nModeling relational data with graph convolutional networks. In European Semantic Web Conference (ESWC) .\\n[109] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point: Summarization with Pointer-Generator\\nNetworks. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[110] Rico Sennrich and Barry Haddow. 2016. Linguistic Input Features Improve Neural Machine Translation. In Conference\\non Machine Translation (WMT) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='on Machine Translation (WMT) .\\n[111] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for\\nmachine comprehension. In International Conference for Learning Representations (ICLR) .\\n[112] Sifatullah Siddiqi and Aditi Sharan. 2015. Keyword and keyphrase extraction techniques: a literature review. In\\nInternational Journal of Computer Applications (IJCA) . Foundation of Computer Science.\\n[113] Haoyu Song, Wei-Nan Zhang, Yiming Cui, Dong Wang, and Ting Liu. 2019. Exploiting persona information for\\ndiverse generation of conversational responses. In International Joint Conference on Artificial Intelligence (IJCAI) .\\n[114] Zhenqiao Song, Xiaoqing Zheng, Lu Liu, Mu Xu, and Xuan-Jing Huang. 2019. Generating responses with a specific\\nemotion in dialog. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[115] Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general\\nknowledge. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[116] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al .2015. End-to-end memory networks. In Advances in Neural\\nInformation Processing Systems (NeurIPS) .\\n[117] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances\\nin Neural Information Processing Systems (NeurIPS) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='in Neural Information Processing Systems (NeurIPS) .\\n[118] Bowen Tan, Lianhui Qin, Eric Xing, and Zhiting Hu. 2020. Summarizing Text on Any Aspects: A Knowledge-Informed\\nWeakly-Supervised Approach. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[119] Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing, and Zhiting Hu. 2019. Target-Guided\\nOpen-Domain Conversation. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[120] Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Carapcea, Chris Thrasher, and Chris Buehler. 2016. Rich\\nimage captioning in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR) .\\n[121] Yi-Lin Tuan, Yun-Nung Chen, and Hung-yi Lee. 2019. DyKgChat: Benchmarking Dialogue Generation Grounding on\\nDynamic Knowledge Graphs. In Conference on Empirical Methods in Natural Language Processing and International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP) .\\n[122] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS) .\\n[123] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph\\nattention networks. In International Conference for Learning Representation (ICLR) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='attention networks. In International Conference for Learning Representation (ICLR) .\\n[124] Cunxiang Wang, Shuailong Liang, Yili Jin, Yilong Wang, Xiaodan Zhu, and Yue Zhang. 2020. SemEval-2020 Task 4:\\nCommonsense Validation and Explanation. In Proceedings of the Fourteenth Workshop on Semantic Evaluation .\\n[125] Hao Wang, Bin Guo, Wei Wu, and Zhiwen Yu. 2020. Towards information-rich, logical text generation with knowledge-\\nenhanced neural models. In arXiv preprint arXiv:2003.00814 .\\n[126] Han Wang, Yang Liu, Chenguang Zhu, Linjun Shou, Ming Gong Gong, Yichong Xu, and Michael Zeng. 2021. Retrieval\\nEnhanced Model for Commonsense Generation. In Annual Meeting of Association for Computational Linguistics (ACL) .\\n[127] Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao, Wenjie Li, and Zhongyuan Wang. 2019.\\nKnowledge-aware graph neural networks with label smoothness regularization for recommender systems. In ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) .\\n[128] Jian Wang, Junhao Liu, Wei Bi, Xiaojiang Liu, Kejing He, Ruifeng Xu, and Min Yang. 2020. Improving Knowledge-aware\\nDialogue Generation via Knowledge Base Question Answering. In Conference on Artificial Intelligence (AAAI) .\\n[129] Kai Wang, Xiaojun Quan, and Rui Wang. 2019. BiSET: Bi-directional Selective Encoding with Template for Abstractive\\nSummarization. In Annual Meeting of the Association for Computational Linguistics (ACL) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Summarization. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[130] Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, and Yi Luan. 2019. PaperRobot:\\nIncremental Draft Generation of Scientific Ideas. In Annual Meeting of Association Computational Linguistics (ACL) .\\n[131] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of approaches and\\napplications. In IEEE Transactions on Knowledge and Data Engineering (TKDE) .\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:38 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\n[132] Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen, Changyou Chen, and Lawrence\\nCarin. 2019. Topic-Guided Variational Auto-Encoder for Text Generation. In Conference of the North American Chapter\\nof the Association for Computational Linguistics (NAACL) .\\n[133] Zhen Wang, Siwei Rao, Jie Zhang, Zhen Qin, Guangjian Tian, and Jun Wang. 2020. Diversify Question Generation\\nwith Continuous Content Selectors and Question Type Modeling. In Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP-Findings) .\\n[134] Xiangpeng Wei, Yue Hu, Luxi Xing, Yipeng Wang, and Li Gao. 2019. Translating with Bilingual Topic Knowledge for\\nNeural Machine Translation. In AAAI Conference on Artificial Intelligence (AAAI) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Neural Machine Translation. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[135] Chien-Sheng Wu, Richard Socher, and Caiming Xiong. 2019. Global-to-local memory pointer networks for task-\\noriented dialogue. In International Conference for Learning Representation (ICLR) .\\n[136] Sixing Wu, Ying Li, Dawei Zhang, Yang Zhou, and Zhonghai Wu. 2020. Diverse and Informative Dialogue Generation\\nwith Context-Specific Commonsense Knowledge Awareness. In Annual Meeting of the Association for Computational\\nLinguistics (ACL) .\\n[137] Sixing Wu, Ying Li, Dawei Zhang, Yang Zhou, and Zhonghai Wu. 2020. TopicKA: Generating Commonsense\\nKnowledge-Aware Dialogue Responses Towards the Recommended Topic Fact. In International Joint Conference on\\nArtificial Intelligence (IJCAI) .\\n[138] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive\\nsurvey on graph neural networks. In IEEE Transactions on Neural Networks and Learning Systems (TNNLS) .\\n[139] Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. 2017. Topic aware neural response\\ngeneration. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[140] Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2020. Pretrained Encyclopedia: Weakly\\nSupervised Knowledge-Pretrained Language Model. In International Conference of Learning Representation (ICLR) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='[141] Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. DeepPath: A Reinforcement Learning Method for\\nKnowledge Graph Reasoning. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[142] Jun Xu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, and Ting Liu. 2020. Conversational graph grounded\\npolicy learning for open-domain conversation generation. In Annual Meeting of the Association for Computational\\nLinguistics (ACL) .\\n[143] Minghong Xu, Piji Li, Haoran Yang, Pengjie Ren, Zhaochun Ren, Zhumin Chen, and Jun Ma. 2020. A Neural Topical\\nExpansion Framework for Unstructured Persona-oriented Dialogue Generation. In European Conference on Artificial\\nIntelligence (ECAI) .\\n[144] Pengcheng Yang, Lei Li, Fuli Luo, Tianyu Liu, and Xu Sun. 2019. Enhancing Topic-to-Essay Generation with External\\nCommonsense Knowledge. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\n[145] Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and\\nMichael Zeng. 2022. Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering.\\nAnnual Meeting of the Association for Computational Linguistics (ACL) .\\n[146] Donghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. 2022. Jaket: Joint pre-training of knowledge graph\\nand language understanding. AAAI Conference on Artificial Intelligence (AAAI) .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='and language understanding. AAAI Conference on Artificial Intelligence (AAAI) .\\n[147] Wenhao Yu, Mengxia Yu, Tong Zhao, and Meng Jiang. 2020. Identifying referential intention with heterogeneous\\ncontexts. In Proceedings of The Web Conference (WebConf) .\\n[148] Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael Zeng, and Meng Jiang.\\n2021. Dict-BERT: Enhancing Language Model Pre-training with Dictionary. arXiv preprint arXiv:2110.06490 .\\n[149] Wenhao Yu, Chenguang Zhu, Tong Zhao, Zhichun Guo, and Meng Jiang. 2021. Sentence-Permuted Paragraph\\nGeneration. Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\n[150] Qingkai Zeng, Jinfeng Lin, Wenhao Yu, Jane Cleland-Huang, and Meng Jiang. 2021. Enhancing Taxonomy Completion\\nwith Concept Generation via Fusing Relational Representations. ACM SIGKDD International Conference on Knowledge\\nDiscovery & Data Mining (KDD) .\\n[151] Houyu Zhang, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2020. Grounded conversation generation as guided\\ntraverses in commonsense knowledge graphs. In Annual Meeting of the Association for Computational Linguistics .\\n[152] Jian Zhang, Liangyou Li, Andy Way, and Qun Liu. 2016. Topic-Informed Neural Machine Translation. In International\\nConference on Computational Linguistics: Technical Papers (COLING) .\\n[153] Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu, and Maosong Sun. 2017. Prior Knowledge Integration for'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Neural Machine Translation using Posterior Regularization. In Annual Meeting of the Association for Computational\\nLinguistics (ACL) .\\n[154] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing\\nDialogue Agents: I have a dog, do you have pets?. In Annual Meeting of Association Computational Linguistics (ACL) .\\n[155] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced Language\\nRepresentation with Informative Entities. In Annual Meeting of the Association for Computational Linguistics (ACL) .\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:39\\n[156] Liang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, and Xu Sun. 2020. Graph-based multi-hop\\nreasoning for long text generation. In arXiv preprint arXiv:2009.13282 .\\n[157] Yinhe Zheng, Rongsheng Zhang, Minlie Huang, and Xiaoxi Mao. 2020. A Pre-Training Based Personalized Dialogue\\nGeneration Model with Persona-Sparse Data.. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[158] Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. 2018. Emotional chatting machine: Emotional\\nconversation generation with internal and external memory. In AAAI Conference on Artificial Intelligence (AAAI) .\\n[159] Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018. Commonsense knowledge'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='aware conversation generation with graph attention. In International Joint Conference on Artificial Intelligence (IJCAI) .\\n[160] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. 2017. Neural question generation\\nfrom text: A preliminary study. In Conference on Natural Language Processing and Chinese Computing (NLPCC) .\\n[161] Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee, Bill Yuchen Lin, and Xiang Ren. 2021. Pre-training\\ntext-to-text transformers for concept-centric common sense. International Conference for Learning Representation .\\n[162] Yimin Zhou, Yiwei Sun, and Vasant Honavar. 2019. Improving image captioning by leveraging knowledge graphs. In\\nIEEE Winter Conference on Applications of Computer Vision (WACV) .\\n[163] Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng, Xuedong Huang, and Meng Jiang.\\n2021. Boosting Factual Correctness of Abstractive Summarization with Knowledge Graph. In Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics (NAACL) .\\n[164] Jun Zhu, Ning Chen, and Eric P Xing. 2014. Bayesian inference with posterior regularization and applications to\\ninfinite latent SVMs. In The Journal of Machine Learning Research (JMLR) .\\n[165] Wenya Zhu, Kaixiang Mo, Yu Zhang, Zhangbin Zhu, Xuezheng Peng, and Qiang Yang. 2017. Flexible end-to-end\\ndialogue system for knowledge grounded conversation. In CoRR, abs/1709.04264 .'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='dialogue system for knowledge grounded conversation. In CoRR, abs/1709.04264 .\\n[166] Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. In Journal of the royal\\nstatistical society . Wiley Online Library.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:40 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\nA APPENDIX\\nFigure 8 demonstrates the statistics of selected publications in this survey. The left figure shows the\\npaper publishing venues. Most papers were published in top machine learning, artificial intelligence,\\nand natural language processing conferences, such as ACL, EMNLP, AAAI, ICLR, NeurIPS. Besides,\\nmany selected papers were published in high-impact journals, such as TNNLS, JMLR, TACL. The\\nright figure shows the paper categories. Among 160 selected papers, 87 papers (“general methods\\n(General)”, “topic”, “keyword”, “knowledge base (KG)”, “knowledge graph (KG)”, “grounded text\\n(Text)”) are directly relevant to the different kinds of knowledge-enhanced text generation methods;\\n10 papers are relevant to benchmark datasets; 10 papers are related survey papers. Besides, other\\n43 papers are about basic (pre-trained) generation methods (e.g., Seq2Seq, CopyNet, BART, T5), or\\nnecessary background (e.g., TransE, OpenIE, GNN, LDA), or future direction.\\nFigure 9 summarized different papers according to years, knowledge sources, and methods.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='Figure 9 summarized different papers according to years, knowledge sources, and methods.\\nTable 10 lists the leaderboard performance on ten knowledge-enhanced generation benchmarks.\\nTable 11 lists code links and programming language of representative open-source knowledge-\\nenhanced text generation systems that have been introduced in this survey.\\n312685204106644\\n01020304050\\nACLEMNLPNAACLCOLINGAAAIIJCAIICLRNeruIPSICMLOthers# PapersPublishing venus\\n13111314102016101043\\n01020304050\\nGeneralTopicKeywordLinguisticKBKGTextDatasetSurveyOthers# PapersPaper categories\\nFig. 8. Paper statistics of selected publications in this survey.\\n251035911\\n3591010\\n2246731015\\n224714\\n0102030405060708020162017201820192020\\n# Publications (including arXiv)YearTopicKeywordLinguistic featuresKnowledge baseKnowledge graphGrounded text\\nFig. 9. Knowledge-enhanced text generation has been gaining emerging interests in the recent five years.\\nA.1 Evaluation Metrics\\nBLEU-𝑚(short as B- 𝑚):BLEU is a weighted geometric mean of 𝑛-gram precision scores.\\nROUGE-𝑚(short as R- 𝑚):ROUGE measures the overlap of n-grams between the reference and\\nhypothesis; ROUGE-L measures the longest matched words using longest common sub-sequence.\\nDistinct-𝑘(short as D-k): Distinct measures the total number of unique 𝑘-grams normalized by\\nthe total number of generated 𝑘-gram tokens to avoid favoring long sentences.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='the total number of generated 𝑘-gram tokens to avoid favoring long sentences.\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:41\\nTable 10. Leaderboard performance on ten knowledge-enhanced generation benchmarks.\\n(a) Leaderboard performance on two summarization benchmark datasets with different knowledge-enhanced\\nNLG methods. Evaluation metrics are standard n-gram based metrics: ROUGE-2 and ROUGE-L.\\nMethods Ref.Knowledge Method CNN/DM Gigaword\\nsource category R-2 R-L R-2 R-L\\nBaseline methods (w/o KG)\\nSeq2Seq [117] 11.81 28.83 11.32 26.42 with attention mechanism\\nPG [109] 15.66 33.42 17.63 33.66 w/o coverage mechanism\\nKnowledge enhanced methods\\nVHTM [33] Topic M3 18.05 37.18 - - -\\nSELECTOR [22] Keyword M2 18.31 - - - * Improve generation diversity\\nFASUM [163] OpenKG - 17.84 37.40 - - * Improve factual correctness\\nKIGN [69] Keyword M1 17.12 35.68 17.93 34.44 -\\nBottomUp [41] Keyword M2 18.68 38.34 17.61 33.54 -\\nTGVAE [132] Topic M3 - - 17.27 33.02 -\\nHierDualPG [70] Keyword M2 - - 18.06 34.39 -\\nR3Sum [13] Text M1 - - 19.03 34.46 -\\nBiSET [129] Text M1 - - 19.78 36.87 -\\nSemSUM [59] DepGraph - - - 19.75 36.09 -\\nASGARD [54] OpenKG - 20.37 40.48 - - -\\n(b) Leaderboard performance on 𝛼NLG-ART dataset.\\nBoth B-4 and R-L are commonly used.\\nMethod Ref. Source B-4 R-L\\nBaseline methods\\nSeq2Seq [117] - 2.37 22.30\\nGPT-2 [103] - 9.80 32.90\\nKnowledge-enhanced methods\\nGPT-COMeT [7] KG 9.62 32.88'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='GPT-2 [103] - 9.80 32.90\\nKnowledge-enhanced methods\\nGPT-COMeT [7] KG 9.62 32.88\\nGRF [80] KG 11.62 34.62(c) Leaderboard performance on ComVE dataset.\\nBoth B-4 and R-L are commonly used.\\nMethod Ref. Source B-4 R-L\\nBaseline methods\\nSeq2Seq [117] - 6.10 25.80\\nGPT-2 [103] - 15.70 36.50\\nKnowledge-enhanced methods\\nCE-PR [56] KG 17.10 37.90\\nGRF [57] KG 17.19 38.10\\n(d) Leaderboard performance on CommonGen\\ndataset. SPICE is the primary evaluation metric.\\nMethod Ref. Source B-4 SPICE\\nBaseline methods\\nBART [67] - 31.83 27.99\\nT5 [104] - 31.96 28.86\\nKnowledge-enhanced methods\\nEKI-BART [32] Text 35.95 29.59\\nKG-BART [80] KG 33.87 29.63\\nRE-T5 [126] Text 40.87 31.08(e) Leaderboard performance on Holl-E (mix-short\\nsetting) dataset. R-L are the primary metric.\\nMethod Ref. Source R-L B-4\\nBaseline methods\\nSeq2Seq [117] - 21.48 5.26\\nBiDAF [111] - 35.09 27.44\\nKnowledge-enhanced methods\\nAKGCM [81] KG 34.72 30.84\\nRefNet [87] Text 36.17 29.38\\nGLKS [107] Text 39.63 -\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.1:42 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang\\n(f) Leaderboard performance on Wizard of Wikipedia\\nwith seen (S) and unseen (UnS) test set.\\nMethod Ref. R-1/R-2 (S) R-1/R-2 (UnS)\\nBaseline methods\\nTransformer [122] 17.8/ — 14.0/ —\\nKnowledge-enhanced methods\\nMemNet [27] 16.9/ — 14.4/ —\\nPostKS [74] 18.1/5.3 13.5/2.0\\nSKT [61] 19.3/6.8 16.1/4.2\\nPIPM+KDBTS [18] 19.9/7.3 17.6/5.4(g) State-of-the-art performance on SQuAD.'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='PIPM+KDBTS [18] 19.9/7.3 17.6/5.4(g) State-of-the-art performance on SQuAD.\\nMethod Ref. Source B-4\\nBaseline methods\\nSeq2Seq [117] - 3.01\\nTransformer [122] - 3.09\\nKnowledge-enhanced methods\\nNQG++ [160] LF 13.27\\nSELECTOR [22] LF+Keyword 15.87\\nG2S+BERT [19] LF+DepGraph 17.49\\nG2S+BERT+RL [19] LF+DepGraph 18.30\\n(h) Leaderboard performance on ELI5 dataset. The\\nKilt R-L (KRL) is the primary evaluation metric.\\nMethod Ref. Source KRL R-L\\nBaseline methods\\nT5 [104] - 0.0 19.1\\nBART [67] - 0.0 20.1\\nKnowledge-enhanced methods\\nRAG [68] Text 1.7 17.4\\nBART+DPR [99] Text 1.9 17.4\\nRT+c-REALM [61] Text 2.4 23.2(i) Some state-of-the-art performance on Per-\\nsonaChat dataset (no leaderboard on this dataset).\\nMethod Ref. B-1/B-2 D-1/D-2\\nBaseline methods\\nSeq2Seq [117] 18.2/9.3 2.6/7.4\\nKnowledge-enhanced methods\\nMemNet(soft) [27] 17.7/9.1 3.5/9.6\\nMemNet(hard) [27] 18.6/9.7 3.7/9.9\\nPostKS [74] 19.0/9.8 4.6/13.4\\nPEE [143] 23.2/11.5 - / -\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.A Survey of Knowledge-Enhanced Text Generation 1:43\\nTable 11. A list of representative open-source knowledge-enhanced text generation systems.\\nTask Ref. Paper title and open source code/toolkitProgramming Venue\\nlanguage & Year\\nTopic-enhanced methods\\nSummarization[94]Topic-Aware Convolutional Neural Networks for Extreme SummarizationPyTorchEMNLP\\n—— Code: https://github.com/EdinburghNLP/XSum 2018\\n[135]Friendly Topic Assistant for Transformer Based Abstractive Summarization-EMNLP'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='[135]Friendly Topic Assistant for Transformer Based Abstractive Summarization-EMNLP\\n—— Code: https://github.com/BoChenGroup/TA 2020\\nKeyword-enhanced methods\\nDialogue\\nsystem[94]A Content-Introducing Approach to Generative Short-Text ConversationTensorflowCOLING\\n—— Code: https://github.com/MaZhiyuanBUAA/Seq2BFforDialogueGeneration 2016\\n[135]Emotional Chatting Machine: Emotional Conversation Generation withPyTorchAAAI\\nInternal and External Memory —— Code: https://github.com/loadder/ECM-tf 2018\\nSummarization [94]Coherent Comment Generation with a Graph-to-Sequence ModelPyTorchACL\\n—— Code: https://github.com/lancopku/Graph-to-seq-comment-generation 2018\\nKB-enhanced methods\\nDialogue\\nsystem[82]Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-EndPyTorchACL\\nDialog Systems —— Code: https://github.com/HLTCHKUST/Mem2Seq 2019\\n[135]Global-to-local Memory Pointer Networks for Task-Oriented DialoguePyTorchICLR\\n—— Code: https://github.com/jasonwu0731/GLMP 2019\\n[128]Improving Knowledge-aware Dialogue Generation via Knowledge BasePyTorchAAAI\\nQuestion Answering —— Code: https://github.com/siat-nlp/TransDG 2020\\n[136]Diverse and Informative Dialogue Generation with Context-Specific KnowledgeTensorflowACL\\nAwareness —— Code: https://github.com/pku-sixing/ACL2020-ConKADI 2020\\n[137]TopicKA: Generating Commonsense Knowledge-Aware Dialogue ResponsesTensorflowIJCAI\\n—— Code: https://github.com/pku-sixing/IJCAI2020-TopicKA 2020\\nKG-enhanced methods\\nDialogue'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='—— Code: https://github.com/pku-sixing/IJCAI2020-TopicKA 2020\\nKG-enhanced methods\\nDialogue\\nsystem[159]Commonsense Knowledge Aware Conversation Generation with GraphTensorflowIJCAI\\nAttention —— Code: https://github.com/thu-coai/ccm 2018\\n[121]DyKgChat: Benchmarking Dialogue Generation Grounding on DynamicTensorflowEMNLP\\nKnowledge Graphs —— Code: https://github.com/Pascalson/DyKGChat 2019\\n[151]Grounded Conversation Generation as Guided Traverses in CommonsensePyTorchACL\\nKnowledge Graphs —— Code: https://github.com/thunlp/ConceptFlow 2020\\nScientific\\nwriting[63]Text Generation from Knowledge Graphs with Graph TransformersPyTorchNAACL\\n—— Code: https://github.com/rikdz/GraphWriter 2019\\n[130]PaperRobot: Incremental Draft Generation of Scientific IdeasPyTorchACL\\n—— Code: https://github.com/EagleW/PaperRobot 2019\\nCommonsense\\nreasoning\\n&\\nStory\\ngeneration[46]Story Ending Generation with Incremental Encoding and CommonsenseTensorflowAAAI\\nKnowledge —— Code: https://github.com/JianGuanTHU/StoryEndGen 2019\\n[57]Language Generation with Multi-Hop Reasoning on CommonsensePyTorchEMNLP\\nKnowledge Graph —— Code: https://github.com/cdjhz/multigen 2020\\n[80]KG-BART: Knowledge Graph-Augmented BART for GenerativePyTorchAAAI\\nCommonsense Reasoning —— Code: https://github.com/yeliu918/KG-BART 2021\\n[21]ENT-DESC: Entity Description Generation by Exploring Knowledge GraphMXNetEMNLP\\n—— Code: https://github.com/LiyingCheng95/EntityDescriptionGeneration 2020\\nQuestion'),\n",
       " Document(metadata={'title': 'A Survey of Knowledge-Enhanced Text Generation', 'year': 2022}, page_content='—— Code: https://github.com/LiyingCheng95/EntityDescriptionGeneration 2020\\nQuestion\\nanswering[5]Commonsense for Generative Multi-Hop Question Answering TasksTensorflowEMNLP\\n—— Code: https://github.com/yicheng-w/CommonSenseMultiHopQA 2018\\nGround text-enhanced methods\\nDialogue\\nsystem[27]Wizard of Wikipedia: Knowledge-Powered Conversational agentsPyTorchICLR\\n—— Code: https://github.com/facebookresearch/ParlAI 2019\\n[101]Conversing by Reading: Contentful Neural Conversation with On-demandPyTorchACL\\nMachine Reading —— Code: https://github.com/qkaren/converse_reading_cmr 2019\\n[61]Sequential Latent Knowledge Selection for Knowledge-Grounded DialogueTensorflowICLR\\n—— Code: https://github.com/bckim92/sequential-knowledge-transformer 2020\\n[87]RefNet: A Reference-aware Network for Background BasedTensorflowAAAI\\nConversation —— Code: https://github.com/ChuanMeng/RefNet 2020\\nSummarization [129]BiSET: Bi-directional Selective Encoding with Template forPyTorchACL\\nAbstractive Summarization —— Code: https://github.com/InitialBug/BiSET 2019\\nQuestion\\nanswering[68]Retrieval-Augmented Generation for Knowledge-Intensive NLP TasksPyTorchNeurips\\n—— Code in https://github.com/huggingface/transformers 2020\\n[99]KILT: a Benchmark for Knowledge Intensive Language TasksPyTorchNAACL\\n—— Code: https://github.com/facebookresearch/KILT 2021\\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='Received March 14, 2020, accepted April 21, 2020, date of publication April 29, 2020, date of current version May 15, 2020.\\nDigital Object Identifier 10.1 109/ACCESS.2020.2991328\\nA Constant Time Complexity Spam Detection\\nAlgorithm for Boosting Throughput on\\nRule-Based Filtering Systems\\nTIAN XIA\\nComputer and Information Engineering Department, Shanghai Polytechnic University, Shanghai 201209, China\\ne-mail: xiatian@sspu.edu.cn\\nThis work was supported in part by the Key Disciplines of Computer Science and Technology of Shanghai Polytechnic University\\nunder Grant No. xxkzd1604.\\nABSTRACT Along with the barbarous growth of spams, anti-spam technologies including rule-based\\napproaches and machine-learning thrive rapidly as well. In antispam industry, the rule-based systems (RBS)\\nbecomes the most prominent methods for \\x1cghting spam due to its capability to enrich and update rules\\nremotely. However, the antispam \\x1cltering throughput is always a great challenge of RBS. Especially,\\nthe explosively spreading of obfuscated words leads to frequent rule update and extensive rule vocabulary\\nexpansion. These incremental obfuscated words make the \\x1cltering speed slow down and the throughput\\ndecrease. This paper addresses the challenging throughput issue and proposes a constant time complexity\\nrule-based spam detection algorithm. The algorithm has a constant processing speed, which is independent of'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='rule and its vocabulary size. A new special data structure, namely, Hash Forest, and a rule encoding method\\nare developed to make constant time complexity possible. Instead of traversing each spam term in rules,\\nthe proposed algorithm manages to detect spam terms by checking a very small portion of all terms. The\\nexperiment results show effectiveness of proposed algorithm.\\nINDEX TERMS Constant time complexity, hash forest, rule-based \\x1cltering, spam detection, throughput.\\nI. INTRODUCTION\\nThe widespread use of Internet had grown explosively since\\nthe \\x1crst establishment of Internet in 1969. Internet had con-\\nnected each individual together via computers and mobile\\ndevices. Along with it, the scale of data is overwhelmingly\\nincreased as well [1], especially after the wide use of social\\nnetworks, personal communication tools, emails and short\\nmessages (SMS). This easy-communication circumstance\\nalso encouraged the numerous emerge of spams. Such kind\\nof activities turned into one of the most pro\\x1ctable businesses\\nfor spammers and criminals.\\nSpams \\x1crst spread explosively but mainly in emails in the\\n\\x1crst decade of 21thcentury, indicated by the statistic results\\nprovided in [2], [3]. Spam e-mails grew exponentially from\\n8% in 2001 up to 90% during 2009 [2]. Fortunately, the trend\\nturned downward to around 8.2% of junk e-mail volumes\\nworldwide because of the withdraw of rogue Internet Service\\nProviders such as 3FN, Bredolab, Rustock, and Grum.'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='Providers such as 3FN, Bredolab, Rustock, and Grum.\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Choon Ki Ahn\\n .Then, spams spread widely into the fast growing SMS\\nservice, because it has reached more than 6 billion users\\nglobally with approximately 9.5 trillion SMS sent globally\\nin the year 2009. At \\x1crst, spam SMS generally less perva-\\nsively than email spam [4]. Then, mobile spam has steadily\\nincreased from 2008 to 2012 and recently account for half\\nof all North America mobile phone traf\\x1cc in 2019 [5]. The\\nincreased use of SMS service has sustained great pro\\x1cts\\nclose to 117.2 billion dollars in 2017 [6]. The great interests\\nhave attracted malicious spammers to spread unsolicited,\\ncommercial, bulk electronic message [7]. Such SMS may\\nsometimes convey undemand adverts, viruses, malware or\\nother annoying contents targeted at consumers, businesses\\nor government organizations. Recently, security has become\\nmain threatening concern because spam SMS often attract\\nusers to reveal critical personal information by promising free\\ngifts, cheap product offers, credit cards or debt relief services.\\nAnti-spam technologies have been developed much along\\nwith the growth of spams. Compared with machine learning\\napproaches boosting in antispam research, the rule-based\\nsystems (RBS) have been much predominant in the \\x1cltering'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='systems (RBS) have been much predominant in the \\x1cltering\\nVOLUME 8, 2020This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/82653T. Xia: Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\\nindustry since it integrates various classi\\x1ccation methods\\nand are able to update \\x1clters online remotely. Recent years,\\nmotivated by the bene\\x1cts provided by SaaS (Software as\\na Service), RBS are running on cyber security compa-\\nnies, such as Symantec Cloud, McAfee Cloud Security,\\nor Kasper-sky Hosted Security [8]. The most popular one is\\nSpamAssassin [9].\\nHowever, such cloud-based spam-\\x1cltering services con-\\ncentrate \\x1cltering tasks and demand much high throughput\\ncapabilities. In addition, the variety of spam always chal-\\nlenges the limited RBS \\x1cltering throughput. The overwhelm-\\ningly emerged obfuscated words, as shown in TABLE 1, are\\nthe most dif\\x1ccult and challenging issue. They are possible to\\nget through RBS spam \\x1clters as they always contain unusual\\nsymbols or blanks. However, they are still able to be associate\\nto spam words in mind.\\nTABLE 1. Obfuscated words in spam SMS messages.\\nObfuscated words directly lead to rules spam term vocab-\\nulary explosion and rule quantity increasing extraordinarily.\\nConsequently, the computation time for detecting and \\x1clter-\\ning grows dramatically, making the running RBS slow down'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"ing grows dramatically, making the running RBS slow down\\nsigni\\x1ccantly and even impractical.\\nTo overcome the drawback, some researchers studied\\nscheduling strategies [10], [11], some researchers provided\\nsimulation tools of \\x1clters' throughput for research [12] and\\nsome companies relied on upgrading hardware equipment.\\nHowever, an inconstant time complexity algorithm\\nstretches the computation time longer while rules quantity\\nand its spam term vocabulary increase. If the time complex-\\nity of \\x1cltering algorithms of RBS can reduce to constant,\\nthe throughput issue can be solved since the expansion of\\nrule and its vocabulary size will not slow down \\x1cltering speed\\never.\\nRBS rules include a Boolean expression and a rank. The\\nBoolean expression is a combination of spam terms and\\nlogical operators. An RBS is capable to detect spam SMS\\nrapidly based on the Boolean expressions and return spam\\nSMS ID and matching rule ID pairs. In our research, several\\ncreative methods manage to complete the \\x1cltering process\\nwithin an extremely short and constant time. We \\x1crst studied\\non the rule representation in computer memory. The rule data\\nare organized in a speed-optimized structure, namely, Hash\\nForest, which is capable to avoid accessing each spam term\\nwhile detecting spams. Then, we studied on the method to\\ncalculate Boolean operators automatically through the spam\\ndetection process. Currently, we support the use of &&(AND\"),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='detection process. Currently, we support the use of &&(AND\\noperator ),jj(OR operator ) and ( ) ( bracket operator ) in ruleBoolean expressions. Other operators will be supported in our\\nfuture research.\\nThis research was motivated by the cooperation with a\\ncommunication service company. This company provides\\nSMS service in Shanghai and serval provinces near around.\\nTheir SMS includes veri\\x1ccation codes SMS, e-commercial\\nSMS, express delivery SMS, government SMS, promote sale\\nSMS of malls or grocery stores, etc. The company sends more\\nthan 80 million business short messages per day on average\\nand about 150 million per day on peak. The project was car-\\nried out to increase \\x1cltering speed to meet its overwhelming\\nSMS sending throughput requirement. This project success-\\nfully addressed the throughput issue and decreased the time\\ncomplexity of the spam detection algorithm to constant O(1).\\nAbout 150 million SMS can be \\x1cltered in less than one hour.\\nCurrently, the company is running 110k rules, including 10k\\nblack rules and 100k white rules. With the rise of rule number,\\nthe algorithm maintains its constant spam detection speed.\\nThe main contributions are following.\\n1) Propose a new rule data structure, namely, Hash Forest.\\nIt rearranges spam terms into search routes branches. Hash\\nForest helps the algorithm detect spam term by checking a\\nvery small portion of them.\\n2) Propose an encoding method for rule Boolean expres-\\nsions. The encoding method helps the \\x1clter calculate opera-'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"sions. The encoding method helps the \\x1clter calculate opera-\\ntors in expressions automatically. This process has constant\\ntime complexity.\\n3) Develop a spam detection algorithm that only processes\\nsymbols (i.e. letters in English) instead of whole spam terms.\\nSince symbols in languages are limited, the time complexity\\nis greatly reduced.\\n4) Support sequential matching. A rule contains a logical\\nexpression. The logical brackets in an expression can be\\ndetected one by one from beginning to the end with constant\\ntime complexity.\\nThe rest of the paper is organized as follows. Section II\\ndiscusses the related work. Section III presents the pro-\\nposed Constant Time Complexity Spam Detection algorithm.\\nSection IV outlines the experiment's results to show the\\nconstant computational complexity. The conclusion is drawn\\nand future work is discussed in Section V.\\nII. RELATED WORK\\nAlong with the barbarous growth of spams, antispam tech-\\nnologies also thrive prosperously. The antispam technologies\\ninclude content \\x1cltering systems and pattern detecting sys-\\ntems. Both have advantages and limitations.\\nA. THE CONTENT FILTERING TECHNOLOGIES\\nThe content \\x1cltering systems utilize statistical machine learn-\\ning approaches. Many models have been applied to obtain\\nbetter spam detection results, such as Support Vector Machine\\n(SVM) [13], Bayesian methods [14], Decision Tree [15], etc.\\nSVM creates a multiclass, SVM-based classi\\x1cer from a\\nset of binary SVM classi\\x1cer. SVM becomes popular because\"),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='set of binary SVM classi\\x1cer. SVM becomes popular because\\nit is robust for many circumstances. SVM trains a decision\\n82654 VOLUME 8, 2020T. Xia: Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\\nequation from a high-dimensional feature space, which leads\\nto high accuracy. In addition, it has been developed into\\nserval types, including one-against-rest SVM (OAR-SVM),\\none-against-one SVM (OAO-SVM), directed acyclic graph\\nSVM (DAG-SVM), adaptive directed acyclic graph SVM\\n(ADAG-SVM), and error-correcting output code SVM\\n(ECOC-SVM) [16].\\nBayesian methods, such as Naive Bayes, are regarded\\nas the effective and important machine learning algorithms\\nin information retrieval. Teraguchi et al. [17] proposed a\\nBayesian algorithm to defeat spammers. To enhance its accu-\\nracy, Bayesian methods are often hybrid with other algo-\\nrithms. Ebadati and Ahmadzadeh [18] proposed a GA-Naive\\nBayes for spam email detection with GA algorithm for feature\\nsection. Ari\\x1cn et al. [19] focused on spam detection for SMS\\nby Naive Bayes Classi\\x1cer and FP-Growth since FP-Growth\\nis utilized for mining frequent patterns.\\nIn addition, other machine learning methods, such as\\nboosting trees [20], Neural Networks [21], are also applied\\nin spam detection and outperform Naive Bayes and Decision\\ntrees.\\nMachine learning approaches usually require a beginning\\ntraining for spam \\x1clter and training again if rules are updated.'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='training for spam \\x1clter and training again if rules are updated.\\nThen, the \\x1clter may be required to restart to load the updated\\nmodels. Therefore, although many progresses have been\\nmade in research, their deployments in antispam industry are\\nnot often reported.\\nB. RULE-BASED SYSTEMS\\nRule-based systems, one of the most prominent methods\\nfor \\x1cghting spam in industry, combine Pattern Detection\\ntechnologies, such as operation research, graph theory, data\\nanalysis, clustering. It has been deployed and spread in the\\nantispam industry because they are capable to update rules\\nonline remotely.\\nAnti-spam RBS \\x1clter spam based on a set of pares of\\nrules and its scores. Whenever a rule is matched through\\nits logical test, along with Boolean TRUE returns, its score\\nis accumulated into a global counter. A spam is determined\\nby whether the global counter comes up to a precon\\x1cgured\\nthreshold.\\n1) THE SpamAssassin FRAMEWORK\\nSpamAssassin [9], a successful forerunner of typical\\nRBS, was developed from two rudimentary rule engines\\n\\x1clter.plx [22] and Spamometer [23]. In a long time, SpamAs-\\nsassin play a vital role [24] and has been adopted by antispam\\nindustry companies (such as Symantec or McAfee) [10].\\nSpamAssassin is integrated with the most popular Mail\\nTransfer Agent (MTA) packages (e.g. Post\\x1cx, Exim or\\nQMail). It is designed to listen to TCP port 783. All received\\npackages are analyzed and several features are extracted as\\ninputs of rules \\x1clter. Then, spam emails are determined by'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='inputs of rules \\x1clter. Then, spam emails are determined by\\nthe accumulation of a global counter. SpamAssassin supports\\nNaïve Bayes classi\\x1cer, Sender Policy Framework (SPF) ver-\\ni\\x1cer as build-in modules and third party \\x1clter as plugins.However, SpamAssassin was reported to have a low \\x1cl-\\ntering throughput [25]. Therefore, a new-generation RBS\\nmiddleware has been developed from SpamAssassin, such as\\nWirebrush4SPAM, to relieve its heavy throughput pressure.\\n2) THROUGHPUT IMPROVEMENTS\\nThroughput capability is always challenging essential fea-\\ntures of RBS. Fortunately, achievements have been made to\\ncope with performance issues [11].\\nThe smart \\x1clter evaluation (SFE), introduced in Wire-\\nbrush4SPAM [25], is able to terminate a rule execution at a\\nproper point based on evaluation. The termination is aimed to\\nsave computational recourses.\\nLearning After Report (LAR) [25] in Wirebrush4SPAM\\ngenerates a new thread to process auto-learning tasks which\\nis inherited from SpamAssassin.\\nIdenti\\x1ccation of Bayes Useless Information (IBUI) in [26]\\nre\\x1cnes Naïve Bayes databases by removing unhelpful tokens\\nif it has an over-threshold Inverse Document Frequency (IDF)\\nvalue.\\nPer Rule Parallelization (PRP) [25] in Wirebrush4SPAM\\nupgrades parallel computing of SpamAssassin. It has a con-\\ncurrent rule execution scheme capable to take advantage of\\nparallelizing computation regardless of the number of pend-\\ning classi\\x1ccations.\\nSuf\\x1ccient Condition Rules First (SCRF) [9] in a plugin of'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='ing classi\\x1ccations.\\nSuf\\x1ccient Condition Rules First (SCRF) [9] in a plugin of\\nSpamAssassin is able to terminate \\x1clter execution as soon as\\nenough matching rules are found.\\nThe improvements discussed above mainly focus on sim-\\nplifying the \\x1cltering stages, omitting unnecessary process and\\nparallel computing. However, despite of the progress made\\nby these technologies, they did not report to decrease the\\ntime complexity of \\x1cltering algorithms to an acceptable level.\\nTherefore, along with the explosion of term vocabulary in\\nrules, the \\x1clter speed will still keep slowing down.\\nThis context provides a solution for throughput issue in\\nRBS by downgrading the computational complexity to con-\\nstant. That is, the speed of \\x1cltering algorithm is irrelevant to\\nrule size or rule term vocabulary.\\nIII. THE CONSTANT TIME COMPLEXITY SPAM\\nDETECTION ALGORITHM\\nA. PROBLEM FORMULATION\\nThe method presented in this paper is able to scan candidate\\nSMS to detect their all matching rules and return their rule\\nIDs. The \\x1cltering speed is independent of the increasing rule\\namounts and rule term vocabulary.\\nTypical Boolean expressions of an English rule, a Chinese\\nrule and a mixed-language rule are shown in TABLE 2.\\nApparently, rules always contain symbols including let-\\nters, Chinese characters, numbers and some special symbols.\\nTherefore, a typical Boolean expression can be represented\\nas a uni\\x1ced formula shown in (1):\\n.T11jjT12jj\\x01\\x01\\x01jj T1n/&&\\n.T21jjT22jj\\x01\\x01\\x01jj T2n/&&\\n\\x01\\x01\\x01\\n.Tm1jjTm2jj\\x01\\x01\\x01jj Tmn/ (1)'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='.T11jjT12jj\\x01\\x01\\x01jj T1n/&&\\n.T21jjT22jj\\x01\\x01\\x01jj T2n/&&\\n\\x01\\x01\\x01\\n.Tm1jjTm2jj\\x01\\x01\\x01jj Tmn/ (1)\\nVOLUME 8, 2020 82655T. Xia: Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\\nTABLE 2. Typical Boolean expression of rules.\\nwhere Tijrepresents a certain spam term in the Boolean\\nexpression.\\nFor a single term Tij, letSij\\n1;Sij\\n2;Sij\\n3;\\x01\\x01\\x01Sij\\nk;\\x01\\x01\\x01Sij\\nKrepresent\\nthe symbols in Tij. Here, symbols are letters in alphabet\\nlanguage, like English, or hieroglyphic characters in pic-\\ntographic language. In this paper, we say symbols instead\\nof letters because there are different symbols in different\\nlanguages. Please note that this approach can also apply to\\nother language, regardless alphabet language, like English,\\nor pictograph language, like Chinese, even multiple language\\nmixture.\\nFilter speed is the main challenge of RBS throughput. The\\npaper focuses this issue and reduces the SMS \\x1cltering time\\nto constant. For a given set of SMS, the overall speed of\\nthe proposed algorithm is only related to spam SMS amount\\nand remains stable while rule amount and its vocabulary are\\nexpanding.\\nB. THE HASH FOREST\\nThe Hash Forest, a special data structure of all spam terms,\\nforms the foundation of the constant time complexity of term\\ndetection in the proposed spam detection algorithm.\\n1) PROCESSING SYMBOLS INSTEAD OF WORDS\\nThere are numerous words in each language whereas symbols\\nare limited. Processing a relatively small set of symbols make'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='are limited. Processing a relatively small set of symbols make\\nit possible to deploy O(1) search algorithms, like Hash search,\\non symbols searching.\\nApparently, if traversing each word to detect term,\\nas shown in FIGURE 1, time complexity is uncontrollable.\\nHowever, the detecting direction can be changed to ver-\\ntical if symbols are processed. The detection process is to\\n\\x1cnd a path getting through each symbols of a term. By this\\nmeans, it only checks a small portion of all terms. An exam-\\nple of searching for term CA\\nshis shown in FIGURE 2.\\nFIGURE 1. Term by term searching direction.\\nFIGURE 2. Searching route of the term CA\\nsh.\\nApparently, other terms, such as Debt, Discount, Pay Loan,\\nare left untouched.\\nSince word length is limited, the searching time for term\\ndetection is limited below a constant.\\nTherefore, to achieve constant time complexity, Boolean\\nrule expressions are \\x1crst represented in a uni\\x1ced formula and\\ntheir terms are represented symbol by symbol. Symbols from\\nall languages come from a limited set. Processing symbols\\ninstead of terms takes the \\x1crst step towards the goal: constant\\ntime complexity. English language has 52 symbols, including\\nall letters and their capitals. Also, taking special symbols\\nin spam terms in consideration, approximately 200 or more\\nsymbols will be added to the set. Therefore, symbol size of\\nspam terms is expected to be limited under 300 in English.\\nEven for Chinese, 3000 frequently used Chinese characters'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='Even for Chinese, 3000 frequently used Chinese characters\\ncan cover 99% Chinese documents and 1000 can cover 90%.\\nTherefore, in Chinese, not more than 2000 characters are\\nexpected in terms of Boolean expressions.\\nC. THE DATA STRUCTURE OF THE HASH FOREST\\nThe Hash Forest is a data structure to represent all terms in\\nall Boolean expressions.\\nAs shown in FIGURE 2, many spam terms have the \\x1crst\\nletter in common. This happens more likely in English than\\nin Chinese. In the Hash Forest, these common symbols are\\nmerged together to form the root of each tree in the forest.\\nIn addition, the second successive symbol of terms may\\nalso be the same. They are merged to form the branch nodes.\\nA root node and a branch node represent a bunch of terms\\nwith the \\x1crst and second symbols in common.\\nMoreover, in turn, the rest common symbols follow the\\nsame process to merge together to form branch tree nodes\\n82656 VOLUME 8, 2020T. Xia: Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\\nin next levels. When no more common symbols are found,\\nthe trees continue growing according to symbols of each term\\nuntil reaching the last symbol which is the leaf node.\\nFor example, as shown in FIGURE 3, suppose there are\\nonly 3 terms in rules. They are Cash ,C@sh ,Ca$h . They all\\nbegin with the letter C. So, they will form a single tree with\\nroot node C. Then, Cash andCa$h have the second letter a\\nin common. Therefore, the both terms will continue to merge'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='in common. Therefore, the both terms will continue to merge\\ntogether with a branch node a. The rest uncommon letters link\\none after another until the last one.\\nFIGURE 3. A 3 terms in a single-tree hash forest example.\\nMore generally, for instance, suppose there are two terms\\nTijandTpq, They contain a few common symbols from the\\nbeginning. let Sij\\n1;Sij\\n2;Sij\\n3;\\x01\\x01\\x01Sij\\nk;\\x01\\x01\\x01Sij\\nKrepresents the sym-\\nbols in TijandSpq\\n1;Spq\\n2;Spq\\n3;\\x01\\x01\\x01Spq\\nr;\\x01\\x01\\x01Spq\\nRrepresents the\\nsymbols in Tpq. Also, Sij\\n1,Spq\\n1andSij\\n2,Spq\\n2are the same\\nrespectively. They form the tree as shown in FIGURE 4.\\nFIGURE 4. A two terms in a single-tree hash forest example.\\nAll terms in rule Boolean expressions are divided symbol\\nby symbol and same divided symbols are merged. Normally,\\ngiven enough terms, their symbols should be able to form\\na multiple-tree, namely, Hash Forest. Each different \\x1crst\\nsymbol is the root of each tree. Suppose a black solid circle\\nin FIGURE 5 represents a symbol. The common symbols are\\nmerged together. Such example of a multiple-tree Hash Forest\\nis shown in FIGURE 5.\\n1) SPAM TERM DETECTION VIA THE HASH FOREST\\nThe Hash Forest is designed to speed up the existence detec-\\ntion of spam terms in SMS. It rearranges all spam terms\\ntogether and builds searching routes for each term. Such\\nsearching process avoids checking the majority of terms in\\neach rule. The detection algorithm has O(1) time complexity.\\nTo detect symbols in spam terms, a Hash search runs on'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='To detect symbols in spam terms, a Hash search runs on\\neach level of the Hash Forest. If the Hash search \\x1cnds the\\nmatching symbol, another Hash search is performed on the\\nFIGURE 5. A multiple-tree hash forest examples.\\nnext level of the matching node. And so on. When a level only\\nhas one node, Hash search is substituted by a matching check.\\nIf all Hash searches and matching checks return a matching\\nnode until leaf node, a detected term is reported. By this way,\\nHash searches are able to \\x1clter out all other un-matching\\nterms by checking a few symbols of the matching term. Such\\nprocess only checks a very small portion of term vocabulary\\nlike route \\x1cnding in the Hash Forest.\\nFor instance, as illustrated in Figure 6, if trying to detect\\nca$h in a term set { ca$h, cost, loan, debt, credit, ... }, the \\x1crst\\nHash search will locate the tree of symbol cas the beginning\\nof this searching route. Then, the second Hash Search is\\nperformed on the nodes of level 2, tree c.It locates the branch\\nof symbol a. Then, the searching route continues by directly\\ngetting through the remaining symbols by a few matching\\nchecks since no branches exist in deeper levels. If the search-\\ning route reaches the leaf node h, it indicates that spam term\\ncash is detected. Apparently, it is unnecessary to traverse into\\nother trees of other symbols, such as bfordebt,lforloan, etc.\\nThe dash line in FIGURE 6 is the searching route.\\nFIGURE 6. An example of spam term detection and its searching route in'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='FIGURE 6. An example of spam term detection and its searching route in\\nthe hash forest.\\n2) TIME COMPLEXITY ANALYSIS OF TERM DETECTION\\nAs mentioned above, existence detection of a term is achieved\\nby a few Hash searches in the Hash Forest.\\nA Hash table (Hash map) is a data structure that imple-\\nments an associative array abstract data type, a structure that\\ncan map keys to values. A hash table uses a hash function\\nto compute a key, also called index or a hash code, and map\\nwith an array of buckets or slots, from which the desired value\\nVOLUME 8, 2020 82657T. Xia: Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\\nFIGURE 7. A general format (a) and an Example (b) of Boolean expression encoding.\\ncan be found. Hash search gets O(1) search time on average\\nand O(n) in worst case [27]. The worst case happens when\\nall values are mapped to the same bucket. Such scenario is\\nunlikely to happen because the nodes amounts of each level\\nin the Hash Forest is quite limited. Therefore, the Hash search\\ntime on each level has O(1) time complexity.\\nIn addition, obviously, the times of Hash searches needed\\nwhen searching through the Hash Forest depend on the\\naverage depth of the Hash Forest. The average depth, then,\\nis determined by the average length of terms. Actually, for\\nEnglish, the average word length is 4.7 characters [28]. This\\nmeans the average depth of the Hash Forest is 4.7 in English'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='means the average depth of the Hash Forest is 4.7 in English\\nand the average times of Hash searches are 4.7. Furthermore,\\nsince the algorithm also accelerate the search speed by getting\\nthrough the remaining symbols together when there is only\\none node on these levels, the actual average times of Hash\\nsearches are lower than 4.7.\\nOn summary, a searching through the Hash Forest only\\nperforms a few Hash searches which time complexity is\\nO(1) on average. Therefore, the overall time complexity for\\ndetecting terms should be O(1).\\nD. RULE IDENTIFICATION\\nThe Rule identi\\x1ccation algorithm is designed to calculate\\nthe logical operators rapidly and return ID of all matching\\nBoolean expressions of rules.\\n1) THE ENCODING OF THE BOOLEAN EXPRESSIONS\\nTake a look at the typical logical rule expression in (1) again.\\n.T11jjT12jj\\x01\\x01\\x01jj T1n/&&\\n.T21jjT22jj\\x01\\x01\\x01jj T2n/&&\\n\\x01\\x01\\x01\\n.Tm1jjTm2jj\\x01\\x01\\x01jj Tmn/\\nwhere Tijrepresents a spam term in logical rule expression.\\nApparently, expression always contains a few brackets.\\nAlso, an identi\\x1ccation of any term in a bracket indicates\\nBoolean true of the bracket. The rule is identi\\x1ced matching\\nonly when all pair of brackets in a rule Boolean expression\\nequal TRUE. The aim of expression encoding is to make the\\ncode equals 0 while all brackets equal true and the rule is\\nmatching.Therefore, the Boolean expressions are encoded at bracket\\nlevel as shown in FIGURE 7(a). Each bracket is encoded\\nas one Boolean value. To do so, a code of any expression'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"as one Boolean value. To do so, a code of any expression\\nis \\x1crst initialized as a ``1...1'' string. Each character ``1''\\nrepresents a bracket in an expression. Once a term in a bracket\\nis detected, the bracket is TURE and the character ``1''\\nresponding to the bracket is set to ``0'' at once. Also, if all\\nbrackets are true, the code is ``0...0'' and its numerical\\nvalue equals 0. At this time, a rule matching is reported.\\nSuch codes can represent and calculate &&(AND operator ),\\njj(OR operator ) and ( ) ( bracket operator ). Furthermore, this\\nprocess is O(1) time complexity.\\nFIGURE 7(b) shows a speci\\x1cc example of Boolean\\nexpression encoding. In the example, the rule expression\\nhas 4 brackets. It is encoded as a four-character string\\n``1111''. Each character ``1'' represents a bracket from left\\nto right respectively. Also, a detection of any term in each\\nbracket indicates the logical Boolean TRUE of the bracket.\\nFor instance, if Company is detected, the bracket (Corp.\\njjCompanyjjCo.jjDiscountjjIMPORTANTjjDebt) is TRUE\\nand its corresponding character, or called bit, is set to `` 0''.\\nAt this time, the string is ``0111''. With more terms are\\ndetected, more brackets are TRUE and more characters ``1''\\nare set to ``0''. When the string is ``0000'', its numerical value\\nequals 0. The matching rule is reported for logging.\\n2) HASH FOREST WITH EXPRESSION CODE EXTENSIONS\\nLIST\\nExpression code extensions lists are the lists of rule expres-\"),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='LIST\\nExpression code extensions lists are the lists of rule expres-\\nsion ID and term position pairs. The term position value logs\\nthat bracket the term exists in the rule.\\nFor example, as shown in FIGURE 7(b), the term Contact\\nexist in the 4thbracket of the rule expression. The expression\\ncode expressions should be [ <rule expression ID >, 4]. The\\n\\x1clter will use the position 4to set the 4thbit of the code\\nto 0 when detecting term Contact .\\nUsually, a term may exist in more than one rules.\\nTherefore, a list of rule expression ID and term position pairs\\nlinks to the leaf node of the term indicating all rules in the list\\ncontains the term.\\nWhile initializing the \\x1clter, all rule Boolean expres-\\nsions are traversed once to form the Hash Forest.\\n82658 VOLUME 8, 2020T. Xia: Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\\nAlso, the initializing preprocess simultaneously collects term\\nposition data, generates the lists of expression ID and term\\nposition ID pairs and links them to the Hash Forest.\\nFor example, suppose a rule with ID 0001 is (ca$hjj\\ncredit)&&(cost)&&(loan jjdebt) . The position number of\\neach term depends on the number of bracket where it exists.\\nTheir code extensions link to the Hash Forest, as shown in\\nFIGURE 8. The position of term ca$h andcredit is1as\\nthey exist in the \\x1crst bracket. The term costis in position 2\\nbecause it is in the second bracket. Thus, loan anddebt has\\nthe position 3.'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='because it is in the second bracket. Thus, loan anddebt has\\nthe position 3.\\nFIGURE 8. An example rule in hash forest with code extensions list.\\nA more general example of Hash Forest with Code\\nExtensions Lists is shown in FIGURE 9. Note the black solid\\ncircles represent symbols of terms.\\nFIGURE 9. Hash forest with code extensions list.\\n3) TIME COMPLEXITY ANALYSIS OF RULE IDENTIFICATION\\nOnce a term is detected, all rules containing the term are\\nmatching candidates. Their codes are set by bit based on the\\ncode extensions list of the term. The corresponding bit of code\\nis set to 0. When the algorithm ends, the only remaining work\\nis to \\x1cnd all matching expression by checking straight-zero\\ncodes.\\nFurthermore, the codes are stored as a String. Each bit of\\ncodes can be accessed by a char array, such as String[]. So, the\\ntime complexity of setting codes by bit is O(1). Once a term\\nis detected, the codes of corresponding rules are set. If eachbit of a code is set to 0, it indicates that all brackets in the rule\\nis TRUE and the rule is matched. At this time, the numerical\\nvalue of the String equals 0, a matching event is reported and\\nthe rule ID is logged in the matching rules list.\\nThe time complexity of these steps is all irrelevant to either\\nrule size or rule term vocabulary. The overall time complexity\\nof rule identi\\x1ccation is O(1).\\nE. THE SPAM SMS DETECTION ALGORITHM\\nThis part will take SMS spam \\x1cltering as an example to'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='E. THE SPAM SMS DETECTION ALGORITHM\\nThis part will take SMS spam \\x1cltering as an example to\\nillustrate how a SMS message is traversed and how the logical\\nrule expressions are identi\\x1ced during the SMS traversing\\nprocess.\\nA SMS message is traversed only once. Since SMS\\nmessage always has a limit length, in this respect, the time\\ncomplexity of processing a single SMS message is also\\nconstant.\\nFIGURE 10 shows the traversing process of a SMS\\nmessage.\\nFIGURE 10. The SMS traversing process.\\nIV. EXPERIMENTAL RESULTS AND DISCUSSION\\nA. EXPERIMENTAL RESULTS\\nThe experiment is based on production environmental data of\\nthe SMS service company cooperated with us mentioned in\\nIntroduction.\\nThe experiment is designed to validate the constant time\\ncomplexity of the proposed spam detection algorithm. The\\n\\x1clter processes a given set of SMS and \\x1clters certain spam\\nSMS in it. Then, we will check if the processing time\\nremains stable while adding more rules with term vocabulary\\nexpanding.\\nVOLUME 8, 2020 82659T. Xia: Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\\nAlgorithm 1 Single SMS Filtering\\nStep 1. Start processing from the 1st symbol of the SMS\\nmessage.\\nStep 2. Perform a Hash search on 1st level of the Hash\\nForest. If not found, go to Step 7.\\nStep 3. Perform a Hash search on 2nd level of the Hash\\nForest. If not found, go to Step 7.\\nStep 4. Perform a Hash search on ithlevel of the Hash\\nForest. If not found, go to Step 7.'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"Step 4. Perform a Hash search on ithlevel of the Hash\\nForest. If not found, go to Step 7.\\nStep 5. Perform a Hash search on the leaf level of the Hash\\nForest. If not found, go to Step 7.\\nStep 6. Term detected and set code extensions by bit.\\nIf the numerical value of the code is 0, a matching rule is\\nreported.\\nStep 7. If the current symbol is not the last one, process the\\nnext symbol and go to step 2.\\nStep 8. Check if any matching rules are reported. If so,\\nreport spam SMS with the rules' IDs.\\nThe \\x1clter speed is too fast to notice the time difference for\\na small size of SMS set. To make the processing time visible,\\nthe given set of SMS has a size of 500,000.\\nTo verify the overall spam detection time, the experiment\\nis designed to perform on 9 different sizes of rules. The \\x1crst\\nbatch has 120 rules and they \\x1clter out 28533 spam SMS.\\nThen, 10 rules are added each time. The 2ndone contains\\n130 in total. And so on, the 9thgroup contains 200 rules. The\\nterm vocabulary expands while adding more rules.\\nThe experiment is run on a MacBook Pro 2018 and the\\nresults are shown in TABLE 3.\\nTABLE 3. Experimental results.\\nPlease note that 500,000 SMS are \\x1cltered below\\n26 seconds. The \\x1cltering speed of the algorithm running on a\\nlaptop is 70,000,000 SMS per hour. Actually, our cooperated\\nSMS service company reports a nearly 10 times faster speed\\non their server.\\nAs shown in FIGURE 11, the time consumed to \\x1clter the\\n50,000 spam remains stable while rules consistently increase.\"),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='50,000 spam remains stable while rules consistently increase.\\nIn this procedure, term vocabulary expands much larger than\\nits original. Therefore, the experiment results validate the\\nconstant time complexity of the spam detection algorithm\\nproposed in this paper.\\nFIGURE 11. The experimental results: consumed time (seconds) and\\nexpanding term vocabulary.\\nIn addition, the methods and algorithms in this paper have\\nalso been implemented in a SMS service company in anti-\\nspam industry. 80 million SMS are sent per day on average\\nand 150 million are sent on peak days. The rules keep raising.\\nAccording to our latest contact, the company is currently\\nholding 110k rules, including 10k black rules and 100k white\\nrules. The processing speed remains stable regardless the\\nincreasing size of rules and its vocabulary. Before imple-\\nmenting the proposed algorithm, the company had 6 powerful\\nservers to parallel compute their old \\x1cltering program. Now,\\nonly one virtual server is dedicated to \\x1clter spam SMS and\\nits CPU occupation is low.\\nB. AN ADDITIONAL FEATURE: SEQUENTIAL MATCHING\\nSequential Matching is an additional feature of the method\\npresented in this paper. In addition to Boolean expression\\nmatching, the SMS service company also demands each pair\\nof brackets of an expression must be satis\\x1ced one after\\nanother sequentially.\\nTake the rule (ca$hjjcredit)&&(cost)&&(loan jjdebt) for\\ninstance, the second bracket is TRUE not only when term cost'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content='instance, the second bracket is TRUE not only when term cost\\nis detected but also when the \\x1crst bracket is already TRUE.\\nAlso, the third bracket is TRUE not only when term loan or\\ndebt is detected but also when the \\x1crst and second brackets\\nare already TRUE.\\nThis feature can be achieved by setting a criterion to\\nexpression code setting process. That is, a bit of a code can\\nonly be modi\\x1ced unless the preceding bits have been set to 0.\\nThis can be done by checking the numerical value of the\\npreceding bits.\\nV. CONCLUSION AND FUTURE WORK\\nIn this paper, a constant time complexity spam detection\\nalgorithm was put forward to boost throughout on rule-based\\n\\x1cltering systems and the computational complexity of the\\nproposed algorithm was analyzed. Since each step in the pro-\\nposed algorithm, including detecting a term and calculating\\nlogical operators of expressions, has O(1) time complexity,\\n82660 VOLUME 8, 2020T. Xia: Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems\\nthe overall time complexity for spam detection is O(1). That\\nis, the speed of the spam detection algorithm presented in this\\npaper is independent of rule size and rule term vocabulary.\\nThe experiment results validated the O(1) time complexity as\\nthe spam detection rules and its terms increasing.\\nWe are now working on upgrading the algorithm to make\\nit much more \\x1dexible. Actually, the ! (NOT operator) can be\\nsupported by a simple modi\\x1ccation. More operators such as'),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"supported by a simple modi\\x1ccation. More operators such as\\n`<D', `<', `>D' or `>' will be supported in the future.\\nIn addition, more technics will be engaged in our future\\nresearch to work on incremental rules and their vocabulary,\\nincluding one-pass compressing techniques [29] and rough\\nset [30].\\nREFERENCES\\n[1] Internet World Stats. (Jun. 30, 2019). Internet Usage Statistics The Internet\\nBig Picture . Accessed: Feb. 21, 2020. [Online]. Available: https://www.\\ninternetworldstats.com/stats.htm\\n[2] Symantec Corporation. (Jan. 12, 2010). 2000-2009 The Spam Explo-\\nsion. Accessed: Feb. 21, 2020. [Online]. Available: https://www.\\nsymantec.com/connect/blogs/2000-2009-spam-explosions\\n[3] V. V. Arutyunov, ``Spam: Its past, present, and future,'' Sci. Tech. Inf.\\nProcess. , vol. 40, no. 4, pp. 205\\x15211, Apr. 2014.\\n[4] S. Corporation. (Feb. 2019). Internet Security Threat Report 2019 Volumn\\n24. Accessed: Feb. 21, 2020. [Online]. Available: https://www.symantec.\\ncom/content/dam/symantec/docs/reports/istr-24-2019-en.pdf\\n[5] H. Shaban. (Sep. 19, 2019). Nearly Half of Cellphone Calls\\nWill be Scams by 2019, Report Says . The Washington Post.\\nAccessed: Feb. 21, 2020. [Online]. Available: https://www.\\nwashingtonpost.com/technology/2018/09/19/nearly-half-cellphone-\\ncalls-will-be-scams-by-report-says/\\n[6] O. Abayomi-Alli, S. Misra, A. Abayomi-Alli, and M. Odusami, ``A review\\nof soft techniques for SMS spam classi\\x1ccation: Methods, approaches\"),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"of soft techniques for SMS spam classi\\x1ccation: Methods, approaches\\nand applications,'' Eng. Appl. Artif. Intell. , vol. 86, pp. 197\\x15212,\\nNov. 2019.\\n[7] S. M. Abdulhamid, M. S. A. Latiff, H. Chiroma, O. Osho,\\nG. Abdul-Salaam, A. I. Abubakar, T. Herawan, ``A review on mobile\\nSMS spam \\x1cltering techniques,'' IEEE Access , vol. 5, pp. 15650\\x1515666,\\n2017.\\n[8] E. Mathisen, ``Security challenges and solutions in cloud computing,''\\ninProc. 5th IEEE Int. Conf. Digit. Ecosystems Technol. (IEEE DEST) ,\\nDaejeon, South Korea, Jun. 2011, pp. 208\\x15212.\\n[9] The Apache Spam Assassin Group. (2003). The First Enterprise Open-\\nSource Spam Filter . Accessed: Dec. 1, 2019. [Online]. Available:\\nhttp://spamassassin.apache.org/\\n[10] D. Ruano-Ordás, J. Fdez-Glez, F. Fdez-Riverola, and J. R. Méndez,\\n``Effective scheduling strategies for boosting performance on rule-based\\nspam \\x1cltering frameworks,'' J. Syst. Softw. , vol. 86, no. 12, pp. 3151\\x153161,\\nDec. 2013.\\n[11] D. Ruano-Ordás, J. Fdez-Glez, F. Fdez-Riverola, and J. R. Méndez,\\n``Using new scheduling heuristics based on resource consumption infor-\\nmation for increasing throughput on rule-based spam \\x1cltering systems,''\\nSoftw., Pract. Exper. , vol. 46, no. 8, pp. 1035\\x151051, Jul. 2015.\\n[12] S. Mostinckx, T. Van Cutsem, S. Timbermont, E. G. Boix, É. Tanter,\\nand W. De Meuter, ``RuleSIM: A toolkit for simulating the operation and\\nimproving throughput of rule-based spam \\x1clter,'' Softw., Pract. Exper. ,\\nvol. 46, no. 8, pp. 1091\\x151108, Aug. 2016.\"),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"vol. 46, no. 8, pp. 1091\\x151108, Aug. 2016.\\n[13] H. Drucker, D. Wu, and V. N. Vapnik, ``Support vector machines for spam\\ncategorization,'' IEEE Trans. Neural Netw. , vol. 10, no. 5, pp. 1048\\x151054,\\nSep. 1999.\\n[14] V. P. Deshpande, R. F. Erbacher, and C. Harris, ``An evaluation of naive\\nBayesian anti-spam \\x1cltering techniques,'' in Proc. IEEE SMC Inf. Assur-\\nance Secur. Workshop , Jun. 2007, pp. 9\\x1517.[15] M. Z. Gashti, ``Detection of spam email by combining harmony search\\nalgorithm and decision tree,'' Eng. Techlonogy Appl. Sci. Res. , vol. 7,\\npp. 1713\\x151718, Jun. 2017.\\n[16] A. A. Aburomman and M. B. Ibne Reaz, ``A novel weighted support vector\\nmachines multiclass classi\\x1cer based on differential evolution for intrusion\\ndetection systems,'' Inf. Sci. , vol. 414, pp. 225\\x15246, Nov. 2017.\\n[17] T. Teraguchi, K. Nakamura, S. Tanaka, and K. Kitano, ``Detection method\\nof blog spam based on categorization and time series information,'' in\\nProc. 26th Int. Conf. Adv. Inf. Netw. Appl. Workshops (WAINA) , 2012,\\npp. 801\\x15808.\\n[18] O. M. E. Ebadati and F. Ahmadzadeh, ``Classi\\x1ccation spam email with\\nelimination of unsuitable features with hybrid of GA-naive Bayes,'' J. Inf.\\nKnowl. Manage. , vol. 18, no. 1, Mar. 2019, Art. no. 1950008.\\n[19] D. Delvia Ari\\x1cn, Shau\\x1cah, and M. A. Bijaksana, ``Enhancing spam detec-\\ntion on mobile phone short message service (SMS) performance using\\nFP-growth and naive bayes classi\\x1cer,'' in Proc. IEEE Asia Paci\\x1cc Conf.\\nWireless Mobile (APWiMob) , Sep. 2016, pp. 80\\x1584.\"),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"Wireless Mobile (APWiMob) , Sep. 2016, pp. 80\\x1584.\\n[20] X. Carreras and L. Marquez, ``Boosting trees for anti-spam email \\x1clter-\\ning,'' in Proc. 4th Int. Conf. Recent Adv. Natural Lang. Process. , Sep. 2001,\\npp. 58\\x1564.\\n[21] S. Madisetty and M. S. Desarkar, ``A neural network-based ensemble\\napproach for spam detection in Twitter,'' IEEE Trans. Comput. Social Syst. ,\\nvol. 5, no. 4, pp. 973\\x15984, Dec. 2018.\\n[22] M. Jeftovic. (Jan. 2, 1998). Filter.Plx: A Context/Keyword\\nBased Spam Filter, Internet Archive WayBack Machine .\\nAccessed: Feb. 21, 2020. [Online]. Available: http://web.archive.\\norg/web/19981207030000/antispam.schmooze.net/\\x1clter/File:\\x1clter.plx\\n[23] G. Robinson. (1997). The Spamometer, Spamometer .\\nAccessed: Feb. 21, 2020. [Online]. Available: http://jon.es/\\nspamometer/File:spamometer.tar.gz\\n[24] A. Garg, R. Battiti, and R. G. Cascella, ```May I borrow your \\x1clter?'\\nExchanging \\x1clters to combat spam in a community,'' in Proc. 20th Int.\\nConf. Adv. Inf. Netw. Appl. (AINA) , vol. 1, Apr. 2006, pp. 1\\x155.\\n[25] N. Pérez-Díaz, D. Ruano-Ordas, F. Fdez-Riverola, and J. R. Méndez,\\n``Wirebrush4SPAM: A novel framework for improving ef\\x1cciency on spam\\n\\x1cltering services,'' Softw., Pract. Exper. , vol. 43, no. 11, pp. 1299\\x151318,\\nJun. 2012.\\n[26] J. R. Méndez, M. Reboiro-Jato, F. Díaz, E. Díaz, and F. Fdez-Riverola,\\n``Grindstone4Spam: An optimization toolkit for boosting e-mail classi\\x1c-\\ncation,'' J. Syst. Softw. , vol. 85, no. 12, pp. 2909\\x152920, Dec. 2012.\"),\n",
       " Document(metadata={'title': 'A Constant Time Complexity Spam Detection Algorithm for Boosting Throughput on Rule-Based Filtering Systems', 'year': 2020}, page_content=\"cation,'' J. Syst. Softw. , vol. 85, no. 12, pp. 2909\\x152920, Dec. 2012.\\n[27] B. Goi, M. U. Siddiqi, and H.-T. Chuah, ``Computational complexity and\\nimplementation aspects of the incremental hash function,'' IEEE Trans.\\nConsum. Electron. , vol. 49, no. 4, pp. 1249\\x151255, Nov. 2003.\\n[28] M. Mayzner and E. Srhldcu. English Letter Frequency\\nCounts . Accessed: Dec. 1, 2019. [Online]. Available: http://\\nnorvig.com/mayzner.html\\n[29] C. Hou and Z.-H. Zhou, ``One-pass learning with incremental and decre-\\nmental features,'' IEEE Trans. Pattern Anal. Mach. Intell. , vol. 40, no. 11,\\npp. 2776\\x152792, Nov. 2018.\\n[30] P. K. Roy, J. P. Singh, and S. Banerjee, ``Deep learning to \\x1clter SMS spam,''\\nFuture Gener. Comput. Syst. , vol. 102, pp. 524\\x15533, Jan. 2020.\\nTIAN XIA received the M.S. and Ph.D. degrees\\nin computer application from East China Normal\\nUniversity, Shanghai, China, in 2004 and 2007,\\nrespectively.\\nSince 2007, he has been working as the Director\\nof the Digital Media Technology Department,\\nShanghai Polytechnic University, and promoted\\nto an Associate Professor, in 2009. He also\\ncooperated with companies on Computing Adver-\\ntisements, Big Data Mining, and Anti-spam tech-\\nnologies. His research interests include machine learning, natural language\\nprocessing, and deep learning.\\nVOLUME 8, 2020 82661\"),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='604 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\nA Survey of the Usages of Deep Learning for\\nNatural Language Processing\\nDaniel W. Otter, Julian R. Medina, and Jugal K. Kalita\\nAbstract — Over the last several years, the ﬁeld of natural\\nlanguage processing has been propelled forward by an explosionin the use of deep learning models. This article provides abrief introduction to the ﬁeld and a quick overview of deeplearning architectures and methods. It then sifts through theplethora of recent studies and summarizes a large assortment ofrelevant contributions. Analyzed research areas include severalcore linguistic processing issues in addition to many applicationsof computational linguistics. A discussion of the current state ofthe art is then provided along w ith recommendations for future\\nresearch in the ﬁeld.\\nIndex Terms — Computational linguistics, deep learning,\\nmachine learning, natural language processing (NLP), neuralnetworks.\\nI. I NTRODUCTION\\nTHE ﬁeld of natural language processing (NLP) encom-\\npasses a variety of topics, which involves the compu-\\ntational processing and understanding of human languages.\\nSince the 1980s, the ﬁeld has increasingly relied on data-driven\\ncomputation involving statistics, probability, and machinelearning [1], [2]. Recent increases in computational power\\nand parallelization, harnessed by graphical processing units'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='and parallelization, harnessed by graphical processing units\\n(GPUs) [3], [4], now allow for “deep learning,” which utilizesartiﬁcial neural networks (ANNs), sometimes with billions\\nof trainable parameters [5]. In addition, the contemporary\\navailability of large data sets, facilitated by sophisticateddata collection processes, enab les the training of such deep\\narchitectures [6]–[8].\\nIn recent years, researchers and practitioners in NLP have\\nleveraged the power of modern ANNs with many propitious\\nresults, beginning in large part with the pioneering work of\\nCollobert et al. [9]. In the very recent past, the use of deep\\nlearning has considerably upsurged [10], [11]. This has led\\nto signiﬁcant advances both in core areas of NLP and inareas in which it is directly app lied to achieve practical and\\nuseful objectives. This article provides a brief introduction\\nto both NLP and deep neural networks (DNNs) and thenpresents an extensive discussion on how deep learning is being\\nused to solve current problems in NLP. While several other\\narticles and books on the topic have been published [10], [12],\\nManuscript received December 6, 2018; revised July 11, 2019 and\\nNovember 11, 2019; accepted December 3, 2019. Date of publication April 21,2020; date of current version February 4, 2021. This work was supportedin part by the National Science Foundation under Grant IIS-1359275 and\\nGrant IIS-1659788. (Corresponding author: Jugal K. Kalita.)'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Grant IIS-1659788. (Corresponding author: Jugal K. Kalita.)\\nThe authors are with the Department of Computer Science, University of\\nColorado at Colorado Springs, Colorado Springs, CO 80918 USA (e-mail:\\njkalita@uccs.edu).\\nColor versions of one or more of the ﬁgures in this article are available\\nonline at https://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TNNLS.2020.2979670none of them have extensively covered the state of the art\\nin as many areas within it. Furthermore, no other survey\\nhas examined not only the applications of deep learning tocomputational linguistics but also the underlying theory and\\ntraditional NLP tasks. In addition to the discussion of recent\\nrevolutionary developments in the ﬁeld, this article will beuseful to readers who want to familiarize themselves quickly\\nwith the current state of the art before embarking upon further\\nadvanced research and practice.\\nThe topics of NLP and AI, including deep learning, are\\nintroduced in Section II. The w ays in which deep learning\\nhas been used to solve problems in core areas of NLPare presented in Section III. The section is broken down\\ninto several subsections, namely natural language modeling\\n(Section III-A), morphology (Section III-B), parsing\\n(Section III-C), and semantics (Section III-D). Applications\\nof deep learning to more practical areas are discussed inSection IV. Speciﬁcally discu ssed are information retrieval\\n(IR) (Section IV-A), information extraction Section (IV-B),'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='(IR) (Section IV-A), information extraction Section (IV-B),\\ntext classiﬁcation (Sectio n IV-C), text generation\\n(Section IV-D), summarization (Section IV-E), question ans-\\nwering (QA) (Section IV-F), and machine translation\\n(Section IV-G). Conclusions are then drawn in Section Vwith a brief summary of the state of the art as well as\\npredictions, suggestions, and other thoughts on the future of\\nthis dynamically evolving area.\\nII. O\\nVERVIEW OF NLP AND DEEPLEARNING\\nIn this section, signiﬁcant issues that draw the attention of\\nresearchers and practitioners are introduced, followed by a\\nbrisk explanation of the deep l earning architectures commonly\\nused in the ﬁeld.\\nA. Natural Language Processing\\nThe ﬁeld of NLP, also known as computational linguis-\\ntics, involves the engineering of computational models and\\nprocesses to solve practical problems in understanding humanlanguages. These solutions are used to build useful software.\\nWork in NLP can be divided into two broad subareas: core\\nareas and applications, although it is sometimes difﬁcultto distinguish clearly to which areas issues belong. The\\ncore areas address fundamental problems such as language\\nmodeling, which underscores quantifying associations among\\nnaturally occurring words; morphological processing, dealing\\nwith segmentation of meaningful components of words andidentifying the true parts of speech (POSs) of words as\\nused; syntactic processing, or parsing, which builds sentence'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='used; syntactic processing, or parsing, which builds sentence\\ndiagrams as possible precursors to semantic processing; and\\n2162-237X © 2020 I EEE. Personal u se is perm itted, but republication/redistri bution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 605\\nFig. 1. Encoder–decoder architectures. While there are multiple options of encoders and decoders available, RNN variants are a common choice for eac h,\\nparticularly the latter. Such a network is shown in (a). Attention mechanisms, such as that present in (b), allow the decoder to determine which portio ns of\\nthe encoding are most relevant at each output step.\\nsemantic processing, which attempts to distill meaning of\\nwords, phrases, and higher level components in text. The\\napplication areas involve topics, such as extraction of usefulinformation (e.g., named entities and relations), translation of\\ntext between and among languages, summarization of written\\nworks, automatic answering of questions by inferring answers,and classiﬁcation and clustering of documents. Often, one\\nneeds to handle one or more of the core issues successfully and\\napply those ideas and procedures to solve practical problems.\\nCurrently, NLP is primarily a data-driven ﬁeld using sta-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Currently, NLP is primarily a data-driven ﬁeld using sta-\\ntistical and probabilistic computations along with machine\\nlearning. In the past, machine learning approaches, such as\\nnaïve Bayes, k-nearest neighbors, hidden Markov models, con-\\nditional random ﬁelds (CRFs), decision trees, random forests,and support vector machines, were widely used. However,\\nduring the past several years, there has been a wholesale trans-\\nformation, and these approach es have been entirely replaced,\\nor at least enhanced, by neural models, discussed next.\\nB. Neural Networks and Deep Learning\\nNeural networks are composed of interconnected nodes,\\nor neurons, each receiving so me number of inputs and supply-\\ning an output. Each of the nodes in the output layers performs\\nweighted sum computation on the values they receive from theinput nodes and then generate outputs using simple nonlinear\\ntransformation functions on these summations. Corrections to\\nthe weights are made in response to individual errors or losses\\nthat the networks exhibit at the output nodes. Such corrections\\nare usually made in modern networks using stochastic gradientdescent, considering the derivatives of errors at the nodes,\\nan approach called backpropagation [13]. The main factors\\nthat distinguish different type s of networks from each other are\\nhow the nodes are connected and the number of layers. Basic\\nnetworks in which all nodes can be organized into sequential\\nlayers, with every node recei ving inputs onl y from nodes'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='layers, with every node recei ving inputs onl y from nodes\\nin earlier layers, are known as f eedforward neural networks\\n(FFNNs). While there is no cl ear consensus on exactly what\\ndeﬁnes a DNN, generally, networks with multiple hiddenlayers are considered deep and those with many layers are\\nconsidered very deep [7].\\n1) Convolutional Neural Networks: Convolutional neural\\nnetworks (CNNs) [14], [15], built upon Fukashima’s neocogni-tron [16], [17], derive the name from the convolution operation\\nin mathematics and signal processing. CNNs use functions,\\nknown as ﬁlters, allowing for simultaneous analysis of dif-ferent features in the data [18], [19]. CNNs are extensively\\nused in image and video processing, as well as speech and\\nNLP [20]–[23]. Often, it is not important precisely wherecertain features occur, but rather whether or not they appear\\nin particular localities. Ther efore, pooling operations can be\\nused to minimize the size of feature maps (the outputs of theconvolutional ﬁlters). The sizes of such pools are generally\\nsmall to prevent the loss of too much precision.\\n2) Recursive Neural Networks: Much like CNNs, recur-\\nsive networks [24], [25] use a form of weight sharing tominimize training. However, whereas CNNs share weights\\nhorizontally (within a layer), recursive nets share weights\\nvertically (between layers). This is particularly appealing, as itallows for easy modeling of structures such as parse trees.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='In recursive networks, a single tensor (or a generalized matrix)\\nof weights can be used at a low level in the tree and then usedrecursively at successively higher levels [26].\\n3) Recurrent Neural Networks and Long Short-Term Mem-\\nory Networks: A type of recursive neural network that has\\nbeen used heavily is the recurrent neural network (RNN) [27],\\n[28]. Since much of NLP is dependent on the order of wordsor other elements such as phonemes or sentences, it is useful\\nto have memory of the previous elements when processing\\nnew ones [29]–[31]. Sometimes, backward dependencies exist,i.e., correct processing of some words may depend on words\\nthat follow. Thus, it is beneﬁcial to look at sentences in\\nboth directions, forward and backward, using two RNN layersand combining their outputs. This arrangement of RNNs is\\ncalled a bidirectional RNN. It may also lead to a better ﬁnal\\nrepresentation if there is a sequence of RNN layers. This mayallow the effect of an input to linger longer than a single RNN\\nlayer, allowing for longer term effects. This setup of sequential\\nRNN cells is called an RNN stack [32], [33].\\nOne highly engineered RNN is the long short-term mem-\\nory (LSTM) network [34], [35]. In LSTMs, the recursive nodesare composed of several individual neurons connected in a\\nmanner designed to retain, forget, or expose speciﬁc infor-\\nmation. Whereas generic RNNs with single neurons feedingback to themselves technically have some memory of long'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='passed results, these results are diluted with each successive\\niteration. Oftentimes, it is important to remember informationfrom the distant past, while at th e same time, other very recent\\ninformation may not be important. By using LSTM blocks,\\nthis important information can be retained much longer, whileirrelevant information can be forgotten. A slightly simpler\\nvariant of the LSTM, called the gated recurrent unit (GRU),\\nhas been shown to perform as well as or better than standard\\nLSTMs in many tasks [36], [37].\\n4) Attention Mechanisms and Transformer: For tasks such\\nas machine translation, text summarization, or captioning,\\nthe output is in textual form. Typically, this is done through\\nthe use of encoder–decoder pairs. An encoding ANN is usedto produce a vector of a particular length and a decoding ANN\\nis used to return variable-length text based on this vector. The\\nproblem with this scheme, which is shown in Fig. 1(a), is thatthe RNN is forced to encode an entire sequence to a ﬁnite\\nlength vector, without regard to whether or not any of the\\ninputs are more important than others.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 606 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\nFig. 2. Transformer model. (a) Transformer with four “encoders” followe d by four “decoders,” all following a “ positional encoder.” (b) Inner workin gs of'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='each “encoder,” which contains a self-attention layer followed by a feed fo rward layer. (c) Inner workings of each “decoder,” which contains a self-a ttention\\nlayer followed by an attentional encoder–d ecoder layer and then a feed forward layer.\\nA robust solution to this is that of attention. The ﬁrst\\nnoted use of an attention mechanism [38] used a dense\\nlayer for annotated weighting of an RNN’s hidden state,\\nallowing the network to learn what to pay attention to inaccordance with the current hidden state and annotation.\\nSuch a mechanism is present in Fig. 1(b). Variants of the\\nmechanism have been introduced, popular ones includingconvolutional [39], intratemporal [40], gated [41], and self-\\nattention [42]. Self-attention involves providing attention to\\nwords in the same sentence. For example, during encoding a\\nword in an input sentence, it is beneﬁcial to project variable\\namounts of attention to other words in the sentence. Duringdecoding to produce a resulting sentence, it makes sense\\nto provide appropriate attention to words that have already\\nbeen produced. Self-attention, in particular, has become widelyused in a state-of-the-art encode r–decoder model called trans-\\nformer [42]. The transformer model, shown in Fig. 2, has\\na number of encoders and decoders stacked on top of eachother, self-attention in each of the encoder and decoder units,\\nand cross attention between the encoders and the decoders.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='and cross attention between the encoders and the decoders.\\nIt uses multiple instances of attention in parallel and eschewsthe use of recurrences and convolutions. The transformer has\\nbecome a quintessential component in most state-of-the-art\\nneural networks for NLP.\\n5) Residual Connections and Dropout: In deep networks,\\ntrained via backpropagation [13], the gradients used to correctfor error often vanish or explode [43]. This can be mitigated by\\nchoosing activation functions, such as the rectiﬁed linear unit\\n(ReLU) [44], which do not exhibit regions that are arêticallysteep or have bosonically small gradients. Also, in response\\nto this issue, as well as others [45], residual connections are\\noften used. Such connections are simply those that skip layers(usually one). If used in every alternating layer, this cuts in\\nhalf the number of layers through which the gradient must\\nbackpropagate. Such a network is known as a residual network\\n(ResNet). A number of variants exist, including highway\\nnetworks [46] and DenseNets [47].\\nAnother important method used in training ANNs is\\ndropout. In dropout, some connections and maybe even nodes\\nare deactivated, usually randomly, for each training batch(small set of examples), varying which nodes are deactivated\\neach batch. This forces the network to distribute its memory\\nacross multiple paths, helping with generalization and lessen-ing the likelihood of overﬁtting to the training data.\\nIII. D\\nEEPLEARNING IN CORE AREAS OF NLP'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='III. D\\nEEPLEARNING IN CORE AREAS OF NLP\\nThe core issues are those that are inherently present in\\nany computational linguistic system. To perform translation,text summarization, image captioning, or any other linguistic\\ntask, there must be some understanding of the underlying\\nlanguage. This understanding can be broken down into at least\\nfour main areas: language modeling, morphology, parsing, andsemantics. The number of scholarly works in each area over\\nthe last decade is shown in Fig. 3.\\nLanguage modeling can be viewed in two ways. First,\\nit determines which words follow which. By extension, how-\\never, this can be viewed as determining what words mean,\\nas individual words are only weakly meaningful, deriving\\ntheir full value only from their interactions with other words.\\nMorphology is the study of how words themselves are formed.It considers the roots of words and the use of preﬁxes and\\nsufﬁxes, compounds, and other intraword devices, to display\\ntense, gender, plurality, and a other linguistic constructs.Parsing considers which words modify others, forming con-\\nstituents, leading to a sentential structure. The area of seman-\\ntics is the study of what words mean. It considers the meaningsof the individual words and how they relate to and modify\\nothers, as well as the contexts these words appear in and some\\ndegree of world knowledge, i.e., “common sense.”\\nThere is a signiﬁcant amount of overlap between each of\\nthese areas. Therefore, many models analyzed can be classiﬁed'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='these areas. Therefore, many models analyzed can be classiﬁed\\nas belonging in multiple sections. As such, they are discussed\\nin the most relevant sections with logical connections to those\\nother places where they also interact.\\nA. Language Modeling and Word Embeddings\\nArguably, the most important task in NLP is that of language\\nmodeling. Language modeling is an essential piece of almost\\nany application of NLP. Language modeling is the process\\nof creating a model to predict words or simple linguistic\\ncomponents given previous words or components [48]. This\\nis useful for applications in which a user types input, toprovide predictive ability for fast text entry. However, its power\\nand versatility emanate from the fact that it can implicitly\\ncapture syntactic and semantic relationships among wordsor components in a linear neighborhood, making it useful\\nfor tasks such as machine translation or text summarization.\\nUsing prediction, such programs can generate more relevant,human-sounding sentences.\\n1) Neural Language Modeling: A problem with statisti-\\ncal language models was the inability to deal well with\\nsynonyms or out-of-vocabulary (OOV) words that were not\\npresent in the training corpus. Progress was made in solvingthe problems with the introduction of the neural language\\nmodel [49]. While much of NLP took another decade to\\nbegin to use ANNs heavily, the language modeling community'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='begin to use ANNs heavily, the language modeling community\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 607\\nFig. 3. Publication volume for core areas of NLP. The number of publicati ons, indexed by Google Scholar, relating to each topic over the last decade is\\nshown. While all areas have experienced growth, language modeling has grown the most.\\nimmediately took advantage of them and continued to develop\\nsophisticated models, many of which were summarized by DeMulder et al. [50].\\n2) Evaluation of Language Models: While neural networks\\nhave made breakthroughs in the language modeling ﬁeld, it ishard to quantify improvements. It is desirable to evaluate\\nlanguage models independently of the applications in which\\nthey appear. A number of metrics have been proposed, butno perfect solution has yet been found. [51]–[53] The most\\ncommonly used metric is perplexity, which is the inverse\\nprobability of a test set normalized by the number of words.Perplexity is a reasonable measurement for language model-\\nings trained on the same data sets, but when they are trained\\non different vocabularies, the m etric becomes less meaningful.\\nLuckily, there are several benchmark data sets that are used\\nin the ﬁeld, allowing for comparison. Two such data setsare the Penn Treebank (PTB) [54] and the Billion Word\\nBenchmark [55].'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Benchmark [55].\\n3) Memory Networks and Attention Mechanisms in Lan-\\nguage Modeling: Daniluk et al. [56] tested several networks\\nusing variations of attention mechanisms. The ﬁrst network\\nhad a simple attention mechanism, which was not fully con-nected, having a window length of ﬁve. They hypothesized\\nthat using a single value to predict the next token, encode\\ninformation for the attentional unit, and decode the informationin the attentional unit hinders a network, as it is difﬁcult to\\ntrain a single parameter to perform three distinct tasks simulta-\\nneously. Therefore, in the second network, they designed eachnode to have two outputs: one to encode and decode the infor-\\nmation in the attentional unit, and another to predict the next\\ntokens explicitly. In the third network, they further separatedthe outputs, using separate values to encode the information\\nentering the attentional unit and decode the information being\\nretrieved from it. Tests on a Wikipedia corpus showed thatthe attention mechanism improved perplexity compared to the\\nbaseline and that successively adding the second and third\\nparameters led to further increases. It was also noted that only\\nthe previous ﬁve or so tokens carried much value (hence the\\nselection of the window size of ﬁve). Therefore, they testeda fourth network that simply used residual connections from\\neach of the previous ﬁve units. It was found that this network\\nalso provided results compar able to many larger RNNs and'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='also provided results compar able to many larger RNNs and\\nLSTMs, suggesting that reasonable results can be achieved\\nusing simpler networks.\\nAnother recent study was done on the usage of residual\\nmemory networks (RMNs) for language modeling [57]. The\\nauthors found that residual connections skipping two layers\\nwere most effective, followed closely by those skipping asingle layer. In particular, a residual connection was present\\nbetween the ﬁrst layer and the fourth layer, as was betweenthe ﬁfth layer and the eighth, and between the ninth and the\\ntwelfth. It was found that increasing network depth improved\\nresults, but that when using large batch sizes, memory con-straints were encountered. Network width was not found to\\nbe of particular importance for performance; however, wide\\nnetworks were found to be harder to train. It was found that\\nRMNs are capable of outperforming LSTMs of similar size.\\n4) Convolutional Neural Networks in Language Modeling:\\nA CNN used recently in language modeling replaced thepooling layers with fully connected layers [58]. These layers\\nallowed the feature maps to be reduced to lower dimen-\\nsional spaces just like the poolin g layers. However, whereas\\nany references to the locatio n of such features are lost in\\npooling layers, fully connected layers somewhat retain thisinformation. Three different ar chitectures were implemented:\\na multilayer perceptron CNN (MLPConv) in which the ﬁlters'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='a multilayer perceptron CNN (MLPConv) in which the ﬁlters\\nwere not simply linear, but instead small MLPs [59]; amultilayer CNN (ML-CNN) in which multiple convolutional\\nlayers were stacked on top of each other; and a combination of\\nthese networks called COM, in which kernel sizes for ﬁltersvaried (in this case, they were three and ﬁve). The results\\nshowed that stacking convoluti onal layers was detrimental in\\nlanguage modeling, but both MLPConv and COM reducedperplexity. Combining MLPConv with the varying kernel sizes\\nof COM provided even better results. Analysis showed that the\\nnetworks learned speciﬁc patterns of words, such as, “as... as.”Finally, this study showed that CNNs can be used to capture\\nlong-term dependencies in sentences. Closer words were found\\nto be of greatest importance, but words located farther away\\nwere of some signiﬁcance as well.\\n5) Character-Aware Neural Language Models: While\\nmost CNNs used in NLP receive word embeddings (seeSection III-A6) as input, recent networks have analyzed\\ncharacter-level input instead. For example, the network of\\nKim et al. [60], unlike previous networks [61], accepted only\\ncharacter-level input, rather than combining it with word\\nembeddings. A CNN was used to process the character-levelinput to provide the representations of the words. In a\\nmanner similar to how word embeddings usually are these\\nrepresentations were then fed into an encoder–decoder paircomposed of a highway network (a gated network resembling'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='an LSTM) [46] and an LSTM. They trained the network on\\nthe English PTB, as well as on data sets for Czech, German,Spanish, French, Russian, and Arabic. For every non-English\\nlanguage except Russian, the network outperformed previously\\npublished results [61] in both the large and small data sets.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 608 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\nOn the PTB, the results were produced on par with the\\nexisting state of the art [62]. However, the network had only\\n19 million trainable parameters, which is considerably lower\\nthan others. Since the network focused on morphologicalsimilarities produced by charact er-level analysis, it was more\\ncapable than previous models of handling rare words. Analysis\\nshowed that without the use of highway layers, many wordshad nearest neighbors that were orthographically similar but\\nnot necessarily semantically similar. In addition, the network\\nwas capable of recognizing misspelled words or words notspelled in the standard way (e.g., looooook instead of look)\\nand of recognizing out of vocabulary words. The analysis also\\nshowed that the network was cap able of identifying preﬁxes,\\nroots, and sufﬁxes, as well as understanding hyphenated words,\\nmaking it a robust model.\\nJozefowicz et al. [63] tested a number of architectures'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='making it a robust model.\\nJozefowicz et al. [63] tested a number of architectures\\nproducing character-level outputs [55], [64]–[66]. While many\\nof these models had only been tested on small-scale language\\nmodeling, this study tested them on a large scale, testing them\\nwith the Billion Word Benchmark. The most effective model,\\nachieving a state-of-the-art (for single models) perplexityof 30.0 with 1.04 billion trainable parameters (compared to\\na previous best by a single model of 51.3 with 20 billion\\nparameters [55]), was a large LSTM using a character-levelCNN as an input network. The best performance, however,\\nwas achieved using an ensemble of ten LSTMs. This ensemble,\\nwith a perplexity of 23.7, far surpassed the previous state-of-the-art ensemble [65], which had a perplexity of 41.0.\\n6) Development of Word Embeddings: N o to n l yd on e u r a l\\nlanguage models allow for the prediction of unseen synony-\\nmous words, but also they allow for modeling the relationshipsbetween the words [67], [68]. Vectors with numeric compo-\\nnents, representing individual words, obtained by language\\nmodeling techniques are called embeddings. This is usuallydone either by the use of principle component analysis or by\\ncapturing internal states in a neural language model. (Note\\nthat these are not standard language modelings, but rather arelanguage modelings constructed speciﬁcally for this purpose.)\\nTypically, word embeddings have between 50 and 300 dimen-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Typically, word embeddings have between 50 and 300 dimen-\\nsions. An overused example is that of the distributed represen-tations of the words king,queen ,man,a n d woman . If one takes\\nthe embedding vectors for each of these words, computation\\ncan be performed to obtain highly sensible results. If thevectors representing these words a re, respectively, represented\\nas\\x02k,\\x02q,\\x02m,a n d \\x02w, it can be observed that \\x02k−\\x02q≈\\x02m−\\x02w,\\nwhich is extremely intuitive to human reasoning. In recent\\nyears, word embeddings have been the standard form of input\\nto NLP systems.\\n7) Recent Advances and Challenges: Language model-\\ning has been evolving on a weekly basis, beginning with\\nthe works of Radford et al. [69] and Peters et al. [70].\\nRadford et al. [69] introduced generative pretraining (GPT)\\nwhich pretrained a language model based on the transformer\\nmodel [42] (Section IV-G), learning dependencies of words\\nin sentences and longer segments of text, rather than justthe immediately surrounding words. Peters et al. [70] incorpo-\\nrated bidirectionalism to capture backward context in addition\\nto the forward context, in their Embeddings from LanguageModels (ELMo). In addition, they captured the vectorizations\\nat multiple levels, rather than just the ﬁnal layer. This allowed\\nfor multiple encodings of the same information to be captured,which was empirically shown to signiﬁcantly boost the per-\\nformance.\\nDevlin et al. [71] added the additional unsupervised training'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='formance.\\nDevlin et al. [71] added the additional unsupervised training\\ntasks of random masked neighbor word prediction and next-sentence-prediction (NSP), in w hich given a sentence (or other\\ncontinuous segment of text), another sentence was predicted\\nto either be the next sentence or not. These Bidirectional\\nEncoder Representations from Transformers (BERT) were\\nfurther built upon by Liu et al. [72] to create multitask DNN\\n(MT-DNN) representations, which are the current state of theart in language modeling. The model used a stochastic answer\\nnetwork (SAN) [73], [74] ontop of a BERT-like model. After\\npretraining, the model was trained on a number of differenttasks before being ﬁne-tuned to the task at hand. Using\\nMT-DNN as the language modeling, they achieved state-of-\\nthe-art results on ten out of eleven of the attempted tasks.\\nWhile these pretrained models have made excellent head-\\nway in “understanding” language, as is required for some tasks\\nsuch as entailment inference, it has been hypothesized by some\\nthat these models are learning templates or syntactic patterns\\npresent within the data sets, unr elated to logic or inference.\\nWhen new data sets are created by removing such patterns\\ncarefully, the models do not perform well [75]. In addition,\\nwhile there has been recent wor k on cross-language modeling\\nand universal language modeling, the amount and level of\\nwork need to pick up to address low-resource languages.\\nB. Morphology'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='work need to pick up to address low-resource languages.\\nB. Morphology\\nMorphology is concerned with ﬁnding segments within\\nsingle words, including roots and stems, preﬁxes, sufﬁxes,and—in some languages—inﬁxes. Afﬁxes (preﬁxes, sufﬁxes,\\nand inﬁxes) are used to overtly modify stems for gender,\\nnumber, person, and so on.\\nLuong et al. [76] constructed a morphologically aware lan-\\nguage modeling. An RvNN was used to model the morpho-logical structure. A neural l anguage model was then placed\\non top of the RvNN. The model was trained on the WordSim-\\n353 data set [77], and segmentation was performed using Mor-fessor [78]. Two models were constructed—one using context\\nand one not. It was found that the model that was insensitive\\nto context overaccounted for certain morphological structures.In particular, words with the same stem were clustered together\\neven if they were antonyms. The context-sensitive model\\nperformed better, noting the relationships between the stemsbut also accounting for other features such as the preﬁx “un.”\\nThe model was also tested on several other popular data\\nsets [79]–[81], signiﬁcantly outperforming previous embed-ding models on all.\\nA good morphological analyzer is often important for many\\nNLP tasks. As such, one recent study by Belinkov et al. [82]\\nexamined the extent to which morphology was learned and\\nused by a variety of neural machine translation (NMT)models. A number of translation models were constructed,'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='all translating from English to French, German, Czech,\\nArabic, or Hebrew. Encoders and decoders were LSTM-basedmodels (some with attention mechanisms) or character\\naware CNNs, and the models were trained on the WIT\\n3\\ncorpus [83], [84]. The decoders were then replaced with POS\\ntaggers and morphological taggers, ﬁxing the weights of the\\nencoders to preserve the interna l representations. The effects\\nof the encoders were examined as were the effects of the\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 609\\ndecoders attached during training. The study concluded that\\nthe use of attention mechanis ms decreases the performance\\nof encoders but increases the performance of decoders.\\nFurthermore, it was found that character-aware models aresuperior to others for learning morphology and that the output\\nlanguage affects the performance of the encoders. Speciﬁcally,\\nthe more morphologically rich the output language, the worsethe representations created by the encoders.\\nMorita et al. [85] analyzed a new morphological lan-\\nguage model for unsegmented languages such as Japanese.They constructed an RNN-based model with a beam search\\ndecoder and trained it on an automatically labeled [86] corpus\\nand a manually labeled corpus. The model performed a num-ber of tasks jointly, including morphological analysis, POS'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='tagging, and lemmatization. The model was then tested on\\nthe Kyoto Text Corpus [87] and the Kyoto University WebDocument Leads Corpus [88], outperforming all baselines on\\nall tasks.\\nA recent line of work in morphology is universal mor-\\nphology. This task considers the relationships between the\\nmorphologies of different languages and how they relateto each other, aiming toward the ultimate goal of a single\\nmorphological analyzer. However, to the authors’ knowledge,\\nthere has been only a single study applying deep learningto this area [89] and, even then, only as a supporting task\\nto universal parsing (Section III-C4). For those wishing to\\napply deep learning to this task, several data sets are alreadyavailable, including one from a CoNLL shared task [90].\\nIn addition to universal mor phology, the development of\\nmorphological embeddings, which considers the structures ofwords could aid in multilanguage processing. They could\\npossibly be used across cognate languages, which would be\\nvaluable when some languages are more resourced than others.\\nIn addition, morphological structures may be important in\\nhandling specialized language, such as that used in biomedicalliterature. Since deep learning has become quite entrenched in\\nNLP, better handling of morphological components is likely\\nto improve the performance of overall models.\\nC. Parsing\\nParsing examines how different words and phrases relate'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='C. Parsing\\nParsing examines how different words and phrases relate\\nto each other within a sentence. There are at least twodistinct forms of parsing: constituency parsing and dependency\\nparsing [48]. In constituency parsing, phrasal constituents are\\nextracted from a sentence in a hierarchical fashion. Depen-dency parsing looks at the relationships between the pairs of\\nindividual words.\\nMost recent uses of deep learning in parsing have been in\\ndependency parsing, within which there exists another major\\ndivide in types of solutions. Graph-based parsing constructs a\\nnumber of parse trees that are t hen searched to ﬁnd the correct\\none. Most graph-based approaches are generative models,\\nin which a formal grammar, based on the natural language,is used to construct the trees [48]. More popular in recent\\nyears than graph-based approaches have been transition-based\\napproaches that usually construct only one parse tree. Whilea number of modiﬁcations have been proposed, the standard\\nmethod of transition-based dependency parsing is to create a\\nbuffer containing all of the wo rds in the sentence and stack\\ncontaining only the ROOT label. Words are then pushed onto\\nthe stack, where connections, known as arcs, are made between\\nthe top two items. Once dependencies have been determined,words are popped off the stack. The process continues until the\\nbuffer is empty and only the ROOT label remains on the stack.\\nThree major approaches are used to regulate the conditions in'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Three major approaches are used to regulate the conditions in\\nwhich each of the previously described actions takes place.In the arc-standard approach [91], [92], all dependents are\\nconnected to a word before the word is connected to its parent.\\nIn the arc-eager approach [91], [92], words are connectedto their parents as soon as possible, regardless of whether\\nor not their children are all connected to them. Finally, in\\nthe swap-lazy approach [93], the arc-standard approach ismodiﬁed to allow swapping of positions on the stack. This\\nmakes the graphing of nonprojective edges possible.\\n1) Early Neural Parsing: One early application of deep\\nlearning to NLP, that of Socher et al. [94], [95], included\\nthe use of RNNs with probabilistic context-free grammars\\n(PCFGs) [96], [97]. As far as the authors are aware, the ﬁrst\\nneural model to achieve state-of-the-art performance in parsingwas that of Le and Zuidema [98]. Such performance was\\nachieved on the PTB for both labeled attachment score (LAS)\\nand unlabeled attachment score (UAS) by using an inside-outrecursive neural network, which used two vector represen-\\ntations (an inner and an outer) to allow both top-down and\\nbottom-up ﬂows of data. Vinyals et al. [99] created an LSTM\\nwith an attention mechanism in a syntactic constituency parser,\\nwhich they tested on data from domains different from those\\nof the test data (the English Web Treebank [100] and the\\nQuestion Treebank [101] as opposed to the Wall Street Journal'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Question Treebank [101] as opposed to the Wall Street Journal\\nportion of the PTB [54]), showing that neural models cangeneralize between domains. Embeddings were ﬁrst used in\\ndependency parsing by Stenetorp [102]. This approach used\\nan RNN to create a directed acyclic graph. While this modeldid produce results within 2% of the state of the art (on the\\nWall Street Journal portion of the CoNLL 2008 Shared Task\\ndata set [103]), by the time it reached the end of a sentence,it seemed to have difﬁculty in re membering phrases from early\\nin the sentence.\\n2) Transition-Based Dependency Parsing: Chen and\\nManning [104] pushed the state of the art in both UAS andLAS on both English and Chinese data sets on the English\\nPTB. They accomplished this by using a simple FFNN as\\nthe decision-maker in a transition-based parser. By doing so,they were able to subvert the problem of sparsity persistent\\nin the statistical models.\\nChen and Manning used a simple greedy search, which was\\nreplaced by Zhou et al. [105] with a beam search, achieving\\na signiﬁcant improvement. Weiss et al. [106] improved upon\\nChen and Manning’s work by using a deeper neural network\\nwith residual connections and a perceptron layer placed after\\nthe softmax layer. They were able to train on signiﬁcantly more\\nexamples than typical by using tritraining [107], a process in\\nwhich potential data samples ar e fed to two other parsers, and\\nthose samples upon which both of the parsers agree are usedfor training the primary parser.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='those samples upon which both of the parsers agree are usedfor training the primary parser.\\nAnother model was produced using an LSTM instead of\\na feedforward network [108]. Unlike previous models, thismodel was given knowledge of the entire buffer and the entire\\nstack and had knowledge of the entire history of transition\\ndecisions. This allowed for better predictions, generating state-of-the-art scores on the Stanford Dependency Treebank [109],\\nas well as state-of-the-art results on the CTB5 Chinese data\\nset [110]. Finally, Andor et al. [111] used a feedforward\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 610 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\nnetwork with global normalization on a number of tasks,\\nincluding POS tagging, sentence compression, and dependency\\nparsing. State-of-the-art results were obtained on all tasks on\\nthe Wall Street Journal data set. Notably, their model requiredsigniﬁcantly less computation than comparable models.\\nMuch like Stenentorp [102], Wang et al. [112] used an\\nalternative algorithm to produce directed acyclic graphs, fora task called semantic parsing, where deeper relationships\\nbetween the words are found. The task seeks to identify what\\ntypes of actions are taking place and how words modifyeach other. In addition to the typical stack and buffer used'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='in transition-based parsing, the algorithm employed a deque.\\nThis allowed for the representation of multiparented words,which although rare in English, are common in many natural\\nlanguages. Furthermore, it a llowed for multiple children of\\nthe ROOT\\nlabel. In addition to producing said graphs, this\\narticle is novel in its use of two new LSTM-based techniques:\\nBi-LSTM subtraction and incr emental Tree-LSTM. Bi-LSTM\\nsubtraction built on previous work [41], [113] to represent the\\nbuffer as a subtraction of the vectors from the head and tail\\nof the LSTM, in addition to using an additional LSTM torepresent the deque. Incremental Tree-LSTM is an extension\\nof Tree-LSTM [114], modiﬁed for directed acyclic graphs,\\nby connecting children to parents incrementally, rather thanconnecting all children to a parent simultaneously. The model\\nachieved the best published scores at the time for 14 of\\nthe 16 evaluation metrics used on SemEval-2015 Task 18(English) [115] and SemEval-2016 Task 9 (Chinese) [116].\\nWhile deep learning had been applied to semantic parsing in\\nparticular domains, such as QA [117], [118], to the authors’knowledge, this was the ﬁrst time it was applied in large scale\\nto semantic parsing as a whole.\\n3) Generative Dependency and Constituent Parsing:\\nDyer et al. [119] proposed a model that used RNN grammars\\nfor parsing and language modeling. While most approachestake a bottom–up approach to parsing, this took a top–down'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='approach, taking as input the full sentence in addition to the\\ncurrent parse tree. This allowed the sentence to be viewed asa whole, rather than simply allowing local phrases within it to\\nbe considered. This model achieved the best results in English\\ngenerative parsing as well as in single sentence languagemodeling. It also attained results close to the best in Chinese\\ngenerative parsing.\\nChoe and Charniak [120] treated parsing as a language\\nmodeling problem and used an LSTM to assign probabilities\\nto the parse trees, achieving state of the art. Fried et al. [121]\\nwanted to determine whether the power of the models camefrom the reranking process or simply from the combined\\npower of two models. They found that while using one parser\\nfor producing candidate trees and another for ranking them\\nwas superior to a single parser approach, combining two\\nparsers explicitly was preferable. They used two parsers toboth select the candidates and rerank them, achieving state-of-\\nthe-art results. They extended this model to use three parsers,\\nachieving even better results. Finally, an ensemble of eightsuch models (using two parsers) was constructed and achieved\\nthe best results on PTB at the time.\\nA model created by Dozat and Manning [122] used a\\ngraph-based approach with a self-attentive network. Similarly,\\nTan et al. [123] used a self-attentional model for semantic\\nrole labeling and a subtask of semantic parsing, achievingexcellent results. They experimented with recurrent and con-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='volutional replacements to th e feedforward portions of the\\nself-attention mechanism, ﬁnding that the feedforward variant\\nhad the best performance. Another novel approach is thatof Duong et al. [124], who used active learning. While not\\nperfect, this is a possible solution to one of the biggest\\nproblems in semantic parsing—the availability of data.\\n4) Universal Parsing: Much like universal morphology,\\nuniversal dependency parsing, or universal parsing, is the\\nrelatively new task of parsing language using a standardizedset of tags and relationships across all languages. While\\ncurrent parsing varies drastically from language to language,\\nthis attempts to make it uniform between them, in orderto allow for easier processing between and among them.\\nNivre [125] discussed the recent development of universal\\ngrammar and presented the challenges that lie ahead, mainlythe development of tree banks in more languages and the\\nconsistency of labeling between tree banks in different (and\\neven the same) languages. This task has gained traction inlarge part because it has been a CoNLL shared task for the past\\ntwo years [126]. A number of approaches from the 2018 task\\nincluded using deep transition parsing [127], graph-based\\nneural parsing [128], and a competitive model, which used\\nonly a single neural model, rather than an ensemble [129].The task has begun to be examined outside of CoNLL, with\\nLiuet al. [130] applying universal dependencies to the parsing'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Liuet al. [130] applying universal dependencies to the parsing\\nof tweets, using an ensemble of bidirectional LSTM.\\n5) Remaining Challenges: Outside of universal parsing,\\na parsing challenge that needs to b e further investigated is the\\nbuilding of syntactic structures without the use of treebanks fortraining. Attempts have been made using attention scores and\\nTree-LSTMs, as well as “outside-inside” autoencoders. If such\\napproaches are successful, they have the potential use inmany environments, including in the context of low-resource\\nlanguages and out-of-domain scenarios. While a number of\\nother challenges remain, these are the largest and are expectedto receive the most focus.\\nD. Semantics\\nSemantic processing involves understanding the meaning of\\nwords, phrases, sentences, or documents at some level. Wordembeddings, such as Word2Vec [67], [68] and GloVe [131],\\nclaim to capture meanings of words, following the distribu-\\ntional hypothesis of meaning [132]. As a corollary, when vec-\\ntors corresponding to phrases, sentences, or other components\\nof text are processed using a neural network, a representationthat can be loosely thought to be semantically representative\\nis computed compositionally. In this section, neural semantic\\nprocessing research is separated into two distinct areas: workon comparing the semantic similarity of two portions of text\\nand work on capturing and transferring meaning in high-level\\nconstituents, particularly sentences.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='and work on capturing and transferring meaning in high-level\\nconstituents, particularly sentences.\\n1) Semantic Comparison: One way to test the efﬁcacy of\\nan approach to computing semantics is to see if two similar\\nphrases, sentences, or documents, judged by humans to havesimilar meaning also are judged similarly by a program.\\nHuet al. [133] proposed two CNNs to perform a seman-\\ntic comparison task. The ﬁrst model, ARC-I, inspired byBordes et al. [134], used a Siamese network, in which two\\nCNNs sharing weights evaluated two sentences in parallel.\\nIn the second network, connections were placed between the\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 611\\ntwo, allowing for sharing before the ﬁnal states of the CNNs.\\nThe approach outperformed a number of existing models in\\ntasks in English and Chinese.\\nBuilding on prior work [21], [26], [133], Yin and\\nSchütze [135] proposed a Bi-CNN-MI (MI for multigranular\\ninteraction features), consisting of a pretrained CNN sentence\\nmodel, a CNN interaction model, and a logistic regressor.They modiﬁed a Siamese network using dynamic CNNs [21]\\n(Section III-D2). In addition, the feature maps from each level\\nwere used in the comparison, rather than simply the top-levelfeature maps. They achieved state-of-the-art results on the\\nMicrosoft Research Paraphrase Corpus (MSRP) [136].'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Microsoft Research Paraphrase Corpus (MSRP) [136].\\nHeet al. [137] constructed feature maps, which were then\\ncompared using a “similarity m easurement layer” followed\\nby a fully connected layer and then a log-softmax output\\nlayer within a CNN. The windows used in the convolutionallayers ranged in length from one to four. The network was\\ntrained and evaluated on three data sets: MSRP, the Sentences\\nInvolving Compositional Knowledge (SICK) data set [138],\\nand the Microsoft Video Paraphrase Corpus (MSRVID) [139].\\nState-of-the-art results were achieved on the ﬁrst and the third.\\nTaiet al. [114] concocted a model using an RvNN with\\nLSTM-like nodes called a Tree-LSTM. Two variations were\\nexamined (constituency- and dependency-based) and tested onboth the SICK data set and Stanford Sentiment Treebank [94].\\nThe constituency-based model achieved state-of-the-art results\\non the Stanford Sentiment Treebank and the dependency-basedone achieved state-of-the-art results on SICK.\\nHe and Lin [140] presented another model, which out-\\nperformed that of Tai et al. on SICK. The model formed a\\nmatrix of the two sentences before applying a “similarity focus\\nlayer” and then a 19-layer CNN followed by dense layers\\nwith a softmax output. The similarity focus layer matched\\nsemantically similar pairs of words from the input sentences\\nand applied weights to the matrix locations representingthe relations between the words in each pair. They also\\nobtained state-of-the-art resuults on MSRVID, SemEval 2014'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='obtained state-of-the-art resuults on MSRVID, SemEval 2014\\nTask 10 [141], WikiQA [142], and TreeQA [143] data sets.\\n2) Sentence Modeling: Extending from neural language\\nmodeling, sentence modeling attempts to capture the meaning\\nof sentences in vectors. Taking this a step further are models,such as that of Le and Mikolov [144], which attempt to model\\nparagraphs or larger bodies of text in this way.\\nKalchbrenner et al. [21] generated the representations of\\nsentences using a dynamic convolutional neural network\\n(DCNN), which used a number of ﬁlters and dynamic\\nk-max-pooling layers. Due to dynamic pooling, features of\\ndifferent types and lengths could be identiﬁed in sentences\\nwith varying structures without padding of the input. This\\nallowed not only short-range de pendencies but also long-range\\ndependencies to be identiﬁed. The DCNN was tested in applied\\ntasks that require semantic understanding. It outperformed allcomparison models in predicting sentiment of movie reviews\\nin the Stanford Sentiment Treebank [95] and in identiﬁcation\\nof sentiment in tweets [145]. It was also one of the topperformers in classifying types of questions using the TREC\\ndatabase [146].\\nBetween their requirement for such understanding and\\ntheir ease of examination due to the typical encoder–decoder\\nstructure they use, NMT systems (Section IV-G) are splen-\\ndid testbeds for researching int ernal semantic representations.Poliak et al. [147] trained encoders on four different language'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='pairs: English and Arabic, English and Spanish, English and\\nChinese, and English and German. The decoding classiﬁers\\nwere trained on four distinct data sets: Multi NLI [148],which is an expanded version of SNLI [149], as well\\nas three recast data sets from the JHU Decompositional\\nSemantics Initiative [150] (FrameNet Plus or FN+ [151],Deﬁnite Pronoun Resolution or DPR [152], and Semantic\\nProto-Roles or SPR [153]). None of the results were particu-\\nlarly strong, although they were strongest in SPR. This led tothe conclusion that NMT models do a poor job of capturing\\nparaphrased information and fail to capture inferences that\\nhelp in anaphora resolution (e.g., resolving gender). Theydid, however, ﬁnd that the models learn about protoroles\\n(e.g., who or what is the recipient of an action). A concurrent\\nwork [154] analyzed the quality of many data sets used fornatural language inference.\\nHerzig and Berant [155] found that training semantic parsers\\non a single domain, as is often done, is less effective than\\ntraining across many domains. This conclusion was drawn\\nafter testing three LSTM-based models. The ﬁrst model wasa one-to-one model, in which a single encoder and a single\\ndecoder were used, requiring the network itself to determine\\nthe domain of the input. In the second model, a many-to-manymodel, a decoder was used for each domain, as were two\\nencoders: the domain-speciﬁc encoder and a multidomain'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='encoders: the domain-speciﬁc encoder and a multidomain\\nencoder. The third model was a one-to-many model, usinga single encoder but separate decoders for each domain. Each\\nmodel was trained on the “OVERNIGHT” data set [156].\\nExceptional results were achieved for all models, with a state-of-the-art performance exhibited by the one-to-one model.\\nSimilar conclusions were drawn by Brunner et al. [157].\\nwho created several LSTM-based encoder–decoder networks\\nand analyzed the embedding vectors produced. A single\\nencoder accepting English sentences as input was used,as were four different decoders. The ﬁrst such decoder was\\na replicating decoder, which reproduced the original English\\ninput. The second and third decoders translated the text intoGerman and French. Finally, the fourth decoder was a POS\\ntagger. Different combinations of decoders were used; one\\nmodel had only the replicating decoder, while others had two,three, or all four. Sentences of 14 different structures from\\nthe EuroParl data set [158] were used to train the networks.\\nA set of test sentences were then fed to the encoders and\\ntheir output analyzed. In all cases, 14 clusters were formed,\\neach corresponding to one of the sentence structures. Analy-\\nsis showed that adding more decoders led to more correctand more deﬁnitive clusters. In particular, using all four of\\nthe decoders led to zero error. Fu rthermore, the researchers\\nconﬁrmed a hypothesis that just as logical arithmetic can be'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='conﬁrmed a hypothesis that just as logical arithmetic can be\\nperformed on word embeddings, so can it be performed on\\nsentence embeddings.\\n3) Semantic Challenges: In addition to the challenges\\nalready mentioned, researchers b elieve that being able to solve\\ntasks well does not indicate actual understanding. Integratingdeep networks with general word graphs (e.g., WordNet [159])\\nor knowledge graphs (e.g., DBPedia [160]) may be able to\\nendow a sense of understanding. Graph embedding is an activearea of research [161], and work on integrating language-based\\nmodels and graph models has only recently begun to take off,\\ngiving hope for better machine understanding.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 612 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\nFig. 4. Publication volume for applied areas of NLP. All areas of applied NLP discussed have witnessed growth in recent years, with the largest growth\\noccurring in the last two to three years.\\nE. Summary of Core Issues\\nDeep learning has generally performed very well, surpassing\\nexisting states of the art in many individual core NLP tasks\\nand has thus created the foundation on which useful naturallanguage applications can and are being built. However, it is\\nclear from examining the research reviewed here that natural'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='clear from examining the research reviewed here that natural\\nlanguage is an enigmatically complex topic, with myriadcore or basic tasks, of which deep learning has only grazed\\nthe surface. It is also not clear how architectures for ably\\nexecuting individual core tasks can be synthesized to build\\na common ediﬁce, possibly a much more complex distributed\\nneural architecture, to show competence in multiple or “all”core tasks. More fundamentally, it is also not clear, how\\nmastering of basic tasks, may lead to superior performance\\nin applied tasks, which are the ultimate engineering goals,especially in the context of building effective and efﬁcient deep\\nlearning models. Many, if not most, successful deep learning\\narchitectures for applied tasks , discussed in Section IV , seem\\nto forgo explicit architectural components for core tasks and\\nlearn such tasks implicitly. Thus, some researchers argue that\\nthe relevance of the large amount of work on core issues isnot fully justiﬁed, while others argue that further extensive\\nresearch in such areas is necessary to better understand and\\ndevelop systems which more perfectly perform these tasks,\\nwhether explicitly or implicitly.\\nIV . A\\nPPLICATIONS OF NLP U SING DEEPLEARNING\\nWhile the study of core areas of NLP is important to\\nunderstanding how neural models work, it is meaningless in\\nand of itself from an engineerin g perspective, which values the\\napplications that beneﬁt humanity, not pure philosophical and'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='applications that beneﬁt humanity, not pure philosophical and\\nscientiﬁc inquiry. Current approaches to solving several imme-diately useful NLP tasks are summarized here. Note that the\\nissues included here are only those involving the processing\\nof text, not the processing of verbal speech. Because speechprocessing [162], [163] requires expertise on several other\\ntopics including acoustic processing, it is generally considered\\nanother ﬁeld of its own, sharing many commonalities with theﬁeld of NLP. The number of studies in each discussed area\\nover the last decade is shown in Fig. 4.\\nA. Information Retrieval\\nThe purpose of IR systems is to help people ﬁnd the right\\n(most useful) information in the right (most convenient) formatat the right time (when they need it) [164]. Among many\\nissues in IR, a primary problem that needs addressing pertainsto ranking documents with respect to a query string in terms\\nof relevance scores for ad hoc retrieval tasks, similar to what\\nhappens in a search engine.\\nDeep learning models for ad hoc retrieval match texts of\\nqueries to texts of documents to obtain relevance scores.Thus, such models have to focus on producing repre-\\nsentations of the interactions among individual words in\\nthe query and the documents. Some representation-focused\\napproaches build deep learning models to produce good rep-\\nresentations for the texts and then match the representationsstraightforwardly [133], [165], [166], whereas interaction-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='focused approaches ﬁrst build lo cal interactions directly and\\nthen use DNNs to learn how the two pieces of text matchbased on word interactions [133], [167], [168]. When matching\\na long document to a short query, the relevant portion can\\npotentially occur anywhere in the long document and mayalso be distributed, thus, ﬁ nding how each word in the query\\nrelates to portions of the document is helpful.\\nMindful of the speciﬁc needs for IR, Guo et al. [169]\\nbuilt a neural architecture called DRMM, enhancing an\\ninteraction-focused model that feeds quantized histograms of\\nthe local interaction intensities to an MLP for matching.In parallel, the query terms go through a small subnetwork on\\ntheir own to establish term importance and term dependencies.\\nThe outputs of the two parallel networks are mixed at the top\\nso that the relevance of the document to the query can be better\\nlearned. DRMM achieved the state-of-the-art performance forits time.\\nMost current neural IR models are not end-to-end rele-\\nvance rankers, but are rerankers for documents a ﬁrst-stageefﬁcient traditional ranker has deemed relevant to a query.\\nThe representations that the neural rerankers learn are dense\\nfor both documents and queries, i.e., most documents in acollection seem to be relevant to a query, making it impossible\\nto use such ANNs for ranking an entire collection of docu-\\nments. In contrast, Zamani et al. [170] presented a standalone\\nneural ranking model called SNRM_PRF that learned sparse'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='neural ranking model called SNRM_PRF that learned sparse\\nrepresentations for both queries and documents, mimicking\\nwhat traditional approaches do. Since queries are much shorter\\nthan documents and queries contain much less information\\nthan documents, it makes sense for query representationsto be denser. This was achieved by using, during training,\\na sparsity objective combined with hinge loss. In particular,\\nann-gram representation for queries and documents was used.\\nIt passed the embedding of each word separately through\\nan individual MLP and performed average pooling on top.\\nDuring training, the approach used pseudorelevant documentsobtained by retrieving documents using the existing models\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 613\\nsuch as TF-IDF and BM25, because of the lack of enough\\ncorrectly labeled documents to train large ANN models. The\\napproach created a 20 000-bit-long inverted index for each\\ndocument using the trained network, just like a traditional end-to-end approach. For retrieval, a dot product was computed\\nbetween query and document representations to obtain the\\nretrieval relevance score. The SNRM_PRF system obtainedthe best metrics (measured by MAP, P@20, nDCG@20, and\\nRecall) across the board for two large data sets, Robust and\\nClueWeb.\\nMacAveney et al. [171] extracted query term representa-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='ClueWeb.\\nMacAveney et al. [171] extracted query term representa-\\ntions from two pretrained contextualized language models,\\nELMo [70] and BERT [71], and used the representationsto augment three existing competitive neural ranking archi-\\ntectures for ad hoc document ranking, one of them being\\nDRMM [169]. They also presented a joint model that com-bined BERT’s classiﬁcation vector with these architectures\\nto get beneﬁts from both approaches. MacAveney’s sys-\\ntem called contextualized embeddings for document rank-\\ning (CEDR) improved the performance of all three prior\\nmodels and produced state-of-the-art results using BERT’stoken representations.\\nB. Information Extraction\\nInformation extraction extracts explicit or implicit informa-\\ntion from the text. The outputs of systems vary, but often,\\nthe extracted data and the relationships within it are saved inrelational databases [172]. Commonly extracted information\\nincludes named entities and relations, events and their partic-\\nipants, temporal information, and tuples of facts.\\n1) Named Entity Recognition: Named entity recogni-\\ntion (NER) refers to the identiﬁcation of proper nouns as\\nwell as information such as dates, times, prices, and product\\nIDs. The multitask approach of Collobert et al. [9] included\\nthe task although no results were reported. In their approach,a simple feedforward network was used, having a context\\nwith a ﬁxed-sized window around each word. Presumably, this'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='with a ﬁxed-sized window around each word. Presumably, this\\nmade it difﬁcult to capture long-distance relations between thewords.\\nLSTMs were ﬁrst used for NER by Hammerton [173]. The\\nmodel, which was ahead of its time, had a small networkdue to the lack of available computing power at the time.\\nIn addition, sophisticated numeric vector models for words\\nwere not yet available. Results were slightly better than thebaseline for English and much better than the baseline for\\nGerman. Dos Santos et al. [174] used a DNN architecture,\\nknown as CharWNN, which jointly used word-level andcharacter-level inputs to perform sequential classiﬁcation. In\\nthis article, a number of experiments were performed using\\nthe HAREM I annotated Portuguese corpus [175], and the\\nSPA CoNLL2002 annotated Spanish corpus [176]. For the Por-\\ntuguese corpus, CharWNN outperformed the previous state-of-the-art system across ten named entity classes. It also achieved\\nstate-of-the-art performance in Spanish. The authors noted that\\nwhen used alone, neither word embeddings nor character levelembeddings worked. This revalidated a fact long known: joint\\nuse of word-level and character-level features is important to\\neffective NER performance.\\nChiu and Nichols [177] used a bidirectional LSTM with\\na character-level CNN resembling those used by dos Santos\\nand Guimarães [174]. Without using any private lexicons,detailed information about linked entities, or elaborate'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='hand-crafted features they produced state-of-the-art results on\\nthe CoNLL-2003 [178] and OntoNotes [179], [180] data sets.\\nLample et al. [181] developed an architecture based\\non bidirectional LSTMs and conditional random ﬁelds.\\nThe model used both character-level inputs and word\\nembeddings. The inputs were combined and then fedto a bidirectional LSTM, whose outputs were in turn\\nfed to a layer that performed CRF computations [182].\\nThe model, when trained using dropout, obtained state-of-the-art performance in both German and Spanish.\\nThe LSTM-CRF model was also very close in both English\\nand Dutch. The main claim of this study was that state-of-the-art results were achieved without the use of any hand-\\nengineered features or gazetteers.\\nAkbik et al. [183] achieved the state-of-the-art performance\\nin German and English NER using a pretrained bidirectional\\ncharacter language model. They retrieved for each word a\\ncontextual embedding that they passed into a BiLSTM-CRF\\nsequence labeler to perform NER.\\n2) Event Extraction: Event extraction is concerned with\\nidentifying words or phrases that refer to the occurrence of\\nevents, along with participants such as agents, objects, recip-\\nients, and times of occurrence. Ev ent extraction usually deals\\nwith four subtasks: identifying event mentions, or phrases\\nthat describe events; identifying event triggers, which are\\nthe main words—usually verbs or gerunds—that specify theoccurrence of the events; identifying arguments of the events;'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='and identifying arguments’ roles in the events.\\nChen et al. [184] argued that CNNs that use max-pooling\\nare likely to capture only the most important information in\\na sentence, and as a result, might miss valuable facts when\\nconsidering sentences that refe r to several events. To address\\nthis drawback, they divided the feature map into three parts,\\nand instead of using one maximum value, kept the maximumvalue of each part. In the ﬁrst stage, they classiﬁed each word\\nas either being a trigger word or nontrigger word. If triggers\\nwere found, the second stage aligned the roles of arguments.Results showed that this appr oach signiﬁcantly outperformed\\nother state-of-the-art methods of the time. The following year,\\nNguyen et al. [185] used an RNN-based encoder–decoder pair\\nto identify event triggers and rol es, exceeding earlier results.\\nLiuet al. [186] presented a latent variable neural model to\\ninduce event schemas and extract open domain events, achiev-ing the best results on a data set they created and released.\\n3) Relationship Extraction: Another important type of\\ninformation extracted from the text is that of relation-ships. These may be possessive, antonymous, or synony-\\nmous relationships, or more natural, familial, or geographic\\nrelationships. The ﬁrst deep learning approach was that of\\nZeng et al. [23], who used a simple CNN to classify a number\\nof relationships between the elements in sentences. Using onlytwo layers, a window size of three and word embeddings'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='with only 50 dimensions, they attained better results than any\\nprior approach. Further work, by Zheng et al. [187], used a\\nbidirectional LSTM and a CNN for relationship classiﬁcation\\nas well as entity recognition. More recently, Sun et al. [188]\\nused an attention-based GRU model with a copy mechanism.This network was novel in its use of a data structure known\\nas a coverage mechanism [189], which helped ensure that\\nall important information was extracted the correct number\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 614 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\nof times. Lin et al. [190] achieved the state-of-the-art per-\\nformance in clinical temporal relation extraction using the\\npretrained BERT [71] model with supervised training on a\\nbiomedical data set.\\nC. Text Classiﬁcation\\nAnother classic application for NLP is text classiﬁcation or\\nthe assignment of free-text documents to predeﬁned classes.Document classiﬁcation h as numerous applications.\\nKim [20] was the ﬁrst to use pretrained word vectors\\nin a CNN for sentence-level classiﬁcation. Kim’s work wasmotivating, and showed that simple CNNs, with one convo-\\nlutional layer followed by a dense layer with dropout and\\nsoftmax output, could achieve excellent results on multiplebenchmarks usin g little hyperparamet er tuning. The CNN'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='models proposed were able to improve upon the state of the art\\non four out of seven different tasks cast as sentence classiﬁca-tion, including sentiment analysis and question classiﬁcation.\\nConneau et al. [191] later showed that networks that employ a\\nlarge number of convolutional layers work well for documentclassiﬁcation.\\nJiang et al. [192] used a hybrid architecture combining\\na deep belief network [193] and softmax regression [194].\\n(A deep belief network is a feedforward network where pairs\\nof hidden layers are designed to resemble restricted Boltzmannmachines [195], which are trained using unsupervised learning\\nand are designed to increase or decrease dimensionality of\\ndata.) This was achieved by making passes over the datausing forward and backward propagation many times until\\na minimum engery-based loss was found. This process was\\nindependent of the labeled or classiﬁcation portion of thetask and was therefore initia lly trained without the softmax\\nregression output layer. Once both sections of the architecture\\nwere pretrained, they were combined and trained such as a reg-ular deep neural net with backpropagation and quasi-Newton\\nmethods [196].\\nAdhikari et al. [197] used BERT [71] to obtain state-of-\\nthe-art classiﬁcation resu lts on four document data sets.\\nWhile deep learning is promising for many areas of NLP,\\nincluding text classiﬁcation, it is not necessarily the end-\\nall-be-all, and many hurdles are still present. Worsham and'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='all-be-all, and many hurdles are still present. Worsham and\\nKalita [198] found that for the task of classifying longfull-length books by genre, gradient boosting trees are superior\\nto neural networks, including both CNNs and LSTMs.\\nD. Text Generation\\nMany NLP tasks require the generation of human-like\\nlanguage. Summarization and machine translation convert one\\ntext to another in a sequence-to-sequence (seq2seq) fashion.\\nOther tasks, such as image and video captioning and automatic\\nweather and sports reporting, convert nontextual data to text.\\nSome tasks, however, produce text without any input data toconvert (or with only small amounts used as a topic or guide).\\nThese tasks include poetry generation, joke generation, and\\nstory generation.\\n1) Poetry Generation: Poetry generation is arguably the\\nhardest of the generation subtasks, as in addition to pro-ducing creative content, the content must be delivered in\\nan esthetic manner, usually following a speciﬁc structure.As with most tasks requiring textual output, recurrent models\\nare the standard. However, while recurrent networks are great\\nat learning internal language models, they do a poor job of\\nproducing structured output or adhering to any single style.Wei et al. [199] addressed the style issue by training using\\nparticular poets and controlling for style in Chinese poetry.\\nThey found that with enough training data, adequate resultscould be achieved. The structure problem was addressed by'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Hopkins and Kiela [200], who generated rhythmic poetry by\\ntraining the network on only a single type of poem to ensurethe produced poems adhered to a single rhythmic structure.\\nHuman evaluators judged poems produced to be of lower\\nquality than, but indistingui shable from, human-produced\\npoems.\\nAnother approach to poetry generation, beginning this year,\\nhas been to use pretrained language models. Speciﬁcally,Radford et al. ’s GPT-2 model [201], the successor of the GPT\\nmodel (Section III-A7), has been used. Radford et al. [201]\\nhypothesized that alongside sequence-to-sequence learning\\nand attention, language models can inherently start to learn\\ntext generation while training over a vast data set. As oflate 2019, these pretrained GPT-2 models are arguably the\\nmost effective and proliﬁc neural natural language generators.\\nBena and Kalita [202] used the 774 million parameter GPT-2 model to generate high-quality poems in English, demon-\\nstrating and eliciting emotional response in readers. (Two\\nother GPT-2 models are available: 355 million parameters, andas of Novemeber 2019, 1.5 billio n parameters.) Tucker and\\nKalita [203] generated poems in several languages—English,\\nSpanish, Ukrainian, Hindi, Bengali, and Assamese—using the774 M model as well. This study provided astonishing results\\nin the fact that GPT-2 was pretrained on a large English corpus,\\nyet with further training on only a few hundred poems in\\nanother language, it turns into a believable generator in that'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='another language, it turns into a believable generator in that\\nlanguage, even for poetry.\\n2) Joke and Pun Generation: Another area, which has\\nreceived little attention, is the use of deep learning for jokeand pun generation. Yu et al. [204] generated homographic\\npuns (puns that use multiple meanings of the same written\\nword) using a small LSTM. The network produced sentences\\nin which ambiguities were int roduced by words with multiple\\nmeanings although it did a poor job of making the punshumorous. The generated puns were classiﬁed by human\\nevaluators as machine generat ed a majority of the time. The\\nauthors noted that training on pun data alone is not sufﬁcientfor generating good puns. Ren and Yang [205] used an LSTM\\nto generate jokes, training on two data sets, one of which\\nwas a collection of short jokes from Conan O’Brien. Sincemany of these jokes pertain to current events, the network\\nwas also trained on a set of news articles. This gave context to\\nthe example jokes. Chippada and Saha [206] generated jokes,quotes, and tweets using the same neural network, using an\\nadditional input to specify which should be produced. It was\\nfound that providing more general knowledge of other typesof language, and examples of nonjokes, increased the quality\\nof the jokes produced.\\n3) Story Generation: While poetry and especially humor\\ngeneration have not gained much traction, story generationhas seen a recent rise in interest. Jain et al. [207] used'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='RNN variants with attention to produce short stories from\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 615\\n“one-liner” story descrip tions. Another recent study of\\ninterest is that by Peng et al. [208], who used LSTMs to\\ngenerate stories, providing an input to specify whether the\\nstory should have a happy or sad ending. Their modelsuccessfully did so while at the same time providing better\\ncoherence than noncontrolled stories. More recent attempts\\nat the task have used special mechanisms focusing on the\\n“events” (or actions) in the stories [209] or on the entities\\n(characters and important objects) [210]. Even with such\\nconstraints, generated stories generally become incoherent or\\nlose direction rather shortly. Xu et al. [211] addressed this by\\nusing a “skeleton”-based model to build general sentences and\\nﬁll in important information. This did a great job of capturingonly the most important information but still provided only\\nmodest end results in human evaluation. Drissi et al. [212]\\nfollowed a similar approach.\\nThe strongest models to date focus on creating high-level\\noverviews of stories before breaking them down into smaller\\ncomponents to convert to text. Huang et al. [213] generated\\nshort stories from images using a two-tiered network. The ﬁrst'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='short stories from images using a two-tiered network. The ﬁrst\\nconstructed a conceptual overview, while the second convertedthe overview into words. Fan et al. [214] used a hierarchical\\napproach, based on CNNs, which beat out the nonhierarchical\\napproach in blind comparison by human evaluators. In addi-tion, they found that self-attention leads to better perplexity.\\nThey also developed a fusion model with a pretrained language\\nmodel, leading to greater impr ovements. These results concur\\nwith those of an older study by Li et al. [215] who read\\ndocuments in a hierarchical fashion and reproduced them in a\\nhierarchical fashion, achieving great results.\\n4) Text Generation With GANs: In order to make sto-\\nries seem more human-like, Lin et al. [216] used generative\\nadversarial networks (GANs) to measure human likeness of\\ngenerated text, forcing the network toward more natural read-ing output. GANs are based on the concept of a minimax\\ntwo-player game, in which a gen erative network and a discrim-\\ninative network are designed to work against each other withthe discriminator attempting to determine whether examples\\nare from the generative network or the training set, and the\\ngenerator trying to maximize the number of mistakes made\\nby the discriminator. RankGAN, the GAN used in the study,\\nmeasured differences in embedding space, rather than in outputtokens. This meant that the story content was evaluated more\\ndirectly, without respect to the speciﬁc words and grammars'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='directly, without respect to the speciﬁc words and grammars\\nused to tell it. Rather than simply using standard metrics andminimizing loss, Tambwekar et al. [217] used reinforcement\\nlearning to train a text generation model. This taught the\\nmodel to not only attempt to optimize metrics but also togenerate stories that humans evaluated to be meaningful.\\nZhang et al. [218] used another modiﬁed GAN, referred to as\\ntextGAN, for text generation, employing an LSTM generatorand a CNN discriminator, achieving a promising bilingual\\nevaluation understudy (BLEU) score and a high tendency\\nto reproduce realistic-looking sentences. GANs have seenincreasing use in text generation recently [219], [220].\\n5) Text Generation With VAEs: Another interesting type of\\nnetwork is the variational autoencoder (V AE) [221]. While\\nGANs attempt to produce output i ndistinguishable (at least\\nto the model’s discriminator) from actual samples, V AEs\\nattempt to create output similar to samples in the trainingset [222]. Several recent studies have used V AEs for text\\ngeneration [223], [224], including Wang et al. [225], who\\nadapted it by adding a module for learning a guiding topic\\nfor sequence generation, producing good results.\\n6) Summary of Text Generation: Humor and poetry gener-\\nation are still understudied topics. As machine-generated texts\\nimprove, the desire for more character, personality, and colorin the texts will almost certainly emerge. Hence, it can be'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='expected that research in these areas will increase.\\nWhile story generation is improving, coherence is still a\\nmajor problem, especially for longer stories. This has been\\naddressed in part, by Haltzman et al. [226], who have pro-\\nposed “nucleus sampling” to help counteract this problem,performing their experiments using the GPT-2 model.\\nIn addition to issues with lack of creativity and coherence,\\ncreating metrics to measure any sort of creative task isdifﬁcult, and therefore, human evaluations are the norm, often\\nutilizing Amazon’s Mechanical Turk. However, recent works\\nhave proposed metrics that make a large step toward reliable\\nautomatic evaluation of generat ed text [227], [228]. In addition\\nto the more creative tasks surveyed here, a number of others\\nwere previously discussed by Gatt and Krahmer [229]. The\\nuse of deep learning for image captioning has been surveyed\\nvery recently [230], [231], and tasks that generate text giventextual inputs are discussed in Sections IV-E–IV-G.\\nE. Summarization\\nSummarization ﬁnds elements of interest in documents in\\norder to produce an encapsulation of the most importantcontent. There are two primary types of summarization: extrac-\\ntive and abstractive. The ﬁrst focuses on sentence extrac-\\ntion, simpliﬁcation, reordering, and concatenation to relay the\\nimportant information in documents using text taken directly\\nfrom the documents. Abstractive summaries rely on express-ing documents’ contents through generation-style abstraction,'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='possibly using words never seen in the documents [48].\\nRush et al. [39] introduced deep learning to summarization,\\nusing an FFNN. The language model used an encoder and a\\ngenerative beam search decoder. The initial input was given\\ndirectly to both the language model and the convolutionalattention-based encoder, which determined contextual impor-\\ntance surrounding the summary sentences and phrases. The\\nperformance of the model was co mparable to other state-of-\\nthe-art models of the time.\\nAs in other areas, attention mechanisms have improved\\nthe performance of encoder–decoder models. Krantz andKalita [232] compared various attention models for abstrac-\\ntive summarization. A state-of-the-art approach developed\\nby Paulus et al. [40] used a multiple intratemporal atten-\\ntion encoder mechanism that considered not only the input\\ntext tokens but also the output tokens used by the decoderfor previously generated words. They also used similar\\nhybrid cross-entropy loss functions to those proposed by\\nRanzato et al. [233], which led to decreases in training and\\nexecution by orders of magnitude. Finally, they recommended\\nusing strategies seen in rein forcement learning to modify\\ngradients and reduce exposure bias, which has been noted inmodels trained exclusively via supervised learning. The use\\nof attention also boosted accura cy in the fully convolutional'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='of attention also boosted accura cy in the fully convolutional\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 616 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\nmodel proposed by Gehring et al. [234], who implemented an\\nattention mechanism for each layer.\\nZhang et al. [235] proposed an encoder–decoder frame-\\nwork, which generated an output sequence based on an inputsequence in a two-stage manner. They encoded the input\\nsequence using BERT [71]. The decoder had two stages.\\nIn the ﬁrst stage, a transformer-based decoder generated adraft output sequence. In the second stage, they masked each\\nword of the draft sequence and fed it to BERT, and then\\nby combining the input sequence and the draft representationgenerated by BERT, they used a transformer-based decoder to\\npredict the reﬁned word for each masked position. Their model\\nachieved state-of-the-art performance on the CNN/Daily Mailand New York Times data sets.\\nF . Question Answering\\nSimilar to summarization and information extraction,\\nquestion answering (QA) gathers relevant words, phrases,\\nor sentences from a document. QA coherently returns thisinformation in response to a request. Current methods resem-\\nble those of summarization.\\nWang et al. [41] used a gated attention-based recurrent\\nnetwork to match the question with an answer-containing'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='network to match the question with an answer-containing\\npassage. A self-matching atte ntion mechanism was used to\\nreﬁne the machine representation by mapping the entire pas-sage. Pointer networks were used to predict the location and\\nboundary of an answer. These networks used attention-pooling\\nvector representations of passages, as well as the words beinganalyzed, to model the critical tokens or phrases necessary.\\nMulticolumn CNNs were used by Dong et al. [236] to\\nautomatically analyze questions from multiple viewpoints.Parallel networks were used to e xtract pertinent information\\nfrom input questions. Separate networks were used to ﬁnd\\ncontext information and relationships and to determine which\\nforms of answers should be returned. The output of these\\nnetworks was combined and used to rank possible answers.\\nSantoro et al. [237] used relational networks (RNs) for\\nsummarization. First proposed by Raposo et al. [238], RNs\\nare built upon an MLP architecture, with a focus on rela-tional reasoning, i.e., deﬁning relationships among entities in\\nthe data. These feedforward n etworks implement a similar\\nfunction among all pairs of objects in order to aggregatecorrelations among them. For input, the RNs took ﬁnal LSTM\\nrepresentations of document sentences. These inputs were\\nfurther paired with a representation of the information requestgiven [237].\\nBERT [71] achieved state of the art in QA experiments on\\nthe SQuAD 1.1 and SQuAD 2.0 data sets. Yang et al. [239]'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='the SQuAD 1.1 and SQuAD 2.0 data sets. Yang et al. [239]\\ndemonstrated an end-to-end QA system that integrates BERT\\nwith the open-source Anserini IR toolkit. This system canidentify answers from a large corpus of Wikipedia articles in\\nan end-to-end fashion, obtaining the best results on a standard\\nbenchmark test collection.\\nG. Machine Translation\\nMachine translation is the quintessential application of NLP.\\nIt involves the use of mathematical and algorithmic techniquesto translate the documents in one language to another. Per-\\nforming effective translation is intrinsically onerous even for\\nhumans, requiring proﬁciency in areas such as morphology,syntax, and semantics, as well as an adept understanding and\\ndiscernment of cultural sensiti vities, for both of the languages\\n(and associated societies) under consideration [48].\\nThe ﬁrst attempt at NMT was that by Schwenk [240],\\nalthough neural models had previously been used for the simi-\\nlar task of transliteration, converting certain parts of text, such\\nas proper nouns, into different languages [241]. Schwenk useda feedforward network with seven-word inputs and outputs,\\npadding and trimming when necessary. The ability to translate\\nfrom a sentence of one length to a sentence of another lengthcame about with the introduction of encoder–decoder models.\\nThe ﬁrst use of such a model, by Kalchbrenner and\\nBlumson [242], stemmed from the success of continuousrecurrent representations in cap turing syntax, semantics, and'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='morphology [243] in addition to the ability of RNNs to\\nbuild robust language models [29]. This original NMTencoder–decoder model used a combination of generative con-\\nvolutional and recurrent layers to encode and optimize a source\\nlanguage model and cast this into a target language. The model\\nwas quickly reworked and further studied by Cho et al. [244],\\nand numerous novel and effective advances to this modelhave since been made [38], [245]. Encoder–decoder mod-\\nels have continuously deﬁned the state of the art, being\\nexpanded to contain dozens of layers, with residual con-nections, attention mechanisms, and even residual attention\\nmechanisms allowing the ﬁnal decoding layer to attend to the\\nﬁrst encoding layer [246]. State-of-the-art results have alsobeen achieved by using numerous convolutional layers in both\\nthe encoder and decoder, allowing information to be viewed in\\nseveral hierarchical layers rather than a multitude of recurrentsteps [234]. Such derived models are continually improving,\\nﬁnding answers to the shortcomings of their predecessors\\nand overcoming any need for h and engineering [247]. Recent\\nprogress includes effective initialization of decoder hidden\\nstates, use of conditional gated attentional cells, removal ofbias in embedding layers, use of alternative decoding phases,\\nfactorization of embeddings, and test time use of the beam\\nsearch algorithm [248], [249].\\nThe standard initialization for the decoder state is that'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='search algorithm [248], [249].\\nThe standard initialization for the decoder state is that\\nproposed by Bahdanau et al. [38], using the last backward\\nencoder state. However, as noted by Britz\\net al. [247], using\\nthe average of the embedding or annotation layer seems to\\nlead to the best translations. Gated recurrent cells have been\\nthe gold standard for sequence-to-sequence tasks, a variationof which is a conditional GRU (cGRU) [248], most effectively\\nutilized with an attention mechanism. A cGRU cell consists\\nof three key components: two GRU transition blocks andan attention mechanism betw een them. These three blocks\\ncombine the previous hidden state, along with the attention\\ncontext window to generate the next hidden state. Altering\\nthe decoding process [38] from look at input, generate output\\ntoken, update hidden representation to a process of look,\\nupdate ,a n d generate can simplify the ﬁnal decoding. Adding\\nfurther source attributes, such as morphological segmentation\\nlabels, POS tags, and syntactic dependency labels, improvesmodels, and concatenating or factorizing these with embed-\\ndings increases robustness further [248], [250]. For remem-\\nbering long-term dependencies, vertically stacked recurrentunits have been the standard, with the optimum number of\\nlayers having been determined to be roughly between 2 and\\n16 [247], depending on the desired input length as well as'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='16 [247], depending on the desired input length as well as\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 617\\nthe presence and density of residual connections. At test time,\\na beam search algorithm can be used beside the ﬁnal softmax\\nlayer for considering multiple target predictions in a greedy\\nfashion, allowing the best predictions to be found withoutlooking through the entire hypothesis space [249].\\nIn a direction diverging from previous work,\\nVaswani et al. [42] and Ahmed et al. [251] proposed\\ndiscarding the large number of recurrent and convolutional\\nlayers and instead focusing exclusively on attention\\nmechanisms to encode a language globally from inputto output. Preferring such “self-attention” mechanisms\\nover traditional layers is motivated by the following three\\nprinciples: reducing the complexity of computations requiredper layer, minimizing sequential training steps, and, ﬁnally,\\nabating the path length from input to output and its handicap\\non the learning of the long-range dependencies that arenecessary in many sequencing tasks [252]. Apart from\\nincreased accuracy across trans lation tasks, self-attention\\nmodels allow more parallelization throughout architectures,\\ndecreasing the training times and minimizing necessary'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='decreasing the training times and minimizing necessary\\nsequential steps. At time of writing, the state-of-the-art modelgenerating the best results for English to German and English\\nto French on the International Workshop on Spoken Language\\nTranslation (IWSLT) 2014 test corpus [253] is that of Medinaand Kalita [254], which modiﬁed the model proposed by\\nVaswani to use parallel self-attention mechanisms, rather than\\nstacking them as was done in the original model. In additionto improving BLEU scores [255], this also reduced training\\ntimes. Ghazvininejad et al. [256] recently applied BERT to\\nthe machine translation task using constant-time models.They were able to achieve relatively competitive performance\\nin a fraction of the time. Lample and Conneau [257] attained\\nstate-of-the-art results, performing unsupervised machine\\ntranslation using multiple languages in their language model\\npretraining.\\nSeveral of the recent state-of-the-art models were examined\\nby Chen et al. [258]. The models were picked apart to deter-\\nmine which features were truly responsible for their strengthand to provide a fair comparison. Hybrid models were then\\ncreated using this knowledge, and incorporating the best parts\\nof each previous model, outperfo rming the previous models.\\nIn addition to creating two models with both a self-attentive\\ncomponent and a recurrent component (in one model, they\\nwere stacked, in the other parallel), they determined fourtechniques that they believe should always be employed,'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='as they are crucial to some mode ls, at best, and neutral to all\\nmodels examined, at worst. These are label smoothing, multi-head attention, layer normalization, and synchronous training.\\nAnother study, by Denkowski and Neubig [259], examined\\na number of other techniques, recommending three: using\\nAdam optimization, restarting multiple times, with learning\\nrate annealing; performing subword translation; and using anensemble of decoders. Furthermore, they tested a number\\nof common techniques on models that were strong to begin\\nand determined that three of the four provided no additionalbeneﬁts to, or actually hurt, the model, those three being lexi-\\ncon bias (priming the outputs with directly translated words),\\npretranslation (using translations from another model, usuallyof lower quality, as additional input), and dropout. They did\\nﬁnd, however, that data bootstrapping (using phrases that are\\nparts of training examples as a dditional independent smallersamples) was advantageous even to models that are already\\nhigh performing. They recommended that future developments\\nbe tested on top-performing models in order to determine their\\nrealm of effectiveness.\\nIn addition to studies presenting recommendations, one\\nstudy has listed a number of challenges facing the ﬁeld [260].\\nWhile neural machine translation models are superior to otherforms of statistical machine translation models (as well as\\nrule-based models), they require signiﬁcantly more data, per-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='rule-based models), they require signiﬁcantly more data, per-\\nform poorly outside of the domain in which they are trained,fail to handle rare words adequately, and do not do well\\nwith long sentences (more than about 60 words). Furthermore,\\nattention mechanisms do not perform as well as their statisticalcounterparts for aligning words, and beam searches used for\\ndecoding only work when the search space is small. Surely,\\nthese six drawbacks will be, or in some cases, will continueto be, the focus of much research in the coming years.\\nIn addition, as mentioned in Section III-D2, NMT models still\\nstruggle with some semantic concepts, which will also be a\\nlikely area of focus in the upcoming years. While examining\\nsome of these failings of NMT can help, predicting the futureof research and development in t he ﬁeld is nearly impossible.\\nNew models and methods are being reported daily with\\nfar too many advancements to survey, and state-of-the-artpractices are becoming outdated in a matter of months.\\nNotable recent advancements in clude using caching to provide\\nnetworks with greater context than simply the individualsentences being translated [261], the ability to better handle\\nrare words [262], [263], and th e ability to translate to and\\nfrom understudied languages, such as those that are polysyn-thetic [264]. In addition, work has been conducted on the\\nselection, sensitivity, and tuning of hyperparameters [265],\\ndenoising of data [266], and a number of other important'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='denoising of data [266], and a number of other important\\ntopics surrounding NMT. Finally, a new branch of machine\\ntranslation has been opened up by groundbreaking research:multilingual translation.\\nA fairly recent study [267] show ed that a single, simple (but\\nlarge) neural network could be trained to convert a number(up to at least 12) of different languages to each other, auto-\\nmatically recognizing the source language and simply needing\\nan input token to identify the output language. Furthermore,the model was found to be capable of understanding, at least\\nsomewhat, multilingual input, a nd of producing mixed outputs\\nwhen multiple language tokens ar e given, sometimes even in\\nlanguages related to, but not actually, those selected. This\\nsuggests that DNNs may be capable of learning universal\\nrepresentations for informati on, independent of language, and\\neven more, that they might possibly be capable of learning\\nsome etymology and relationships between and among fami-\\nlies of different languages.\\nH. Summary of Deep Learning NLP Applications\\nNumerous other applications of NLP exist including gram-\\nmar correction, as seen in word processors, and author mim-\\nicking, which, given sufﬁcient data, generates text replicatingthe style of a particular writer. Many of these applications are\\ninfrequently used, understudied, or not yet exposed to deep\\nlearning. However, the area of sentiment analysis should benoted, as it is becoming increasingly popular and utilizing'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='deep learning. In large part a semantic task, it is the extraction\\nof a writer’s sentiment—their positive, negative, or neutral\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 618 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\ninclination toward some subject or idea [268]. Applications are\\nvaried, including product research, futures prediction, social\\nmedia analysis, and classiﬁcation of spam [269], [270]. The\\ncurrent state of the art uses an ensemble, including bothLSTMs and CNNs [271].\\nThis section has provided a number of select examples of\\nthe applied usages of deep learning in NLP. Countless studieshave been conducted in these and similar areas, chronicling\\nthe ways in which deep learning has facilitated the successful\\nuse of natural language in a wide variety of applications. Onlya minuscule fraction of such work has been referred to in this\\nsurvey.\\nWhile more speciﬁc recommendations for practitioners have\\nbeen discussed in some individual sections, the current trend\\nin state-of-the-art models in all application areas is to use\\npretrained stacks of transformer units in some conﬁguration,whether in encoder–decoder conﬁgurations or just as encoders.\\nThus, self-attention, which is the mainstay of transformer,\\nhas become the norm, along with cross attention between the\\nencoder and decoder units, if decoders are present. In fact,'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='encoder and decoder units, if decoders are present. In fact,\\nin many recent articles, if not most, transformers have begun toreplace LSTM units that were preponderant just a few months\\nago. Pretraining of these large transformer models has also\\nbecome the accepted way to endow a model with generalizedknowledge of language. Models such as BERT, which have\\nbeen trained on corpora of billions of words, are available\\nfor download, thus providing a practitioner with a model thatpossesses a great amount of general knowledge of language\\nalready. A practitioner can further train it with one’s own\\ngeneral corpora, if desired, but such training is not alwaysnecessary, considering the enormous sizes of the pretraining\\nthat downloaded models have received. To train a model to\\nperform a certain task well, the last step that a practitioner\\nmust go through is to use available downloadable task-speciﬁc\\ncorpora or build one’s own task-speciﬁc corpus. This lasttraining step is usually supervised. It is also recommended\\nthat if several tasks are to be performed, multitask training is\\nused wherever possible.\\nV. C\\nONCLUSION\\nEarly applications of NLP i ncluded a well-acclaimed but\\nsimpleminded algebra word problem solver program called\\nSTUDENT [272], as well as interesting but severely con-\\nstrained conversational systems such as Eliza, which acted asa “psychotherapist” [273], and another that conversed about\\nmanipulating blocks in a microworld [274]. Nowadays, highly'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='manipulating blocks in a microworld [274]. Nowadays, highly\\nadvanced applications of NLP are ubiquitous. These includeGoogle’s and Microsoft’s machine translators, which translate\\nmore or less competently from a language to scores of other\\nlanguages, as well as a number of devices which process\\nvoice commands and respond in like. The emergence of these\\nsophisticated applications, par ticularly in deployed settings,\\nacts as a testament to the impr essive accomplishments that\\nhave been made in this domain over the last sixty or so\\nyears. Without a doubt, incredi ble progress has taken place,\\nparticularly in the last several years.\\nAs has been shown, this recent progress has a clear causal\\nrelationship with the remarkable advances in ANNs. Consid-ered an “old” technology just a decade ago, these machine\\nlearning constructs have ushered in progress at an unprece-\\ndented rate, breaking perform ance records in myriad tasks inmiscellaneous ﬁelds. In particular, deep neural architectures\\nhave instilled models with higher performance in natural lan-\\nguage tasks, in terms of “imperfect” metrics. Consolidating the\\nanalysis of all the models surveyed, a few general trends canbe surmized. Both convolutional and recurrent specimens had\\ncontributed to the state of the art in the recent past; however,\\nof very late, stacks of attention-powered transformer unitsas encoders and often decoders have consistently produced\\nsuperior results across the rich and varying terrain of the'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='superior results across the rich and varying terrain of the\\nNLP ﬁeld. These models are generally heavily pretrained ongeneral language knowledge in an unsupervised or supervised\\nmanner and somewhat lightly trained on speciﬁc tasks in\\na supervised fashion. Second, attention mechanisms alone,without recurrences or convolutions, seem to provide the best\\nconnections between encoders and decoders. Third, forcing\\nnetworks to examine different features (by performing multipletasks) usually improves results. Finally, while highly engineer-\\ning networks usually optimizes results, there is no substitute\\nfor cultivating networks with large quantities of high-quality\\ndata, although pretraining on large generic corpora seems\\nto help immensely. Following from this ﬁnal observation,it may be useful to direct more research effort toward pretrain-\\ning methodologies, rather than developing highly specialized\\ncomponents to squeeze the last drops of performance fromcomplex models.\\nWhile the numerous stellar architectures being proposed\\neach month are highly competitive, muddling the process ofidentifying a winning architecture, the methods of evaluation\\nused add just as much complexity to the problem. Data sets\\nused to evaluate new models are often generated speciﬁcallyfor those models and are then used only several more times,\\nif at all, although consolidated data sets encompassing several\\ntasks such as GLUE [275] have started to emerge. As the'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='tasks such as GLUE [275] have started to emerge. As the\\nfeatures and sizes of these dat a sets are highly variable, this\\nmakes comparison difﬁcult. Most subﬁelds of NLP, as well asthe ﬁeld as a whole, would beneﬁ t from extensive, large-scale\\ndiscussions regarding the necessary contents of such data\\nsets, followed by the compilation of such sets. In additionto high variability in evaluation data, there are numerous\\nmetrics used to evaluate perfo rmance on each task. Oftentimes,\\ncomparing similar models is difﬁ cult because different metrics\\nare reported for each. Agreement on particular sets of metrics\\nwould go a long way toward ensuring clear comparisons in\\nthe ﬁeld.\\nFurthermore, metrics are usually only reported for the best\\ncase, with few mentions of average cases and variability, or of\\nworst cases. While it is important to understand the possibleperformance of new models, it is just as important to under-\\nstand the standard performance. If models produce highly\\nvariable results, they may take many attempts to train to the\\ncutting-edge levels reported. In most cases, this is undesirable,\\nand models that can be consistently trained to relatively highlevels of performance are preferable. While increasingly large\\nnumbers of randomized parameters do reduce variation in\\nperformance, some variance will always exist, necessitating\\nthe reporting of more than just best case metrics.\\nOne ﬁnal recommendation for future work is that it is'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='One ﬁnal recommendation for future work is that it is\\ndirected toward a wider variety of languages than it is atpresent. Currently, the vast majority of research in NLP is\\nconducted on the English language, with another sizeable\\nportion using Mandarin Chinese. In translation tasks, English\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 619\\nis almost always either the input or output language, with\\nthe other end usually being one of a dozen major European\\nor Eastern Asian languages. This neglects entire families of\\nlanguages, as well as the people who speak them. Manylinguistic intricacies may not be expressed in any of the\\nlanguages used and, therefore, are not captured in current\\nNLP software. Furthermore, there are thousands of languagesspoken throughout the world, with at least 80 spoken by more\\nthan 10 million people, meaning that current research excludes\\nan immense segment of humankind. Collection and validationof data in underanalyzed languages, as well as testing NLP\\nmodels using such data, will be a tremendous contribution\\nto not only the ﬁeld of NLP but also to human societyas a whole.\\nDue to the small amounts of data available in many lan-\\nguages, the authors do not foresee the complete usurpationof traditional NLP models by deep learning any time in the\\nnear future. Deep learning models (and even shallow ANNs)'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='near future. Deep learning models (and even shallow ANNs)\\nare extremely data hungry. Cont rastingly, many traditional\\nmodels require only relatively small amounts of training data.\\nHowever, looking further forward, it can be anticipated thatdeep learning models will become the norm in computational\\nlinguistics, with pretraining and transfer learning playing\\nhighly impactful roles. Collobert et al. [9] sparked the deep\\nlearning revolution in NLP, although one of the key contri-\\nbutions of their work—that of a single uniﬁed model—was\\nnot realized widely. Instead, n eural networks were introduced\\ninto traditional NLP tasks and are only now reconnecting.\\nIn the ﬁeld of parsing, for example, most models continue\\nto implement nonneural structures, simply using ANNs onthe side to make the decisions that were previously done\\nusing rules and probability models. While more versatile and\\ngeneral architectures are obviously becoming more and more\\nof a reality, understanding the abstract concepts handled by\\nsuch networks is important to understanding how to build andtrain better networks. Furtherm ore, as abstraction is a hallmark\\nof human intelligence, understanding of the abstractions that\\ntake place inside an ANN may aid in the understanding ofhuman intelligence and the pro cesses that underlie it. Just as\\nhuman linguistic ability is only a piece of our sentience, so is\\nlinguistic processing just a sma ll piece of artiﬁcial intelligence.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='linguistic processing just a sma ll piece of artiﬁcial intelligence.\\nUnderstanding how such components are interrelated is impor-\\ntant in constructing more complete AI systems, and creating a\\nuniﬁed NLP architecture is another step toward making sucha system a reality.\\nThis goal will also be aided by f urther advances in computa-\\ntional equipment. While GPUs have signiﬁcantly improved theability to train deep networks, they are only a step in the right\\ndirection [276]. The next step is the wider availa bility of chips\\ndesigned speciﬁcally for this purpose, such as Google’s Tensor\\nProcessing Unit (TPU), Microsoft’s Catapult, and Intel’s Lake\\nCrest [277]. Ultimately, ANNs implemented in traditional vonNeumann style computers m ay not be able to reach their\\nfull potential. Luckily, another old line of work in computer\\nscience and engineering has seen a resurgance in recent years:neuromorphic computing. With neuromorphic chips, which\\nimplement neural structures a t the hardware level, expected\\nmuch more widely in the coming years [278], the continuationof deep learning and the longevity of its success can be highly\\nanticipated, ensuring the opportunity for sustained progress\\nin NLP.A\\nCKNOWLEDGMENT\\nThe authors would like to thank the anonymous reviewers\\nof this survey for their valuable comments, critiques, and\\nrecommendations. Any opinions, ﬁndings, conclusions, and'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='recommendations. Any opinions, ﬁndings, conclusions, and\\nrecommendations expressed in this material are those of theauthors and do not necessarily reﬂect the views of the National\\nScience Foundation.\\nR\\nEFERENCES\\n[1] K. S. Jones, “Natural language processing: A historical review,” in\\nCurrent Issues in Computational Linguistics: In Honour of Don Walker .\\nDordrecht, The Netherlands: Springer, 1994, pp. 3–16.\\n[2] E. D. Liddy, “Natural language processing,” in Encyclopedia of Library\\nand Information Science , 2nd ed. New York, NY , USA: Marcel Decker,\\nInc., 2001.\\n[3] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, and N. Andrew,\\n“Deep learning with cots HPC systems,” in Proc. ICML , 2013,\\npp. 1337–1345.\\n[4] R. Raina, A. Madhavan, and A. Y . Ng, “Large-scale deep unsu-\\npervised learning using graphics processors,” in Proc. ICML , 2009,\\npp. 873–880.\\n[5] I. Goodfellow, Y . Bengio, A. Courville, and Y . Bengio, Deep Learning ,\\nvol. 1. Cambridge, MA, USA: MIT Press, 2016.\\n[6] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature , vol. 521,\\nno. 7553, pp. 436–444, 2015.\\n[7] J. Schmidhuber, “Deep learning in neural networks: An overview,”\\nNeural Netw. , vol. 61, pp. 85–117, Jan. 2015.\\n[ 8 ] D .C .C i r e s a n et al. , “Flexible, high performance convolutional neural\\nnetworks for image classiﬁcation,” in Proc. IJCAI , 2011, vol. 22, no. 1,\\np. 1237.\\n[9] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='p. 1237.\\n[9] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\\nand P. Kuksa, “Natural language pro cessing (almost) from scratch,”\\nJ. Mach. Learn. Res. , vol. 12 pp. 2493–2537, Aug. 2011.\\n[10] Y . Goldberg, “Neural network m ethods for natural language process-\\ning,” Synth. Lect. Hum. Lang. Technol. , vol. 10, no. 1, pp. 1–309, 2017.\\n[11] Y . Liu and M. Zhang, “Neural network methods for natural lan-\\nguage processing,” Comput. Linguistics , vol. 44, no. 1, pp. 193–195,\\nMar. 2018.\\n[12] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural language processing,” IEEE Comput. Intell.\\nMag. , vol. 13, no. 3, pp. 55–75, Aug. 2018.\\n[13] D. Rumelhart, G. Hinton, and R. Williams, “Learning internal represen-\\ntations by error propagation,” UCS D, La Jolla, CA, USA, Tech. Rep.\\nICS-8506, 1985.\\n[14] Y . LeCun et al. , “Backpropagation applied to handwritten zip\\ncode recognition,” Neural Comput. , vol. 1, no. 4, pp. 541–551,\\nDec. 1989.\\n[15] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based\\nlearning applied to document recognition,” Proc. IEEE , vol. 86, no. 11,\\npp. 2278–2324, Nov. 1998.\\n[16] K. Fukushima, “Neocognitron: A sel f-organizing neural network model\\nfor a mechanism of pattern recognition unaffected by shift in position,”\\nBiol. Cybern. , vol. 36, no. 4, pp. 193–202, Apr. 1980.\\n[17] K. Fukushima and S. Miyake, “Neocognitron: A new algorithm for\\npattern recognition tolerant of deformations and shifts in position,”'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='pattern recognition tolerant of deformations and shifts in position,”\\nPattern Recognit. , vol. 15, no. 6, pp. 455–469, Jan. 1982.\\n[18] Y . LeCun et al. , “Convolutional networks for images, speech, and\\ntime series,” in The Handbook of Brain Theory and Neural Networks ,\\nvol. 3361, no. 10. Cambridge, MA, USA: MIT Press, 1995.\\n[19] A. Krizhevsky, “One weird trick for parallelizing convolutional\\nneural networks,” 2014, arXiv:1404.5997 . [Online]. Available: http://\\narxiv.org/abs/1404.5997\\n[20] Y . Kim, “Convolutional neural networks for sentence classiﬁca-\\ntion,” 2014, arXiv:1408.5882 . [Online]. Available: http://arxiv.org/abs/\\n1408.5882\\n[21] N. Kalchbrenner, E. Grefenste tte, and P. Blunsom, “A convolutional\\nneural network for modelling sentences,” 2014, arXiv:1404.2188 .\\n[Online]. Available: http://arxiv.org/abs/1404.2188\\n[22] C. N. Dos Santos and M. Gatti, “Deep convolutional neural net-\\nworks for sentiment analysis of short texts,” in Proc. COLING , 2014,\\npp. 69–78.\\n[23] D. Zeng et al. , “Relation classiﬁcation via convolutional deep neural\\nnetwork,” in Proc. COLING , 2014, pp. 2335–2344.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 620 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\n[24] M. Kawato, K. Furukawa, and R. Suzuki, “A hierarchical neural-\\nnetwork model for control and l earning of voluntary movement,” Biol.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='network model for control and l earning of voluntary movement,” Biol.\\nCybern. , vol. 57, no. 3, pp. 169–185, Oct. 1987.\\n[25] C. Goller and A. Kuchler, “Learning task-dependent distributed repre-\\nsentations by backpropagation through structure,” in Proc. IEEE Int.\\nConf Neural Netw. , vol. 1, Jun. 1996, pp. 347–352.\\n[26] R. Socher, E. Huang, J. Pennin, C. Manning, and A. Ng, “Dynamic\\npooling and unfolding recursive autoe ncoders for paraphrase detection,”\\ninProc. NIPS , 2011, pp. 801–809.\\n[27] J. L. Elman, “Finding structure in time,” Cognit. Sci. , vol. 14, no. 2,\\npp. 179–211, 1990.\\n[28] L. Fausett, Fundamentals of Neural Networks: Architectures, Algo-\\nrithms, and Applications . Upper Saddle River, NJ, USA: Prentice-Hall,\\n1994.\\n[29] T. Mikolov, M. Karaﬁát, L. Burget, J. ˇCernock` y, and S. Khudanpur,\\n“Recurrent neural network based language model,” in Proc. 11th Annu.\\nConf. Int. Speech Commun. Assoc. , vol. 2, 2010, p. 3.\\n[30] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur,\\n“Extensions of recurrent neural network language model,” in Proc.\\nIEEE ICASSP , May 2011, pp. 5528–5531.\\n[31] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ˇCernock` y, “Strate-\\ngies for training large scale neural network language models,” in Proc.\\nIEEE Workshop Autom. Speech Recognit. Understand. , Dec. 2011,\\npp. 196–201.\\n[32] J. Schmidhuber, “Learning comp lex, extended sequences using the\\nprinciple of history compression,” Neural Comput. , vol. 4, no. 2,\\npp. 234–242, Mar. 1992.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='principle of history compression,” Neural Comput. , vol. 4, no. 2,\\npp. 234–242, Mar. 1992.\\n[33] S. El Hihi and Y . Bengio, “Hierarc hical recurrent neural networks for\\nlong-term dependencies,” in Proc. NIPS , 1996, pp. 493–499.\\n[34] S. Hochreiter and J. Schmi dhuber, “Long short-term memory,” Neural\\nComput. , vol. 9, no. 8, pp. 1735–1780, 1997.\\n[35] K. Greff, R. K. Srivastava, J . Koutník, B. R. Steunebrink, and\\nJ. Schmidhuber, “LSTM: A search space odyssey,” IEEE Trans. Neural\\nNetw. Learn. Syst. , vol. 28, no. 10, pp. 2222–2232, Oct. 2017.\\n[36] K. Cho, B. van Merriënboe r, D. Bahdanau, and Y . Bengio,\\n“On the properties of neural machin e translation: Encoder-decoder\\napproaches,” 2014, arXiv:1409.1259 . [Online]. Available: http://arxiv.\\norg/abs/1409.1259\\n[37] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Empirical evaluation\\nof gated recurrent neural netw orks on sequence modeling,” 2014,\\narXiv:1412.3555 . [Online]. Available: http://arxiv.org/abs/1412.3555\\n[38] D. Bahdanau, K. Cho, and Y . Be ngio, “Neural machine translation\\nby jointly learning to align and translate,” 2014, arXiv:1409.0473 .\\n[Online]. Available: http://arxiv.org/abs/1409.0473\\n[39] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model\\nfor abstractive sentence summarization,” 2015, arXiv:1509.00685 .\\n[Online]. Available: http://arxiv.org/abs/1509.00685\\n[40] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model for\\nabstractive summarization,” 2017, arXiv:1705.04304 . [Online]. Avail-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='abstractive summarization,” 2017, arXiv:1705.04304 . [Online]. Avail-\\nable: http://arxiv.org/abs/1705.04304\\n[41] W. Wang, N. Yang, F. Wei, B. Chang, and M. Zhou, “Gated self-\\nmatching networks for reading comprehension and question answer-\\ning,” in Proc. ACL , vol. 1, 2017, pp. 189–198.\\n[42] A. Vaswani et al. , “Attention is all you need,” in Proc. NIPS , 2017,\\npp. 6000–6010.\\n[43] Y . Bengio, P. Simard, and P. Fras coni, “Learning long-term depen-\\ndencies with gradient descent is difﬁcult,” IEEE Trans. Neural Netw. ,\\nvol. 5, no. 2, pp. 157–166, Mar. 1994.\\n[44] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nBoltzmann machines,” in Proc. ICML , 2010, pp. 807–814.\\n[45] K. He, X. Zhang, S. Ren, and J. S un, “Deep residual learning for image\\nrecognition,” in Proc. IEEE CVPR , Jun. 2016, pp. 770–778.\\n[46] R. Kumar Srivastava, K. Greff, and J. Schmidhuber, “Highway\\nnetworks,” 2015, arXiv:1505.00387 . [Online]. Available: http://arxiv.\\norg/abs/1505.00387\\n[47] G. Huang, Z. Liu, L. V . D. Maaten , and K. Q. Weinberger, “Densely\\nconnected convolutional networks,” in Proc. IEEE CVPR , Jul. 2017,\\nvol. 1, no. 2, pp. 4700–4708.\\n[48] D. Jurafsky and J. Martin, Speech & Language Processing . London,\\nU.K.: Pearson Education, 2000.\\n[49] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-\\nbilistic language model,” J .M a c h .L e a r n .R e s . , vol. 3, pp. 1137–1155,\\nFeb. 2003.\\n[50] W. De Mulder, S. Bethard, and M.-F. Moens, “A survey on the appli-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Feb. 2003.\\n[50] W. De Mulder, S. Bethard, and M.-F. Moens, “A survey on the appli-\\ncation of recurrent neural networks to statistical language modeling,”Comput. Speech Lang. , vol. 30, no. 1, pp. 61–98, Mar. 2015.[51] R. Iyer, M. Ostendorf, and M. Meteer, “Analyzing and predicting\\nlanguage model improvements,” in Proc. IEEE Workshop Autom.\\nSpeech Recognit. Understand. , Dec. 1997, pp. 254–261.\\n[52] S. F. Chen, D. Beeferman, and R. Rosenfeld, “Evaluation metrics\\nfor language models,” School Comput . Sci., Carnegie Mellon Univ.,\\nPittsburgh, PA, USA, Tech. Rep., Jan. 2008. [Online]. Available:https://kilthub.cmu.edu/articles/Evaluation_Metrics_For_Language_\\nModels/6605324, doi: 10.1184/R1/6605324.v1.\\n[53] P. Clarkson and T. Robinson, “Improved language modelling through\\nbetter language model evaluation measures,” Comput. Speech Lang. ,\\nvol. 15, no. 1, pp. 39–53, Jan. 2001.\\n[54] M. Marcus, M. A. Marcinkiewicz, and B. Santorini, “Building a large\\nannotated corpus of English: The Penn Treebank,” Comput. Linguistics ,\\nvol. 19, no. 2, pp. 313–330, 1993.\\n[55] C. Chelba et al. , “One billion word benchmark for measuring progress\\nin statistical language modeling,” 2013, arXiv:1312.3005 . [Online].\\nAvailable: http://arxiv.org/abs/1312.3005\\n[56] M. Daniluk, T. Rocktäschel, J. Welbl, and S. Riedel, “Frustrat-\\ningly short attention spans in neural language modeling,” 2017,\\narXiv:1702.04521 . [Online]. Available: http://arxiv.org/abs/1702.04521'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='arXiv:1702.04521 . [Online]. Available: http://arxiv.org/abs/1702.04521\\n[57] K. Beneš, M. K. Baskar, and L. Bu rget, “Residual memory networks\\nin language modeling: Improving the reputation of feed-forward net-\\nworks,” in Proc. InterSpeech , Aug. 2017, pp. 284–288.\\n[58] N.-Q. Pham, G. Kruszewski, and G. Boleda, “Convolutional neural\\nnetwork language models,” in Proc. EMNLP , 2016, pp. 1153–1162.\\n[59] M. Lin, Q. Chen, and S. Yan, “Network in network,” 2013,\\narXiv:1312.4400 . [Online]. Available: http://arxiv.org/abs/1312.4400\\n[60] Y . Kim, Y . Jernite, D. Sontag, and A. M. Rush, “Character-aware neural\\nlanguage models,” in Proc. AAAI , 2016, pp. 2741–2749.\\n[61] J. Botha and P. Blunsom, “Compositional morphology for word\\nrepresentations and language modelling,” in Proc. ICML , 2014,\\npp. 1899–1907.\\n[62] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural net-\\nwork regularization,” 2014, arXiv:1409.2329 . [Online]. Available:\\nhttp://arxiv.org/abs/1409.2329\\n[63] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y . Wu,\\n“Exploring the limits of language modeling,” 2016, arXiv:1602.02410 .\\n[Online]. Available: https://arxiv.org/abs/1602.02410\\n[64] Y . Ji, T. Cohn, L. Kong, C. Dyer, and J. Eisenstein, “Document con-\\ntext language models,” 2015, arXiv:1511.03962 . [Online]. Available:\\nhttp://arxiv.org/abs/1511.03962\\n[65] N. Shazeer, J. Pelemans, and C. Chelba, “Sparse non-negative matrix\\nlanguage modeling for skip-grams,” in Proc. InterSpeech , 2015,\\npp. 1428–1432.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='language modeling for skip-grams,” in Proc. InterSpeech , 2015,\\npp. 1428–1432.\\n[66] W. Williams, N. Prasad, D. Mrva, T. Ash, and T. Robinson, “Scaling\\nrecurrent neural network language models,” in Proc. IEEE ICASSP ,\\nApr. 2015, pp. 5391–5395.\\n[67] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation\\nof word representations in vector space,” 2013, arXiv:1301.3781 .\\n[Online]. Available: http://arxiv.org/abs/1301.3781\\n[68] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distrib-\\nuted representations of words and phrases and their compositionality,”inProc. NIPS , 2013, pp. 3111–3119.\\n[69] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\\n(2018). Improving Language Underst anding by Generative Pre-\\nTraining . [Online]. Available: https://s3-us-west-2.amazonaws.com/\\nopenai-assets/research-covers/language-unsupervised/language_\\nunderstanding_paper.pdf\\n[70] M. E. Peters et al. , “Deep contextualized word representations,” 2018,\\narXiv:1802.05365 . [Online]. Available: http://arxiv.org/abs/1802.05365\\n[71] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\\ntraining of deep bidirectiona l transformers for language under-\\nstanding,” 2018, arXiv:1810.04805 . [Online]. Available: http://arxiv.\\norg/abs/1810.04805\\n[72] X. Liu, P. He, W. Chen, and J. Gao, “Multi-task deep neural networks\\nfor natural language understanding,” 2019, arXiv:1901.11504 . [Online].\\nAvailable: http://arxiv.org/abs/1901.11504'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Available: http://arxiv.org/abs/1901.11504\\n[73] X. Liu, Y . Shen, K. Duh, and J. Gao, “Stochastic answer networks for\\nmachine reading comprehension,” 2017, arXiv:1712.03556 . [Online].\\nAvailable: http://arxiv.org/abs/1712.03556\\n[74] X. Liu, K. Duh, and J. Gao, “Stochastic answer networks for nat-\\nural language inference,” 2018, arXiv:1804.07888 . [Online]. Available:\\nhttp://arxiv.org/abs/1804.07888\\n[75] T. McCoy, E. Pavlick, and T. Linzen, “Right for the wrong reasons:\\nDiagnosing syntactic heuristics in n atural language inference,” in Proc.\\nACL, 2019, pp. 3428–3448.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 621\\n[76] T. Luong, R. Socher, and C. Manni ng, “Better word representations\\nwith recursive neural networks for morphology,” in Proc. CoNLL , 2013,\\npp. 104–113.\\n[77] L. Finkelstein et al. , “Placing search in context: The concept revisited,”\\ninProc. Int. Conf. World Wide Web , 2001, pp. 406–414.\\n[78] M. Creutz and K. Lagus, “Unsupervised models for morpheme segmen-\\ntation and morphology learning,” ACM Trans. Speech Lang. Process. ,\\nvol. 4, no. 1, pp. 1–34, Jan. 2007.\\n[79] G. A. Miller and W. G. Charles, “Contextual correlates of semantic\\nsimilarity,” Lang. Cognit. Processes , vol. 6, no. 1, pp. 1–28, Jan. 1991.\\n[80] H. Rubenstein and J. B. Goode nough, “Contextual correlates of syn-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='[80] H. Rubenstein and J. B. Goode nough, “Contextual correlates of syn-\\nonymy,” Commun. ACM , vol. 8, no. 10, pp. 627–633, Oct. 1965.\\n[81] E. Huang, R. Socher, C. Manning, and A. Ng, “Improving word\\nrepresentations via global context and multiple word prototypes,” in\\nProc. ACL , vol. 1, 2012, pp. 873–882.\\n[82] Y . Belinkov, N. Durrani, F. Dalvi, H. Sajjad, and J. Glass, “What do\\nneural machine translation models learn about morphology?” 2017,arXiv:1704.03471 . [Online]. Available: http://arxiv.org/abs/1704.03471\\n[83] M. Cettolo, C. Girardi, and M. Federico, “WIT3: Web inventory of\\ntranscribed and translated talks,” in Proc. Conf. Eur. Assoc. Mach.\\nTransl. , 2012, pp. 261–268.\\n[84] M. Cettolo, “An Arabic–Hebrew parallel corpus of TED talks,” 2016,\\narXiv:1610.00572 . [Online]. Available: http://arxiv.org/abs/1610.00572\\n[85] H. Morita, D. Kawahara, and S . Kurohashi, “Morphological analysis\\nfor unsegmented languages using r ecurrent neural network language\\nmodel,” in Proc. EMNLP , 2015, pp. 2292–2297.\\n[86] D. Kawahara and S. Kurohashi, “Case frame compilation from the\\nWeb using high-performance computing,” in Proc. LREC , 2006,\\npp. 1344–1347.\\n[87] D. Kawahara, S. Kurohashi, and K. Hasida, “Construction of a Japanese\\nrelevance-tagged corpus,” in Proc. LREC , 2002, pp. 2008–2013.\\n[88] M. Hangyo, D. Kawahara, and S. Kurohashi, “Building a diverse\\ndocument leads corpus annotated with semantic relations,” in Proc.\\nPaciﬁc–Asia Conf. Lang., Inf., Comput. , 2012, pp. 535–544.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Paciﬁc–Asia Conf. Lang., Inf., Comput. , 2012, pp. 535–544.\\n[89] M. Dehouck and P. Denis, “A frame work for understanding the role of\\nmorphology in universal dependency parsing,” in Proc. Conf. Empirical\\nMethods Natural Lang. Process. , 2018, pp. 2864–2870.\\n[90] A. More et al. , “CONLL-UL: Universal morphological lattices for\\nuniversal dependency parsing,” in Proc. 11th Int. Conf. Lang. Resour.\\nEval. , 2018, pp. 3847–3853.\\n[91] J. Nivre, “An efﬁcient algorithm for projective dependency parsing,” in\\nProc. Int. Workshop Parsing Technol. , 2003, pp. 149–160.\\n[92] J. Nivre, “Incrementality in deterministic dependency parsing,” in\\nProc. Workshop Incremental Parsing Bringing Eng. Cognition Together(IncrementParsing) , 2004, pp. 50–57.\\n[93] J. Nivre, M. Kuhlmann, and J. Hall, “An improved oracle for depen-\\ndency parsing with online reordering,” in Proc. 11th Int. Conf. Parsing\\nTechnol. (IWPT) , 2009, pp. 73–76.\\n[94] R. Socher et al. , “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proc. EMNLP , 2013, pp. 1631–1642.\\n[95] R. Socher, J. Bauer, A. Y . Ng, and C. D. Manning, “Parsing with\\ncompositional vector grammars,” in Proc. ACL , vol. 1, Aug. 2013,\\npp. 455–465.\\n[96] T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and T. Nishino, “A prob-\\nabilistic parsing method for sentence disambiguation,” in Current\\nIssues in Parsing Technology\\n. Boston, MA, USA: Springer, 1991,\\npp. 139–152.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Issues in Parsing Technology\\n. Boston, MA, USA: Springer, 1991,\\npp. 139–152.\\n[97] F. Jelinek, J. Lafferty, and R. Mercer, “Basic methods of probabilistic\\ncontext free grammars,” in Speech Recognition and Understanding .\\nBerlin, Germany: Springer, 1992, pp. 345–360.\\n[98] P. Le and W. Zuidema, “The inside-outside recursive neural network\\nmodel for dependency parsing,” in Proc. EMNLP , 2014, pp. 729–739.\\n[99] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and\\nG. Hinton, “Grammar as a foreign Language,” in Proc. NIPS , 2015,\\npp. 2773–2781.\\n[100] S. Petrov and R. McDonald, “Overview of the 2012 shared task on\\nparsing the Web,” in Proc. Notes 1st Workshop Syntactic Anal. Non-\\nCanonical Lang. , vol. 59, 2012, pp. 1–8.\\n[101] J. Judge, A. Cahill, and J. Van Genabith, “Questionbank: Creating\\na corpus of parse-annotated questions,” in Proc. COLING , 2006,\\npp. 497–504.\\n[102] P. Stenetorp, “Transition-based dependency parsing using recur-\\nsive neural networks,” in Proc. Deep Learn. Workshop Conf.\\nNeural Inf. Process. Syst. (NIPS) , Dec. 2013. [Online]. Available:\\nhttps://pontus.stenetorp.se/res/pdf/stenetorp2013transition.pdf[103] M. Surdeanu, R. Johansson, A. Meyers, L. Màrquez, and J. Nivre,\\n“The CoNLL-2008 shared task on joint parsing of syntactic andsemantic dependencies,” in Proc. CONLL , 2008, pp. 159–177.\\n[104] D. Chen and C. Manning, “A fast and accurate dependency parser using\\nneural networks,” in Proc. EMNLP , 2014, pp. 740–750.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='neural networks,” in Proc. EMNLP , 2014, pp. 740–750.\\n[105] H. Zhou, Y . Zhang, S. Huang, and J. Chen, “A neural probabilistic\\nstructured-prediction model for tran sition-based dependency parsing,”\\ninProc. ACL IJCNLP , vol. 1, 2015, pp. 1213–1222.\\n[106] D. Weiss, C. Alberti, M. Collins, and S. Petrov, “Structured training\\nfor neural network transition-based parsing,” 2015, arXiv:1506.06158 .\\n[Online]. Available: http://arxiv.org/abs/1506.06158\\n[107] Z. Li, M. Zhang, and W. Chen, “Ambiguity-aware ensemble training\\nfor semi-supervised dependency parsing,” in Proc. ACL , vol. 1, 2014,\\npp. 457–467.\\n[108] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith,\\n“Transition-based dependency parsing with stack long short-term\\nmemory,” 2015, arXiv:1505.08075 . [Online]. Available: http://arxiv.\\norg/abs/1505.08075\\n[109] M.-C. de Marneffe and C. D. Ma nning, “The Stanford typed depen-\\ndencies representation,” in Proc. Coling, Workshop Cross-Framework\\nCross-Domain Parser Eval. (CrossParser) , 2008, pp. 1–8.\\n[110] N. Xue, F. Xia, F.-D. Chiou, and M. Palmer, “The Penn Chinese\\nTreeBank: Phrase structure annotation of a large corpus,” Natural Lang.\\nEng., vol. 11, no. 2, pp. 207–238, Jun. 2005.\\n[111] D. Andor et al. , “Globally normalized transition-based neural net-\\nworks,” 2016, arXiv:1603.06042 . [Online]. Available: http://arxiv.org/\\nabs/1603.06042\\n[112] Y . Wang, W. Che, J. Guo, and T. Liu, “A neural transition-based'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='abs/1603.06042\\n[112] Y . Wang, W. Che, J. Guo, and T. Liu, “A neural transition-based\\napproach for semantic dependency graph parsing,” in Proc. 32nd AAAI\\nConf. Artif. Intell. , 2018, pp. 5561–5568.\\n[113] J. Cross and L. Huang, “Incremental parsing with minimal features\\nusing bi-directional LSTM,” 2016, arXiv:1606.06406 . [Online]. Avail-\\nable: http://arxiv.org/abs/1606.06406\\n[114] K. Sheng Tai, R. Socher, and C. D. Manning, “Improved\\nsemantic representations from tr ee-structured long short-term\\nmemory networks,” 2015, arXiv:1503.00075 . [Online]. Available:\\nhttp://arxiv.org/abs/1503.00075\\n[115] S. Oepen et al. , “SemEval 2015 task 18: Broad-coverage semantic\\ndependency parsing,” in Proc. 9th Int. Workshop Semantic Eval.\\n(SemEval) , 2015, pp. 915–926.\\n[116] W. Che, Y . Shao, T. Liu, and Y . Ding, “SemEval-2016 task 9: Chinese\\nsemantic dependency parsing,” in Proc. 10th Int. Workshop Semantic\\nEval. (SemEval) , 2016, pp. 378–384.\\n[117] W.-T. Yih, X. He, and C. Meek, “Semantic parsing for single-relation\\nquestion answering,” in Proc. 52nd Annu. Meeting Assoc. Comput.\\nLinguistics , vol. 2, 2014, pp. 643–648.\\n[118] J. Krishnamurthy, P. Dasigi, an d M. Gardner, “Neural semantic pars-\\ning with type constraints for semi-structured tables,” in Proc. Conf.\\nEmpirical Methods Natural Lang. Process. , 2017, pp. 1516–1526.\\n[119] C. Dyer, A. Kuncoro, M. Ballesteros, and N. A. Smith, “Recurrent\\nneural network grammars,” 2016, arXiv:1602.07776 . [Online]. Avail-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='neural network grammars,” 2016, arXiv:1602.07776 . [Online]. Avail-\\nable: http://arxiv.org/abs/1602.07776\\n[120] D. K. Choe and E. Charniak, “Parsing as language modeling,” in Proc.\\nEMNLP , 2016, pp. 2331–2336.\\n[121] D. Fried, M. Stern, and D. Klein, “Improving neural parsing\\nby disentangling model combina tion and reranking effects,” 2017,\\narXiv:1707.03058 . [Online]. Available: http://arxiv.org/abs/1707.03058\\n[122] T. Dozat and C. D. Manning, “Simpler but more accurate semantic\\ndependency parsing,” 2018, arXiv:1807.01396 . [Online]. Available:\\nhttp://arxiv.org/abs/1807.01396\\n[123] Z. Tan, M. Wang, J. Xie, Y . Chen, and X. Shi, “Deep semantic role\\nlabeling with self-attention,” in Proc. 32nd AAAI Conf. Artif. Intell. ,\\n2018, pp. 4929–4936.\\n[124] L. Duong, H. Afshar, D. Estival, G. Pink, P. Cohen, and M. Johnson,\\n“Active learning for deep semantic parsing,” in Proc. 56th Annu.\\nMeeting Assoc. for Comput. Linguistics , vol. 2, 2018, pp. 43–48.\\n[125] J. Nivre, “Towards a universal grammar for natural language process-\\ning,” in Proc. Int. Conf. Intell. Text Process. Comput. Linguistics .C h a m ,\\nSwitzerland: Springer, 2015, pp. 3–16.\\n[126] D. Zeman et al. , “CoNLL 2018 shared task: Multilingual parsing\\nfrom raw text to universal dependencies,” in Proc. CoNLL Shared\\nTask, Multilingual Parsing Raw Text Universal Dependencies , 2018,\\npp. 1–21.\\n[127] D. Hershcovich, O. Abend, an d A. Rappoport, “Universal depen-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='pp. 1–21.\\n[127] D. Hershcovich, O. Abend, an d A. Rappoport, “Universal depen-\\ndency parsing with a general transition-based DAG parser,” 2018,arXiv:1808.09354 . [Online]. Available: http://arxiv.org/abs/1808.09354\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 622 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\n[128] T. Ji, Y . Liu, Y . Wang, Y . Wu, and M. Lan, “AntNLP at CoNLL 2018\\nshared task: A graph-based parser for universal dependency parsing,”inProc. CoNLL Shared Task, Multilingual Parsing Raw Text Universal\\nDependencies , 2018, pp. 248–255.\\n[129] P. Qi, T. Dozat, Y . Zhang, and C. D . Manning, “Universal dependency\\nparsing from scratch,” 2019, arXiv:1901.10457 . [Online]. Available:\\nhttp://arxiv.org/abs/1901.10457\\n[130] Y . Liu, Y . Zhu, W. Che, B. Qin, N. Schneider, and N. A. Smith, “Parsing\\ntweets into universal dependencies,” 2018, arXiv:1804.08228 . [Online].\\nAvailable: http://arxiv.org/abs/1804.08228\\n[131] J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for\\nword representation,” in Proc. EMNLP , 2014, pp. 1532–1543.\\n[132] Z. S. Harris, “Distributional structure,” Word , vol. 10, nos. 2–3,\\npp. 146–162, 1954.\\n[133] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network\\narchitectures for matching n atural language sentences,” in Proc. NIPS ,\\n2014, pp. 2042–2050.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='architectures for matching n atural language sentences,” in Proc. NIPS ,\\n2014, pp. 2042–2050.\\n[134] A. Bordes, X. Glorot, J. Weston, and Y . Bengio, “A semantic matching\\nenergy function for learning with multi-relational data,” Mach. Learn. ,\\nvol. 94, no. 2, pp. 233–259, Feb. 2014.\\n[135] W. Yin and H. Schütze, “Convoluti onal neural network for paraphrase\\nidentiﬁcation,” in Proc. NAACL, HLT , 2015, pp. 901–911.\\n[136] B. Dolan, C. Quirk, and C. Brock ett, “Unsupervised construction of\\nlarge paraphrase corpora: Exploiting massively parallel news sources,”\\ninProc. COLING , 2004, p. 350.\\n[137] H. He, K. Gimpel, and J. Lin, “Mu lti-perspective sentence similarity\\nmodeling with convolutional neural networks,” in Proc. EMNLP , 2015,\\npp. 1576–1586.\\n[138] M. Marelli, L. Bentivogli, M. Ba roni, R. Bernardi, S. Menini, and\\nR. Zamparelli, “SemEval-2014 task 1: Evaluation of compositional\\ndistributional semantic models on full sentences through semanticrelatedness and textual entailment,” in Proc. 8th Int. Workshop Semantic\\nEval. (SemEval) , 2014, pp. 1–8.\\n[139] E. Agirre, M. Diab, D. Cer, an d A. Gonzalez-Agirre, “SemEval-2012\\ntask 6: A pilot on semantic textual similarity,” in Proc. Joint Conf.\\nLexical Comput. Semantics , vol. 1, 2012, pp. 385–393.\\n[140] H. He and J. Lin, “Pairwise word interaction modeling with deep neural\\nnetworks for semantic similarity measurement,” in Proc. NAACL, HLT ,\\n2016, pp. 937–948.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='networks for semantic similarity measurement,” in Proc. NAACL, HLT ,\\n2016, pp. 937–948.\\n[141] E. Agirre et al. , “SemEval-2014 task 10: Multilingual semantic textual\\nsimilarity,” in Proc. 8th Int. Workshop Semantic Eval. (SemEval) , 2014,\\npp. 81–91.\\n[142] Y . Yang, W.-T. Yih, and C. Meek, “WikiQA: A challenge dataset\\nfor open-domain question answering,” in Proc. EMNLP , 2015,\\npp. 2013–2018.\\n[143] M. Wang, N. A. Smith, and T. Mitamura, “What is the jeopardy model?\\nA quasi-synchronous grammar for QA,” in Proc. Joint EMNLP CoNLL ,\\n2007, pp. 22–32.\\n[144] Q. Le and T. Mikolov, “Distribute d representations of sentences and\\ndocuments,” in Proc. ICML , 2014, pp. 1188–1196.\\n[145] A. Go, R. Bhayani, and L. Huang, “Twitter sentiment classiﬁca-\\ntion using distant supervision,” Project Rep., Stanford, CA, USA,\\nTech. Rep. CS224N, 2009, vol. 1, no. 12.\\n[146] X. Li and D. Roth, “Learning question classiﬁers,” in Proc. COLING ,\\nvol. 1, 2002, pp. 1–7.\\n[147] A. Poliak, Y . Belinkov, J. Glass, and B. Van Durme, “On the evaluation\\nof semantic phenomena in neural machine translation using natural\\nlanguage inference,” 2018, arXiv:1804.09779 . [Online]. Available:\\nhttp://arxiv.org/abs/1804.09779\\n[148] A. Williams, N. Nangia, and S. R. Bowman, “A broad-coverage\\nchallenge corpus for sentence unde rstanding through inference,” 2017,\\narXiv:1704.05426 . [Online]. Available: http://arxiv.org/abs/1704.05426\\n[149] N. Nangia, A. Williams, A. Lazaridou, and S. R. Bowman, “The'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='[149] N. Nangia, A. Williams, A. Lazaridou, and S. R. Bowman, “The\\nRepEval 2017 shared task: Multi-ge nre natural language inference with\\nsentence representations,” 2017, arXiv:1707.08172 . [Online]. Avail-\\nable: http://arxiv.org/abs/1707.08172\\n[150] A. S. White, P. Rastogi, K. Duh, and B. Van Durme, “Inference\\nis everything: Recasting semantic resources into a uniﬁed evaluation\\nframework,” in Proc. IJCNLP , vol. 1, 2017, pp. 996–1005.\\n[151] E. Pavlick, T. Wolfe, P. Rastogi, C. Callison-Burch, M. Dredze, and\\nB. Van Durme, “FrameNet +: Fast paraphrastic tripling of FrameNet,”\\ninProc. ACL , vol. 2, 2015, pp. 408–413.\\n[152] A. Rahman and V . Ng, “Resolving complex cases of deﬁnite pronouns:\\nThe winograd schema challenge,” in Proc. Joint EMNLP CoNLL , 2012,\\npp. 777–789.[153] D. Reisinger, R. Rudinger, F. Fe rraro, C. Harman, K. Rawlins, and\\nB. Van Durme, “Semantic proto-roles,” Trans. Assoc. Comput. Linguis-\\ntics, vol. 3, pp. 475–488, Dec. 2015.\\n[154] A. Poliak, J. Naradowsky, A. Hal dar, R. Rudinger, and B. Van Durme,\\n“Hypothesis only baselines in na tural language inference,” 2018,\\narXiv:1805.01042 . [Online]. Available: http://arxiv.org/abs/1805.01042\\n[155] J. Herzig and J. Berant, “Neu ral semantic parsing over multi-\\nple knowledge-bases,” 2017, arXiv:1702.01569 . [Online]. Available:\\nhttp://arxiv.org/abs/1702.01569\\n[156] Y . Wang, J. Berant, and P. Liang, “Building a semantic parser\\novernight,” in Proc. ACL IJCNLP , vol. 1, 2015, pp. 1332–1342.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='overnight,” in Proc. ACL IJCNLP , vol. 1, 2015, pp. 1332–1342.\\n[157] G. Brunner, Y . Wang, R. Wattenhofer, and M. Weigelt, “Natural\\nlanguage multitasking: Analyzing and improving syntactic saliency of\\nhidden representations,” 2018, arXiv:1801.06024 . [Online]. Available:\\nhttp://arxiv.org/abs/1801.06024\\n[158] P. Koehn, “Europarl: A parallel cor pus for statistical machine transla-\\ntion,” in Proc. MT Summit , vol. 5, 2005, pp. 79–86.\\n[159] G. A. Miller, “WordNet: A lexical database for English,” Commun.\\nACM , vol. 38, no. 11, pp. 39–41, 1995.\\n[160] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives,\\n“DBpedia: A nucleus for a Web of open data,” in The Semantic Web .\\nBerlin, Germany: Springer, 2007, pp. 722–735.\\n[161] Q. Wang, Z. Mao, B. Wang, and L. Guo, “Knowledge graph embed-\\nding: A survey of approaches and applications,” IEEE Trans. Knowl.\\nData Eng. , vol. 29, no. 12, pp. 2724–2743, Dec. 2017.\\n[162] G. Hinton et al. , “Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups,” IEEE Signal\\nProcess. Mag. , vol. 29, no. 6, pp. 82–97, Nov. 2012.\\n[163] A. Graves, A.-R. Mohamed, and G. Hinton, “Speech recognition with\\ndeep recurrent neural networks,” in Proc. IEEE Int. Conf. Acoust.,\\nSpeech Signal Process. , May 2013, pp. 6645–6649.\\n[164] T. Kenter, A. Borisov, C. Van Gysel, M. Dehghani, M. de Rijke,'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='[164] T. Kenter, A. Borisov, C. Van Gysel, M. Dehghani, M. de Rijke,\\nand B. Mitra, “Neural networks for information retrieval,” inProc. 40th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr. , 2017,\\npp. 1403–1406.\\n[165] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning\\ndeep structured semantic models for Web search using clickthrough\\ndata,” in\\nProc. ACM CIKM , 2013, pp. 2333–2338.\\n[166] Y . Shen, X. He, J. Gao, L. Deng, and G. Mesnil, “Learning semantic\\nrepresentations using convolutional neural networks for Web search,”inProc. 23rd Int. Conf. World Wide Web (WWW Companion) , 2014,\\npp. 373–374.\\n[167] Z. Lu and H. Li, “A deep architecture for matching short texts,” in\\nProc. Adv. Neural Inf. Process. Syst. , 2013, pp. 1367–1375.\\n[168] L. Pang, Y . Lan, J. Guo, J. Xu, S. Wan, and X. Cheng, “Text matching\\nas image recognition,” in Proc. 13th AAAI Conf. Artif. Intell. , 2016,\\npp. 2793–2799.\\n[169] J. Guo, Y . Fan, Q. Ai, and W. B. Croft, “A deep relevance matching\\nmodel for ad-hoc retrieval,” in Proc. 25th ACM Int. Conf. Inf. Knowl.\\nManage. (CIKM) , 2016, pp. 55–64.\\n[170] H. Zamani, M. Dehghani, W. B. Croft, E. Learned-Miller, and\\nJ. Kamps, “From neural re-ranking to neural ranking: Learning a sparse\\nrepresentation for inverted indexing,” in Proc. 27th ACM Int. Conf. Inf.\\nKnowl. Manage. (CIKM) , 2018, pp. 497–506.\\n[171] S. MacAvaney, A. Yates, A. Coha n, and N. Goharian, “CEDR: Contex-\\ntualized embeddings for document ranking,” in Proc. 42nd Int. ACM'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='tualized embeddings for document ranking,” in Proc. 42nd Int. ACM\\nSIGIR Conf. Res. Develop. Inf. Retr. , 2019, pp. 1101–1104.\\n[172] J. Cowie and W. Lehnert, “ Information extraction,” Commun. ACM ,\\nvol. 39, no. 1, pp. 80–91, 1996.\\n[173] J. Hammerton, “Named entity recognition with long short-term mem-\\nory,” in Proc. HLT-NAACL , vol. 4, 2003, pp. 172–175.\\n[174] C. Nogueira dos Santos and V . Guimarães, “Boosting named\\nentity recognition with neural character embeddings,” 2015,\\narXiv:1505.05008 . [Online]. Available: http://arxiv.org/abs/1505.05008\\n[175] D. Santos, N. Seco, N. Cardoso, and R. Vilela, “Harem: An advanced\\nner evaluation contest for portuguese,” in Proc. LREC , Genoa, Italy,\\n2006, pp. 1986–1991.\\n[176] X. Carreras, L. Màrquez, and L. Padró, “Named entity extraction using\\nAdaBoost,” in Proc. CoNLL , 2002, pp. 1–4.\\n[177] J. P. C. Chiu and E. Nichols, “Named entity recognition with bidi-\\nrectional LSTM-CNNs,” 2015, arXiv:1511.08308 . [Online]. Available:\\nhttp://arxiv.org/abs/1511.08308\\n[178] E. F. Sang and F. De Meulder, “Introduction to the CoNLL-2003 shared\\ntask: Language-inde pendent named entity recognition,” in Proc. HLT-\\nNAACL , vol. 4, 2003, pp. 142–147.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. OTTER et al. : SURVEY OF THE USAGES OF DEEP LEARNING FOR NLP 623\\n[179] E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel,'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='[179] E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel,\\n“Ontonotes: The 90% solution,” in Proc. Hum. Lang. Technol. Conf,\\nCompanion , 2006, pp. 57–60.\\n[180] S. Pradhan et al. , “Towards robust linguistic analysis using ontonotes,”\\ninProc. CoNLL , 2013, pp. 143–152.\\n[181] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and\\nC. Dyer, “Neural architectures for named entity recognition,” 2016,\\narXiv:1603.01360 . [Online]. Available: http://arxiv.org/abs/1603.01360\\n[182] J. D. Lafferty, A. McCallum, and F. C. N. Pereira, “Conditional random\\nﬁelds: Probabilistic models for segmenting and labeling sequence data,”\\ninProc. 18th Int. Conf. Mach. Learn. San Mateo, CA, USA: Morgan\\nKaufmann, 2001, pp. 282–289.\\n[183] A. Akbik, D. Blythe, and R. V ollg raf, “Contextual string embeddings\\nfor sequence labeling,” in Proc. COLING , 2018, pp. 1638–1649.\\n[184] Y . Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via\\ndynamic multi-pooling convolutional neural networks,” in Proc. ACL ,\\nvol. 1, 2015, pp. 167–176.\\n[185] T. H. Nguyen, K. Cho, and R. Grishman, “Joint event extraction via\\nrecurrent neural networks,” in Proc. Conf. North Amer. Chapter Assoc.\\nComput. Linguistics, Hum. Lang. Technol. , 2016, pp. 300–309.\\n[186] X. Liu, H. Huang, and Y . Zhang, “Open domain event extraction\\nusing neural latent variable models,” 2019, arXiv:1906.06947 . [Online].\\nAvailable: http://arxiv.org/abs/1906.06947'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Available: http://arxiv.org/abs/1906.06947\\n[187] S. Zheng et al. , “Joint entity and relation extraction based on a hybrid\\nneural network,” Neurocomputing , vol. 257, pp. 59–66, Sep. 2017.\\n[188] M. Sun, X. Li, X. Wang, M. Fan, Y . Feng, and P. Li, “Logician:\\nA uniﬁed End-to-End neural approach for open-domain information\\nextraction,” in Proc. 11th ACM Int. Conf. Web Search Data Mining\\n(WSDM) , 2018, pp. 556–564.\\n[189] Z. Tu, Z. Lu, Y . Liu, X. Liu, and H. Li, “Modeling coverage for neural\\nmachine translation,” 2016, arXiv:1601.04811 . [Online]. Available:\\nhttp://arxiv.org/abs/1601.04811\\n[190] C. Lin, T. Miller, D. Dligach, S. Bethard, and G. Savova, “A bert-based\\nuniversal model for both within-and cross-sentence clinical temporal\\nrelation extraction,” in Proc. Clin. NLP Workshop , 2019, pp. 65–71.\\n[191] A. Conneau, H. Schwenk, L. Barrault, and Y . Lecun, “Very deep\\nconvolutional networks for text classiﬁcation,” in Proc. 15th Conf. Eur.\\nChapter Assoc. Comput. Linguistics , vol. 1, 2017, pp. 1107–1116.\\n[192] M. Jiang et al. , “Text classiﬁcation based on deep belief network and\\nsoftmax regression,” Neural Comput. Appl. , vol. 29, no. 1, pp. 61–70,\\nJan. 2018.\\n[193] G. E. Hinton, S. Osindero, and Y . -W. Teh, “A fast learning algorithm\\nfor deep belief nets,” Neural Comput. , vol. 18, no. 7, pp. 1527–1554,\\nJul. 2006.\\n[194] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction ,\\nvol. 1, no. 1. Cambridge, MA, USA: MIT Press, 1998.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='vol. 1, no. 1. Cambridge, MA, USA: MIT Press, 1998.\\n[195] P. Smolensky, “Information processing in dynamical systems: Founda-\\ntions of harmony theory,” Dept. Com put. Sci., Univ. Colorado Boulder,\\nBoulder, CO, USA, Tech. Rep. CU-CS-321-86, 1986.\\n[196] R. Fletcher, Practical Methods of Optimization . Hoboken, NJ, USA:\\nWiley, 2013.\\n[197] A. Adhikari, A. Ram, R. Tang, and J. Lin, “DocBERT: BERT for\\ndocument classiﬁcation,” 2019, arXiv:1904.08398 . [Online]. Available:\\nhttp://arxiv.org/abs/1904.08398\\n[198] J. Worsham and J. Kalita, “Genre identiﬁcation and the compositional\\neffect of genre in literature,” in Proc. COLING , 2018, pp. 1963–1973.\\n[199] J. Wei, Q. Zhou, and Y . Cai, “Poet-based poetry generation: Controlling\\npersonal style with recurrent neural networks,” in Proc. Int. Conf.\\nComput., Netw. Commun. (ICNC) , Mar. 2018, pp. 156–160.\\n[200] J. Hopkins and D. Kiela, “Automa tically generating rhythmic verse\\nwith neural networks,” in Proc. 55th Annu. Meeting Assoc. Comput.\\nLinguistics , vol. 1, 2017, pp. 168–178.\\n[201] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\\n“Language models are unsupervised multitask learners,” OpenAI Blog ,\\nvol. 1, no. 8, p. 9, 2019.\\n[202] B. Bena and J. Kalita, “Introducin g aspects of creativity in automatic\\npoetry generation,” in Proc. Int. Conf. NLP , 2019.\\n[203] S. Tucker and J. Kalita, “Genra ting believable poetry in multiple\\nlanguages using GPT-2,” Univ. Colo rado, Colorado Springs, CO, USA,\\nTech. Rep., 2019.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='languages using GPT-2,” Univ. Colo rado, Colorado Springs, CO, USA,\\nTech. Rep., 2019.\\n[204] Z. Yu, J. Tan, and X. Wan, “A neural approach to pun generation,”\\ninProc. 56th Annu. Meeting Assoc. Comput. Linguistics , vol. 1, 2018,\\npp. 1650–1660.\\n[205] H. Ren and Q. Yang, “Neural joke generation,” Dept. Elect. Eng.,\\nStanford Univ., Stanford, CA, US A, Final Project Rep. Course CS224n,\\n2017.[206] B. Chippada and S. Saha, “Kno wledge amalgam: Generating jokes\\nand quotes together,” 2018, arXiv:1806.04387 . [Online]. Available:\\nhttp://arxiv.org/abs/1806.04387\\n[207] P. Jain, P. Agrawal, A. Mishra, M. Sukhwani, A. Laha, and\\nK. Sankaranaraya nan, “Story generation from sequence of indepen-\\ndent short descriptions,” 2017, arXiv:1707.05501 . [Online]. Available:\\nhttp://arxiv.org/abs/1707.05501\\n[208] N. Peng, M. Ghazvininejad, J. May, and K. Knight, “Towards con-\\ntrollable story generation,” in Proc. 1st Workshop Storytelling , 2018,\\npp. 43–49.\\n[209] L. J. Martin et al. , “Event representations for automated story gener-\\nation with deep neural nets,” in Proc. 32nd AAAI Conf. Artif. Intell. ,\\n2018, pp. 868–875.\\n[210] E. Clark, Y . Ji, and N. A. Smith, “Neural text generation in stories\\nusing entity representations as context,” in Proc. Conf. North Amer.\\nChapter Assoc. Comput. Linguistics, Hum. Lang. Technol. , vol. 1, 2018,\\npp. 2250–2260.\\n[211] J. Xu, X. Ren, Y . Zhang, Q. Zeng, X. Cai, and X. Sun, “A skeleton-\\nbased model for promoting cohere nce among sentences in narra-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='based model for promoting cohere nce among sentences in narra-\\ntive story generation,” 2018, arXiv:1808.06945 . [Online]. Available:\\nhttp://arxiv.org/abs/1808.06945\\n[212] M. Drissi, O. Watkins, and J. Kalita, “Hierarchical text generation using\\nan outline,” in Proc. Int. Conf. NLP , 2018, pp. 180–187.\\n[213] Q. Huang, Z. Gan, A. Celikyilmaz, D. Wu, J. Wang, and X. He,\\n“Hierarchically structured reinforcement learning for topically coherent\\nvisual story generation,” 2018, arXiv:1805.08191 . [Online]. Available:\\nhttp://arxiv.org/abs/1805.08191\\n[214] A. Fan, M. Lewis, and Y . Dauphin, “Hierarchical neural story gen-\\neration,” 2018, arXiv:1805.04833 . [Online]. Available: http://arxiv.\\norg/abs/1805.04833\\n[215] J. Li, M.-T. Luong, and D. Jurafsky, “A hierarchical neural autoencoder\\nfor paragraphs and documents,” 2015, arXiv:1506.01057 . [Online].\\nAvailable: http://arxiv.org/abs/1506.01057\\n[216] K. Lin, D. Li, X. He, Z. Zhang, and M.-T. Sun, “Adversarial ranking\\nfor language generation,” in Proc. Adv. Neural Inf. Process. Syst. , 2017,\\npp. 3155–3165.\\n[217] P. Tambwekar, M. Dhuliawala, L. J. Martin, A. Mehta, B. Harrison,\\nand M. O. Riedl, “Controllable n eural story plot generation via\\nreinforcement learning,” 2018, arXiv:1809.10736 . [Online]. Available:\\nhttp://arxiv.org/abs/1809.10736\\n[218] Y . Zhang et al. , “Adversarial feature matching for text generation,” in\\nProc. 34th Int. Conf. Mach. Learn. , vol. 70, 2017, pp. 4006–4015.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='Proc. 34th Int. Conf. Mach. Learn. , vol. 70, 2017, pp. 4006–4015.\\n[219] L. Chen et al. , “Adversarial text generation via feature-mover’s dis-\\ntance,” in Proc. Adv. Neural Inf. Process. Syst. , 2018, pp. 4666–4677.\\n[220] J. Guo, S. Lu, H. Cai, W. Zhang, Y . Yu, and J. Wang, “Long text\\ngeneration via adversarial training with leaked information,” in Proc.\\n32nd AAAI C onf. Artif. Intell. , 2018, pp. 5141–5148.\\n[221] D. P. Kingma and M. Welling, “Auto-encoding varia-\\ntional Bayes,” 2013, arXiv:1312.6114 . [Online]. Available:\\nhttp://arxiv.org/abs/1312.6114\\n[222] C. Doersch, “Tutorial on v ariational autoencoders,” 2016,\\narXiv:1606.05908 . [Online]. Available: http://arxiv.org/abs/1606.05908\\n[223] I. V . Serban et al. , “A hierarchical latent variable encoder-decoder\\nmodel for generating dialogues,” in Proc. 31st AAAI Conf. Artif. Intell. ,\\n2017, pp. 3295–3301.\\n[224] Z. Hu, Z. Yang, X. Liang, R. Sala khutdinov, and E. P. Xing, “Toward\\ncontrolled generation of text,” in Proc. 34th Int. Conf. Mach. Learn. ,\\nvol. 70, 2017, pp. 1587–1596.\\n[225] W. Wang et al. , “Topic-guided variationa l autoencoders for text\\ngeneration,” 2019, arXiv:1903.07137 . [Online]. Available: http://arxiv.\\norg/abs/1903.07137\\n[226] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi, “The curious\\ncase of neural text degeneration,” 2019, arXiv:1904.09751 . [Online].\\nAvailable: http://arxiv.org/abs/1904.09751\\n[227] E. Clark, A. Celikyilmaz, and N. A. Smith, “Sentence Mover’s sim-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='[227] E. Clark, A. Celikyilmaz, and N. A. Smith, “Sentence Mover’s sim-\\nilarity: Automatic evaluation for multi-sentence texts,” in Proc. 57th\\nAnnu. Meeting Assoc. Comput. Linguistics , 2019, pp. 2748–2760.\\n[228] T. B. Hashimoto, H. Zhang, and P. Liang, “Unifying human\\nand statistical evaluation for na tural language generation,” 2019,\\narXiv:1904.02792 . [Online]. Available: http://arxiv.org/abs/1904.02792\\n[229] A. Gatt and E. Krahmer, “Survey of the state of the art in natural\\nlanguage generation: Core tasks, a pplications and evaluation,” J. Artif.\\nIntell. Res. , vol. 61, pp. 65–170, Jan. 2018.\\n[230] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A compre-\\nhensive survey of deep learning for image captioning,” ACM Comput.\\nSurveys , vol. 51, no. 6, pp. 1–36, Feb. 2019.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply. 624 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 2, FEBRUARY 2021\\n[231] X. Liu, Q. Xu, and N. Wang, “A survey on deep neural network-\\nbased image captioning,” Vis. Comput. , vol. 35, no. 3, pp. 445–470,\\nMar. 2019.\\n[232] J. Krantz and J. Kalita, “Abstractive summarization using attentive\\nneural techniques,” in Proc. Int. Conf. NLP , 2018, pp. 1–9.\\n[233] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence level\\ntraining with recurrent neural networks,” 2015, arXiv:1511.06732 .\\n[Online]. Available: http://arxiv.org/abs/1511.06732'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='[Online]. Available: http://arxiv.org/abs/1511.06732\\n[234] J. Gehring, M. Auli, D. Grangier , D. Yarats, and Y . N. Dauphin, “Con-\\nvolutional sequence to s equence learning,” 2017, arXiv:1705.03122 .\\n[Online]. Available: http://arxiv.org/abs/1705.03122\\n[235] H. Zhang, J. Xu, and J. Wang, “P retraining-based natural language\\ngeneration for text summarization,” 2019, arXiv:1902.09243 . [Online].\\nAvailable: http://arxiv.org/abs/1902.09243\\n[236] L. Dong, F. Wei, M. Zhou, and K. Xu, “Question answering over\\nfreebase with multi-column convolutional neural networks,” in Proc.\\n53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf.Natural Lang. Process. , vol. 1, 2015, pp. 260–269.\\n[237] A. Santoro et al. , “A simple neural network module for relational\\nreasoning,” in Proc. NIPS , vol. 2017, pp. 4974–4983.\\n[238] D. Raposo, A. Santoro, D. Barrett, R. Pascanu, T. Lillicrap, and\\nP. Battaglia, “Discovering objects a nd their relations from entangled\\nscene representations,” 2017, arXiv:1702.05068 . [Online]. Available:\\nhttp://arxiv.org/abs/1702.05068\\n[239] W. Yang et al. , “End-to-End open-domain question answering\\nwith BERTserini,” 2019, arXiv:1902.01718 . [Online]. Available:\\nhttp://arxiv.org/abs/1902.01718\\n[240] H. Schwenk, “Continuous space translation models for phrase-\\nbased statistical machine translation,” in Proc. COLING , 2012,\\npp. 1071–1080.\\n[241] T. Deselaers, S. Hasan, O. Be nder, and H. Ney, “A deep learning'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='pp. 1071–1080.\\n[241] T. Deselaers, S. Hasan, O. Be nder, and H. Ney, “A deep learning\\napproach to machine transliteration,” in Proc. 4th Workshop Stat. Mach.\\nTransl. (StatMT) , 2009, pp. 233–241.\\n[242] N. Kalchbrenner and P. Blunsom, “ Recurrent continuous translation\\nmodels,” in Proc. EMNLP , 2013, pp. 1700–1709.\\n[243] R. Collobert and J. Weston, “A uniﬁe d architecture for natural language\\nprocessing: Deep neural networks with multitask learning,” in Proc.\\nICML , 2008, pp. 160–167.\\n[244] K. Cho et al. , “Learning phrase representations using RNN encoder-\\ndecoder for statistical machine translation,” 2014, arXiv:1406.1078 .\\n[Online]. Available: http://arxiv.org/abs/1406.1078\\n[245] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\\nwith neural networks,” in Proc. NIPS , 2014, pp. 3104–3112.\\n[246] Y . Wu et al. , “Google’s neural machine translation system: Bridging the\\ngap between human and machine translation,” 2016, arXiv:1609.08144 .\\n[Online]. Available: http://arxiv.org/abs/1609.08144\\n[247] D. Britz, A. Goldie, M.-T. Luong, and Q. Le, “Massive exploration\\nof neural machine translation architectures,” 2017, arXiv:1703.03906 .\\n[Online]. Available: http://arxiv.org/abs/1703.03906\\n[248] R. Sennrich et al. , “Nematus: A toolkit for neural machine transla-\\ntion,” 2017, arXiv:1703.04357 . [Online]. Available: http://arxiv.org/abs/\\n1703.04357\\n[249] G. Klein, Y . Kim, Y . Deng, J. Se nellart, and A. M. Rush, “Open-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='1703.04357\\n[249] G. Klein, Y . Kim, Y . Deng, J. Se nellart, and A. M. Rush, “Open-\\nNMT: Open-source toolkit for ne ural machine translation,” 2017,\\narXiv:1701.02810 . [Online]. Available: http://arxiv.org/abs/1701.02810\\n[250] R. Sennrich and B. Haddow, “Lingui stic input features improve neural\\nmachine translation,” 2016, arXiv:1606.02892 . [Online]. Available:\\nhttp://arxiv.org/abs/1606.02892\\n[251] K. Ahmed, N. Shirish Keskar, and R. Socher, “Weighted transformer\\nnetwork for machine translation,” 2017, arXiv:1711.02132 . [Online].\\nAvailable: http://arxiv.org/abs/1711.02132\\n[252] S. Hochreiter, “Gradient ﬂow in recurrent nets: The difﬁculty of\\nlearning long-term dependencies,” in A Field Guide to Dynamical\\nRecurrent Neural Networks . Piscataway, NJ, USA: IEEE Press, 2001.\\n[253] M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, and M. Fed-\\nerico, “Report on the 11th IWSLT evaluation campaign, IWSLT2014,” in Proc. Int. Workshop Spoken Lang. Transl. ,H a n o i ,V i e t n a m ,\\n2014, pp. 2–17.\\n[254] J. Richard Medina and J. Kalita , “Parallel attention mechanisms in\\nneural machine translation,” 2018, arXiv:1810.12427 . [Online]. Avail-\\nable: http://arxiv.org/abs/1810.12427\\n[255] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: A method\\nfor automatic evaluation of machine translation,” in Proc. ACL , 2002,\\npp. 311–318.[256] M. Ghazvininejad, O. Levy, Y . Liu, and L. Zettlemoyer, “Mask-\\npredict: Parallel decoding of c onditional masked language mod-'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='predict: Parallel decoding of c onditional masked language mod-\\nels,” 2019, arXiv:1904.09324 . [Online]. Available: http://arxiv.org/\\nabs/1904.09324\\n[257] G. Lample and A. Conneau, “Cross-lingual language model pre-\\ntraining,” 2019, arXiv:1901.07291 . [Online]. Available: http://arxiv.org/\\nabs/1901.07291\\n[258] M. Xu Chen et al. , “The best of both worlds: Combining recent\\nadvances in neural machine translation,” 2018, arXiv:1804.09849 .\\n[Online]. Available: http://arxiv.org/abs/1804.09849\\n[259] M. Denkowski and G. Neubig, “Stronger baselines for trustable results\\nin neural machine translation,” 2017, arXiv:1706.09733 . [Online].\\nAvailable: http://arxiv.org/abs/1706.09733\\n[260] P. Koehn and R. Knowles, “Six challenges for neural machine transla-\\ntion,” 2017, arXiv:1706.03872 . [Online]. Available: http://arxiv.org/abs/\\n1706.03872\\n[261] S. Kuang, D. Xiong, W. Luo, an d G. Zhou, “Modeling coherence for\\nneural machine translation with dynamic and topic caches,” in Proc.\\nCOLING , 2018, pp. 596–606.\\n[262] M.-T. Luong, I. Sutskever, Q. V . Le, O. Vinyals, and W. Zaremba,\\n“Addressing the rare word probl em in neural machine transla-\\ntion,” 2014, arXiv:1410.8206 . [Online]. Available: http://arxiv.org/abs/\\n1410.8206\\n[263] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation\\nof rare words with subword units,” 2015, arXiv:1508.07909 . [Online].\\nAvailable: http://arxiv.org/abs/1508.07909\\n[264] M. Mager, E. Mager, A. Medi na-Urrea, I. Meza, and K. Kann,'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='[264] M. Mager, E. Mager, A. Medi na-Urrea, I. Meza, and K. Kann,\\n“Lost in translation: Analysis of information loss during machine\\ntranslation between polysynthe tic and fusional languages,”\\n2018, arXiv:1807.00286 . [Online]. Available: http://arxiv.org/\\nabs/1807.00286\\n[265] M. Ott, M. Auli, D. Grangier, and M. Ranzato, “Analyzing uncertainty\\nin neural machine translation,” 2018, arXiv:1803.00047 . [Online].\\nAvailable: http://arxiv.org/abs/1803.00047\\n[266] W. Wang, T. Watanabe, M. Hughe s, T. Nakagawa, and C. Chelba,\\n“Denoising neural machine translation training with trusted data and\\nonline data selection,” 2018, arXiv:1809.00068 . [Online]. Available:\\nhttp://arxiv.org/abs/1809.00068\\n[267] M. Johnson et al. , “Google’s multilingual neural machine transla-\\ntion system: Enabling zero-shot translation,” 2016, arXiv:1611.04558 .\\n[Online]. Available: http://arxiv.org/abs/1611.04558\\n[268] D. Jurafsky and J. Martin, Speech & Language Processing ,3 r de d .\\nLondon, U.K.: Pearson Education, 2017.\\n[269] L. Zheng, H. Wang, and S. Gao, “Sentimental feature selection for\\nsentiment analysis of Chinese online reviews,” Int. J. Mach. Learn.\\nCybern. , vol. 9, no. 1, pp. 75–84, Jan. 2018.\\n[270] M. Etter, E. Colleoni, L. Illia, K. Meggiorin, and A. D’Eugenio,\\n“Measuring organizational legitimacy in social media: Assessing cit-izens’ judgments with sentiment analysis,” Bus. Soc. , vol. 57, no. 1,\\npp. 60–97, Jan. 2018.\\n[271] M. Cliche, “BB_twtr at SemEval-2017 task 4: Twitter sentiment'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='pp. 60–97, Jan. 2018.\\n[271] M. Cliche, “BB_twtr at SemEval-2017 task 4: Twitter sentiment\\nanalysis with CNNs and LSTMs,” 2017, arXiv:1704.06125 . [Online].\\nAvailable: http://arxiv.org/abs/1704.06125\\n[272] D. G. Bobrow, “Natural la nguage input for a computer problem\\nsolving system,” Massachusetts Inst. Technol., Ca mbridge, MA, USA,\\nTech. Rep. MAC-TR-1, 1964.\\n[273] J. Weizenbaum, “ELIZA—A computer program for the study of natural\\nlanguage communication between man and machine,” Commun. ACM ,\\nvol. 9, no. 1, pp. 36–45, Jan. 1966.\\n[274] T. Winograd, “Procedures as a representation for data in a computer\\nprogram for understanding natura l language,” MIT, Cambridge, MA,\\nUSA, Tech. Rep. MAC-TR-84, 1971.\\n[275] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\\n“GLUE: A multi-task benchmark and analysis platform for natural\\nlanguage understanding,” 2018, arXiv:1804.07461 . [Online]. Available:\\nhttp://arxiv.org/abs/1804.07461\\n[276] C. D. Schuman et al. , “A survey of neuromorphic computing and neural\\nnetworks in hardware,” 2017, arXiv:1705.06963 . [Online]. Available:\\nhttp://arxiv.org/abs/1705.06963\\n[277] J. Hennessy and D. Patterson, Computer Architecture: A Quantitative\\nApproach . Amsterdam, The Netherlands: Elsevier, 2017.\\n[278] D. Monroe, “Neuromorphic computing gets ready for the (really) big\\ntime,” Commun. ACM , vol. 57, no. 6, pp. 13–15, Jun. 2014.'),\n",
       " Document(metadata={'title': 'A Survey of the Usages of Deep Learning for Natural Language Processing', 'year': 2021}, page_content='time,” Commun. ACM , vol. 57, no. 6, pp. 13–15, Jun. 2014.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:18:30 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/358749331\\nA Combined approach of Base and Meta Learners for Hybrid System\\nArticle \\xa0\\xa0 in\\xa0\\xa0Turkish Journal of Engineering  · Januar y 2023\\nDOI: 10.31127/ tuje.1007508\\nCITATIONS\\n12READS\\n113\\n5 author s, including:\\nAbdul Ahad\\nIqra Univ ersity\\n34 PUBLICA TIONS \\xa0\\xa0\\xa0200 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nWaqas ahmed Siddique\\nMiTE univ ersity\\n14 PUBLICA TIONS \\xa0\\xa0\\xa082 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nMir Sajjad Hussain T alpur\\nSindh Agricult ure Univ ersity\\n37 PUBLICA TIONS \\xa0\\xa0\\xa0214 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAwais Khan Jumani\\nILMA Univ ersity Kar achi\\n73 PUBLICA TIONS \\xa0\\xa0\\xa0708 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Abdul Ahad  on 03 April 2022.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.* Corresponding Author  Cite this article  \\n*(abdulahadabro1@gmail.com ) ORCID ID 0000 -0002 -3591 -9231  \\n (eng.waqas@outlook.com ) ORCID ID 0000 -0001 -8206 -4451  \\n (mirsajjadhussain@sau.edu.pk ) ORCID ID 0000 -0001 -9897 -3916  \\n(awaisjumani@yahoo.com ) ORCID ID 0000 -0001 -9468 -0446  \\n(erkan.yasar@ege.edu.tr ) ORCID ID 0000 -0001 -7333 -6320  \\n Abro, A. A., Siddique, W. A., Talpur, M. S. H., Jumani, A. K. & Yaşar, E. (2023). A combined \\napproach of base and meta learners for hybrid system. Turkish Journal of Engineering , \\n7(1), 25 -32 \\n \\n \\n Turkish  Journa l of Engineering – 2023, 7(1), 25-32 \\n \\n Turkish  Journa l of Engineering'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='Turkish  Journa l of Engineering – 2023, 7(1), 25-32 \\n \\n Turkish  Journa l of Engineering  \\nhttps://dergipark.org.tr/en/pub/tuje  \\ne-ISSN  2587 -1366  \\n \\n \\n \\nA combined approach of base and meta learners for hybrid system  \\n \\nAbdul Ahad Abro*1, Waqas Ahmed Siddique 2, Mir Sajjad Hussain Talpur 3, Awais Khan Jumani2, Erkan \\nYaşar1  \\n \\n1Ege University, Department of Computer Engineering, T ürkiye  \\n2Ilma University, Depa rtment of Computer Science, Pakistan  \\n3Sindh Agriculture University, Information Technology Centre, Pakistan  \\n \\n \\n \\n \\n \\nKeywords   Abstract  \\nArtificial Intelligence  \\nMachine Learning  \\nEnsemble Learning  \\nPattern Recognition  \\nData Mining  \\nClassification  \\n  The ensemble learning method is considered a meaningful yet challenging task. To enhance \\nthe performance of binary classification and predictive analysis, this paper proposes an \\neffective ensemble learning approach by applying multiple models to produce effici ent and \\neffective outcomes. In these experimental studies, three base learners, J48, Multilayer \\nPerceptron (MP), and Support Vector Machine (SVM) are being utilized. Moreover, two meta -\\nlearners, Bagging and Rotation Forest are being used in this analysis. Firstly, to produce \\neffective results and capture productive data, the base learner, the J48 decision tree is \\naggregated with the rotation forest. Secondly, machine learning and ensemble learning'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='aggregated with the rotation forest. Secondly, machine learning and ensemble learning \\nclassification algorithms along with the five UCI Datasets a re being applied to progress the \\nrobustness of the system. Whereas, the recommended mechanism is evaluated by \\nimplementing five performance standards concerning the accuracy, AUC (Area Under Curve), \\nprecision, recall and F -measure values. In this regard, e xtensive strategies and various \\napproaches were being studied and applied to obtain improved results from the current \\nliterature; however, they were insufficient to provide successful results. We present \\nexperimental results which demonstrate the efficienc y of our approach to well -known \\ncompetitive approaches. This method can be applied to image identification and machine \\nlearning problems, such as binary classification.  Research  Article  \\nDOI: 10.31127/tuje.1007508  \\n \\nReceived:  09.10.2021  \\nAccepted:  20.02. 2022  \\nPublished:  01.04.2022  \\n \\n \\n \\n \\n \\n \\n \\n1. Introduction  \\n \\nIn the field of data mining, the classification task is to \\ncorrectly predict the class of a given instance. Several \\ntheoretical and empirical studies have been published \\nthat demonstrate the advantages of the hybrid model. \\nThese approaches are known as multi -classifiers or \\nensembles. A huge number of research was carried out to \\nproduce multiple classifier systems based on the same \\nclassifier models trained on different data or feature \\nsubsets [1 -2].'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='classifier models trained on different data or feature \\nsubsets [1 -2]. \\nThe primary agenda of the resea rch is to evaluate and \\ncompare various techniques (J48, MP, SVM) with Bagging \\nand Rotation Forest for binary classification. In this \\npaper, we provide a technique based on the J48 Machine \\nLearning algorithm integrated with the rotation forest \\nensemble lear ning algorithm [3 -4]. Decision tree J48 is the execution of an algorithm \\n(Iterative Dichotomiser 3). J48 algorithm is a \\nclassification algorithm producing a decision tree \\nfocused on information theory. It is one of the best \\nmachine learning algorithms for categorising and \\ncontinuously examining data [5]. To produce accurate \\nclassification results, the J48 method is utilised to classify \\nnumerous applications.  \\nOn the other side, Rotation Forest is a method focused \\non feature extraction for generating classifi er ensembles \\n[6]. It has been broadly used to resolve a variety of tasks \\nrelating to medical images, computer vision and machine \\nlearning to achieve outstanding performances.  \\nIn [7], bagging and classification tree methods were \\ncombined to introduced the B AGCT and BAGCT‐SVM \\nframework to improve the reliability and robustness. The \\nTurkish  Journa l of Engineering – 202 3, 7(1), 25-32 \\n \\n  26  \\n outcomes indicate that the BAGCT‐SVM contributes \\nimproved analytical capability than CT and SVM.  \\nThis paper has been structured with several sections.'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content=\"This paper has been structured with several sections. \\nSection 2, discusses the rela ted works about machine \\nlearning and ensemble learning algorithms, mainly \\nfocusing on J48 and Rotation Forest. Section 3, presents \\nthe proposed methodology in detail. Section 4, describes \\nthe datasets, experimental methods, and outcomes. \\nFinally, the concl usion and future works are stated in \\nSection 5.  \\n \\n2. Related work  \\n \\nIn general, hybrid system base and meta -approaches  \\ncan enhance the effects and integrate the dynamic \\napproaches in the system. A surge of research efforts has \\nbeen recently witnessed for the c lassifications based on \\nJ48, MP, SVM and Bagging along with Rotation Forest. In \\nthis paper, we have included the classification of datasets \\nconcerning the base learners and meta -learners.  \\nWe analyzed various research articles to find current \\nstate -of-the-art developments in the domain of the \\nHybrid System. A few of them are discussed as follows:  \\nIn [8], proposed a hybrid model for Parkinson's \\ndiagnosis using machine learning techniques. The hybrid \\nmodel includes feature selection methods such as an \\nextra t ree and mutual information gain and three \\nclassifiers k -nearest -neighbors, random forest and naive \\nbayes. The combination of random forest and the genetic \\nalgorithm was performed and 95.58% accuracy was \\nachieved.  \\nIn [9], the model is suggested primarily to  assist and \\noptimize the movement patterns of aged people. A new\"),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='optimize the movement patterns of aged people. A new \\nclassifier named Apriori based Probability Tree Classifier \\n(APTC) is integrated into the bagged J48 machine \\nlearning algorithm to yield a better outcome.  \\nIn [10], multiple ensemble methods Ra ndom \\nSubSpace, Rotation Forest, Bagging, MultiBoost, Dagging \\nand AdaBoost with the base classifier of Multiple \\nPerceptron Neural Networks. The execution of the base \\nclassifier of MLP significantly improved concerning AUC. \\nThe results of the review are indi cated in the current \\nresearch, and paradigms using machine learning \\nensemble frameworks have worked properly.  \\nIn [11], extreme learning machine (ELM) created \\nhierarchical learning structure was proposed for MP.  \\nThe architecture of ELM based on feature ext raction and \\nrandom has initialized hidden weights. This method had \\nbetter learning efficiency than Deep learning. The \\nproposed algorithm achieved better and faster \\nconvergence than the existing state -of-the-art \\nhierarchical learning methods.  \\nIn [12], robu st machine learning SVM -based \\nalgorithms has been suggested. It is based on the \\nframework of the double duality strategy of the decision -\\nmaking process to get the additional constraints for \\noptimization variables incorporated of imprecise \\ninformation.  \\nIn [13], a hybrid ensemble learning method bagging, \\nboosting, random forest and rotation forest along with \\nlogistic regression with stacking classifiers were'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='logistic regression with stacking classifiers were \\nintroduced, which resultant occupy more space and \\nconsume more time for computations.  In [14], a rotat ion forest algorithm created on \\nheterogeneous classifiers ensemble is applied to \\nclassified the gene expression outline. The local optimum \\nand overfitting were improved through heterogeneous \\nrotation forests. It improves the high stability, \\nclassification accuracy and time efficiency.  \\nIn [15], proposed a collaborative approach of \\nblockchain and metaheuristic -enabled genetic \\nalgorithms. Blockchain technology provides a secure \\ncommunication channel between stakeholders where a \\nmetaheuristic -enabled genetic algorithm, process and \\nanalyze t he forecast pricing from records by scheduling, \\nmanaging and monitoring them in real -time from day -to-\\nday agriculture production detail. This approach \\nachieved 95.3% accuracy and maintains transparency, \\nintegrity, availability and secure operational contro l \\naccess.  \\nIn [16], propsed the state -of-the-art utilization of ML \\nalgorithm, which are C4.5 (J48), K -Nearest Neighbor \\n(KNN), Logistic Regression (LR), Naive Bayes (NB), \\nSupport Vector Machine (SVM), and One Rule (OneR) \\nalong with the five UCI Datasets. A retrospective study \\nthat looked at different sizes of training and test sets had \\na significant impact on the sensitivity and specificity of \\nthe same algorithm. The collaborative nature of the \\nproposed system is to improve the efficiency of binary'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='proposed system is to improve the efficiency of binary \\nclassific ation.  \\nIn [17], rotation forest algorithms are proposed for \\ngene appearance of data classification. Three types of \\nclassification named; misclassification, test and rejection \\ncost were integrated into the framework to make it more \\nreliable and efficient. The experimental results have \\nshown that the overall classification accuracy was \\nimproved significantly.  \\nIn [18], proposed a Parkinson’s diagnosis system by \\nusing an optimized version of the BAT algorithm. Only 23 \\nfeatures were selected from the UCI Parkinson’s disease \\nclassification data set and directly feed into the 23 \\nneurons in the input layer of the model. The 96.74% \\naccuracy was achieved by the proposed method with a \\n3.27% loss.  \\nIn [19], address the state -of-the-art utilization of ML \\nin compute r vision and image processing. This survey \\nwill provide details about the type of tools and \\napplications and datasets. Multiple techniques and \\nvarious types of supervised and unsupervised ML \\nalgorithms, the overview of image processing and the \\nresults base d on the impact; neural network -enabled \\nmodels, limitations, tools and application of computer \\nvision have been discussed.  \\nIn [20], the metaheuristic optimization procedure \\nalong with the whale optimization set of rules and \\nrotation forest algorithm was a pplied for the selection of \\nemail features and categorising the emails as spam and'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='email features and categorising the emails as spam and \\nnon-spam. The results obtained showed that the \\nsuggested technique generated notable improvement as \\ncompared with some previous methods.  \\nIn [21], compared and investigated s tate -of-the-art \\nensemble techniques Bagging, AdaBoost and Rotation \\nForest with the base classifier of J48 for the susceptibility \\nof the landslide. The performance was assessed through \\nROC, AUC and statistical indexes. The J48 with the \\nRotation Forest model  presented the highest prediction Turkish  Journa l of Engineering – 202 3, 7(1), 25-32 \\n \\n  27  \\n capability followed by AdaBoost and Bagging \\nrespectively. Moreover, J48 with Rotation Forest has \\nproved the best -enhanced approach and promising one \\nfor better accuracy.  \\nIn [22], SVM, Naïve Bayes, Logistic Regression and K -\\nNearest Neighbor classifier had been utilized for binary \\nclassification. In supervised ML algorithms, the \\nrobustness of the method is progressed accordingly.  \\nIn the literature, some features have a negative \\nimpact on classification algorithms. The primar y goal of \\nclassification is to reliably predict the target class for \\neach occurrence in the data. A classification algorithm \\ncoordinates between the values of the predictors and the \\nvalues during the model build training process.  \\n \\n \\n3. Methodology  \\n \\nThis secti on provides an overview of the proposed \\nmethod, which describes the pre -processing stage of data \\nand classification algorithms.'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='method, which describes the pre -processing stage of data \\nand classification algorithms.  \\n \\n3.1. Overview of the proposed system  \\n \\nAn overview of the proposed framework is given in \\nFig. 1. This system is composed of many phas es: datasets, \\nbase learners, meta -learners and comparative analysis of \\nthe results. In addition, method generalization efficiency, \\n10-fold cross -validation, is used for all learners and \\ndatasets of the classifier.  \\n \\n3.2. Data pre -processing  \\n \\nIn this phase, the r anges of the values of the data in \\ndatasets may be high. In such a scenario, certain features \\ncan significantly or negatively affect algorithms for \\nclassification accuracy. Therefore, the data assessments \\nare normalized to the [0,1] range using the min -max  \\nnormalization technique [23 -24]. For mapping a value, of \\na feature x i from the range [min(x i ), max(x i)] to a new \\nrange [minx new, maxx new ], the normalized feature x̑i is \\ncomputed as Eq. 1.  \\n \\n (1) \\n \\n \\n3.3. Classification of algorithms  \\n \\nIn this study, three base learners, including J48, MP, \\nSVM and two Meta -Learners Rotation Forest and Bagging \\nare employed as shown in Figure 1.  \\nThere are numerous phases of methods related to the \\ndatasets and classifiers. In this work, base learners and \\nmeta -learner along with several datasets, are \\nexperienced for binary classification . \\n \\n4. Experimental work  \\n \\nIn these subsections, we define and present the \\nexperimental procedure, measurements of evaluation'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='experimental procedure, measurements of evaluation \\nand results of the experiment.  \\n  \\nFigure 1.  The framework of the method  \\n \\n \\n4.1. Experimental process  \\n \\nIn the experimental process, five datasets have been \\nused from the UCI ML Repository [25].  \\nAll experiments are performed on base and meta -\\nlearners by using WEKA (Waikato Environment for \\nKnowledge Analysis) ML toolkit and JAVA programming \\nlanguage [24]. We have utilized default parameter values \\nfor all the classifiers in WEKA.  \\nOn the other hand, we have carried out 10 -fold cross -\\nvalidation to all datasets to yield reliable results. The \\ncross -validat ion is imposed on the original dataset \\nrandomly partitioned into 10 equally sized sets, one of \\nwhich is used as test validation, while the remaining sets \\nare used for training operations. The method is recurring \\n10 times and calculates the averages of the results.  \\nDataset characteristics are evaluated concerning the \\nattributes and the number of instances. There are various \\nnumerical attribute descriptions illustrated in Table I. \\nThe number of instances, attributes, and classes for each \\ndataset are presented  in Table I. It is determined by \\ninvestigating the appropriate data or datasets which are \\nbeing utilized for binary classification problems.  \\n \\nTable 1.  This is the example of table formatting  \\nDatasets  Instances  Attributes  Classes  \\nAbalone  4177  8 29 \\nBalance Scale  625  4 3 \\nDiabetes  768  8 2 \\nGerman Credit  1000  21 2'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content=\"Abalone  4177  8 29 \\nBalance Scale  625  4 3 \\nDiabetes  768  8 2 \\nGerman Credit  1000  21 2 \\nSonar  208  60 2 \\n \\nTurkish  Journa l of Engineering – 202 3, 7(1), 25-32 \\n \\n  28  \\n In this work, various approaches have been carried \\nout along with several datasets, which are considered \\nsuitable for the classification. However, the performance \\nmetrics are calculated according to the binary \\nclassification problems based on the confusion matrix.  \\n \\n4.2. Assessment of measures  \\n \\nThis section explains the proposed method's five \\nperformance assessment metrics, consisting of accuracy, \\nAUC, precision, recall, and F -measure.  \\nAccuracy reflects how close an agreed number is to a \\nmeasurement. It is specified further in Eq. (2).  \\n \\nAcc = (𝑇𝑃+𝑇𝑁\\n𝑇𝑃 + 𝐹𝑃+𝐹𝑁+𝑇𝑁) (2) \\n \\nIn equation 2, TN, FN, FP and TP show the number of \\nTrue Negatives, False Negatives, False Positives and True \\nPositives [13 ,16].  \\nAUC represents the area under the ROC Curve. AUC \\ncalculates the whole two -dimensional area beneath the \\nwhole ROC curve from (0,0) to (1,1).  \\nPrecision is a positive analytical value [22 ,24]. \\nPrecision defines how reliable measurements are, \\nalthough they are farther from the accepted value.  \\nThe equation of precision is shown in Eq. (3).  \\n \\nPrecision = (𝑇𝑃\\n𝑇𝑃+𝐹𝑃) (3) \\n \\nThe recall is the hit rate [13 ,16,22,24]. The recall is \\nthe reverse of precision; it calculates false negatives \\nagainst true positives. The equation is illustrated in Eq \\n(4). \\n \\nRecall    (𝑇𝑃\\n𝑇𝑃+𝐹𝑁) (4)\"),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='against true positives. The equation is illustrated in Eq \\n(4). \\n \\nRecall    (𝑇𝑃\\n𝑇𝑃+𝐹𝑁) (4) \\n \\nF-measure can be defined as the weighted average \\n[13,16,22,24], of precision and recall. This rating \\nconsiders both false positives and false negatives. The \\nequation is illustrated in Eq (5).  \\n 𝐹= 2 𝑥𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛  ∗ 𝑅𝑒𝑐𝑎𝑙𝑙\\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑅𝑒𝑐𝑎𝑙𝑙 (5) \\n \\nThese criteria are adjusted proportionally in the data \\nby the reference class prevalence in the weighting \\noperation.  \\n \\n4.3. Experimental results  \\n \\nTable 2 presents classification accuracies for all \\ndatasets, base and ensemble learners. As it can be \\nobserved from Table 2, Rotation Forest with J48 gives \\nhighly accurate results than other approaches. In \\nadditi on to the fact that meta learner bagging produces \\nmore accurate results than J48, MP and SVM base \\nlearners.  \\nIn Table 3, weighted precision values obtained by all \\nbase and ensemble classifiers for all datasets are \\npresented.  \\nIn Table 4 and 5, weighted reca ll and weighted F -\\nmeasure values are illustrated for all datasets, base and \\nensemble classifiers, respectively.  \\nIn Table 6, weighted AUC values are introduced for all \\ndatasets, base and ensemble classifiers. According to \\nTable VI, Rotation Forest gives the  best results very close \\nor equal to 1.0. So, Rotation Forest can be determined to \\nbe a very powerful and effective classifier.  \\nThe balance scale, sonar and diabetes datasets have \\nsignificant outputs concerning the accuracy, precision,'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='significant outputs concerning the accuracy, precision, \\nrecall, F -measure an d AUC parameters; however German \\nCredit has somehow satisfactory output and Abalone \\nshows lower outcomes in Table 2 -6. \\nFurthermore, it is analyzed that the Meta learner’s \\nrotation forest provides a more accurate outcome. \\nLikewise, Meta learners bagging ind icates adequate \\nconsequences. In addition, base learners provide positive \\nfindings.  \\nSimilarly, Figure 2 -6, indicates the accuracy, AUC, \\nprecision, recall and F -measures values accordingly.  \\n \\n \\n \\nTable 2.  Classif ication accuracies (%) For Uci datasets  \\n \\nTable 3.  Weighted precision values for Uci datasets  \\n  Base Learner  Meta Learner Bagging  Meta Learner  \\nRotation Forest  \\nDatasets  J48 MP SVM  J48 MP SVM  J48 MP SVM  \\nAbalone  21.12  26.24  24.11  23.10  27.15  23.63  24.61  27.00  27.48  \\nBalance Scale  76.64  90.72  89.76  84 92.48  90.08  90.72  94.24  90.40  \\nDiabetes  73.82  75.39  65.10  74.61  76.82  65.10  76.30  76.30  76.30  \\nGerman Credit  70.50  71.50  68.70  73.30  76.10  68.60  74.80  75.40  76.70  \\nSonar  71.15  82.21  65.87  77.88  83.65  62.98  79.81  80.77  85.10  \\n Base Learner  Meta Learner Bagging  Meta Learner  \\nRotation Forest  \\nDatasets  J48 MP SVM  J48 MP SVM  J48 MP SVM  \\nAbalone  0.36  0.43  0.23  0.17  0.37  0.17  0.30  0.40  0.43  \\nBalance Scale  0.73  0.92  0.83  0.81  0.92  0.86  0.89  0.94  0.83  \\nDiabetes  0.73  0.75  0.65  0.74  0.76  0.65  0.76  0.76  0.76'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='Diabetes  0.73  0.75  0.65  0.74  0.76  0.65  0.76  0.76  0.76  \\nGerman Credit  0.69  0.71  0.49  0.72  0.75  0.52  0.73  0.75  0.76  \\nSonar  0.71  0.82  0.72  0.78  0.84  0.66  0.80  0.81  0.85  Turkish  Journa l of Engineering – 202 3, 7(1), 25-32 \\n \\n  29  \\n Table 4.  Weighted Recall Values For Uci  Dataset  \\n \\n \\nTable 5 . Weighted F -Measure Values For Uci  Datasets  \\n \\n \\nTable 6 . Weighted Auc values for Uci  datasets  \\n- High Acc, AUC, Precision, Recall and F - measure is shown in Bold.  \\n \\n \\n \\n \\nFigure 2.  The chart showing the effects between datasets and accuracies  \\n \\n Base Learner  Meta Learner Bagging  Meta Learner  \\nRotation Forest  \\nDatasets  J48 MP SVM  J48 MP SVM  J48 MP SVM  \\nAbalone  0.21  0.26  0.24  0.23  0.27  0.24  0.25  0.27  0.27  \\nBalance Scale  0.77  0.91  0.89  0.84  0.92  0.90  0.91  0.94  0.90  \\nDiabetes  0.74  0.75  0.65  0.75  0.77  0.65  0.76  0.76  0.76  \\nGerman Credit  0.70  0.71  0.69  0.73  0.76  0.69  0.75  0.75  0.77  \\nSonar  0.71  0.82  0.66  0.78  0.84  0.63  0.79  0.81  0.85  \\n Base Learner  Meta Learner Bagging  Meta Learner  \\nRotation Forest  \\nDatasets  J48 MP SVM  J48 MP SVM  J48 MP SVM  \\nAbalone  0.40  0.47  0.10  0.15  0.38  0.03  0.24  0.41  0.39  \\nBalance Scale  0.75  0.91  0.86  0.82  0.92  0.87  0.89  0.93  0.87  \\nDiabetes  0.74  0.75  0.79  0.74  0.76  0.79  0.75  0.76  0.75  \\nGerman Credit  0.69  0.71  0.57  0.72  0.75  0.57  0.74  0.75  0.74  \\nSonar  0.71  0.82  0.62  0.78  0.84  0.59  0.79  0.81  0.85'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content=\"Sonar  0.71  0.82  0.62  0.78  0.84  0.59  0.79  0.81  0.85  \\n Base Learner  Meta Learner Bagging  Meta Learner  \\nRotation Forest  \\nDatasets  J48 MP SVM  J48 MP SVM  J48 MP SVM  \\nAbalone  0.59  0.77  0.56  0.70  0.77  0.59  0.72  0.78  0.58  \\nBalance Scale  0.81  0.98  0.91  0.93  0.99  0.96  0.99  0.99  0.94  \\nDiabetes  0.75  0.79  0.50  0.79  0.82  0.50  0.82  0.82  0.73  \\nGerman Credit  0.64  0.73  0.49  0.75  0.78  0.49  0.78  0.39  0.69  \\nSonar  0.74  0.88  0.64  0.89  0.91  0.70  0.90  0.89  0.88  Turkish  Journa l of Engineering – 202 3, 7(1), 25-32 \\n \\n  30  \\n  \\nFigure 3.  The chart showing the effects between datasets and weighted precision values  \\n \\n \\nFigure 4.  The chart showing the effects between datasets and weighted recall values  \\n \\n \\nFigure 5.  The chart showing the effects between datasets and weighted F -measure  \\nTurkish  Journa l of Engineering – 202 3, 7(1), 25-32 \\n \\n  31  \\n  \\nFigure 6. The chart showing the effects between datasets and weighted AUC values  \\n \\n \\n5. Conclusions and future work  \\n \\nThis section discusses Base and Meta Learners \\noutcomes and future challenges in the existing Hybrid \\nsystem. We investigated the various kinds of solutions to \\nrelevant problems and analyzed different types of \\napproaches, tools, and techniques, but we couldn't find a \\nsingle one that could do th e entire task at once. Thus, the \\ncollaborative approach was proposed to analyze the \\nHybrid system. This collaborative nature of the proposed\"),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='Hybrid system. This collaborative nature of the proposed \\nsystem is dependent on two different folds, such as the \\nBase and Meta learner’s approach. For the process of data \\ncollection, multivariate, categorical, integer and efficient \\nrecords have been utilized for the Hybrid system. For \\nfinding the best results one has to try different methods. \\nWe have tried different methods and found the best \\ncombination. The results suggest  that the use of the \\nfeature selection method is advantageous because it \\nreduces complexity and increases accuracy. The \\nperformance of J48, MP and SVM with Rotation Forest \\nhas been studied using 05 datasets. The main objectives, \\npriority of this proposed s ystem and the key findings of \\nthis research work can be summarized as follows, based \\non the experimental and numerical results:  \\nThe Rotation Forest meta -ensemble learning method \\nbased on J48 is proposed in this paper. Although Rotation \\nForest can take more space and consume more time for \\ncomputations, this method yields more efficient results \\nby using hybrid advantages of base learners’ algorithms.  \\nThe integration of other hybridization ensemble \\nlearning algorithms/approaches and deployment of \\nemerging challenges is the primary focus of our future \\nresearch.  \\n \\nAuthor contributions  \\n \\nAbdul Ahad Abro : Original draft and preparation;  \\nWaqas Ahmed Siddique, Mir Sajjad Hussain Talpur, \\nAwais Khan Jumani and Erkan Yaşar : Reviewed,'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='Waqas Ahmed Siddique, Mir Sajjad Hussain Talpur, \\nAwais Khan Jumani and Erkan Yaşar : Reviewed, \\nrewrote, performed part of the lit erature survey, edited, \\ninvestigated and designed the architecture and explored \\nsoftware tools.   Conflicts of interest  \\n \\nThe authors declare no conflicts of interest.  \\n \\n  \\nReferences  \\n \\n1. Urso, A., Fiannaca, A., La Rosa, M., Ravì, V., & Rizzo, R.  \\n(2018). Data mining: Classification and prediction.  \\nEncyclopedia of Bioinformatics and Computational  \\nBiology: ABC of Bioinformatics, 1 , 3, 384 -402.  \\n2. Galdi, P., & Tagliaferri, R. (2018). Data minin g: \\naccuracy and error measures for classification and \\nprediction.  Encyclopedia of Bioinformatics and \\nComputational Biology , 431 -436. \\n3. Ozcift, A., & Gulten, A. (2011). Classifier ensemble \\nconstruction with rotation forest to improve medical \\ndiagnosis perform ance of machine learning \\nalgorithms. Computer Methods and Programs in \\nBiomedicine, 104(3), 443 -451.  \\n4. Panigrahi, R., & Borah, S. (2018). Rank allocation to \\nJ48 group of decision tree classifiers using binary and \\nmulticlass intrusion detection datasets.  Proce dia \\nComputer Science , 132 , 323 -332.  \\n5. Bansal, D., Chhikara, R., Khanna, K., & Gupta, P. (2018). \\nComparative analysis of various machine learning \\nalgorithms for detecting dementia. Procedia \\nComputer Science, 132, 1497 -1502.  \\n6. Zhang, C. X., & Zhang, J. S. (2008). RotBoost: A \\ntechnique for combining Rotation Forest and'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='6. Zhang, C. X., & Zhang, J. S. (2008). RotBoost: A \\ntechnique for combining Rotation Forest and \\nAdaBoost. Pattern Recognition Letters, 29(10), 1524 -\\n1536.  \\n7. Chen, S. F., Gu, H., Tu, M. Y., Zhou, Y. P., & Cui, Y. F. \\n(2018). Robust variable selection based on bagg ing \\nclassification tree for support vector machine in \\nmetabonomic data analysis. Journal of Chemometrics, \\n32(11), e2921.  \\n8. Lamba, R., Gulati, T., Alharbi, H. F., & Jain, A. (2021). A \\nhybrid system for Parkinson’s disease diagnosis using \\nmachine learning tech niques. International Journal of \\nSpeech Technology, 1 -11. \\nTurkish  Journa l of Engineering – 202 3, 7(1), 25-32 \\n \\n  32  \\n 9. Nandhini, M. (2021). Ensemble human movement \\nsequence prediction model with Apriori based \\nProbability Tree Classifier (APTC) and Bagged J48 on \\nMachine learning. Journal of King Saud University -\\nCompu ter and Information Sciences, 33(4), 408 -416.  \\n10. Pham, B. T., Bui, D. T., Prakash, I., & Dholakia, M. B. \\n(2017). Hybrid integration of Multilayer Perceptron \\nNeural Networks and machine learning ensembles for \\nlandslide susceptibility assessment at Himalayan ar ea \\n(India) using GIS. Catena, 149, 52 -63. \\n11. Sun, X., Xu, J., Jiang, C., Feng, J., Chen, S. S., & He, F.  \\n(2016). Extreme learning machine for multi -label \\nclassification. Entropy, 18(6), 225.  \\n12. Utkin, L. V. (2019). An imprecise extension of SVM -\\nbased machine lea rning models. Neurocomputing, \\n331, 18 -32.'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content='based machine lea rning models. Neurocomputing, \\n331, 18 -32. \\n13. Abro, A. A., T aşci, E., & Aybars, U. (2020). A Stacking -\\nbased Ensemble Learning Method for Outlier \\nDetection. Balkan Journal of Electrical and Computer \\nEngineering, 8(2), 181 -185.  \\n14. Chen, T. (2017). An improved rotat ion forest  \\nalgorithm based on heterogeneous classifiers \\nensemble for classifying gene expression profile. \\nAdvances in Modelling and Analysis B, 60(1), 1 -24. \\n15. Khan, A. A., Shaikh, Z. A., Belinskaja, L., Baitenova, L., \\nVlasova, Y., Gerzelieva Z, Laghari A . A., Abro , A.A., & \\nBarykin , S. (2022). A Blockchain and Metaheuristic -\\nEnabled Distributed Architecture for Smart \\nAgricultural Analysis and Ledger Preservation \\nSolution: A Collaborative Approach. Applied Sciences, \\n12(3), 1487.  \\n16. Abro, A. A., Khan, A. A., Talpur,  M. S. H., & Kayijuka, I. \\n(2021). Machine Learning Classifiers: A Brief Primer. \\nUniversity of Sindh Journal of Information and \\nCommunication Technology, 5(2), 63 -68. 17. Lu, H., Yang, L., Yan, K., Xue, Y., & Gao, Z. (2017). A cost -\\nsensitive rotation forest alg orithm for gene \\nexpression data classification. Neurocomputing, 228, \\n270 -276.  \\n18. Olivares, R., Munoz, R., Soto, R., Crawford, B., \\nCárdenas, D., Ponce, A., & Taramasco, C. (2020).  An \\noptimized brain -based algorithm for classifying \\nParkinson’s disease. Applied Sciences, 10(5), 1827.  \\n19. Khan, A. A., Laghari, A. A., & Awan, S. A. (2021).'),\n",
       " Document(metadata={'title': 'A Combined approach of Base and Meta Learners for Hybrid System', 'year': 2023}, page_content=\"19. Khan, A. A., Laghari, A. A., & Awan, S. A. (2021). \\nMachine learning in computer vision: A review. EAI \\nTransactions on Scalable Information Systems, e4.  \\n20. Shuaib, M., Abdulhamid, S. I. M., Adebayo, O. S., Osho, \\nO., Idris, I., Al hassan, J. K., & Rana, N. (2019). Whale \\noptimization algorithm -based email spam feature \\nselection method using rotation forest algorithm for \\nclassification. SN Applied Sciences, 1(5), 1 -17. \\n21. Hong, H., Liu, J., Bui, D. T., Pradhan, B., Acharya, T. D., \\nPham, B. T., ... & Ahmad, B. B. (2018). Landslide \\nsusceptibility mapping using J48 Decision Tree with \\nAdaBoost, Bagging and Rotation Forest ensembles in \\nthe Guangchang area (China). Catena, 163, 399 -413.  \\n22. Abro, A. A., Yimer, M. A., & Bhatti, Z. (2020). \\nIdentifyin g the Machine Learning Techniques for \\nClassification of Target Datasets. Sukkur IBA Journal \\nof Computing and Mathematical Sciences, 4(1), 45 -52. \\n23. Singh, B. K., Verma, K., & Thoke, A. S. (2015). \\nInvestigations on impact of feature normalization \\ntechniques on  classifier's performance in breast \\ntumor classification. International Journal of \\nComputer Applications, 116(19).  \\n24. Abro, A. A. (2021). Vote -Based: Ensemble Approach. \\nSakarya University Journal of Science, 25(3), 858 -866.  \\n25. UCI Machine Learning Repository, 20 18, \\nhttps://archive.ics.uci.edu/ml/index.php.  \\n \\n \\n \\n © Author(s) 202 3. This work is distributed under https://creativecommons.org/licenses/by -sa/4.0/  \\n \\nView publication stats\"),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='REVIEW\\nAdvances in natural\\nlanguage processing\\nJulia Hirschberg1*and Christopher D. Manning2,3\\nNatural language processing employs computati onal techniques for the purpose of learning,\\nunderstanding, and producing human languag e content. Early computational approaches to\\nlanguage research focused on automating the an alysis of the linguistic structure of language\\nand developing basic technologi es such as machine translation, speech recognition, and speech\\nsynthesis. Today ’s researchers refine and make use of such tools in real-world applications,\\ncreating spoken dialogue systems and speech-to-speech translation engines, mining socialmedia for information about health or finance, and identifying sentiment and emotion toward\\nproducts and services. We describe successes and challenges in this rapidly advancing area.\\nOver the past 20 years, computational lin-\\nguistics has grown into both an exciting\\narea of scientific research and a practicaltechnology that is increasingly being in-corporated into consumer products (for\\nexample, in applications such as Apple ’sS i r ia n d\\nSkype Translator). Four key factors enabled thesedevelopments: (i) a vast increase in computing\\npower, (ii) the availability of very large amounts\\nof linguistic data, (iii) the development of highlysuccessful machine learning (ML) methods, and(iv) a much richer understanding of the structure\\nof human language and its deployment in social'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='of human language and its deployment in social\\ncontexts. In this Review, we describe some cur-rent application areas of interest in languageresearch. These efforts illustrate computational\\napproaches to big data, based on current cutting-\\nedge methodologies that c ombine statistical anal-\\nysis and ML with knowledge of language.\\nComputational linguistics, also known as nat-\\nural language processing (NLP), is the subfieldof computer science concerned with using com-putational techniques to learn, understand, and\\nproduce human language content. Computation-\\nal linguistic systems can have multiple purposes:The goal can be aiding human-human commu-\\nnication, such as in machine translation (MT);\\naiding human-machine communication, such as\\nwith conversational agents; or benefiting bothhumans and machines by analyzing and learn-ing from the enormous quantity of human lan-\\nguage content that is now available online.\\nDuring the first several decades of work in\\ncomputational linguistics, scientists attempted\\nto write down for computers the vocabularies\\nand rules of human languages. This proved adifficult task, owing to the variability, ambiguity,and context-dependent interpretation of human\\nlanguages. For instance, a star can be either an\\nastronomical object or a person, and “star”can\\nbe a noun or a verb. In another example, two in-terpretations are possible for the headline “Teacherstrikes idle kids, ”depending on the noun, verb, and\\nadjective assignments of the words in the sentence,'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='adjective assignments of the words in the sentence,\\nas well as grammatical structure. Beginning in the1980s, but more widely in the 1990s, NLP wastransformed by researchers starting to build mod-\\nels over large quantities of empirical language\\ndata. Statistical or corpus ( “body of words ”)–based\\nNLP was one of the first notable successes of\\nthe use of big data, long before the power of\\nML was more generally recognized or the term“big data ”even introduced.\\nA central finding of this statistical approach to\\nNLP has been that simple methods using words,\\npart-of-speech (POS) sequences (such as whethera word is a noun, verb, or preposition), or simpletemplates can often achieve notable results when\\ntrained on large quantities of data. Many text\\nand sentiment classifiers are still based solely onthe different sets of words ( “bag of words ”)t h a t\\ndocuments contain, without regard to sentence\\nand discourse structure or meaning. Achievingimprovements over these simple baselines can bequite difficult. Nevertheless, the best-performing\\nsystems now use sophisticated ML approaches\\nand a rich understanding of linguistic structure.High-performance tools that identify syntacticand semantic information as well as information\\nabout discourse context are now available. One\\nexample is Stanford CoreNLP ( 1), which provides\\na standard NLP preprocessing pipeline that in-cludes POS tagging (with tags such as noun, verb,\\nand preposition); identification of named entities,'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='and preposition); identification of named entities,\\nsuch as people, places, and organizations; parsingof sentences into their grammatical structures;\\nand identifying co-references between noun\\nphrase mentions (Fig. 1).\\nHistorically, two developments enabled the\\ninitial transformation o f NLP into a big data field.\\nThe first was the early availability to researchers\\nof linguistic data in digital form, particularlythrough the Linguistic Data Consortium (LDC)(2), established in 1992. Today, large amounts\\nof digital text can easily be downloaded from\\nthe Web. Available as linguistically annotateddata are large speech and text corpora anno-\\ntated with POS tags, syntactic parses, semantic\\nlabels, annotations of named entities (persons,places, organizations), dialogue acts (statement,question, request), emotions and positive or neg-\\native sentiment, and discourse structure (topicor rhetorical structure). Second, performance im-provements in NLP were spurred on by shared\\ntask competitions. Origi nally, these competitions\\nwere largely funded and organized by the U.S.Department of Defense, but they were later or-\\nganized by the research community itself, such\\nas the CoNLL Shared Tasks ( 3). These tasks were\\na precursor of modern ML predictive modelingand analytics competitions, such as on Kaggle ( 4),\\nin which companies and researchers post their\\ndata and statisticians and data miners from all overt h ew o r l dc o m p e t et op r o d u c et h eb e s tm o d e l s .'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='A major limitation of NLP today is the fact that\\nmost NLP resources and systems are available\\nonly for high-resource languages (HRLs), such asEnglish, French, Spanish, German, and Chinese.In contrast, many low-resource languages (LRLs) —\\nsuch as Bengali, Indonesian, Punjabi, Cebuano,\\nand Swahili —spoken and written by millions of\\npeople have no such resources or systems avail-\\na b l e .Af u t u r ec h a l l e n g ef o rt h el a n g u a g ec o m m u -\\nnity is how to develop resources and tools forhundreds or thousands of languages, not just a few.\\nMachine translation\\nProficiency in languages was traditionally a hall-\\nmark of a learned person. Although the socialstanding of this human skill has declined in the\\nmodern age of science and machines, translation\\nbetween human languages remains crucially im-portant, and MT is perhaps the most substantial\\nway in which computers could aid human-human\\ncommunication. Moreover, the ability of com-puters to translate between human languagesremains a consummate test of machine intel-\\nligence: Correct translation requires not only\\nthe ability to analyze and generate sentences inhuman languages but also a humanlike under-standing of world knowledge and context, de-\\nspite the ambiguities of l anguages. For example,\\nthe French word “bordel ”straightforwardly means\\n“brothel ”; but if someone says “My room is un\\nbordel, ”then a translating machine has to know'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='“brothel ”; but if someone says “My room is un\\nbordel, ”then a translating machine has to know\\nenough to suspect that this person is probably notrunning a brothel in his or her room but rather issaying “My room is a complete mess. ”\\nMachine translation was one of the first non-\\nnumeric applications of computers and was studiedintensively starting in the late 1950s. However, thehand-built grammar-base ds y s t e m so fe a r l yd e c -\\nades achieved very limited success. The field was\\ntransformed in the early 1990s when researchersat IBM acquired a large quantity of English andFrench sentences that wer et r a n s l a t i o n so fe a c h\\nother (known as parallel text), produced as the\\nproceedings of the bilingual Canadian Parliament.These data allowed them to collect statistics of\\nword translations and word sequences and to\\nbuild a probabilistic model of MT ( 5).\\nFollowing a quiet period in the late 1990s,\\nthe new millennium brought the potent combina-\\ntion of ample online text, including considerable\\nquantities of parallel text, much more abundantand inexpensive computing, and a new ideafor building statistical phrase-based MT systems\\nSCIENCE sciencemag.org 17 JULY 2015 \\x7fVOL 349 ISSUE 6245 261\\n1Department of Computer Science, Columbia University, New York,\\nNY 10027, USA.2Department of Linguistics, Stanford University,\\nStanford, CA 94305-2150, USA.3Department of Computer\\nScience, Stanford University, Stanford, CA 94305-9020, USA.'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='Science, Stanford University, Stanford, CA 94305-9020, USA.\\n*Corresponding author. E-mail: julia@cs.columbia.edu\\nDownloaded from https://www.science.org at Peking University on November 22, 2024\\n(6). Rather than translating word by word, the\\nkey advance is to notice that small word groupsoften have distinctive translations. The Japa-nese\\n“mizu iro ”is literally the sequence\\nof two words ( “water color ”), but this is not the\\ncorrect meaning (nor does it mean a type ofpainting); rather, it indicates a light, sky-blue color.\\nSuch phrase-based MT was used by Franz Och in\\nthe development of Google Translate.\\nThis technology enabled the services we have\\ntoday, which allow free and instant translation\\nbetween many language pairs, but it still pro-\\nduces translations that are only just serviceablefor determining the gist of a passage. However,very promising work continues to push MT for-\\nward. Much subsequent research has aimed to\\nbetter exploit the structure of human languagesentences (i.e., their syntax) in translation sys-tems ( 7,8), and researchers are actively building\\ndeeper meaning representations of language ( 9)\\nto enable a new level of semantic MT.\\nFinally, just in the past year, we have seen the\\ndevelopment of an extremely promising approach\\nto MT through the use of deep-learning –based\\nsequence models. The central idea of deep learn-ing is that if we can train a model with several\\nrepresentational levels to optimize a final objec-'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='representational levels to optimize a final objec-\\ntive, such as translation q uality, then the model\\ncan itself learn intermediate representationsthat are useful for the task at hand. This idea\\nhas been explored particularly for neural net-\\nwork models in which information is stored inreal-valued vectors, with the mapping between\\nvectors consisting of a matrix multiplication fol-\\nlowed by a nonlinearity, such as a sigmoid func-tion that maps the output values of the matrixmultiplication onto [ −1, 1]. Building large modelsof this form is much more practical with the\\nmassive parallel computation that is now econo-mically available via graphics processing units. Fortranslation, research ha s focused on a particular\\nversion of recurrent neural networks, with enhanced\\n“long short-term memory ”computational units\\nthat can better maintain contextual information\\nfrom early until late in a sentence ( 10,11)( F i g .2 ) .\\nThe distributed representations of neural networksare often very effective for capturing subtle seman-tic similarities, and neural MT systems have al-\\nready produced some state-of-the-art results ( 12,13).\\nA still-underexplored area in MT is getting ma-\\nchines to have more of a sense of discourse, sothat a sequence of sentences translates naturally —\\nalthough work in the area has begun ( 14). Finally,\\nMT is not necessarily a task for machines to doalone. Rather it can be reconceptualized as an op-portunity for computer-supported cooperative'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='work that also exploits human skills ( 15). In such\\na system, machine intelli gence is aimed at human-\\ncomputer interface capabilities of giving effective\\nsuggestions and reacting productively to human\\ninput, rather than wholly replacing the skills andknowledge of a human translator.\\nSpoken dialogue systems and\\nconversational agents\\nDialogue has been a popular topic in NLP re-\\nsearch since the 1980s. However, early work on\\ntext-based dialogue has now expanded to include\\nspoken dialogue on mobile devices (e.g., Apple ’s\\nSiri, Amtrak ’s Julie, Google Now, and Microsoft ’s\\nCortana) for information access and task-based\\napps. Spoken dialogue systems (SDSs) also allowrobots to help people with simple manual tasks[e.g., Manuela Veloso ’s CoBots ( 16)] or providetherapy for less-abled persons [e.g., Maja Mataric ’s\\nsocially assistive robots ( 17)]. They also enable ava-\\ntars to tutor people in interview or negotiation stra-tegies or to help with health care decisions ( 18,19).\\nThe creation of SDSs, whether between hu-\\nmans or between humans and artificial agents,requires tools for automatic speech recognition\\n(ASR), to identify what a human says; dialogue\\nmanagement (DM), to determine what that hu-man wants; actions to obtain the information orperform the activity requested; and text-to-speech\\n(TTS) synthesis, to convey that information back'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='(TTS) synthesis, to convey that information back\\nto the human in spoken form. (Fig. 3). In addition,SDSs need to be ready to interact with users whenan error in speech recognition occurs; to decide\\nwhat words might be incorrectly recognized; and\\nto determine what the user actually said, eitherautomatically or via dialogue with the user. Inspeech-to-speech translation systems, MT com-\\nponents are also needed to facilitate dialogue\\nbetween speakers of diff erent languages and the\\nsystem, to identify potential mistranslations before\\nthey occur, and to clarify these with the speaker.\\nPractical SDSs have been enabled by break-\\nthroughs in speech recognition accuracy, mainlycoming from replacing tradi tional acoustic feature –\\nmodeling pipelines with deep-learning models that\\nmap sound signals to sequences of human languagesounds and words ( 20). Although SDSs now work\\nfairly well in limited domains, where the topics of\\nthe interaction are known in advance and where\\nt h ew o r d sp e o p l ea r el i k e l yt ou s ec a nb ep r e d e t e r -mined, they are not yet very successful in open-\\ndomain interaction, where users may talk about\\nanything at all. Chatbots following in the traditionof ELIZA ( 21) handle open-domain interaction by\\ncleverly repeating variations of the human input;\\n262 17 JULY 2015 \\x7fVOL 349 ISSUE 6245 sciencemag.org SCIENCE\\ncase auxnsubjMMrs. Clinton previously worked for Mr. Obama , but she is now distancing herself from him  .Part of speech:'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='NNP NNP PRP PRP PRP NNP VBD VBZ VBG NNP RB RB CC IN IN ,\\nMrs.                Clinton previously     worked   for   Mr.                 Obama , but she is now          distancing  herself  from          him  .Basic dependencies:\\nNNP NNP PRP PRP PRP NNP VBD VBZ NNP RB RB CC IN IN , .Mrs. Clinton previously worked for Mr. Obama, but she is now distancing herself from him.Named entity recognition:\\nPerson Date Date Person\\nMrs. Clinton previously worked for Mr. Obama, but she is now distancing herself from him.Co-reference:\\nMention M MentCoref\\ncompound compound case dobj advmodnmod nmodccconj\\nadvmodnsubjCorefCoref\\nMention\\nVBG.\\nFig. 1. Many language technology tools start by doing linguistic structure analysis. Here we show output from Stanford CoreNLP . As shown from top to\\nbottom, this tool determines the parts of speech of each word, tags various words or phrases as semantic named entities of various sorts, determines wh ich entity\\nmentions co-refer to the same person or organization, and then works out the s yntactic structure of each sentence, using a dependency grammar analysi s.ARTIFICIAL INTELLIGENCE  \\nDownloaded from https://www.science.org at Peking University on November 22, 2024\\nthis approach is also being attempted in spoken-\\nchat systems designed to provide a sense of com-\\npanionship for target audiences such as the elderly\\nor individuals with dementia ( 22). In spoken dia-\\nlogue, information about the speaker ’sm e n t a l'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='logue, information about the speaker ’sm e n t a l\\nstate inferred from multimodal information can\\nbe used to supplement the system ’sk n o w l e d g eo f\\nwhat the user is saying.\\nThere are many challenges in building SDSs,\\nin addition to the primary challenge of improv-\\ning the accuracy of the basic ASR, DM, and TTSbuilding blocks and extending their use into less-restricted domains. These include basic problems\\nof recognizing and producing normal human\\nconversational behaviors, such as turn-takingand coordination. Humans interpret subtle cuesin speakers ’voices and facial and body gestures\\n(where available) to determine when the speaker\\nis ready to give up the turn versus simply pausing.These cues, such as a filled pause (e.g., “um”or\\n“uh”), are also used to establish when some\\nfeedback from the listener is desirable, to indi-\\ncate that he or she is listening or working on arequest, as well as to provide “grounding ”(i.e.,\\ninformation about the current state of the con-\\nversation). Non –humanlike latency often makes\\nSDS burdensome, as users must wait seconds toreceive a system response. To address this, re-\\nsearchers are exploring incremental processing\\nof ASR, MT, and TTS modules, so that systemscan respond more quickly to users by beginningthese recognition, trans lation, and generation\\nprocesses while the user is still speaking. Hu-mans can also disambiguate words such as “yeah ”\\nand “okay, ”which may have diverse meanings —\\nincluding agreement, topic shift, and even'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='and “okay, ”which may have diverse meanings —\\nincluding agreement, topic shift, and even\\ndisagreement —when spoken in different ways.\\nIn successful and cooperati ve conversations, hu-\\nmans also tend to entrain to their conversational\\npartners, becoming more similar to each other in\\npronunciation, word choice, acoustic and pro-sodic features, facial expressions, and gestures.\\nThis tendency has long been used to subtly in-\\nduce SDS users to employ terms that the systemcan more easily recognize. Currently, research-ers are beginning to believe that systems (parti-\\ncularly embodied agents) should entrain to their\\nusers in these different modalities, and some ex-perimental results have shown that users prefersuch systems ( 23) and even think they are more\\nintelligent ( 24). Open issues for DM have long been\\nthe determination of how to architect the appro-priate dialogue flow for particular applications,where existing experimental data may be sparse\\nand some aspects of the dialogue state may not yet\\nhave been observed or even be observable from thedata. Currently, the most widely used approach is\\nthe POMDP (partially observable Markov decision\\nprocess), which attempts to identify an optimalsystem policy by maintaining a probability distri-bution over possible SDS states and updating this\\ndistribution as the system observes additional\\ndialogue behavior ( 25). This approach may make\\nuse of the identification of dialogue acts, such aswhether the user input represents a question,'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='use of the identification of dialogue acts, such aswhether the user input represents a question,\\nstatement, or indication of agreement, for example.Machine reading\\nThe printed word has great power to enlighten.Machine reading is the idea that machines couldbecome intelligent, and could usefully integrate\\nand summarize information for humans, by read-\\ning and understanding the vast quantities of textthat are available.\\nIn the early decades of artificial intelligence, many\\nresearchers focused on the approach of trying toenable intelligent machines by manually buildinglarge structured knowled ge bases in a formal logi-\\ncal language and developing automated reasoning\\nmethods for deriving further facts from this knowl-edge. However, with the emergence of the mod-ern online world, what we mainly have instead is\\nhuge repositories of online information coded in\\nhuman languages. One place where this is true isin the scientific literature, where findings are stillreported almost entirely in human language text\\n(with accompanying tables and diagrams). How-\\never, it is equally true for more general knowledge,where we now have huge repositories of infor-\\nmation such as Wikipedia ( 26). The quantity of\\nscientific literature is growing rapidly: For example,t h es i z eo ft h eU . S .N a t i o n a lL i b r a r yo fM e d i c i n e ’s\\nMedline index has grown exponentially ( 27). At\\nsuch a scale, scientists are unable to keep up with'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='Medline index has grown exponentially ( 27). At\\nsuch a scale, scientists are unable to keep up with\\nthe literature, even in their narrow domains ofexpertise. Thus, there is an increased need formachine reading for the purposes of comprehend-\\ning and summarizing the literature, as well as ex-\\ntracting facts and hypoth eses from this material.\\nAn initial goal is to extract basic facts, most\\ncommonly a relation between two entities, such\\nas“child of ”(for instance, Bill Clinton, Chelsea\\nClinton). This is referred to as relation extrac-tion. For particular domain-specific relations,\\nmany such systems have been successfully built.\\nOne technique is to use handwritten patternsthat match the linguistic expression of relations(e.g., <PERSON> ’s daughter, <PERSON>). Bet-\\nter results can be obtained through the use of\\nSCIENCE sciencemag.org 17 JULY 2015 \\x7fVOL 349 ISSUE 6245 263\\ninformation accessDialogue\\nManagementSpeech in\\nText-to-Speech\\nSynthesisAutomatic Speech\\nRecognition\\nSpeech out\\nFig. 3. A spoken dialogue system. The three main\\ncomponents are represented by rectangles; arrows\\ndenote the flow of information.0.1\\n0.30.10.40.2-\\n0.2\\n0.20.10.1\\n0.1--\\n0.2\\n0.60.10.70.1--\\nDie proteste0.2\\n0.60.10.70.1--\\n0.2\\n0.60.10.7\\n0.1--\\n0.4\\n0.60.20.30.4--\\nwaren0.4\\n0.40.30.20.3--\\n0.1\\n0.30.10.7\\n0.1--\\n0.2\\n0.30.10.40.2--\\nam0.5\\n0.50.90.30.2--\\n0.2\\n0.60.10.4\\n0.1--\\n0.2\\n0.40.10.50.2-\\n-\\nwochenende0.2\\n0.60.10.50.1--\\n0.2\\n0.80.10.5\\n0.1---\\n0.4\\n0.20.30.40.2-\\n--\\n-\\neskaliert0.1\\n0.60.10.70.1---\\n0.2\\n0.60.10.7\\n0.1--\\n0.2'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='0.2\\n0.80.10.5\\n0.1---\\n0.4\\n0.20.30.40.2-\\n--\\n-\\neskaliert0.1\\n0.60.10.70.1---\\n0.2\\n0.60.10.7\\n0.1--\\n0.2\\n0.60.10.70.1--\\n<EOS>The\\n0.2\\n0.60.10.70.1--\\n0.2\\n0.10.10.7\\n0.1---\\n0.2\\n0.60.10.70.1--\\nTheprotests\\n0.2\\n0.60.10.70.1--\\n0.4\\n0.60.10.7\\n0.1---\\n0.2\\n0.60.10.70.1--\\nprotestsescalated\\n0.3\\n0.60.10.70.1--\\n0.2\\n0.60.10.3\\n0.1-\\n0.2\\n0.60.10.70.1--\\nescalatedover\\n0.4\\n0.40.10.70.1--\\n0.1\\n0.60.10.3\\n0.1-\\n0.1\\n0.30.10.70.1--\\noverthe\\n0.2\\n0.60.10.70.1---\\n0.2\\n0.40.10.2\\n0.1-\\n0.2\\n0.60.10.30.1-\\ntheweekend\\n0.4\\n0.60.10.70.1---\\n0.3\\n0.60.10.5\\n0.1-\\n-\\n0.4\\n0.50.50.40.1-\\n-\\nweekend<EOS>\\n0.3\\n0.50.10.70.1---\\n0.2\\n0.60.10.7\\n0.1-\\n-\\n0.2\\n0.60.10.70.1--\\nFig. 2. A deep, recurrent neural MT system ( 10).Initially trained on parallel sentences that translate each\\nother, the model learns a representation of each word as a real-valued vector and internal parameter matrices so\\nas to optimize translation quality. The trained network can then translate new sentences. Each arrow representsa computation unit of a matrix multiplication followed by a nonlinear transformation; the small vectors shown inthe illustration might really be 1000-dimensional. The recurrent network first encodes the meaning of the source\\nsentence (left side, blue). It maintains an internal state representing a partial sentence, which is updated after\\neach new word is read (horizontal arrows). Having the upper network layers [mapped to by additional (uppervertical) computation arrows] makes this a deep recurrent network. Adding depth improves the ability of the'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='model to learn, generalize, and remember. Once the end of the sentence (denoted by <EOS>) is reached\\n(middle, dark blue), the network additionally starts to p roduce a word of translated output at each step from its\\ninternal state (using a mult iclass logistic regression –style model). During translation generation (right side,\\ngreen), the last generated word is fed in as the input at each step. From the stored hidden state and this input,\\nthe model calculates the next word of the translati on. This process repeats until <EOS> is generated.\\nDownloaded from https://www.science.org at Peking University on November 22, 2024\\nML. A structured prediction classifier proposes\\ninstances of such relations based on extractedfeatures from the sequence of words and gram-matical structure of a sentence ( 28,29). Such sys-\\ntems are the mainstay of lite rature fact-extraction\\nt o o l si nf i e l d ss u c ha sb i o m e d i c i n e( 30,31).\\nIn many scientific fields, there have been ma-\\njor efforts to build databases of structured infor-\\nmation based on the textual scientific record,such as the Gene Ontology database ( 32)i nb i o -\\nmedicine or the PaleoBiology Database for fossil\\nrecords ( 33). This has generally been done man-\\nually, via concerted work by trained profes-sionals. Using artificial intelligence software toextract these databases, as well as to perform\\nsubsequent reasoning and hypothesis genera-'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='subsequent reasoning and hypothesis genera-\\ntion, has become a major research goal. Onesubfield where these questions have been ac-tively pursued is pharmacogenomics ( 34). For\\nexample, Percha et al.(35) trained a model of\\ndrug-drug interactions based on drug-gene in-teractions extracted from the literature and were\\nable to use it to predict novel drug-drug interactions.\\nIf a partial knowledge base —for instance, Freebase\\n(36), dbpedia ( 37), Wikidata ( 38) (related to Wikipe-\\ndia), or the Gene Ontology database ( 32)—has already\\nbeen extracted from biomedical research articles,\\nthen there is an opportunity to automatically alignknown facts from the knowledge base with puta-tive expressions of those facts in text. The type labels\\nfrom this mapping can then be used as if they were\\nsupervised data for ML information-extraction sys-tems (Fig. 4). This is referred to as distantly super-\\nvised relation extraction. Early systems aligned\\nentity mentions and then made the naïve assump-tion that sentences containing a pair of entitiesexpressed every known relation between the two\\nentities in the database ( 39). More recent systems\\nhave used increasingly sophisticated probabilisticinference to discern which textual clauses map towhich facts in the knowledge base, or to something\\nelse entirely ( 40,41). A dramatic recent applica-\\ntion of this approach has been the DeepDive sys-tem ( 42), which aims to automate the construction\\nof such systems by providing efficient large-scale'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='of such systems by providing efficient large-scale\\nlearning and inference so a user can simply focuson good features for their domain. PaleoDeepDive,its application to the fossil record, has recently been\\nshown to do a better job at fact extraction from\\njournal articles than the scientist volunteers whomaintain the PaleoBiology Database ( 43).\\nThe relation-extraction task is made general,\\nif less semantically precise, by aiming to ex-\\ntract all relations from any piece of text, a tasknormally referred to as open information ex-traction (Open IE) ( 44). Early work emphasized\\nthe development of simple but highly scalable\\nfact-extraction techniques that do not requireany kind of hand-labeled data ( 45). With ever-\\ngrowing computational power, a second genera-\\ntion of work increasingly emphasized careful useof linguistic structure, which can reliably be ex-tracted with the use of detailed NLP ( 46).\\nCurrently, a number of avenues are being\\nexplored to further extend the ability of compu-ters to build and use knowledge bases startingfrom textual information. An exciting unificationis the proposal for universal schemas ( 47), which\\nallow simultaneous inference and knowledge-base completion over both the open set of textualrelations (such as “born in ”)f o u n di nO p e nI E\\nand the more exact schema of databases (such\\nas per:city_of_birth). Even with all of our text-extraction techniques, any knowledge base will\\nonly be partial and incomplete; some recent'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='only be partial and incomplete; some recent\\nwork explores how it can be probabilistically com-pleted to deliver a form of common-sense rea-soning ( 48). Finally, we hope to move beyond\\nsimply extracting relations, events, and facts to\\nbe able to understand the relations betweenevents (such as causation) and complex multistepprocedures and processes. In ( 49), Berant et al.\\nexplore how this can be done for understanding\\nthe steps in biological processes, showing thatextracting explicit process structures can improvethe accuracy of question answering. The flip side of\\nmachine reading is to provide question-answering\\nsystems, by which humans can get answers fromconstructed knowledge bases. There has recently\\nbeen dramatic progress in building such systems\\nby learning semantic parsers ( 50).\\nMining social media\\nThe development of social media has revolution-\\nized the amount and types of information avail-able today to NLP researchers. Data availablefrom sources such as Twitter, Facebook, YouTube,\\nblogs, and discussion forums make it possible\\nto examine relations between demographic in-formation, language use, and social interaction\\n(51). Researchers use Web-scraping techniques,\\noften via application program interfaces pro-vided by websites, to do wnload previously unim-\\naginable amounts and categories of data. Using\\nstatistical and ML techniques, they learn to iden-'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='aginable amounts and categories of data. Using\\nstatistical and ML techniques, they learn to iden-\\ntify demographic information (such as age andgender) from language, track trending topics andpopular sentiment, identify opinions and beliefs\\nabout products and politicians, predict disease\\nspreading (for instance, with Google Flu Trends:www.google.org/flutren ds/) from symptoms men-\\ntioned in tweets or food-related illnesses ( 52),\\nrecognize deception in fake reviews ( 53), and\\nidentify social networks of people who interacttogether online.\\nIn this era of big data, the availability of social\\nmedia has revolutionized the ways advertisers,journalists, businesses, politicians, and medical\\nexperts acquire their data and the ways in which\\nthose data can be put to practical use. Product\\nreviews can be mined to predict pricing trendsand assess advertising campaigns. Political forumscan be searched to predict candidate appeal and\\nperformance in elections. Social networks can be\\nexamined to find indicators of power and influ-ence among different groups. Medical forums can\\nbe studied to discover common questions and\\nmisconceptions about sufferers from particularmedical conditions so that website informationcan be improved.\\nSocial media also provide very large and rich\\nsources of conversational data in Web forumsthat can provide “found ”data for the study of lan-\\nguage phenomena such as code-switching (mixed\\n264 17 JULY 2015 \\x7fVOL 349 ISSUE 6245 sciencemag.org SCIENCE'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='264 17 JULY 2015 \\x7fVOL 349 ISSUE 6245 sciencemag.org SCIENCE\\nJohn CurtinThe Right Honourable\\n14th Prime Minister of Australia\\nElections:  1937 , 1940, 1943\\nPersonal details\\nBorn John Joseph Curtin\\n8 January 1885\\nCreswick , Colony of Victoria\\nBritish Empire\\nDied 5 July 1945 (aged 60)\\nCanberra , Australia\\nJohn Curtin (1885-1945),\\nprime minister and journalist,was born on 8 January 1885at Creswick, Victoria, eldest of four children of Irish-born parents John Curtin and his wife Catherine (Kate) Agnes,née Bourke.  . . .per: city_of_birth per: date_of_birth\\nLearn how to extract\\nslots from free text:\\nwas born on <DATE>\\nborn . . . at <LOCATION>\\nper: city_of_birthper: date_of_birth\\nFig. 4. Distantly supervised learning. In this ap-\\nproach, facts in a structured knowledge repre-\\nsentation are projected onto pieces of text thatmention the people, places, dates, etc., that appearin knowledge-base entries.This projection is noisy,\\nbut when done over large quantities of text, it pro-\\nvides enough signal to successfully learn good clas-sifiers for extracting relations from text. [Photosource: National Library of Australia, http:/ /nla.gov.\\nau/nla.pic-an12267621]ARTIFICIAL INTELLIGENCE  \\nDownloaded from https://www.science.org at Peking University on November 22, 2024\\nlanguage in bilingual speech), hedging behavior\\n(words and phrases indicating lack of commitmentto a proposition such as “sort of ”), and hate speech\\nor bullying behavior. Social media exist in a wide'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='or bullying behavior. Social media exist in a wide\\nvariety of languages, including both HRLs and\\nLRLs. These data can be invaluable for enrichingASR language models and developing TTS synthe-\\nsizers without the need to create costly special-\\npurpose corpora. In turn, these technologies canbe useful in producing SDSs in LRL areas. Suchsystems can provide millions of people with the\\nability to obtain information over their cell phones\\n(which are ubiquitous, even among populationswith low literacy rates or whose languages ordialects have no standard written form), similar to\\nthe residents of HRL countries. The development\\nof tools for LRLs from found LRL data, by adapt-ing HRL tools, is another important way to usefound text data. A particular application of data\\nmining in LRLs is the mining of data collected\\nfrom Twitter or blogs to provide valuable infor-mation for disaster relief organizations, identify-\\ning the most serious problems, where they occur,\\nand who is experiencing them.\\nThere are also some drawbacks to social media\\ndata mining. There is an increasing concern for\\nprivacy issues, particul arly for an individual ’sc o n -\\ntrol over their own data versus researchers ’desire\\nto mine it. Sites such as Twitter severely limit aresearcher ’s ability to download data, which im-\\npedes speedy corpus collection. There is also a\\nmajor issue with discovering “ground truth ”in\\nonline postings, because there is no clear way of\\nvalidating an individual ’s demographic information;'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='validating an individual ’s demographic information;\\nthe validity of posts concerning events; and mostreviews of hotels, restaura nts, and products. Aggre-\\ngating information from multiple sources at similar\\ntimes can address some validity issues, and sites do\\nattempt to identify spurious reviews, but this issuer e m a i n sp e r h a p st h em o s td i f f i c u l to n ef o rt h o s eworking with social media data.\\nAnalysis and generation of speaker state\\nSpeaker states ( 54), also termed “private states ”\\n(55), include opinions, speculations, beliefs, emo-\\ntions, and any other evaluative views that arepersonally held by the speaker or writer of al a n g u a g e .M u c ho ft h ew o r ki nN L Ph a sf o c u s e d\\non sentiment analysis (identification of positive\\nor negative orientation of textual language) andidentification of belief st ates (committed belief,uncommitted belief, or neutrality of a sentence)\\non the basis of lexical and syntactic information.Both sentiment and belief constitute attitudes to-ward events and propositions, although sentiment\\ncan also concern attitudes toward objects such as\\npeople, organizations, and abstract concepts. De-tection of sentiment and emotion in text requires\\nlexical and sentence-level information. Sentiment\\nc a nb es i g n a l e db yw o r d sc o n v e y i n gp o s i t i v eo rnegative orientation: For example, “sad,”“worried, ”\\n“difficult, ”and “weak ”are all words with negative\\norientation, whereas “comfortable, ”“important, ”'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='orientation, whereas “comfortable, ”“important, ”\\n“successful, ”and “interesting ”convey a positive\\nsentiment. Online sentiment dictionaries, such asWhissel ’sD i c t i o n a r yo fA f f e c t( 56), and systems\\ncreated from subject-ranked terms, such as Tausczik\\nand Pennebaker ’sL I W C( L i n g u i s t i cI n q u i r ya n d\\nWord Count) ( 57), can be used to assess positive\\nand negative sentiment in a text. More sophis-\\nticated approaches to sent iment analysis also seek\\nto identify the holder (source) as well as the objectof the sentiment: for instance, who is positive about\\nwhat person, country, activity, or concept ( 55).\\nThe speech community has also studied pos-\\nitive and negative attitudes by focusing moregenerally on the identification of positive and neg-\\native emotions, primarily using acoustic and pro-\\nsodic information. However, more work is currentlybeing done to identify particular emotions, suchas Ekman ’s classic six basic emotions (anger, dis-\\ngust, fear, happiness, sadness, surprise), which\\nmay be reactions to events, propositions, or objects.There has also been considerable research using\\nfeatures that have proven important in recog-\\nnizing classic emotions to identify other speak-er states (such as deception), medical conditions(such as autism and Parkinson ’s disease), speaker\\ncharacteristics (such as age, gender, likeability,'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='characteristics (such as age, gender, likeability,\\npathology, and personality), and speaker conditions(such as cognitive load, drunkenness, sleepiness,interest, and trust). Corpora collected for such\\nstudies have been used in the Interspeech Para-\\nlinguistic Challenges, which have been conductedsince 2009. Emotion generation has proven a\\nmore difficult challenge for TTS synthesis. Although\\nthere are some systems (e.g., MARY) that at-tempt to generate emotions such as depression,aggression, or cheerfulness ( 58), the best synthe-\\nsized emotion still comes from corpora recorded for\\nparticular emotions by voice talent imitating thoseemotions.Sentiment classificatio n is widely used in opin-\\nion identification (positive or negative views ofpeople, institutions, or ideas) in many languagesand genres. Particular applications abound, such\\nas identifying positive and negative movie or pro-\\nduct reviews ( 59,60) and predicting votes from con-\\ngressional records ( 61) or Supreme Court decisions\\nfrom court proceedings. Figure 5 illustrates a typical\\nrestaurant review, annotated for positive, negative,and neutral sentiment, as well as basic emotions.\\nMining social media for sentiment or classic emo-\\ntions has been a particularly popular topic for the\\npurposes of assessing the “public mood ”from Twit-\\nter, predicting stock market trends, or simply eval-uating a community ’s mental state ( 62). Social media\\nsuch as Twitter, blog posts, and forums also provide'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='such as Twitter, blog posts, and forums also provide\\nresearchers with very large amounts of data to use inassessing the role of sentiment and emotion inidentifying other linguistic or social phenomena\\n[e.g., sarcasm ( 63), power relationships, and so-\\ncial influence ( 64) ] ,a sw e l la sm e n t a lh e a l t hi s -\\nsues [e.g., depression ( 65)].\\nConclusion and outlook\\nMany times during the past 50 years, enthusi-\\nastic researchers have had high hopes that the\\nlanguage-understanding ability of robots in sci-\\nence fiction movies was just around the corner.However, in reality, speech and language under-standing did not work well enough at that time\\nto power mainstream applic ations. The situation\\nhas been changing dramatically over the past fiveyears. Huge improvements in speech recognition\\nhave made talking to you r phone a commonplace\\nactivity, especially for young people. Web searchengines are increasingly successful in understandingcomplex queries, and MT can at least yield the gist\\nof material in another language, even if it cannot\\nyet produce human-quality translations. Com-puter systems trade stocks and futures auto-matically, based on the sentiment of reports about\\ncompanies. As a result, there is now great com-\\nmercial interest in the deployment of humanlanguage technology, especially because natural\\nlanguage represents such a natural interface'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='language represents such a natural interface\\nwhen interacting with mobile phones. In theshort term, we feel confident that more dataand computation, in addition to recent advances\\nin ML and deep learning, will lead to further\\nsubstantial progress in NLP. However, the trulydifficult problems of semantics, context, and\\nSCIENCE sciencemag.org 17 JULY 2015 \\x7fVOL 349 ISSUE 6245 265Breakfast on Broadway is a new place focusing on, you guessed \\nit, breakfast/brunch.  Went there last Sunday around 1.  The food \\nwas not bad but  the service was pretty terrible.  We had to wait 15 \\nminutes just to get menus and another 30 to get something to eat. And there were only a few tables occupied!  If you don’t mind \\nthe wait though, the price is right. I’ll probably give it another try.  \\nMaybe they need time to get their act together.Breakfast on Broadway is a new place focusing on, you guessed it, \\nbreakfast/brunch. Went there last Sunday around 1. The food was not bad but [Anger:  the service was pretty terrible ]. [Disgust: We \\nhad to wait 15 minutes just to get menus and another 30 to get something to eat. And there were only a few tables occupied! ] If \\nyou don’t mind the wait though, the price is right. I’ll probably give it another try. [Uncertainty:  Maybe they need time to get their act \\ntogether. ]\\nFig. 5. Manually annotated text analysis on a sample restaurant review. Sentiment analysis is shown on the left (blue, positive sentiments; red, negative; gray,'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='neutral). In the emotion analysis on the right, emotions are shown in bold ty pe and delineated by square brackets. Note in particular the importance of going beyond\\nsimple keyword analysis; for example, “not”has scope over “bad,”which might mislead simple systems. Also, the presence of “hedge ”words and phrases, which muddle\\nthe intended meaning (e.g., “pretty, ”which has a positive connotation, modifying the negative word “terrible ”), somewhat decreases the negative score of the next clause.\\nDownloaded from https://www.science.org at Peking University on November 22, 2024\\nknowledge will probably require new discov-\\neries in linguistics and inference. From this per-spective, it is worth noting that the developmentof probabilistic approaches to language is not\\nsimply about solving engineering problems: Prob-\\nabilistic models of language have also been reflectedback into linguistic science, where researchers are\\nfinding important new applications in describing\\nphonology ( 66), understanding human language\\nprocessing ( 67), and modeling linguistic seman-\\ntics and pragmatics ( 68). Many areas of linguistics\\nare themselves becoming more empirical and\\nmore quantitative in their approaches.\\nREFERENCES AND NOTES\\n1. C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard,\\nD. McClosky, “The Stanford CoreNLP Natural Language Processing\\nToolkit, ”inProceedings of the 52nd Annual Meeting of the\\nAssociation for Computational Linguistics, System Demonstrations'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='Association for Computational Linguistics, System Demonstrations\\n(Association for Computational Linguistics, Stroudsburg, PA,\\n2014), pp. 55 –60.\\n2. Linguistic Data Consortium, www.ldc.upenn.edu/.3. CoNLL Shared Tasks, http://ifarm.nl/signll/conll/.4. Kaggle, www.kaggle.com.5. P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, R. L. Mercer,\\nComput. Linguist. 19, 263 –311 (1993).\\n6. P. Koehn, F. J. Och, D. Marcu, “Statistical phrase-based\\ntranslation, ”inProceedings of the Human Language Technology\\nConference of the North American Chapter of the Association for\\nComputational Linguistics (Association for Computational\\nLinguistics, Stroudsburg, PA, 2003), pp. 48 –54.\\n7. D. Chiang, “A hierarchical phrase-based model for statistical\\nmachine translation, ”Proceedings of the 43rd Annual Meeting\\non Association for Computational Linguistics (Association for\\nComputational Linguistics, Stroudsburg, PA, 2005), pp. 263 –270.\\n8 . M .G a l l e y ,M .H o p k i n s ,K .K n i g h t ,D .M a r c u , “What ’s in a translation\\nrule? ”inProceedings of the Human Language Technology\\nConference of the North American Chapter of the Association for\\nComputational Linguistics (HLT/NAACL 2004) (Association for\\nComputational Linguistics, Stroudsburg, PA, 2004).\\n9. B. Jones, J. Andreas, D. Bauer, K. M. Hermann, K. Knight,\\n“Semantics-based machine translation with hyperedge\\nreplacement grammars, ”inProceedings of COLING 2012\\n(Technical Papers, The COLING 2012 Organizing Committee,'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='(Technical Papers, The COLING 2012 Organizing Committee,\\nMumbai, India, 2012), pp. 1359 –1376.\\n10. I. Sutskever, O. Vinyals, Q. V. Le, “Sequence to sequence\\nlearning with neural networks, ”inAdvances in Neural Information\\nProcessing Systems 27 (NIPS 2014) , Z. Ghahramani, M. Welling,\\nC. Cortes, N. D. Lawrence, K. Q. Weinberger, Eds. (Curran\\nA s s o c i a t e s ,R e dH o o k ,N Y ,2 0 1 4 ) ,p p .3 1 0 4 –3112.\\n11. D. Bahdanau, K. Cho, Y. Bengio, “Neural machine translation by\\njointly learning to align and translate, ”http://arxiv.org/abs/\\n1409.0473 (2015).\\n12. M.-T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, W. Zaremba,\\n“Addressing the rare word problem in neural machine\\ntranslation, ”http://arxiv.org/abs/1410.8206 (2015).\\n13. S. Jean, K. Cho, R. Memisevic, Y. Bengio, “On using very\\nlarge target vocabulary for neural machine translation, ”\\nhttp://arxiv.org/abs/1412.2007 (2015).\\n14. S. Stymne, C. Hardmeier, J. Tiedemann, J. Nivre, “Feature weight\\noptimization for discourse-level SMT, ”inProceedings of the Workshop\\non Discourse in Machine Translation (DiscoMT) (Association for\\nComputational Linguistics, Stroudsburg, PA, 2013), pp. 60 –69.\\n15. S. Green, J. Chuang, J. Heer, C. D. Manning, “Predictive\\ntranslation memory: A mixed-initiative system for humanlanguage translation, ”inProceedings of the 27th Annual ACM\\nSymposium on User Interface Software and Technology ,\\nHonolulu, HI, 5 to 8 October 2014 (Association for Computing\\nMachinery, New York, 2014), pp. 177 –187.'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='Machinery, New York, 2014), pp. 177 –187.\\n16. S. Rosenthal, J. Biswas, M. Veloso, “An effective personal\\nmobile robot agent through symbiotic human-robot interaction, ”\\ninProceedings of the 9th International Conference on\\nAutonomous Agents and Multiagent Systems (AAMAS 2010) ,\\nToronto, Canada, 10 to 14 May 2010 (International\\nFoundation for Autonomous Agents and Multiagent Systems,\\nRichland, SC, 2010), pp. 915 –922.\\n17. J. Fasola, M. J. Matari ć,J. Human-Robot Interact. 2,3–32\\n(2013).\\n18. M. Core, H. C. Lane, D. Traum, “Intelligent tutoring support\\nfor learners interacting with virtual humans, ”inDesignRecommendations for Intelligent Tutoring Systems (U.S. Army\\nResearch Laboratory, Orlando, FL, 2014), vol. 2, pp. 249 –257.\\n19. D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer,\\nK. Georgila, J. Gratch, A. Hartholt, M. Lhommet, G. Lucas,\\nS. Marsella, F. Morbini, A. Nazarian, S. Scherer, G. Stratou,\\nA .S u r i ,D .T r a u m ,R .W o o d ,Y .X u ,A .R i z z o ,L . - P .M o r e n c y , “SimSensei\\nKiosk: A virtual human interviewer f or healthcare decision support, ”in\\nProceedings of the 13th International Conference on Autonomous\\nAgents and Multiagent Systems (AAMAS 2014) , Paris, France, 5 to 9\\nMay 2014 (International Foundation for Autonomous Agents and\\nMultiagent Systems, Richland, SC, 2014), pp. 1061 –1068; http://\\naamas2014.lip6.fr/proceedings/aamas/p1061.pdf.\\n20. G. Hinton et al.,IEEE Signal Process. Mag. 29,8 2 –97 (2012).'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content=\"20. G. Hinton et al.,IEEE Signal Process. Mag. 29,8 2 –97 (2012).\\n21. J. Weizenbaum, Commun. ACM 9,3 6 –45 (1966).\\n22. Y. Nonaka, Y. Sakai, K. Yasuda, Y. Nakano, “Towards assessing\\nthe communication responsiveness of people with dementia, ”\\nin12th International Conference on Intelligent Virtual Agents\\n(IVA'12) (Springer, Berlin, 2012), pp. 496 –498.\\n23. C. Nass, Y. Moon, B. J. Fogg, B. Reeves, D. C. Dryer, Int. J.\\nHum. Comput. Stud. 43, 223 –239 (1995). .\\n24. H. Giles, A. Mulac, J. J. Bradac, P. Johnson, “Speech accommodation\\ntheory: The next decade and beyond, ”inCommunication Yearbook\\n(Sage, Newbury Park, CA, 1987), vol. 10, pp. 13 –48.\\n25. S. Young, M. Gasic, B. Thomson, J. Williams, Proc. IEEE 101,\\n1160 –1179 (2013).\\n26. Wikipedia, www.wikipedia.org/.27. L. Hunter, K. B. Cohen, Mol. Cell 21, 589 –594 (2006).\\n28. A. Culotta, J. Sorensen, “Dependency tree kernels for relation\\nextraction, ”inProceedings of the 42nd Annual Meeting of the\\nAssociation for Computational Linguistics (Association for\\nComputational Linguistics, Stroudsburg, PA, 2004), pp. 423 –429.\\n29. K. Fundel, R. Küffner, R. Zimmer, Bioinformatics 23, 365 –371\\n(2007).\\n30. J. Björne et al.,Comput. Intell. 27, 541 –557 (2011).\\n31. S. Van Landeghem et al.,PLOS ONE 8, e55814 (2013).\\n32. M. Ashburner et al. The Gene Ontology Consortium, Nat. Genet.\\n25,2 5 –29 (2000).\\n33. PaleoBiology Database, https://paleobiodb.org/.\\n34. A. Coulet, K. B. Cohen, R. B. Altman, J. Biomed. Inform. 45,\\n825 –826 (2012).\"),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='34. A. Coulet, K. B. Cohen, R. B. Altman, J. Biomed. Inform. 45,\\n825 –826 (2012).\\n35. B. Percha, Y. Garten, R. B. Altman, Pac. Symp. Biocomput.\\n2012 , 410 –421 (2012).\\n36. Freebase, www.freebase.com/.37. dbpedia, http://dbpedia.org/.38. Wikidata, www.wikidata.org/.39. M. Mintz, S. Bills, R. Snow, D. Jurafsky, “Distant supervision for\\nrelation extraction without labeled data, ”inProceedings of\\nthe Joint Conference of the 47th Annual Meeting of the ACL and\\nthe 4th International Joint Conference on Natural Language\\nProcessing of the AFNLP (Association for Computational\\nLinguistics, Stroudsburg, PA, 2009), vol. 2, pp. 1003 –1011.\\n40. M. Surdeanu, J. Tibshirani, R. Nallapati, C. D. Manning,\\n“Multi-instance multi-label learning for relation extraction, ”\\ninProceedings of the 2012 Conference on Empirical Methods in\\nNatural Language Processing and Natural Language Learning\\n(EMNLP-CoNLL) , Jeju Island, South Korea, 12 to 14 July 2012\\n(Association for Computational Linguistics, Stroudsburg,\\nPA, 2012), pp. 455 –465.\\n41. B. Min, R. Grishman, L. Wan, C. Wang, D. Gondek, “Distant\\nsupervision for relation extraction with an incomplete\\nknowledge base, ”inProceedings of NAACL-HLT 2013 , Atlanta,\\nGA, 9 to 14 June 2013 (Association for Computational\\nLinguistics, Stroudsburg, PA, 2013), pp. 777 –782.\\n42. DeepDive, http://deepdive.stanford.edu/.\\n4 3 . S .E .P e t e r s ,C .Z h a n g ,M .L i v n y ,C .R é , PLOS ONE 9, e113523 (2014).'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='4 3 . S .E .P e t e r s ,C .Z h a n g ,M .L i v n y ,C .R é , PLOS ONE 9, e113523 (2014).\\n44. E. Etzioni, M. Banko, M. J. Cafarella, “Machine reading, ”in\\nProceedings of the 21st National Conference on Artificial\\nIntelligence (AAAI 2006) , Boston, MA, 16 to 20 July 2006\\n(AAAI Press, Menlo Park, CA, 2006), vol. 2, pp. 1517 –1519.\\n45. M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,\\nO. Etzioni, “Open information extraction from the web, ”in\\nProceedings of the 20th International Joint Conference\\non Artifical Intelligence (IJCAI 2007) (Morgan Kaufmann,\\nSan Francisco, 2007), pp. 2670 –2676.\\n46. O. Etzioni, A. Fader, J. Christensen, S. Soderland, Mausam,\\n“Open information extraction: The second generation, ”in\\nProceedings of the 22nd International Joint Conference onArtificial Intelligence , Barcelona, Spain, 16 to 22 July 2011\\n(AAAI Press, Menlo Park, CA, 2011), pp. 3 –10.\\n47. S. Riedel, L. Yao, A. McCallum, B. M. Marlin, “Relation\\nextraction with matrix factorization and universal schemas, ”\\ninProceedings of the 2013 Conference of the North American\\nChapter of the Association for Computational Linguistics\\n(HLT NAACL 2013) (Stroudsburg, PA, 2013), pp. 74 –84.48. G. Angeli, C. D. Manning, “NaturalLI: Natural logic inference for\\ncommon sense reasoning, ”inProceedings of the 2014\\nConference on Emprical Methods in Natural Language\\nProcessing , Doha, Qatar, 25 to 29 October 2014 (Association for\\nComputational Linguistics, Stroudsburg, PA, 2014), pp. 534 –545.'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='Computational Linguistics, Stroudsburg, PA, 2014), pp. 534 –545.\\n49. J. Berant, V. Srikumar, P.-C. Chen, A. Vander Linden, B. Harding,\\nB. Huang, P. Clark, C. D. Manning, “Modeling biological processes\\nfor reading comprehension, ”inProceedings of the 2014 Conference\\non Emprical Methods in Natural Language Processing ,D o h a ,Q a t a r ,\\n25 to 29 October 2014 (Association for Computational Linguistics,\\nStroudsburg, PA, 2014), pp. 1499 –1510.\\n50. A. Fader, L. Zettlemoyer, O. Etzioni, “Open question answering\\nover curated and extracted knowledge bases, ”inProceedings of the\\nConference on Knowledge Discovery and Data Mining (KDD)\\n(Association for Computing Machinery, New York, 2014), pp. 1156 –1165.\\n51. M. A. Russell, Mining the Social Web: Data Mining Facebook,\\nTwitter, LinkedIn, Google+, GitHub, and More (O’Reilly Media,\\nSebastopol, CA, ed. 2, 2013).\\n52. N. Elhadad, L. Gravano, D. Hsu, S. Balter, V. Reddy,\\nH. Waechter, “Information extraction from social media for\\npublic health, ”inKDD at Bloomberg Workshop, Data\\nFrameworks Track (KDD 2014) (Association for Computing\\nMachinery, New York, 2014).\\n53. M. Ott, C. Cardie, J. T. Hancock, “Estimating the prevalence\\nof deception in online review communities. ”inProceedings of\\nthe 21st International Conference on World Wide Web\\nConference , Lyon, France, 16 to 20 April 2012 (Association\\nfor Computing Machinery, New York, 2012), pp. 201 –210.\\n54. J. Liscombe, thesis, Columbia University (2007).'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='54. J. Liscombe, thesis, Columbia University (2007).\\n55. J. Wiebe, T. Wilson, C. Cardie, Lang. Resour. Eval. 39,1 6 5 –210 (2005).\\n56. C. Whissell, “The dictionary of affect in language, ”inEmotion:\\nTheory, Research and Experience , R. Plutchik, H. Kellerman,\\nEds. (Academic Press, London, 1989).\\n57. Y. R. Tausczik, J. W. Pennebaker, J. Lang. Soc. Psychol. 29,2 4–54 (2010).\\n58. O. Türk, M. Schröder, IEEE Trans. Audio Speech Lang. Proc. 18,\\n965 –973 (2010).\\n59. B. Pang, L. Lee, S. Vaithyanathan, “Thumbs up? Sentiment\\nclassification using machine learning techniques, ”in\\nProceedings of the 2002 Conference on Empirical Methods in\\nNatural Language Processing , Philadelphia, PA, July 2002\\n(Association for Computational Linguistics, Stroudsburg, PA,\\n2002), vol. 10, pp. 79 –86.\\n60. H. Wang, M. Ester, “A sentiment-aligned topic model for product\\naspect rating prediction, ”inProceedings of the 2014 Conference on\\nEmpirical Methods in Natural Language Processing ,D o h a ,Q a t a r ,2 5\\nto 29 October 2014 (Association for Computational Linguistics,\\nStroudsburg, PA, 2014), pp. 1192 –1202.\\n61. M. Thomas, Bo Pang, L. Lee, “Get out the vote: Determining\\nsupport or opposition from Congressional floor-debate\\ntranscripts, ”inProceedings of the 2006 Conference on\\nEmprical Methods in Natural Language Processing , Sydney,\\nAustralia, 22 to 23 July 2006 (Association for Computational\\nLinguistics, Stroudsburg, PA, 2006), pp. 327 –335.\\n62. J. Bollen, H. Mao, A. Pepe, “Modeling public mood and'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='62. J. Bollen, H. Mao, A. Pepe, “Modeling public mood and\\nemotion: Twitter sentiment and socio-economic phenomena, ”\\nProceedings of the Fifth International AAAI Conference on\\nWeblogs and Social Media , Barcelona, Spain, 17 to 21 July 2011\\n(AAAI Press, Menlo Park, 2011), pp. 450 –453.\\n63. R. Gonzalez-Ibanez, S. Muresan, N. Wacholder, “Identifying\\nsarcasm in Twitter: A closer look, ”inProceedings of the\\n49th Annual Meeting of the Association for ComputationalLinguistics , Portland, Oregon, 19 to 24 June 2011 (Association\\nfor Computational Linguistics, S t r o u d s b u r g ,P A ,2 0 1 1 ) ,p p .5 8 1 –586.\\n64. O. Biran, S. Rosenthal, J. Andreas, K. McKeown, O. Rambow,\\n“Detecting influencers in written online conversations, ”in\\nProceedings of the 2012 Workshop on Language in Social\\nMedia , Montreal, Canada, 7 June 2012 (Association for\\nComputational Linguistics, Stroudsburg, PA, 2012), pp. 37 –45.\\n65. L.-C. Yu, C.-Y. Ho, “Identifying emotion labels from psychiatric\\nsocial texts using independent component analysis, ”inProceedings\\nof COLING 2014 (Technical Papers, Association for Computational\\nLinguistics, Stroudsburg, PA, 2014), pp. 837 –847.\\n66. B. Hayes, Z. Londe, Phonology 23,5 9 –104 (2006).\\n67. R. Levy, Cognition 106, 1126 –1177 (2008).\\n68. N. D. Goodman, D. Lassiter, “Probabilistic semantics and pragmatics:\\nUncertainty in language and thought, ”inHandbook of Contemporary'),\n",
       " Document(metadata={'title': 'Advances in natural language processing', 'year': 2015}, page_content='Uncertainty in language and thought, ”inHandbook of Contemporary\\nSemantics ,C .F o x ,S .L a p p i n ,E d s .( B l a c k w e l l ,H o b o k e n ,N J ,e d .2 ,2 0 1 5 ) .\\nACKNOWLEDGMENTS\\nC.D.M. holds equity in Google and serves in an advising role to\\nIdibon, Lilt, Lex Machina, and Xseed.\\n10.1126/science.aaa8685\\n266 17 JULY 2015 \\x7fVOL 349 ISSUE 6245 sciencemag.org SCIENCEARTIFICIAL INTELLIGENCE  \\nDownloaded from https://www.science.org at Peking University on November 22, 2024'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 9, NO. 4, AUGUST 2001 483\\nAffect Analysis of Text Using\\nFuzzy Semantic Typing\\nPero Subasic , Member, IEEE, and Alison Huettner\\nAbstract— Weproposeanovel,convenientfusionofnaturallan-\\nguage processing and fuzzy logic techniques for analyzing the af-fectcontentinfreetext.Ourmaingoalsarefastanalysisandvisu-alizationofaffectcontentfordecisionmaking.Themainlinguisticresourceforfuzzysemantictypingisthefuzzy-affectlexicon,fromwhich other important resources—the fuzzy thesaurus and affect\\ncategorygroups—aregenerated.Freetextistaggedwithaffectcat-\\negories from the lexicon and the affect categories’ centralities andintensities are combined using techniques from fuzzy logic to pro-duceaffectsets—fuzzysetsrepresentingtheaffectqualityofadoc-ument.Weshowdifferentaspectsofaffectanalysisusingnewscon-tent and movie reviews. Our experiments show a good correspon-dence between affect sets and human judgments of affect content.Weascribe thistothe representation ofambiguity inourfuzzyaf-fect lexicon and the ability of fuzzy logic to deal successfully withthe ambiguity of words in a natural language. Planned extensionsof the system include personalized profiles for Web-based contentdissemination, fuzzy retrieval,clustering, and classification.\\nIndex Terms— Computing with words, fuzzy logic, knowledge\\nengineering, text mining, world wide web.\\nI. INTRODUCTION\\nTHE huge amount of text stored on computer systems is'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='I. INTRODUCTION\\nTHE huge amount of text stored on computer systems is\\ngettinglargereveryday.Movingbeyondthebasicassump-\\ntion that a given piece of text should be easily located, the next\\ngenerationofsystemsaimstowardintegrated,personalizedser-vices and decision support. In these areas, a quick analysis ofparticular qualities in the text and an intuitive presentation to\\ntheuserbecomeincreasinglyimportant.Tomatchanindividual\\nuser’s profile on the World Wide Web, for example, it is nec-essary to introduce a human dimension into text understandingand representation. Expectations for modeling the purely sub-\\njective, human dimensions of text and data understanding are\\nhigh, on both the users’ and providers’ sides. Here we exper-iment with qualitative analysis of affect-related information infree text. Affect-related information includes words describing\\nemotions; like fear, anger, love, joy , andsorrow; feelings like\\nwarmthandexcitement ; attitudes like helpful, friendly, hostile ;\\nandotherrelatedcategoriessuchastemperament,humor,frameofmind,mood,spirits,morale,anddisposition(includingwords\\nlikesweet,farce,wary,sanguine,depressed,eagerness,selfish ).\\nOurreasonsforselectingthisparticulardomainarethreefold.\\nFirst,affect-related information is pervasive in electronic doc-\\numents: in news stories (on politics, competitive sports, etc.),\\neconomic reports (corporate acquisitions, investor reactions),\\nManuscript received September 11, 2000; revised May 8, 2001.'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='Manuscript received September 11, 2000; revised May 8, 2001.\\nThe authors are with the Clairvoyance Corporation, Pittsburgh, PA 15232\\nUSA (e-mail: p.subasic@clairvoyancecorp.com).\\nPublisher Item Identifier S 1063-6706(01)06537-7.customer-oriented Internet sites (eBay.com, amazon.com),\\nnewsgroups (misc.consumer), corporate customer service,\\nemail (customer complaints, questions, opinions), artisticand cultural material (movie and art reviews), etc. Second,affectinformation is critical to human communication: recent\\nwork shows the importance of emotions in decision-making,\\nperception and learning [8]. And finally, we believe that con-clusions with respect to affectare extensible to other types of\\nsubjectiveinformation,suchasflavors,styles,motivations,and\\nperceptions, in general. Potential applications for qualitative\\ntext mining technology are completely open ended. This paperis an extended version of an earlier paper [9]. Here, we presentourworkinmoredetailandpresentmoreapplicationexamples.\\nAnalyzing affectin a text presents us with two obvious\\nsources of ambiguity and imprecision: the first being emotionsthemselves and the second, words in a natural language [10].Ratherthanattemptingtoconstrainandlimitthisambiguity,we\\nhave taken the opposite approach. We explicitly represent and\\nprocess ambiguity by introducing fuzzy logic into the picture.Specifically,weintegratebasictechniquesfromfuzzylogicandfrom computing with words [13] with techniques from natural'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='language processing (NLP). This work is also very related to\\nthe recent work on representing and manipulating perceptions[14]. Since the central technique we use from NLP is semantictyping [7], werefer to this approach as fuzzy semantic typing .\\nThe fuzzy semantic typing approach is general in scope and\\ncanbeappliedtomanydifferentkindsofanalysis.Weillustrateitsuse inanalyzing affect. Atthe most basic level,it involves:\\n1) isolating a vocabulary of words belonging to a meta-lin-\\nguistic domain (here, affector emotion);\\n2) using multiple categorizations and scalar metrics to rep-\\nresent the meaning of each word in that domain;\\n3) computing profiles for texts based on the categorizations\\nand scores of their component domain words;\\n4) manipulating the profiles to visualize the texts.\\nWetakeamulti-facetedapproachtorepresentingandmanip-\\nulatingqualitativeinformation.Webeginwithanaffectlexicon,\\nwhichcharacterizesalargevocabularyofaffectwordsintermsofasmallsetofbasiccategories,suchas love,hate,happiness ,\\nandanger,eachtosomenumericaldegree.Thecategoriesfrom\\ntheaffectlexiconconstitutesemantictags,whichareassociated\\nwith words within a broad semantic domain.\\nInthepast,semantictagginghasgenerallybeenusedforthe-\\nmatic role assignment [3] or for word sense disambiguation\\n(WSD) [4]. Our approach is similar to the standard WSD ap-'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='(WSD) [4]. Our approach is similar to the standard WSD ap-\\nproach in that the lexicon entry for an ambiguous word rep-resents all of its possible meanings. However, where WSD re-quires selecting a single meaning for the word in context, our\\n1063–6706/01$10.00 © 2001 IEEE\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. 484 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 9, NO. 4, AUGUST 2001\\nsystemsimplyassignsthemall,exploitingratherthanreducing\\nthe word’s ambiguity. For any relevant word that appears in atext,weincludeallofitspossiblemeaningsandconnotationsin\\nour analysis of that text; and, we depend on the associated nu-\\nmerical weightings and the cumulative effect of related vocab-ulary to create arealisticpicture of thetext’saffectivecontent.\\nSemantic treatments of lexical ambiguity are more typically\\ncomponentsofmachinetranslationthanofinformationretrieval(IR) or filtering systems, although there is some evidence that\\nambiguity resolution can improve performance in IR [6]. Our\\napproach is unusual in integrating lexico-semantic tags into ageneral-purpose text management system, capable of IR, fil-tering, categorization, and potentially other text management\\nfunctions.\\nWe have dealt with ambiguity by allowing a single lexicon'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='functions.\\nWe have dealt with ambiguity by allowing a single lexicon\\nentry(domain word) tobelongtomultiplesemanticcategories.Imprecision is handled, not only via multiple category assign-ments, but also by allowing degrees of relatedness ( centrality )\\nbetween lexicon entries and their various categories. Gleeful,\\nforexample,isassignedtoboth happiness andexcitement ,b u t\\nis given a higher centrality score in the happiness category. In\\nadditiontocentralities,lexiconentriesarealsoassignednumer-\\nicalintensities , which represent the strength of the affect level\\ndescribed by that word. Thus, for example, abhorrent anddis-\\ntastefulhaveroughlythesamecentralityonthe repulsion scale,\\nbutabhorrent receives a higher intensity.\\nAfter the affectwords in a document are tagged, the fuzzy\\nlogic part of the system handles them by using fuzzy combina-\\ntionoperators,setextensionoperators,andafuzzythesaurustoanalyze fuzzy sets representing affects. Fuzzy techniques pro-\\nvide anexcellent framework forcomputational management of\\nthe ambiguity and imprecision that are pervasive in the words\\nof a natural language. There are additional reasons why fuzzylogic is a good choice for text management. First, eliminatingambiguity and imprecision from texts is unnatural and leads to\\nmisconceptions about the underlying meaning. When properly'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='misconceptions about the underlying meaning. When properly\\nunderstoodandmanaged,ambiguityandimprecisionleadtoen-hanced, more concrete and precise representations than other(e.g., statistical) methods used for text analysis. This is espe-\\nciallytrueinthecaseofqualitativeanalysis,whenweareinter-\\nestedinwhatessentialfeaturesarepresentinsomecontent.Andsecond,sincetheiremphasisisqualitative,fuzzytechniquesaremoreappealingtohumans,whotendtothinkqualitatively;this\\nmakes them good candidates for any human-friendly applica-\\ntion. The appeal of fuzzy analysis will become apparent whenwe show visualizations of affect sets later in this document.\\nBesides the fuzzy affect lexicon, we generate additional re-\\nsources for enhanced functionality. A fuzzy thesaurus isgener-atedfromtheaffectlexiconandusedtoexpandaffectsets;affect\\ncategorygroupsaregeneratedbyclusteringthefuzzythesaurus\\nto enable easier visualization, navigation, and browsing for theuser. We provide a detailed explanation of these resources andthe ways in which we generate them, with several practical ex-\\namples, in Section II.\\nA primary representation vehicle in our system is a set of\\nfuzzy semantic categories (affect categories) followed by theircentralitiesand/orintensities,calledthe affectset.Anaffectset\\nwith attached centralities is always treated as a pure fuzzy set,and all fuzzy techniques applicable to fuzzy sets are applied to'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='such affect sets. The handling of affect sets with intensities isdifferent and more statistical, since, intensities represent less\\nambiguous, more quantitative features of the text. Affect sets\\nand the fuzzy semantic typing technique are presented in Sec-tion III.\\nFinally,visualizationisaveryimportantissueinoursystem.\\nItdemonstratestherealpoweroffuzzysemantictypingbypre-\\nsenting a concise, to-the-point, qualitative representation of af-fectintexts.Suchvisualizationsconstituteanexcellenttoolfordecision making. We show some interesting visualization sam-\\nples of affect sets for movies and news articles in Section IV.\\nInSectionV,weillustratecomputationalapplicationsoftheap-proach: retrieval of phrases whose affect content is similar tothatofagivenphraseandfilteringofphrasesbasedonapre-set\\nintensity threshold. In Section VI, we discuss ideas for further\\ndevelopment of the system and in Section VII we summarizethe paper and its conclusions.\\nII. L\\nINGUISTIC RESOURCES\\nThis section describes the linguistic resources of the fuzzy\\ntyping system: the affect lexicon , thefuzzy thesaurus and the\\naffect category groups .\\nA. Affect Lexicon\\nThe affect lexicon is a compendium of lexical entries for af-\\nfectwords, withtheir corresponding partsofspeech, affectcat-\\negories, centralities, and intensities. Affectwords were gath-\\nered from several sources. We began with an existing affect\\nwordlistcollectedfromnewspaperarticlesbyMarkKantrowitz'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='wordlistcollectedfromnewspaperarticlesbyMarkKantrowitz\\nofJustsystemPittsburghResearchCenter.Weconvertedthelistto a new format and supplemented it rather haphazardly froman on-line thesaurus. We are currently experimenting with a\\nmore systematic, semi-automatic collection method based on\\nWordNet[2],whichwehopewillprovebothscalableandtrans-ferable.\\nAnaffectwordisanywordhavinganaffect-relatedmeaning\\nor connotation: e.g., abhor, abusive, amity, apprehend, arro-\\ngance,etc. Any given affectword may have multiple entries\\nin the affect lexicon, differing by part of speech value and/orcategory. The expressions of interest are a superset of simple\\n\"emotion\" words: they include emotions ( happiness ), feelings\\n(desire), attitudes ( resentful), temperament ( good-natured ),\\nhumor (hilarious), frame of mind ( cheerful), mood ( sulk),\\nspirits(morale),anddisposition( sunny).Werepresentanaffect\\nword’s meaning by associating the word with one or more\\naffect categories, from our initial inventory of 83 \"atomic\"affects. A relatively straightforward affect word, like terror\\nwill be associated with a single affect category. An affect word\\nwith a more complex meaning will be assigned to multiple\\ncategories—for example, infatuation is assigned to both love\\nandinsanity. An ambiguous word is simply assigned to all the\\ncategories necessary to capture its various meanings: e.g., mad\\nhas three entries, associating it with insanityin its first sense'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='has three entries, associating it with insanityin its first sense\\nand with irritation andangerin its second. All lexicon entries\\nare root forms; forms in the text are part-of-speech tagged andstemmed before lookup.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. SUBASIC AND HUETTNER: AFFECT ANALYSIS OF TEXT USING FUZZY SEMANTIC TYPING 485\\nEntries in the affect lexicon are of this form\\nas in\\nLexicalentryisasingleentryforawordthathasanaffectual\\nconnotation or denotes an affect directly. At present, our fuzzyaffect lexicon contains 3876 lexical entries, about half of what\\nwe plan.\\nPart\\nofspeechtag.Sinceambiguitysometimesdependson\\na word’s part of speech—and since NLP allows us to differ-entiate parts of speech in documents— we have included POS\\ninformationforlexiconentries.Forexample,theword alerthas\\ndifferent category assignments associated with different POSvalues\\nThatis,theadjective alertmeansquicktoperceiveandact —a\\nkind ofintelligence —while the verb alertmeansto call to a\\nstate of readiness —a kind of warning.\\nA word’s POS can affect its centrality or intensity values as\\nwell as its category assignment. For example, lexicon entrieswithPOS,categories,andcentralitydegreesfortheword craze\\ninclude\\nThatis,theverb crazebelongstoaffectcategory insanitywitha\\ndegree of 0.8; the singular noun crazebelongs to the same cat-\\negory with a degree of 0.5. This reflects the fact that the verb'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='egory with a degree of 0.5. This reflects the fact that the verb\\ncrazemeanstomakeinsaneorasifinsane —verycentraltothe\\ninsanitycategory!—while the noun crazemeansan exagger-\\natedandoftentransiententhusiasm —i.e.,itbelongsto insanity\\nonly in a less central, more metaphorical sense.\\nAffectcategory.Manyofourcategorieshavestrayedsome-\\nwhat from the strictly affect domain: for example, deprivation ,\\nhealth,andintelligence areonlymarginallyaffects,and death,\\ndestruction andjusticeare not affects at all. Such categories\\nhave been created in cases where (a) some significant portion\\nof an affect word’s meaning cannot be captured using pure af-fect categories; and (b) the same meaning component recurredagainandagaininthevocabularyweweretryingtohandle.For\\nexample, a word like corpsecertainly entails some affect, and\\ncan plausibly be assigned to categories sadnessandhorror;a t\\nthe same time, a part of its meaning is obviously being missedby those categorizations. Moreover, words like assassination,\\ncyanide, execute, funeral, genocide , andhomicidal share this\\nmissing meaning component. On this first pass, we have goneahead and created extra, not-strictly-affect categories to handlesuch words; in the future, when we review and revise the cate-\\ngory inventory, we may rethink this decision.\\nAtpresent,thereare83affectcategories.Eachaffectcategory\\nhasanexplicitopposite,withthreeexceptions: death,irritation\\nandcrime.Affectwordsarespreadunevenlyacrossaffectcate-'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='andcrime.Affectwordsarespreadunevenlyacrossaffectcate-\\ngories,withtheleastfrequentcategoriesbeing health,sickness\\nandfacilitation (only0.35%ofentries),andmostfrequentbeingconflictandviolence(3.7%ofentries).Thecompletelistofaf-\\nfect categories with their opposites is givenin Appendix A.\\nCentrality . Centrality degrees range from 0 to 1 by incre-\\nments of 0.1. A word that belongs to several affect categories\\nwill generally have different centralities from category to cate-gory, as in this example\\nThat is, the element of weakness is fairly central to the word\\nemasculate (aratingof0.7);thenotionofaspecific lackisalso\\npresentbutlesscentral(ratingof0.4);andanadditionalelement\\nofviolenceis possible butnot really necessary (ratingof 0.3).\\nIn assigning centrality, typical questions the lexicon devel-\\noper should answer for each entry and affect category include:To what extent is affectword X related to category C? To what\\nextentdoes affectwordXco-occurwithcategoryC?Towhatex-\\ntent canaffectword X be replaced with category C in the text,\\nwithout changing the meaning?\\nSince centralities indicate the presence of certain qualities\\n(represented by appropriate affect categories) in a given affect\\nword, centrality computations are handled as computations offuzzy membership degrees.\\nIntensity. In addition to centralities, lexicon entries are as-\\nsignednumericalintensities,whichrepresentthestrengthofthe'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='signednumericalintensities,whichrepresentthestrengthofthe\\naffect level described by that entry. Intensity degrees, like cen-tralitydegrees,rangefrom0to1byincrementsof0.1.Herearesome examples (the second number represents theintensity)\\nAll of these words have some element or connotation of repul-\\nsion. A word like abhorexpresses very intense repulsion (as\\nwell as being very central to the concept of repulsion );con-\\ntempt, aversion , anddispleasure are progressively less intense\\nontherepulsion scale.Awordlike fat—whichisnotatallcen-\\ntral to the repulsion concept, as expressed by its low centrality\\nof0.2,butwhichhassomeslightovertonesof repulsion tomany\\nAmericans—isanobjectivedescription,hence,hardlyanaffectword at all. This is reflected in its low intensity score of 0.1. Ingeneral,scoresbelow0.4onbothscalestendtobethemostsub-\\njectiveandnotional,sinceitiseasiertorateprominentqualities\\nthan backgrounded ones.\\nAwordthatbelongstoseveralaffectcategorieswillgenerally\\nhave different intensities from category to category, as in this\\nexample\\nThatis,avengeisahigh-intensity conflictword,butonlyamod-\\nerate-intensitywordwithrespectto violence;itsintensityrating\\nforjusticeis somewhere in between.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. 486 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 9, NO. 4, AUGUST 2001\\nAssigningcategorylabelsandmembershipdegreestolexicon'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='Assigningcategorylabelsandmembershipdegreestolexicon\\nentriesisaverysubjectiveprocess.Duringthepresentproof-of-conceptphase,theassignmentshavebeenmadebyasinglelin-\\nguist.Theyareobviouslyinfluencedbythelinguist’sownexpe-\\nrience, reading background, and (since affectsare in question)\\npersonal/emotionalbackgroundandprejudices.Thoughsubjec-tive, the process is not completely arbitrary—the assignments\\nare general enough in the main to yield useful results. Ideally,\\nhowever, we would like to involve additional linguists, to re-viewandrefinetheinventoryofatomiccategoriesandtoensuresomeconsensusontherepresentationofdifficultitems.Inafin-\\nished system, repeated iterations and use of additional profiles\\nor personal lexicons will allow the individual user to fine-tunemembership degrees and accommodate his or her own subjec-tive criteria.\\nB. Fuzzy Thesaurus\\nThe fuzzy thesaurus establishes relationships between pairs\\nof affect categories, based on the centralities of items assignedto both categoriesin thelexicon.It contains entries ofthe form\\nas in\\narranged in a matrix. When the relationship degree is equal to0, no entry is recorded in the fuzzy thesaurus. When the rela-\\ntionship degree is equal to 1.0 we say that we have discovered\\naffectual synonyms ,a si n\\nNon-synonymous pairs having entries in the matrix are related\\nto some specified degree.\\nThefuzzythesaurus isgeneratedbythesystemfromtheaffect\\nlexicon. It is generated using max-min composition [12]\\n(1)'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='lexicon. It is generated using max-min composition [12]\\n(1)\\nwhere areaffectcategorieswhoserelationshipdegree\\nwe want to compute and\\nrepresentthecentralitiesofaffectcategories withre-\\nspect to affect . are taken directly from\\nthe affect lexicon.\\nThefuzzythesaurusisprimarilyusedforexpansionofaffect\\nsets. For example, a single affect category humorwith a cen-\\ntrality of 0.7 is expanded using the similarity class for humor\\nfrom the fuzzy thesaurus. Using max-min composition, we ex-pand this affect intensity profile as follows\\nhumor\\nhumor excitement intelligence\\nhumor excitement intelligence(2)\\nwhererepresents the composition operator.Since it is difficult to modify the affect intensity set consis-\\ntentlytoreflectthechangesintheaffectcentralityset,weleaveit to the user to accommodate the intensities of the added cate-\\ngories for his/her particular purposes.\\nExpansion increases the number of categories and the level\\nof detail, as shown in Section IV.\\nNote,thatsincethefuzzythesaurusisgeneratedfromtheaf-\\nfect lexicon, it must be recomputed and re-generated whenever\\ntheaffectlexiconchanges.Forefficiency,onlythoseentriesdi-rectly affected by a change are recomputed.\\nC. Affect Category Groups\\nAffect category groups are generated automatically by clus-\\nteringthefuzzythesaurus.Inthisprocess,affectcategorieswithhigh degrees of similarity (as defined in the fuzzy thesaurus)aregroupedtogether.Forexample,wefindthat love,attraction ,\\nhappiness ,desireandpleasureformoneaffectcategorygroup,'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='happiness ,desireandpleasureformoneaffectcategorygroup,\\nwhilerepulsion ,horror,inferiority andpainform another. If\\nthe automatically-created groups are not as intuitively naturalas these examples, the user can edit them.\\nAffect category group ACGis a set of affect categories\\nsuch that\\n(3)\\nwhere is a user-set threshold. It is worth\\nnotingthat ACGisnotasimilarityrelationinthesenseofZadeh\\n[12], since it lacks the transitivity property. The lack of transi-\\ntivity is a direct consequence of the fact that the ACGis gener-\\nated from the fuzzy thesaurus, which is in turn generated fromthe affect lexicon. Transitivity can be enforced by computingthe missing relationship degrees from the existing ones. How-\\never, we prefer to keep the original relationship degrees intact,\\ninordertoensuretheproperinterpretationoftheoriginalaffectcategory assignments reflected in the affect lexicon.\\nAffectcategorygroupscanbeusedformoreefficientgroup-\\nings of affect categories in visualization charts. An example is\\nshown in Section IV.\\nSincetheaffectcategorygroupsarecomputedfromthefuzzy\\nthesaurus, each time the fuzzy thesaurus is changed, the affect\\ncategorygroupsmustberecomputed.Thisisacomputationally\\ninexpensiveoperation,sincethenumberofaffectcategoriesin-volved is typically small—in our prototype, there are only 83categories.\\nIII. F\\nUZZYSEMANTIC TYPING\\nFuzzysemantictypingistheprocessinwhichdomainwords\\nfromadocumentareidentified.Thewordsareassignedmeta-in-'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='fromadocumentareidentified.Thewordsareassignedmeta-in-\\nformation from the typing lexicon in the form of semantic cat-egories and associated degrees; and the categories and degreesare combined to yield the overall representation of the docu-\\nment’scontent.Inthissection,wedescribeindetailthestepsin\\nthis process for affect analysis.\\nA. Affect Sets\\nA central construct in our affect analysis is the affect set.I t\\ncomprises the set of unique affect categories from a given text,\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. SUBASIC AND HUETTNER: AFFECT ANALYSIS OF TEXT USING FUZZY SEMANTIC TYPING 487\\nFig.1. Generationofthedocumentaffectset,afuzzysetrepresentingaffective\\ncontent of a document.\\nwithattachedcentralitiesandintensities.Thefollowingsections\\ndiscuss thegeneration of anaffect set fora generaldocument.\\nB. Tagging of Free Text\\nThe process for tagging a document with an affect set is\\nshown in Fig. 1. It includes the following steps.\\n1) Normalization and Tagging:\\n1) Thedocumentisparsedandtokens(individualwords)are\\ngenerated one at a time.\\n2) Each token is normalized using normalization rules for\\nEnglish language, shown as Grammar in Fig. 1.\\n3) Thenormalizedtokensarelookedupintheaffectlexicon.\\nIf a token has one or several lexicon entries, we retrieveall affect categories with their associated centrality and\\nintensity scores.\\nUsingthisalgorithm,wegeneratetheinitialaffectsetforeach'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='intensity scores.\\nUsingthisalgorithm,wegeneratetheinitialaffectsetforeach\\ndocument.\\nAs an example, consider a simple document consisting of\\nthis sentence:His first film,\\nUn Chien Andalou (1928),\\nco-directed by Salvador Dali, caused an\\nuproar (he filled his pockets with stones,\\nhe wrote in his autobiography, so he wouldhave something to throw if the audience\\nattacked him).\\nThis document is tagged with\\non the basis of its two affect words, uproarandattacked.\\nNote, that since the word attackedbelongs to both the affect\\ncategories violenceandconflict,bothcategoriesareincludedas\\ndocument tags.\\n2) Combination of Centralities and Intensities and Docu-\\nment Affect Set: The following algorithm describes how to re-\\nduce the initial affect set by combining the centralities and in-tensities of recurring categories.\\n1) Foreach affectcategorythat appears in the tagging set:\\na) Computethemaximalcentrality(fuzzyunion)ofall\\ncentralities attached to that affect category in thetaggeddocument.Theresultisthecentralityofthat\\ncategory for the document as a whole.\\nb) Compute the average intensity of all intensities at-\\ntached to that affect category in the tagged docu-ment.Theresultistheintensityofthatcategoryfor\\nthe document as a whole.\\n2) Countsof affectcategoriesarecombinedwith intensities\\nusingsimpleaveragingtoyieldtheoverallintensityscore\\nfor the document.\\nAs anexample,consider the followingdocument:\\nLuis Bunuel’s The\\nExterminating Angel\\n(1962) is amacabre comedy ,amordant view'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='Luis Bunuel’s The\\nExterminating Angel\\n(1962) is amacabre comedy ,amordant view\\nof human nature that suggests we harbor\\nsavageinstincts and unspeakable secrets .\\nTake a group of prosperous dinner guests\\nand pen them up long enough, he suggests,\\nand they’ll turn on one another like rats\\nin an overpopulation study. Bunuel be-gins with small, alarming portents . The\\ncook and the servants suddenly put on\\ntheir coats and escape , just as the dinner\\nguests are arriving. The hostess is fu-\\nrious ; she planned an after-dinner enter-\\ntainment involving a bear and two sheep.\\nNow it will have to be canceled .I ti s\\ntypical of Bunuel that such surrealistic\\ntouches are dropped in without comment.The dinner party is a success . The guests\\nwhisper slanders about each other, their\\neyesplaying across the faces of their\\nfellow guests with greed ,lustandenvy.\\nAfter dinner, they stroll into the drawing\\nroom, where we glimpse a woman’s purse,\\nfilled with chicken feathers and roosterclaws.The output produced after fuzzy semantic tagging is given in\\nTable I.\\nWe combine recurring affect categories into a set of unique\\ntags, with centralities and intensities that accurately reflect the\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. 488 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 9, NO. 4, AUGUST 2001\\nTABLE I\\nENTRIES AND ASSOCIATED AFFECTCATEGORIES WITHCENTRALITIES AND'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='TABLE I\\nENTRIES AND ASSOCIATED AFFECTCATEGORIES WITHCENTRALITIES AND\\nINTENSITIES FOR THE MOVIEREVIEW OF EXTERMINATING ANGEL\\noveralldocumentcontent.Forthispurpose,wediscardtheorig-\\ninalaffectwordsandthePOSinformation,andcombinethein-tensities and centralitiesof the remaining affect categories.\\nIntensitiesandcentralitiesarehandleddifferently, sincethey\\nrepresentdifferenttypesofinformation.Centralityindicatesthe\\npurity of a quality represented by an affect category. Intensity\\nindicates the strength of that quality. Thus, the number of oc-\\ncurrences of a particular affect category in the document does\\nnotaffectitscentrality,butdoesaffectitsintensity.Centrality,asthe purity of a quality, depends on the maximal centrality over\\nall instances of that affect category in a particular document.Thatistosay,themaximalpurityofthequalityinthedocument\\nalready implies vaguer or more diluted degrees of that quality\\nand, therefore, is appropriate as the combined centrality/purityfor that category. The appropriate operation here is thus fuzzy\\nunion. On the other hand, the more times an affect category is\\npresent in the document and the higher the intensities of its in-\\nstances, the higher will be the combined intensity/strength at-\\ntachedtoit.We,therefore,computetheintensityattachedtoanaffect category as the average of all intensities attached to in-\\nstances ofthat category.Webelievethis modeliscloser tohow'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='stances ofthat category.Webelievethis modeliscloser tohow\\nhumansperceiveintensitiesofwordsastheyread.Forexample,if we encounter only one instance of a “strong” word (for ex-\\nample,greedinthedesirecategory),togetherwithmany“weak”\\nwords(forexample, wish,want,prefer ),the“weak”wordswill\\ninfluencetheintensityproportionallyandreducetheoverallef-\\nfect of the “strong” word. This is why we compute the averageratherthansomeotherfunction,suchasthemaximum—amax-\\nimum would imply that the intensity of the “strongest” word is\\nthe intensity for the whole article, which is not what we expe-\\nrience while reading. Another plausible approach would be to\\nweightintensitiesdependingontheirpositioninthetext,givingheavierweighttowordsnearthebeginningofthedocument,or\\nbelonging to highly exposed partsof thedocument, such as the\\ntitle or abstract.\\nAftercomputingcentralitiesusingfuzzyunionandarranging\\nelements so that the elements with higher membership degrees(centralities) are at the front of the fuzzy set, we have\\nviolence,humor, warning, anger, success, slander, greed\\nhorror, aversion absurdity, excitement,desire\\npleasure,promise, surfeit repulsion, fear\\nlack,death, slyness, intelligence, deception,insanity\\nclarity,innocence,inferiority\\npain, disloyalty, failure, creation, surprise(4)\\nThisrepresentation ofthe fuzzyset of affectcategoriesenables\\nus easily to spot predominant qualities of affect categories in\\nthe document. The meaning of this affect category set is that'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='the document. The meaning of this affect category set is that\\nthe document has a high degree of violence,humor,warning,\\nanger,success,slander,greed,horror,aversion,absurdity ,ex-\\ncitement,desire,pleasure,promise and surfeit ; a medium de-\\ngree ofrepulsion ,fear,lack,death,slyness,intelligence ,de-\\nception,insanity,clarity,innocence andinferiority ; and a low\\ndegree of pain,disloyalty ,failure,creationandsurprise.\\nTocomputetheoverallintensityweuseasimpleaverageover\\nall affect categoryinstances and their respective intensities\\n(5)\\nwhere\\noverall intensity of a document ;\\ntotal number of affect category instances in the\\ndocument ;\\nintensity of an affect categoryinstance .\\nFor the example document, overall intensity is 0.6.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. SUBASIC AND HUETTNER: AFFECT ANALYSIS OF TEXT USING FUZZY SEMANTIC TYPING 489\\nFig. 2. Centralities with positive connotations up are shown separately from those whose connotations are negative. Affect categories are arranged into similar\\ngroups around the circle.\\nFig.3. WhenallcentralitiesfromFig.2areexpandedusingthefuzzythesaurus,wegetagreaterlevelofdetail.Notethatadditionalaffectcategori esexistinthe\\nnew chart. Centrality values are omitted for greater legibility (scale 0–1).\\nOverall intensity is used to detect documents with offensive\\ncontent. For example, high overall intensity (0.7) in com-'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='content. For example, high overall intensity (0.7) in com-\\nbination with the specific centrality profile /100/105/115 /116/97/115/116 /101\\n/118/105 /111/108/101 /110 /99 /101 /112 /97/105/110 may indicate offensive and\\nundesirable content.IV. AFFECTSETVISUALIZATION\\nAninteresting and importantarea related to thefuzzy typing\\nworkisvisualization oftheresults.Wehavedevelopedasimple\\naffecttaggingandaffectprofilebrowsingapplicationcalledthe\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. 490 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 9, NO. 4, AUGUST 2001\\nFig.4. NewsreportontraincrashinLondon,October1999.Centralitiesdescribequality,andintensities,quantity(countandstrength)oftheaffe ctsinadocument.\\nAs might be expected, the affects fear,harm,painandsurprisearemost central. The most intenseaffects are conflict,confusion ,disadvantage andpain.\\nFig. 5. Profile of a recent cult movie Matrixgenerated from a movie review. Opposite affect categories are placed on opposite sides of the circle (the left side\\nshows negative affects, the right side, positive). With a well-developed negative side, this movie is deservedly rated “ /82” in the US.\\nAffectInspector.Weshowabasicbrowsingsetupfromthatap-\\nplication in Appendix B.\\nEachaffectcategory’scentralitiesandintensitiescan berep-\\nresentedasapointontheperimeterofaunitcircle.Centralities'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='resentedasapointontheperimeterofaunitcircle.Centralities\\nandintensitiescanthenbevisualized,asshowninFigs.2–8.Inordertodemonstratevariouswaysthesechartscanbeorganized,we show different charts fordifferent information objects.\\nIn Fig. 2 we show centralities with positive affects separated\\nfromcentralitieswithmainlynegativeaffects,forthemoviere-view ofExterminating Angel . In this way, the positive versus\\nthenegativesideofthedocumentcanbeeasilyanalyzed.More-\\nover,groupsofsimilaraffectcategoriesareshownclosetoeach\\nother. This facilitates a quick overview of aspectsdenoted by\\nthosegroups.Forexample,inFig.2, clarity,creationandintel-\\nligenceare not as well developed as success,desire,pleasure,humorandexcitement . Groups of similar affect categories are\\ngenerated using the technique discussed in Section II-C.\\nWhenallaffectcategoriesfromFig.2areexpandedusingthe\\nfuzzythesaurus,weobtainthechartinFig.3.Thechartcontains\\nboth positive and negative affects, with a higher level of detail,sincenewaffectcategorieshavebeenaddedtothechartthroughexpansion.\\nIn Fig. 4 we show the affect structure of a news report con-\\ncerning a train crash in London. We show both centralities andintensities for qualities typical of news on accidents.\\nOpposingaffectcategoriescanbeplacedonoppositesidesof\\nthe circle with respect to the center point. This is illustrated in\\nFig.5,forcentralitiesina Matrixreview.Withthisarrangement,\\nwecaneasilyspotwhichpartofthecircleisbestdevelopedandunderstand its affective impact.'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='wecaneasilyspotwhichpartofthecircleisbestdevelopedandunderstand its affective impact.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. SUBASIC AND HUETTNER: AFFECT ANALYSIS OF TEXT USING FUZZY SEMANTIC TYPING 491\\nFig. 6. Centralities (up) and intensities in T. S. Eliot’s famous “Rhapsody on a Windy Night.” Even the surrealistic content of this poem can be succesf ully\\nanalyzed. The poem contains mixed affects of weakness, disadvantage, insanity andstrength, clarity andopenness . For intensities, clarity,weakness , and\\ninferiority seem to prevail.\\nAffect categories can be generated for sets of movies. We\\nanalyzed sets in the romance, action, science fiction, comedy,andfamilygenres.Eachsetcontainsabout15moviesandsome\\nmovies belong to multiple sets (for example, 12 Monkeys be-\\nlongsbothto theactionandsciencefiction genres).Theresultsare shown in Table II, along with profiles for news about acci-dents. The generated profiles confirm our expectations about\\naffect categories for different movie genres: romance movies\\nshowedhighlevelsof happiness ,innocence andjustice;action\\nmovies scored high on surprise,strengthandslyness; family\\nmovies emphasized sensitivity ,morality,responsibility and\\nnurturance ;andsoon.Thecompositeprofilesalsorevealsome\\nless obvious features: high levels of immorality andinferiority\\nin the romance movie set, and high levels of inferiority and'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='in the romance movie set, and high levels of inferiority and\\ndestruction in the comedies, for example. Although the results\\ninmanycasesrepresenttypicalcharacteristicsofmoviegenres,\\nwebelieve thatsome aspects of theresults may reflectonly theselected movies within the genre.\\nNext, we show that poetry can be analyzed using this ap-\\nproach. Fig. 6 shows centralities and intensities for a famous\\npoem with a very accurate affect profile.\\nAnother application of this technique is personalization.\\nWe generated affect profiles for different users based on theirmovie preferences and the results clearly reflect different\\npersonal tastes. The illustration in Fig. 7 shows the centrality\\naffectprofilesobtainedaftermergingprofilesforeachperson’sfavoritemovies.Inmergingcentralityaffectprofiles,weusethesame approach as when merging affect profiles of individual\\nwords or sentences, that is, the union (maximum) operator.\\nV. C\\nOMPUTING WITHAFFECTPROFILES\\nToillustratethecomputationalpotentialofthefuzzysemantic\\ntypingframework,wepresenthereanexampleofretrievalbasedon similarities drawn from affect categories. We illustrate thetechnique on retrieval of similar phrases, but it extends equallyTABLE II\\nCENTRALITY RESULTS BY TYPE\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. 492 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 9, NO. 4, AUGUST 2001\\nTABLE III\\nPHRASERETRIEVAL'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='TABLE III\\nPHRASERETRIEVAL\\nwelltoretrievaloflargerportionsoftexts,suchassentencesor\\nparagraphs. Here is the outline of the experiment from which\\nwe produced Table III.\\n1) We found the affect profile of the sentence fear, anger,\\ngrief and pain filled the room .\\n2) We retrieved all phrases from a precompiled list of 34\\nphrases, from four different documents that had affectprofiles most similar to the seed phrase. As a similaritymeasure between the affect profile of the seed phrase\\nandtheaffectprofileofacandidatephrase ,weuse[15]\\n(6)\\nwhere represents the sum of the intersections (min-\\nimum values) of the affect sets’ centralities for the respectiveaffectcategoriesand\\nrepresentsthesumoftheunions\\n(maximumvalues)oftheaffectsets’centralitiesfortherespec-\\ntive affect categories.\\nWe found phrases as presented in Table III, ordered by de-\\ncreasingscoresforsimilaritytotheseedphrase.Eachphraseisshownwithitsrespectiveaffectcentralityandintensityprofiles,\\nsimilarity degrees, and average intensities.This retrieval-by-similarity technique can be complemented\\nwith the filtering-on-intensity technique. For example, we can\\nset the intensity threshold to a certain category’s intensity, and\\nfilter out all expressions whose intensity for that category isgreaterthanthethresholdintensity.Alternatively,wecansetthethreshold on overall intensity and filter out all documents with\\nintensities higher than the given threshold.\\nSuch similarity computations can be conveniently combined'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='Such similarity computations can be conveniently combined\\nwith statistical methods. For short phrases like those shownhere, the similarity measure we used is very convenient since\\nit reflects the overlapping of qualitative features (affect cat-\\negories) without taking into account statistical features likenumber of words or categories. In certain cases, it may bereasonable to use a vector space model with cosine similarity\\nto find similar phrases. One must be careful, however, as that\\napproach will not always return the desired qualitative profile.To illustrate this point, let us imagine that we have submittedthe query love, pain to a text corpus and that we are searching\\nfor sentences that have both qualities—i.e., sentences that\\nmatch both loveandpain. However, vector space retrieval\\nwould return sentences containing many instances of lovewith\\na high score, even if there is no mention of painin them. Our\\nsimilarity computation, on the other hand, gives preference\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. SUBASIC AND HUETTNER: AFFECT ANALYSIS OF TEXT USING FUZZY SEMANTIC TYPING 493\\nFig. 7. Affect profiles generated from personal movie preferences. Each person submitted from 10 to 81 favorite movies. Individual movie profiles ar e merged\\nusing the maximum operator on centralities to yield the personal profiles in this figure.'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='using the maximum operator on centralities to yield the personal profiles in this figure.\\nto documents having higher-level maximal centralities for\\nboth categories, regardless of their total count in the sentence.\\nThis example clearly illustrates the main differences in thequantitative vs. the qualitative approach to loveandpain.\\nIn the quantitative approach, we are concerned with finding\\ninformation that is statistically similar to our query. In the\\nqualitative approach, we are more concerned with a particularqualitative profile of the target information.\\nVI. F\\nURTHERDEVELOPMENT\\nThe fuzzy semantic typing approach deals well with ambi-\\nguityandimprecisioninfreetext.Itcanbeeffectivelycombinedwithasetofvisualizationtoolsforeasy,accurateanalysisoftheaffect content in a document. These results are promising, but\\nwe feel that we have just begun exploration in uncharted terri-\\ntory. Our plans in the immediate future include:\\n1)Enrichment of the existing affect typing mech-\\nanism.To the extent that expressions in free text and\\ntheirconstituentwordshavesyntacticallyandsemanti-cally different roles, we are making an approximation\\nbyusingfuzzyuniontocomputecentralities.Although\\nindividual affect words are treated equally, their rolesare frequently uneven. For example, modifiers ( very,\\nmore or less, not ), comparative and superlative adjec-\\ntives, adverbial phrases, noun phrases, and complex\\nphrases (with and/orconnectives, etc.) all represent'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='phrases (with and/orconnectives, etc.) all represent\\ndifferentclassesofexpressionswithpossiblydifferentroles in a phrase or sentence. A finer-grained analysiswould, therefore, lead to different centrality combina-\\ntion operators for each of these classes.\\nThe same holds true for the hierarchical centrality\\ncomputation, but to a lesser extent, since text hier-archies are more regular and do not have as richly\\nvarying a structure. Still, for many documents, sen-\\ntences and paragraphs can be assigned weights thatreflect their relative importance in centrality compu-tations. For example, since report-style prose is typi-\\ncally “front-loaded,” a possible approach would be to\\nincrease the centrality of categories appearing in thetitle, summary, or lead paragraph of such texts.\\n2)Supportformanagementoflinguisticresources.\\nWe would like to begin experimenting with personal\\n(user-initiated)updatingofgeneral-purposeaffectlex-icons. This would include the modification of central-ities and intensities attached to affect categories, the\\naddition or removal of affect words, the definition of\\ncomplexaffectcategoriesintermsofbasicaffectcate-gories, and the tuning of the fuzzy thesaurus to reflectchanges in the affect lexicon. It would be especially\\ninterestingtoexperimentwithdifferentaffectlexicons\\non the sending and receiving sides of a communica-tion.Insuchanexperiment,amessagecomposedwithahelpofpersonalaffectlexicon\\nwouldbeinterpreted\\nusing personal lexicon .'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='wouldbeinterpreted\\nusing personal lexicon .\\n3)Generalization of the fuzzy typing framework.\\nFuzzy typing may be adapted to many different appli-cationareasbydevelopingappropriateresources: busi-\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. 494 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 9, NO. 4, AUGUST 2001\\nFig. 8. Basic browsing setup in TreeViewer. The tree view at upper right shows a browsable text hierarchy; the text pane at lower right displays analyze d text.\\nAffect centrality and affect intensity sets appear on the left. Circles represent average values.\\nness,food/cooking,fashion,architecturalstyles,cul-\\ntural events and artistic material, psychology-related\\nmaterial, wine and beer, perfumes .\\n4)Analysis of different points of view. Using an\\naffect lexicon, we can analyze affects in a text. Usinga lexicon with expressions and types that describe in-\\ntentions (e.g., would like to, will, is considering, is\\nthinking about ) would give us an intention finger-\\nprint for a document. Using both lexicons at thesame time, to find a text’s affectual and intentionalfingerprints, could reveal interesting juxtapositions in\\nthe data.\\n5)Integration with existing text management\\ntechniques. Although not currently included in our\\nsystem, quasistatistical information can be generatedto complement qualitative analysis. This would\\ninvolve a simple extension of common statistical'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='involve a simple extension of common statistical\\napproaches like statistical indexing, term-weighting,IDF-TF scoring and cosine distance [5], on afull feature space (all terms and phrases), or on\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. SUBASIC AND HUETTNER: AFFECT ANALYSIS OF TEXT USING FUZZY SEMANTIC TYPING 495\\nTABLE IV\\nTHECOMPLETE LIST OFAFFECTCATEGORIES AND RESPECTIVE OPPOSITEAFFECTCATEGORIES\\na significantly reduced feature space consisting\\nof a limited number of affect categories. Fuzzy\\ntyping would then be complemented with retrievaltechniques applied to affect profiles: e.g., queryingand retrieval, clustering and classification. We\\nbelieve the best way to approach this issue is to\\nstart with basic similarity assumptions as given in[11], and further investigate techniques from [12]and [15] for computing similarities between fuzzy\\nobjects. Finally, we hope to devise an algorithm that\\neffectively combines statistical and quasistatisticalsimilarity scores with fuzzy similarity scores tocompute the overall similarity of two texts.6)Extension to additional languages. While the\\nlexicon for our prototype system was created man-\\nually, we are currently experimenting with a moresystematic, semi-automatic collection method basedon WordNet [2]. If this technique proves successful,\\nit could be applied to foreign language semantic nets'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='it could be applied to foreign language semantic nets\\nsuch as EuroWordNet [1], to accommodate a varietyof other languages.\\nVII. C\\nONCLUSION\\nWe describe a novel approach to text analysis that combines\\nsemantic typing techniques from natural language processing\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. 496 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 9, NO. 4, AUGUST 2001\\nwith fuzzy techniques, under the common framework of fuzzy\\nsemantic typing . Fuzzy semantic typing is an innovative way\\nto capture metalinguistic facts about a text while allowing for\\nlinguistic ambiguity and vagueness. From our analysis of gen-\\nerated profiles for news reports and movie reviews, we believethat the metalinguistic representation can be usefully appliedin retrieval, clustering, and classification. The approach is ap-\\nplicable to an indefinite number of domains and lends itself to\\ncustomization for a particular user or task. We look forward tocontinuing our research in these directions.\\nA\\nPPENDIX A\\nSee Table IV.\\nAPPENDIX B\\nAFFECTINSPECTOR APPLICATION\\nInFig.8weshowanexamplescreenfromour affectinspector\\napplication,illustratingthedisplayofaprofile’scentralitiesand\\nintensities atdifferentlevelsof thetexthierarchy. TheessentialpartofthebrowsingsystemistheTreeViewer.ItisaJava/XMLapplication that contains a text hierarchy in the form of a tree.\\nEach node in the tree is clickable, and clicking on it displays'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='Each node in the tree is clickable, and clicking on it displays\\nthe affect profile associated with that node in two panes on theleft—the centrality profile in the upper pane, the intensity pro-file in the lower. The circles represent the average values for\\ncentralities (upper left) and intensities (lower left). The text as-\\nsociated with the node is displayed in the lower right pane.Browsing is possible at any level of the hierarchy from top tobottom:corpus,document,paragraph,sentence,affectcategory.\\nAffect profiles to the left are click-sensitive maps: by clicking\\noncertainpoints,onecansearchonindividualaffects,orchangethe form of the current display to see opposite category place-ments, linear placements, or placements ordered by decreasing\\nscores.Sucharrangementsfacilitateeasybrowsing,verification\\nofaffectprofiles,andcomparisonofaffectprofilesfordifferenttext elements, both vertically (affect-sentence-paragraph-docu-ment-corpus)andlaterally(twoormoredocuments,paragraphs\\nor sentences).\\nR\\nEFERENCES\\n[1] UniversityofAmsterdam,Dep.ComputionalLinguisticsEuroWordNet,\\n. (2000, May). [Online]. Available: http://www.hum.uva.nl/~ewn/\\n[2] C.Fellbaum,Ed., WordNet:AnElectronicLexicalDatabase :MITPress,\\n1998.\\n[3] C. Fillmore and B. T. S. Atkins, “FrameNet and lexicographic rele-\\nvance,”inProc.FirstInt.Conf.onLanguageResourcesandEvaluation, 1998, pp. 417–420.\\n[4] T.Fontanelle,“Semantictagging:Asurvey,”in PapersinComputational'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='[4] T.Fontanelle,“Semantictagging:Asurvey,”in PapersinComputational\\nLexicography, COMPLEX 99 , 1999, pp. 39–56.[5] D. A. Grossman and O. Frieder, Information Retrieval Algorithms and\\nHeuristics . Norwell, MA: Kluwer, 1998.\\n[6] R. Krovetz and W. B. Croft, “Lexical ambiguity and information re-\\ntrieval,”ACM Trans.Inform. Syst. , vol. 10,no. 2, pp.115–141, 1992.\\n[7] G. Millerand C. Walter, “Contextualcorrelates of semanticsimilarity,”\\nLanguage and Cognitive Processes , vol. 6, pp. 1–28, 1991.\\n[8] R.W.Picard, AffectiveComputing . Cambridge,MA:MITPress,1997.\\n[9] P.SubasicandA.Huettner,“Affectanalysisoftextusingfuzzysemantic\\ntyping,” presented at the Proc. of FUZZ-IEEE 2000, The 9th Interna-\\ntional Conference on Fuzzy Systems, San Antonio, Taxas, 2000.\\n[10] M. Sugeno, “On organization of imprecision based on word classifica-\\ntion,” in2nd FuzzySystem Symposium ,1986, pp.148–153. Japanese.\\n[11] A. Tversky, “Features of similarity,” Psychological Rev. 84 , pp.\\n327–352, 1977.\\n[12] L. A. Zadeh, “Similarity relations and fuzzy orderings,” in Inform. Sci.\\n3: Elsevier Sci., 1977, pp. 177–200.\\n[13]\\n,“Fuzzylogic /61computingwithwords,” IEEETrans.FuzzySyst. ,\\npp. 103–111, 1996.\\n[14] , “A new AI: Toward computational theory of perceptions,” AAAI\\nMagazine , vol. 22, no. 1, pp. 73–84, Spring 2001.\\n[15] R. Zwick, E. Carlstein, and D. V. Budescu, “Measures of similarity\\namong fuzzy concepts: A comparative analysis,” International Journal\\nof Approximate Reasoning , vol. 1, pp. 221–242, 1987.'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='of Approximate Reasoning , vol. 1, pp. 221–242, 1987.\\nPero Subasic (M’97) obtained the Dipl. Eng.\\nand M.S. degrees from the School of ElectricalEngineering, Begrade University, Yugoslavia, and\\nthe Dr. Eng. from Yamagata University, Japan, in\\n1989, 1993, and 1996, respectively\\nHehasbeenwiththeInstituteMihajloPupin,Bel-\\ngrade, Yugoslavia; Belgrade University, Yugoslavia;\\nTohoku University, Japan; Yamagata University,\\nJapan;and, the Tokyo Insitute of Technology, Japan,as a Faculty Member or Researcher. He is currently\\nwith Clairvoyance, Corporation, Pittsburgh, PA,\\nwherehehasworkedintextmining,navigationandvisualization,dataanalysis,\\npreprocessing,includingsemanticTypingandResourceManagement.Heisthe\\nPrincipal Inventor of the Fuzzy Semantic Typing framework.He haspublished\\nover 40 research papers and reports in international journals, monographs and\\nconferences. Heis author of abook on fuzzy systems and neuralnetworks.\\nDr.SubasicisamemberofJapanSocietyforFuzzySystems(ACMSIGCHI,\\nSOFT)andYugoslavSocietyforSoftComputingandIntelligentSystems(SO-COIS).\\nAlison Huettner received the Ph.D. in linguistics\\nfrom the University of Massachusetts, Amherst,\\nMA, in 1989.\\nIn 1998, she joined Clairvoyance Corporation,\\nPittsburgh, PA as a Project Manager working on\\nnatural language processing (NLP). She refined the\\nCLARIT NLP resources and experimented with\\nextensions,includingexpandedlexicalequivalences,\\nsemantic typing, specialized affect handling, and'),\n",
       " Document(metadata={'title': 'Affect Analysis of Text Using Fuzzy Semantic Typing', 'year': 2001}, page_content='extensions,includingexpandedlexicalequivalences,\\nsemantic typing, specialized affect handling, and\\na prototype question-answering system; currently,\\nshe is working with e-commerce applications. Prior\\nto joining Clairvoyance, she was a knowledge engineer with Carnegie Group,Inc., and worked on a fact-extraction system and a commercial machine\\ntranslation system. She is also one of the inventors of the associated patent for\\nan integrated authoring and translation system.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='arXiv:1803.07640v2  [cs.CL]  31 May 2018AllenNLP: A Deep Semantic Natural Language Processing Plat form\\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Prad eep Dasigi,\\nNelson F. Liu, Matthew Peters, Michael Schmitz, Luke Zettle moyer\\nAllen Institute for Artiﬁcial Intelligence\\nAbstract\\nModern natural language processing\\n(NLP) research requires writing code.\\nIdeally this code would provide a pre-\\ncise deﬁnition of the approach, easy\\nrepeatability of results, and a basis for\\nextending the research. However, many\\nresearch codebases bury high-level pa-\\nrameters under implementation details,\\nare challenging to run and debug, and are\\ndifﬁcult enough to extend that they are\\nmore likely to be rewritten. This paper\\ndescribes AllenNLP, a library for applying\\ndeep learning methods to NLP research,\\nwhich addresses these issues with easy-\\nto-use command-line tools, declarative\\nconﬁguration-driven experiments, and\\nmodular NLP abstractions. AllenNLP\\nhas already increased the rate of research\\nexperimentation and the sharing of NLP\\ncomponents at the Allen Institute for\\nArtiﬁcial Intelligence, and we are working\\nto have the same impact across the ﬁeld.\\n1 Introduction\\nNeural network models are now the state-of-the-\\nart for a wide range of tasks such as text classiﬁ-\\ncation ( Howard and Ruder ,2018 ), machine trans-\\nlation ( Vaswani et al. ,2017 ), semantic role label-\\ning (Zhou and Xu ,2015 ;He et al. ,2017 ), corefer-\\nence resolution ( Lee et al. ,2017a ), and semantic'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='ence resolution ( Lee et al. ,2017a ), and semantic\\nparsing ( Krishnamurthy et al. ,2017 ). However it\\ncan be surprisingly difﬁcult to tune new models\\nor replicate existing results. State-of-the-art deep\\nlearning models often take over a week to train\\non modern GPUs and are sensitive to initialization\\nand hyperparameter settings. Furthermore, ref-\\nerence implementations often re-implement NLP\\ncomponents from scratch and make it difﬁcult toreproduce results, creating a barrier to entry for\\nresearch on many problems.\\nAllenNLP, a platform for research on deep\\nlearning methods in natural language processing,\\nis designed to address these problems and to sig-\\nniﬁcantly lower barriers to high quality NLP re-\\nsearch by\\n•implementing useful NLP abstractions that\\nmake it easy to write higher-level model code\\nfor a broad range of NLP tasks, swap out\\ncomponents, and re-use implementations,\\n•handling common NLP deep learning prob-\\nlems, such as masking and padding, and\\nkeeping these low-level details separate from\\nthe high-level model and experiment deﬁni-\\ntions,\\n•deﬁning experiments using declarative con-\\nﬁguration ﬁles, which provide a high-level\\nsummary of a model and its training, and\\nmake it easy to change the deep learning ar-\\nchitecture and tune hyper-parameters, and\\n•sharing models through live demos, making\\ncomplex NLP accessible and debug-able.\\nThe AllenNLP website1provides tutorials,\\nAPI documentation, pretrained models, and\\nsource code2. The AllenNLP platform has a per-'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='API documentation, pretrained models, and\\nsource code2. The AllenNLP platform has a per-\\nmissive Apache 2.0 license and is easy to down-\\nload and install via pip, a Docker image, or cloning\\nthe GitHub repository. It includes reference im-\\nplementations for recent state-of-the-art models\\n(see Section 3) that can be easily run (to make\\npredictions on arbitrary new inputs) and retrained\\nwith different parameters or on new data. These\\npretrained models have interactive online demos3\\n1http://allennlp.org/\\n2http://github.com/allenai/allennlp\\n3http://demo.allennlp.org/with visualizations to help interpret model deci-\\nsions and make predictions accessible to others.\\nThe reference implementations provide examples\\nof the framework functionality (Section 2) and\\nalso serve as baselines for future research.\\nAllenNLP is an ongoing open-source effort\\nmaintained by several full-time engineers and re-\\nsearchers at the Allen Institute for Artiﬁcial Intel-\\nligence, as well as interns from top PhD programs\\nand contributors from the broader NLP commu-\\nnity. It is used widespread internally for research\\non common sense, logical reasoning, and state-\\nof-the-art NLP components such as: constituency\\nparsers, semantic parsing, and word representa-\\ntions. AllenNLP is gaining traction externally and\\nwe want to invest to make it the standard for ad-\\nvancing NLP research using PyTorch.\\n2 Library Design\\nAllenNLP is a platform designed speciﬁcally for\\ndeep learning and NLP research. AllenNLP is'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='AllenNLP is a platform designed speciﬁcally for\\ndeep learning and NLP research. AllenNLP is\\nbuilt on PyTorch ( Paszke et al. ,2017 ), which pro-\\nvides many attractive features for NLP research.\\nPyTorch supports dynamic networks, has a clean\\n“Pythonic” syntax, and is easy to use.\\nThe AllenNLP library provides (1) a ﬂexible\\ndata API that handles intelligent batching and\\npadding, (2) high-level abstractions for common\\noperations in working with text, and (3) a modular\\nand extensible experiment framework that makes\\ndoing good science easy.\\nAllenNLP maintains\\na high test coverage of over 90%4to ensure\\nits components and models are working as in-\\ntended. Library features are built with testability\\nin mind so new components can maintain a similar\\ntest coverage.\\n2.1 Text Data Processing\\nAllenNLP’s data processing API is built around\\nthe notion of Field s. EachField represents a\\nsingle input array to a model. Field s are grouped\\ntogether in Instance s that represent the exam-\\nples for training or prediction.\\nTheField API is ﬂexible and easy to extend,\\nallowing for a uniﬁed data API for tasks as diverse\\nas tagging, semantic role labeling, question an-\\nswering, and textual entailment. To represent the\\nSQuAD dataset ( Rajpurkar et al. ,2016 ), for exam-\\nple, which has a question and a passage as inputs\\n4https://codecov.io/gh/allenai/allennlpand a span from the passage as output, each train-\\ningInstance comprises a TextField for the\\nquestion, a TextField for the passage, and a'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='ingInstance comprises a TextField for the\\nquestion, a TextField for the passage, and a\\nSpanField representing the start and end posi-\\ntions of the answer in the passage.\\nThe user need only read data into a set of\\nInstance objects with the desired ﬁelds, and the\\nlibrary can automatically sort them into batches\\nwith similar sequence lengths, pad all sequences\\nin each batch to the same length, and randomly\\nshufﬂe the batches for input to a model.\\n2.2 NLP-Focused Abstractions\\nAllenNLP provides a high-level API for building\\nmodels, with abstractions designed speciﬁcally for\\nNLP research. By design, the code for a model\\nactually speciﬁes a class of related models. The\\nresearcher can then experiment with various ar-\\nchitectures within this class by simply changing\\na conﬁguration ﬁle, without having to change any\\ncode.\\nThe library has many abstractions that encap-\\nsulate common decision points in NLP models.\\nKey examples are: (1) how text is represented as\\nvectors, (2) how vector sequences are modiﬁed to\\nproduce new vector sequences, (3) how vector se-\\nquences are merged into a single vector.\\nTokenEmbedder: This abstraction takes in-\\nput arrays generated by e.g. a TextField and\\nreturns a sequence of vector embeddings. Through\\nthe use of polymorphism and AllenNLP’s exper-\\niment framework (see Section 2.3), researchers\\ncan easily switch between a wide variety of pos-\\nsible word representations. Simply by changing\\na conﬁguration ﬁle, an experimenter can choose'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='sible word representations. Simply by changing\\na conﬁguration ﬁle, an experimenter can choose\\nbetween pre-trained word embeddings, word em-\\nbeddings concatenated with a character-level CNN\\nencoding, or even pre-trained model token-in-\\ncontext embeddings ( Peters et al. ,2017 ), which\\nallows for easy controlled experimentation.\\nSeq2SeqEncoder: A common operation in\\ndeep NLP models is to take a sequence of word\\nvectors and pass them through a recurrent net-\\nwork to encode contextual information, produc-\\ning a new sequence of vectors as output. There\\nis a large number of ways to do this, includ-\\ning LSTMs ( Hochreiter and Schmidhuber ,1997 ),\\nGRUs ( Cho et al. ,2014 ), intra-sentence atten-\\ntion ( Cheng et al. ,2016 ), recurrent additive net-\\nworks ( Lee et al. ,2017b ), and many more. Al-\\nlenNLP’sSeq2SeqEncoder abstracts away thedecision of which particular encoder to use, allow-\\ning the user to build an encoder-agnostic model\\nand specify the encoder via conﬁguration. In this\\nway, a researcher can easily explore new recur-\\nrent architectures; for example, they can replace\\nthe LSTMs in any model that uses this abstrac-\\ntion with any other encoder, measuring the impact\\nacross a wide range of models and tasks.\\nSeq2VecEncoder: Another common op-\\neration in NLP models is to merge a sequence\\nof vectors into a single vector, using either a\\nrecurrent network with some kind of averaging\\nor pooling, or using a convolutional network.\\nThis operation is encapsulated in AllenNLP by a'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='or pooling, or using a convolutional network.\\nThis operation is encapsulated in AllenNLP by a\\nSeq2VecEncoder . This abstraction again al-\\nlows the model code to only describe a class of\\nsimilar models, with particular instantiations of\\nthat model class being determined by a conﬁgu-\\nration ﬁle.\\nSpanExtractor: A recent trend in NLP\\nis to build models that operate on spans of\\ntext, instead of on tokens . State-of-the-art mod-\\nels for coreference resolution ( Lee et al. ,2017a ),\\nconstituency parsing ( Stern et al. ,2017 ), and se-\\nmantic role labeling ( He et al. ,2017 ) all op-\\nerate in this way. Support for building this\\nkind of model is built into AllenNLP, including\\naSpanExtractor abstraction that determines\\nhow span vectors get computed from sequences of\\ntoken vectors.\\n2.3 Experimental Framework\\nThe primary design goal of AllenNLP is to make\\nit easy to do good science with controlled exper-\\niments. Because of the abstractions described in\\nSection 2.2, large parts of the model architecture\\nand training-related hyper-parameters can be con-\\nﬁgured outside of model code. This makes it easy\\nto clearly specify the important decisions that de-\\nﬁne a new model in conﬁguration, and frees the\\nresearcher from needing to code all of the imple-\\nmentation details from scratch.\\nThis architecture design is accomplished in Al-\\nlenNLP using a HOCON5conﬁguration ﬁle that\\nspeciﬁes, e.g., which text representations and en-\\ncoders to use in an experiment. The mapping from'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='speciﬁes, e.g., which text representations and en-\\ncoders to use in an experiment. The mapping from\\nstrings in the conﬁguration ﬁle to instantiated ob-\\njects in code is done through the use of a registry ,\\nwhich allows users of the library to add new im-\\n5We use it as JSON with comments. See\\nhttps://github.com/lightbend/conﬁg/blob/master/HOCO N.md\\nfor the full spec.plementations of any of the provided abstractions,\\nor even to create their own new abstractions.\\nWhile some entries in the conﬁguration ﬁle are\\noptional, many are required and if unspeciﬁed\\nAllenNLP will raise a ConﬁgurationError when\\nreading the conﬁguration. Additionally, when a\\nconﬁguration ﬁle is loaded, AllenNLP logs the\\nconﬁguration values, providing a record of both\\nspeciﬁed and default parameters for your model.\\n3 Reference Models\\nAllenNLP includes reference implementations\\nof widely used language understanding models.\\nThese models demonstrate how to use the frame-\\nwork functionality presented in Section 2. They\\nalso have veriﬁed performance levels that closely\\nmatch the original results, and can serve as com-\\nparison baselines for future research.\\nAllenNLP includes reference implementations\\nfor several tasks, including:\\n•Semantic Role Labeling (SRL) models re-\\ncover the latent predicate argument structure\\nof a sentence ( Palmer et al. ,2005 ). SRL\\nbuilds representations that answer basic ques-\\ntions about sentence meaning; for example,\\n“who” did “what” to “whom.” The Al-'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='tions about sentence meaning; for example,\\n“who” did “what” to “whom.” The Al-\\nlenNLP SRL model is a re-implementation\\nof a deep BiLSTM model ( He et al. ,2017 ).\\nThe implemented model closely matches the\\npublished model which was state of the art\\nin 2017, achieving a F1 of 78.9% on En-\\nglish Ontonotes 5.0 dataset using the CoNLL\\n2011/12 shared task format.\\n•Machine Comprehension (MC) systems\\ntake an evidence text and a question as in-\\nput, and predict a span within the evidence\\nthat answers the question. AllenNLP in-\\ncludes a reference implementation of the\\nBiDAF MC model ( Seo et al. ,2017 ) which\\nwas state of the art for the SQuAD bench-\\nmark ( Rajpurkar et al. ,2016 ) in early 2017.\\n•Textual Entailment (TE) models take a pair\\nof sentences and predict whether the facts\\nin the ﬁrst necessarily imply the facts in\\nthe second. The AllenNLP TE model is a\\nre-implementation of the decomposable at-\\ntention model ( Parikh et al. ,2016 ), a widely\\nused TE baseline that was state-of-the-art on\\nthe SNLI dataset ( Bowman et al. ,2015 ) inlate 2016. The AllenNLP TE model achieves\\nan accuracy of 86.4% on the SNLI 1.0 test\\ndataset, a 2% improvement on most publicly\\navailable implementations and a similar score\\nas the original paper. Rather than pre-trained\\nGlove vectors, this model uses ELMo embed-\\ndings ( Peters et al. ,2018 ), which are com-\\npletely character based and account for the\\n2% improvement.\\n•AConstituency Parser breaks a text into\\nsub-phrases, or constituents. Non-terminals'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='•AConstituency Parser breaks a text into\\nsub-phrases, or constituents. Non-terminals\\nin the tree are types of phrases and the ter-\\nminals are the words in the sentence. The\\nAllenNLP constituency parser is an imple-\\nmentation of a minimal neural model for\\nconstituency parsing based on an indepen-\\ndent scoring of labels and spans ( Stern et al. ,\\n2017 ). This model uses ELMo embed-\\ndings ( Peters et al. ,2018 ), which are com-\\npletely character based and improves single\\nmodel performance from 92.6 F1 to 94.11 F1\\non the Penn Tree bank, a 20% relative error\\nreduction.\\nAllenNLP also includes a token embedder that\\nuses pre-trained ELMo ( Peters et al. ,2018 ) repre-\\nsentations. ELMo is a deep contextualized word\\nrepresentation that models both complex charac-\\nteristics of word use (e.g., syntax and semantics)\\nand how these uses vary across linguistic contexts\\n(in order to model polysemy). ELMo embeddings\\nsigniﬁcantly improve the state of the art across a\\nbroad range of challenging NLP problems, includ-\\ning question answering, textual entailment, and\\nsentiment analysis.\\nAdditional models are currently under de-\\nvelopment and are regularly released, includ-\\ning semantic parsing ( Krishnamurthy et al. ,\\n2017 ) and multi-paragraph reading comprehen-\\nsion ( Clark and Gardner ,2017 ). We expect the\\nnumber of tasks and reference implementations\\nto grow steadily over time. The most up-to-\\ndate list of reference models is maintained at\\nhttp://allennlp.org/models.\\n4 Related Work'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='date list of reference models is maintained at\\nhttp://allennlp.org/models.\\n4 Related Work\\nMany existing NLP pipelines, such as Stan-\\nford CoreNLP ( Manning et al. ,2014 ) and spaCy6,\\nfocus on predicting linguistic structures rather\\n6https://spacy.io/than modeling NLP architectures. While Al-\\nlenNLP supports making predictions using pre-\\ntrained models, its core focus is on enabling\\nnovel research. This emphasis on conﬁgur-\\ning parameters, training, and evaluating is simi-\\nlar to Weka ( Witten and Frank ,1999 ) or Scikit-\\nlearn ( Pedregosa et al. ,2011 ), but AllenNLP fo-\\ncuses on cutting-edge research in deep learning\\nand is designed around declarative conﬁguration\\nof model architectures in addition to model param-\\neters.\\nMost existing deep-learning toolkits\\nare designed for general machine learn-\\ning ( Bergstra et al. ,2010 ;Yu et al. ,2014 ;\\nChen et al. , 2015 ; Abadi et al. , 2016 ;\\nNeubig et al. ,2017 ), and can require signiﬁ-\\ncant effort to develop research infrastructure\\nfor particular model classes. Some, such as\\nKeras ( Chollet et al. ,2015 ), do aim to make it\\neasy to build deep learning models. Similar to\\nhow AllenNLP is an abstraction layer on top of\\nPyTorch, Keras provides high-level abstractions\\non top of static graph frameworks such as Tensor-\\nFlow. While Keras’ abstractions and functionality\\nare useful for general machine learning, they\\nare somewhat lacking for NLP, where input data\\ntypes can be very complex and dynamic graph'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='are somewhat lacking for NLP, where input data\\ntypes can be very complex and dynamic graph\\nframeworks are more often necessary.\\nFinally, AllenNLP is related to toolkits for\\ndeep learning research in dialog ( Miller et al. ,\\n2017 ) and machine translation ( Klein et al. ,2017 ).\\nThose toolkits support learning general functions\\nthat map strings (e.g. foreign language text or user\\nutterances) to strings (e.g. English text or sys-\\ntem responses). AllenNLP, in contrast, is a more\\ngeneral library for building models for any kind\\nof NLP task, including text classiﬁcation, con-\\nstituency parsing, textual entailment, question an-\\nswering, and more.\\n5 Conclusion\\nThe design of AllenNLP allows researchers to fo-\\ncus on the high-level summary of their models\\nrather than the details, and to do careful, repro-\\nducible research. Internally at the Allen Insti-\\ntute for Artiﬁcial Intelligence the library is widely\\nadopted and has improved the quality of our re-\\nsearch code, spread knowledge about deep learn-\\ning, and made it easier to share discoveries be-\\ntween teams. AllenNLP is gaining traction exter-\\nnally and is growing an open-source communityof contributors7. The AllenNLP team is com-\\nmitted to continuing work on this library in or-\\nder to enable better research practices throughout\\nthe NLP community and to build a community of\\nresearchers who maintain a collection of the best\\nmodels in natural language processing.\\nReferences\\nMart´ ın Abadi, Ashish Agarwal, Paul Barham, Eugene'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='References\\nMart´ ın Abadi, Ashish Agarwal, Paul Barham, Eugene\\nBrevdo, Zhifeng Chen, Craig Citro, Greg S Cor-\\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\\net al. 2016. Tensorﬂow: Large-scale machine learn-\\ning on heterogeneous distributed systems. CoRR\\nabs/1603.04467 .\\nJames Bergstra, Olivier Breuleux, Fr´ ed´ eric Bastien,\\nPascal Lamblin, Razvan Pascanu, Guillaume Des-\\njardins, Joseph Turian, David Warde-Farley, and\\nYoshua Bengio. 2010. Theano: A cpu and gpu math\\ncompiler in python. In Proc. 9th Python in Science\\nConf , pages 1–7.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large an-\\nnotated corpus for learning natural language infer-\\nence. In Proceedings of the Conference on Em-\\npirical Methods in Natural Language Processing\\n(EMNLP) . Association for Computational Linguis-\\ntics.\\nTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang,\\nMinjie Wang, Tianjun Xiao, Bing Xu, Chiyuan\\nZhang, and Zheng Zhang. 2015. Mxnet: A ﬂexible\\nand efﬁcient machine learning library for heteroge-\\nneous distributed systems. CoRR abs/1512.01274 .\\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016.\\nLong short-term memory-networks for machine\\nreading. In Proceedings of the Conference on Em-\\npirical Methods in Natural Language Processing\\n(EMNLP) .\\nKyunghyun Cho, Bart van Merrienboer, C ¸ aglar\\nG¨ ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio. 2014.'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='G¨ ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio. 2014.\\nLearning phrase representations using RNN encoder-decode r for statistical machine translation .\\nInProceedings of the Conference on Empiri-\\ncal Methods in Natural Language Processing,\\n(EMNLP) , pages 1724–1734.\\nFranc ¸ois Chollet et al. 2015. Keras.\\nhttps://keras.io .\\nChristopher T Clark and Matthew Gardner. 2017. Sim-\\nple and effective multi-paragraph reading compre-\\nhension. CoRR , abs/1710.10723.\\n7See GitHub stars and issues\\non https://github.com/allenai/allennlp\\nand mentions from publications at\\nhttps://www.semanticscholar.org/search?q=allennlp.Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-\\nmoyer. 2017. Deep semantic role labeling: What\\nworks and whats next. In Proceedings of the Asso-\\nciation for Computational Linguistics (ACL) .\\nSepp Hochreiter and J¨ urgen Schmidhuber. 1997.\\nLong short-term memory. Neural computation ,\\n9(8):1735–1780.\\nJeremy Howard and Sebastian Ruder. 2018. Fine-\\ntuned language models for text classiﬁcation. CoRR ,\\nabs/1801.06146.\\nGuillaume Klein, Yoon Kim, Yuntian Deng,\\nJean Senellart, and Alexander M. Rush. 2017.\\nOpennmt: Open-source toolkit for neural machine translati on.\\nInProceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics, ACL ,\\npages 67–72.\\nJayant Krishnamurthy, Pradeep Dasigi, and Matthew\\nGardner. 2017. Neural semantic parsing with type\\nconstraints for semi-structured tables. In EMNLP .'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='Gardner. 2017. Neural semantic parsing with type\\nconstraints for semi-structured tables. In EMNLP .\\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\\nmoyer. 2017a. End-to-end neural coreference res-\\nolution. In Proceedings of the Conference on Em-\\npirical Methods in Natural Language Processing\\n(EMNLP) .\\nKenton Lee, Omer Levy, and Luke Zettlemoyer.\\n2017b. Recurrent additive networks. CoRR\\nabs/1705.07393 .\\nChristopher D Manning, Mihai Surdeanu, John Bauer,\\nJenny Rose Finkel, Steven Bethard, and David Mc-\\nClosky. 2014. The stanford corenlp natural language\\nprocessing toolkit. In Proceedings of the Associ-\\nation for Computational Linguistics (ACL) (System\\nDemonstrations) .\\nAlexander H. Miller, Will Feng, Dhruv Ba-\\ntra, Antoine Bordes, Adam Fisch, Jiasen\\nLu, Devi Parikh, and Jason Weston. 2017.\\nParlai: A dialog research software platform . In\\nProceedings of the Conference on Empirical Meth-\\nods in Natural Language Processing, EMNLP ,\\npages 79–84.\\nGraham Neubig, Chris Dyer, Yoav Goldberg, Austin\\nMatthews, Waleed Ammar, Antonios Anastasopou-\\nlos, Miguel Ballesteros, David Chiang, Daniel\\nClothiaux, Trevor Cohn, et al. 2017. Dynet:\\nThe dynamic neural network toolkit. CoRR\\nabs/1701.03980 .\\nMartha Palmer, Daniel Gildea, and Paul Kingsbury.\\n2005. The proposition bank: An annotated cor-\\npus of semantic roles. Computational linguistics ,\\n31(1):71–106.\\nAnkur P Parikh, Oscar T¨ ackstr¨ om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='Jakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In Proceed-\\nings of the Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) .Adam Paszke, Sam Gross, Soumith Chintala, Gre-\\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\\ning Lin, Alban Desmaison, Luca Antiga, and Adam\\nLerer. 2017. Automatic differentiation in pytorch.\\nInNIPS-W .\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\\nB. Thirion, O. Grisel, M. Blondel, P. Pretten-\\nhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Pas-\\nsos, D. Cournapeau, M. Brucher, M. Perrot, and\\nE. Duchesnay. 2011. Scikit-learn: Machine learning\\nin Python. Journal of Machine Learning Research ,\\n12:2825–2830.\\nMatthew E Peters, Waleed Ammar, Chandra Bhagavat-\\nula, and Russell Power. 2017. Semi-supervised se-\\nquence tagging with bidirectional language models.\\nInProceedings of the Association for Computational\\nLinguistics (ACL) .\\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\\nMatthew Gardner, Christopher T Clark, Kenton Lee,\\nand Luke S. Zettlemoyer. 2018. Deep contextual-\\nized word representations. CoRR , abs/1802.05365.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) .\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017. Bidirectional attention\\nﬂow for machine comprehension.'),\n",
       " Document(metadata={'title': 'AllenNLP: A Deep Semantic Natural Language Processing Platform', 'year': 2018}, page_content='Hannaneh Hajishirzi. 2017. Bidirectional attention\\nﬂow for machine comprehension.\\nMitchell Stern, Jacob Andreas, and Dan Klein. 2017. A\\nminimal span-based neural constituency parser. In\\nACL.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In NIPS .\\nIan H. Witten and Eibe Frank. 1999. Data mining:\\nPractical machine learning tools and techniques with\\njava implementations.\\nDong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao,\\nZhiheng Huang, Brian Guenter, Oleksii Kuchaiev,\\nYu Zhang, Frank Seide, Huaming Wang, et al. 2014.\\nAn introduction to computational networks and the\\ncomputational network toolkit. Microsoft Technical\\nReport MSR-TR-2014–112 .\\nJie Zhou and Wei Xu. 2015. End-to-end learning of\\nsemantic role labeling using recurrent neural net-\\nworks. In Proceedings of the Association for Com-\\nputational Linguistics (ACL) .'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 4, JULY 2005 959\\nAmbiguity Resolution Analysis in Incremental\\nParsing of Natural Language\\nFabrizio Costa, Paolo Frasconi , Member, IEEE , Vincenzo Lombardo, Patrick Sturt, and Giovanni Soda , Member, IEEE\\nAbstract— Incremental parsing gains its importance in natural\\nlanguage processing and psycholinguistics because of its cognitive'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='language processing and psycholinguistics because of its cognitive\\nplausibility. Modeling the associated cognitive data structures, andtheir dynamics, can lead to a better understanding of the humanparser. In earlier work, we have introduced a recursive neural net-work (RNN) capable of performing syntactic ambiguity resolutionin incremental parsing. In this paper, we report a systematic anal-ysis of the behavior of the network that allows us to gain importantinsights about the kind of information that is exploited to resolvedifferent forms of ambiguity. In attachment ambiguities, in whicha new phrase can be attached at more than one point in the syn-tactic left context, we found that learning from examples allows usto predict the location of the attachment point with high accuracy,while the discrimination amongst alternative syntactic structureswith the same attachment point is slightly better than making a de-cision purely based on frequencies. We also introduce several newideas to enhance the architectural design, obtaining signiﬁcant im-provements of prediction accuracy, up to 25% error reduction onthe same dataset used in previous work. Finally, we report largescale experiments on the entire Wall Street Journal section of thePenn Treebank. The best prediction accuracy of the model on thislarge dataset is 87.6%, a relative error reduction larger than 50%compared to previous results.\\nIndex Terms— First-pass attachment, incremental parsing,'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='Index Terms— First-pass attachment, incremental parsing,\\nlearning preferences, recursive neural networks (RNNs), struc-tured data.\\nI. INTRODUCTION\\nTHE incremental strategy is a largely accepted hypothesis\\nabout the human mechanism of syntactic processing\\nin language comprehension. Under this model, processingproceeds from left to right, and each input word is assigned a\\nstructural position as it is being read [1]. The incrementality\\nhypothesis is supported by several experimental studies thatdemonstrate how humans are able to assign a meaning to “al-most any” initial (left) fragment of a sentence [2], that is, they\\nare capable of anticipating syntactic and semantic decisions\\nbefore reaching the end of the sentence [3]–[5]. In particular,under the strong incrementality framework (assumed in this\\npaper), humans maintain a totally connected parse tree while\\nscanning the input words from left to right, with no input stored\\nin a disconnected state [6].\\nManuscript received March 25, 2003; revised August 26, 2004.\\nF. Costa, P. Frasconi, and G. Soda are with the Dipartimento di Sistemi e Infor-\\nmatica, Università di Firenze, 50139 Firenze, Italy (e-mail: costa@dsi.uniﬁ.it;\\npaolo@dsi.uniﬁ.it; giovanni@dsi.uniﬁ.it).\\nV . Lombardo is with the Dipartimento di Informatica, Università degli Studi\\ndi Torino 10149 Torino, Italy (e-mail: vincenzo@di.unito.it).\\nP.Sturt is with the Human Communication Research Center, University of\\nGlasgow, Glasgow G12 8QB, U.K. (e-mail: patrick@psy.gla.ac.uk).'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='Glasgow, Glasgow G12 8QB, U.K. (e-mail: patrick@psy.gla.ac.uk).\\nDigital Object Identiﬁer 10.1109/TNN.2005.849837Although well accepted in the psycholinguistic community,\\nincremental processing has received relatively modest attention\\nin the computational linguistic community. In this direction,Roark and Johnson [7] have proposed a top-down left-cornerprobabilistic parser that uses a probabilistic best-ﬁrst strategy\\nand a beam-search heuristic to avoid the non termination prob-\\nlems typical of top-down predictive parsers. Their parser pro-ceeds incrementally from left to right, with one item of look-ahead, and maintains a fully connected tree spans the left context\\nand is used to extract nonlocal dependency information. With\\nregards to connectionist architectures, Lane and Henderson [8]have proposed simple synchrony networks and applied them to a\\nsmall scale parsing problem. Their approach combines temporal\\nsynchrony variable binding with simple recurrent networks inorder to output representations of tree structures. More recentwork by Henderson [9] has used neural networks to estimate the\\nparameters of a generative model, resulting in a wide-coverage\\nstatistical parser with state-of-the-art performance. Earlier con-nectionist parsing models tackling smaller-scale problems haveincluded [10] [11][12]— [13].\\nIn this paper, we focus on the development of machine'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='In this paper, we focus on the development of machine\\nlearning methods for ambiguity resolution in ﬁrst-pass at-tachment. By “ﬁrst-pass attachment,” we mean the processof initially combining a new input word with the developing\\nphrase structure analysis of a sentence. This process can be\\nviewed as one component of a full parsing model, in which wewould also need to implement a way of keeping track of alter-native analysis, and recovering them if necessary. The ﬁrst-pass\\nattachment problem is mainly important in psycholinguistics,\\nwhere much of the experimental research involves testing initialpreferences for ambiguity resolution during the processing of awritten or auditorily presented sentence. However, a solution to\\nthe ﬁrst-pass attachment problem could help in building com-\\nputational tools whose behavior is closer to that of humans inambiguous situations. To gain some intuition on the ambiguityresolution problem and why statistical regularities may play\\nan important role, let us consider the ambiguous sentence “the\\nservant of the actress who was on the balcony died.” Cuetosand Mitchell [14] report evidence that English and Spanishspeakers have a different preferential bias in attaching the\\nambiguous relative clause. In particular, English speakers are\\nmore likely to support the interpretation in which the actresswas on the balcony (attaching the clause to the most recentnoun), while for the Spanish translation of the sentence native'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='speakers are more likely to support the interpretation in which\\nthe servant was on the balcony (attaching the clause to the lessrecent noun). Mitchell et al. [15] showed that preferences for\\nthis type of ambiguity could be modulated through exposure,\\n1045-9227/$20.00 © 2005 IEEE\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 960 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 4, JULY 2005\\nand proposed that the difference can be explained as a conse-\\nquence of different structural frequencies in the two languages,independently of lexical preferences. They have formulated\\nthetuning hypothesis , according to which purely structural\\nstatistical regularities determine the earliest stages of syntacticambiguity resolution in human parsing.\\nIn [16], we have proposed a computational model in the at-\\ntempt of verifying the previous hypotheses with the help of ma-\\nchine learning. As revised in Sections II and III, our methodis based on a dynamic grammar as a model of strong incre-mental parsing. States of the dynamic grammar consist of in-\\ncremental trees, i.e., the substructures\\nof a parse tree\\n that\\nspan the ﬁrst words\\n in a sentence. The graph\\ndifference between two consecutive incremental trees\\n and\\nis called in this framework a connection path and can be\\nseen as the syntactic structure that must be added to the in-\\ncremental tree spanning\\n in order to attach the\\nnext word'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='cremental tree spanning\\n in order to attach the\\nnext word\\n . Transitions in this grammar are generated by ex-\\ntracting a set of connection paths from a treebank (using the\\nalgorithm proposed in [17]). Under this model, ambiguity res-\\nolution consists of choosing the correct transition to be appliedat each step in order to continue parsing. The problem can beconveniently modeled as a preference learning task in which\\ninstances consist of alternative incremental trees (each repre-\\nsenting a valid continuation). We propose a recursive neural net-work (RNN) [18], [19] to learn this preference task.\\n1Results\\nin [16] support in a quantitative way the psycholinguistic hy-\\npotheses that structural learning plays a signi ﬁcant role for dis-\\nambiguation. In particular, we found that the RNN trained ona relatively large collection of parse trees (extracted from thePenn treebank [21]) was actually able to reproduce some inter-\\nesting patterns of human syntactic disambiguation on new sen-\\ntences. There have been very few other psycholinguistic modelsof parsing that combine connectionist methods with symbolicrepresentations of syntactic structure in this way. Some excep-\\ntions are the hybrid models proposed by Stevenson [22] and\\nV osse and Kempen [23], both involving the creation of the syn-tactic structure through a network-based process of competitiveactivation. However, unlike [16], these models do not employ\\nconnectionist learning techniques, and are not designed to be'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='connectionist learning techniques, and are not designed to be\\nused in a wide-coverage setting.\\nIn this paper, we report about new results obtained on a sig-\\nniﬁcantly larger data set than that used in [16], and we present a\\nthorough analysis of the properties of the trained network. After\\nperforming several statistical tests that correlate structural fea-tures of the input to the generalization error, we ﬁnd that the\\nlearned solution consistently assigns higher scores to simpler\\nand more frequent structures. Moreover, we ﬁnd that relevant\\nsignals tend to concentrate near the anchoring point between anincremental tree and a connection path. Interestingly, this ob-served behavior can be exploited to improve the design of the\\npredictor by selectively pruning nodes that are too distant from\\nthe candidate attachment points. Selection of relevant portionsof a tree when learning in structured domains can be seen as a\\n1In [20], Collins and Duffy proposed a kernel-based approach for solving a\\nrelated preference learning problem over sets of syntactic trees. In their paper,\\nalternatives consist of complete parse trees for a sentence that are given a highscore by a statistical parser.counterpart of attribute selection for attribute-value data. Here\\nsubstructure selection is driven partly from domain knowledgeand partly from the analysis of prediction errors. Domain parti-\\ntioning is a second technique that we ﬁnd useful to inject prior'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='tioning is a second technique that we ﬁnd useful to inject prior\\nknowledge and to boost prediction accuracy. More precisely,we propose to specialize separate networks on different domainsplits, according to the grammatical category of the word to be\\nattached. All these enhancements produce a signi ﬁcant accuracy\\nimprovement over the previous architecture.\\nThe rest of the paper is organized as follows. In Section II,\\nwe review the incremental dynamic grammar and we formulate\\ndisambiguation as a preference learning task. In Section III, we\\nbrieﬂy revise RNNs for learning preferences on syntactic struc-\\ntures. In Section IV we describe and characterize the data set. InSection V , we study the main properties of the trained network\\nand we correlate prediction error to the structural properties of\\nthe input trees. In Section VI, we describe the enhanced archi-tecture and report the wide coverage experiments.\\nII. I\\nNCREMENTAL DYNAMIC GRAMMAR MODEL\\nIn this section, we give some basic concepts related to ﬁrst-\\npass ambiguity resolution. More details can be found in [17].\\nA. Deﬁnitions\\nWe assume that syntactic processing takes place in a strongly\\nincremental fashion. This means that each word\\n is processed\\nby scanning the sentence from left to right and that we do not\\nallow disconnected substructures to be assembled together atsome later stage in processing.\\nLet\\nbe the parse tree for sentence\\n .W ed e ﬁne\\nfor each\\n theincremental tree\\n as the subtree of'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='Let\\nbe the parse tree for sentence\\n .W ed e ﬁne\\nfor each\\n theincremental tree\\n as the subtree of\\nrecursively built in the following way [see Fig. 1(a)]:2\\n\\x7f\\n consists of the chain of nodes and edges of\\n that goes\\nfrom\\n to its maximal projection.3\\n\\x7f\\n consists of all the nodes and edges in\\n and either:\\n— the chain of nodes and edges of\\n descending from\\nnode\\n where\\n is the lowest node of\\n that dom-\\ninates\\n ;\\n— the chain of nodes and edges of\\n descending from\\nnode\\n where\\n is lowest node of\\n that dominates\\nboth the root of\\n and\\n , and the chain of nodes\\nand edges that connect\\n with the root of\\n .\\nGiven two incremental trees with indexes\\n , and\\n ,\\nwe de ﬁne the difference between\\n and\\n as the set of all the\\nedges that are in\\n but not in\\n and all the nodes touched by\\nthose edges. The difference between\\n and\\n is called the\\nconnection path :\\n [see Fig. 1(b)]. The node that\\nbelongs to\\n and\\n is called the anchor (this is the node\\nwhere\\n attaches to\\n ). The preterminal node for word\\nis called the foot. This is the node whose label is the part\\nof speech (POS) tag of word\\n and in our framework it is a\\n2The examples of syntactic structure in this paper are based on the actual\\nstructures used to train the network (a relatively ﬂat representation derived from\\nthe Penn Treebank format [21]).\\n3A maximal projection of a word /119is the largest non terminal symbol /88\\nthat is related to /119through projection (i.e., /119and /88share head features). For\\nexample, a noun projects onto a noun phrase.'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='example, a noun projects onto a noun phrase.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. COSTA et al. : AMBIGUITY RESOLUTION ANALYSIS IN INCREMENTAL PARSING 961\\nFig. 1. (a) Example of incremental trees. (b) Anchor and Foot nodes.\\nleaf of the syntactic tree. POS-tags are the syntactic category\\nof words and can be predicted with very high accuracy [24].We use the symbol “\\n”to denote the joinoperator, de ﬁned as\\n. According to the previous de ﬁnitions, an incre-\\nmental tree\\n can always be written as the result of a number\\nof joins:\\n .\\nWe assume that the children of each node are ordered from\\nleft to right. The right frontier of an incremental tree is the chain\\nof the current rightmost children, starting from the root node\\nand ending to the rightmost leaf (see Fig. 7). Join operations arealways performed on nodes belonging to the right frontier.\\nLombardo and Sturt [17] describe a procedure that takes as\\ninput a parse tree\\nand computes all the incremental trees\\nand the all the connection paths\\n . By ap-\\nplying this procedure to a treebank\\n (a set of sentences anno-\\ntated with their parse trees) we obtain a set of connection paths\\ncalled the universe of connection paths , denoted\\n .\\nB. First-Pass Attachment Prediction\\nSuppose we are given a new sentence\\n not\\nincluded in the treebank\\n , and suppose that at stage\\n of\\nparsing we know the correct incremental tree\\n spanning'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content=', and suppose that at stage\\n of\\nparsing we know the correct incremental tree\\n spanning\\n. We want to compute the next tree\\n in order to\\naccommodate the next word\\n . Under the implicit hypothesis\\nthat\\n contains the required connection path,\\n can be\\nobtained by joining\\n to some unknown path\\n in\\n .\\nThe prediction problem is then de ﬁned as follows: given\\n ,\\nﬁnd\\n such that\\n is the correct incremental\\ntree spanning\\n . The set of candidate paths can be\\nsigni ﬁcantly reduced by enforcing the following two rules that\\nmust be satis ﬁed by a legal joining:\\n\\x7fthe foot of\\n must match the POS-tag of\\n ;\\n\\x7fthe anchor of\\n must match the one of the nodes in the\\nright frontier of\\n .\\nNote that\\n along with the joining operator and the previous\\nrules can be regarded as a dynamic grammar [25], [26]. Thisgrammar, however, is highly ambiguous as the set of connectionpaths that satisfy the two joining rules may be very large. In\\nparticular, there are three different sources of ambiguity:\\n\\x7fa word can have more than one POS tag;\\n\\x7fthe anchor can be any node of the right frontier [see\\nFig. 2(a)];\\n\\x7ffor each pair\\nanchor tag\\n foot tag\\n there can exist more\\nthan one legal connection path [see Fig. 2(b)].\\nThe set of trees obtained by legally joining\\n to a path in\\nwill be referred to as the forest of candidates for word\\n ,\\ndenoted\\n . Note that, under our assump-\\ntions, one, and only one tree in\\n is the correct incremental tree\\nspanning\\n . Without any loss in generality we will as-'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='is the correct incremental tree\\nspanning\\n . Without any loss in generality we will as-\\nsume that the ﬁrst element in the forest is the correct one.\\nC. Left Recursion and Lexical Information\\nThe top-down parser suffers from the problem of left re-\\ncursion [27]. Since a left recursive structure can be arbitrarilynested, we cannot predict the correct connection-path incre-\\nmentally. There are a few practical and psycholinguistically\\nmotivated solutions in the literature [28], but in the currentwork we have resorted to an immediate approach which isextensively implemented in the Penn Treebank schema: that\\nis we ﬂatten the tree structure and avoid the left recursion\\nissue altogether. Consider as an example the application of theﬂattening procedure to a tree like 1) that produces as a result a\\ntree like 2):\\n1)\\n;\\n2)\\n .\\nSince the main focus of the present linguistic analysis is about\\nsyntax, no lexical information is used for prediction and there-\\nfore two sentences having the same sequence of POS tags are\\nequivalent in our study. We believe that the use of lexical in-formation would further improve the prediction capability ofthe model, although this would result in a more complex net-\\nwork architecture. However, the current network architecture al-\\nlows us to model theories in which purely structural informationplays the major role in ﬁrst-pass ambiguity resolution, such as\\nthe Tuning Hypothesis [15].'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='the Tuning Hypothesis [15].\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 962 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 4, JULY 2005\\nFig. 2. (a) Anchor variability. (b) Connection path variability.\\nD. Connectionist Versus Frequency Approach\\nAccording to the formulation given previously, we can\\nrestate our learning task as the estimation of a utility functionthat, given a forest of incremental trees\\n,\\ncomputes the highest value for the correct element\\n .A\\nﬁrst and direct approach is to derive a probabilistic estimator\\nof such function by collecting information on occurrencesof all the instances of our problem (distinct trees) in a largecorpus. We want to estimate\\n, that is\\n. This approach suffers from a severe\\ndata sparseness problem. The combinatorial nature of thegrammar determines negligible probabilities for the occurrenceof the incremental trees in training sets of any given size (i.e.,\\nsentences) currently available. To quantify this state-\\nment, we have selected a sample of 1000 sentences randomlydivided in two sets of same size: one for a nominal test set andone for a nominal training set. We have calculated the number\\nof trees of the test set present in the training set, counting\\nﬁrst the coincidences among correct incremental trees, and\\nthen among all trees (i.e., the correct incremental trees plus allincorrect trees generated by the dynamic grammar), obtaining'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='for the correct incremental trees: and for the overall dataset\\n(i.e., correct incremental trees plus all incorrect trees generatedby the dynamic grammar):\\nThe small percentage of the training set seen in the test set\\nclearly illustrates the infeasibility of a direct multinomial esti-mator.\\nIn computational linguistics, data sparseness is traditionally\\ndealt with smoothing techniques, that is, by approximatingfrequency estimation through the decomposition of complexand infrequent objects in more frequent subparts [29]. The item\\nprobability is computed by composing the frequencies of the\\nsubparts, under some independency hypothesis. In an incre-mental framework, this decomposition has been attempted by[7]. Our solution does not make any simplifying assumptions,\\nand tries to take advantage of the global information available,\\novercoming at the same time the data sparseness problem. Thisis achieved by resorting to a parametric estimator (the RNN)that makes use of a much smaller set of hyper-parameters. This\\ncan be viewed as a way to perform an adaptive compression of\\nthe information.\\nIII. R\\nECURSIVE NETWORKS FOR PREFERENCE LEARNING\\nWe present a two-step solution to the ﬁrst-pass attachment\\nprediction problem. In the ﬁrst step, RNNs are used to adap-\\ntively build a set of features that describe a parse tree as a ﬁxed-\\nsize real vector. In the second step, we show a utility functionsolution to the preference learning task.\\nA. RNNs'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='A. RNNs\\nThe general theory developed in [18] allows the processing\\nof directed acyclic graphs with a super-source. Here we are in-\\nterested in the case of labeled ordered\\n-ary trees. By ordered\\nwe mean that, for each vertex\\n , a total order is de ﬁned on the\\nchildren of\\n .\\n denotes the label attached to vertex\\n of\\n.\\nIn the case of syntactic trees, labels belong to a ﬁnite alphabet of\\nnonterminal symbols\\n . The set of all trees\\nwith labels in\\n is denoted as\\n#.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. COSTA et al. : AMBIGUITY RESOLUTION ANALYSIS IN INCREMENTAL PARSING 963\\nThe basic neural network architecture computes a vector\\nof\\n features according to the following recursive processing\\nscheme:\\n(1)\\nwhere\\n denotes the (possibly missing)\\n th child\\nof\\n. We can interpret the previous equation as the recursive\\nstate–space representation of a generalized dynamical system\\nthat“evolves ”on a tree domain [18]. Under this interpretation,\\nthe feature vector\\n is a state vector associated\\nwith node\\n of tree\\n is the state transi-\\ntion function that maps states at\\n ’s children and the label at\\ninto the\\n -dimensional state vector at\\n . If a child is missing, the\\ncorresponding argument to\\n is the frontier state\\n 0.\\nStates in (1) are updated bottom-up, yielding a vector based rep-\\nresentation\\n at the root of\\n that can'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='resentation\\n at the root of\\n that can\\nbe seen as the result of applying a feature mapping to the en-tire tree. Using a parameterized function\\n(e.g., realized by a\\nfeedforward neural network) this feature mapping can be made\\nadaptive. In the following, we call state transition network the\\nnetwork implementing the mapping\\n .\\nB. Preference Learning\\nTypical prediction problems are formulated either as classi ﬁ-\\ncation or as regression, which can both be thought of as func-tion approximation problems, where the image of the function\\nis a subset of\\nfor regression or a discrete set for classi ﬁcation.\\nRanking a set of alternatives is a task with characteristics of boththe previous problems: like classi ﬁcation, it has as its image a\\ndiscrete set, and like in regression, there exists an ordering rela-\\ntion among the elements of the image. Ranking problems have\\nbeen studied under different assumptions in machine learning[30], [31]. Here we are interested in the simple case of prefer-ence learning, in which data points are organized into sets of\\ninstances and exactly one instance is preferred to the rest in the\\nset. Without losing generality we can write a preference data setas a collection of (partially ordered) sequences\\nwhere\\n#\\nis the size of the\\n th set and, conventionally,\\ndenotes the preferred instance in its set.\\nIn order to solve the preference learning problem we use the\\nutility function approach in which we learn a function\\n#\\nand, for a future forest of trees'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='utility function approach in which we learn a function\\n#\\nand, for a future forest of trees\\n we predict that the\\npreferred instance is\\n iff\\nWe propose realizing\\n in the following way:\\n(2)where\\n and\\n are adjustable weights and\\n is\\nthe feature vector of\\n . The utility function\\n can be then used\\nto compute the probability of selecting the correct tree\\nand, according to the maximum likelihood principle, we can\\nlearn\\n by maximizing the objective function\\n(3)\\njointly with respect to the parameters\\n of (2) and\\nthe parameters of the network implementing the state transitionfunction of (1). Maximization can be carried out by gradientdescent as explained in the next subsection.\\nC. Parameter Optimization\\nGradients are computed by a special form of backpropaga-\\ntion on the feedforward network obtained by unrolling the statetransition network according to the topology of the input tree\\nas showed in Section III-A. The algorithm was ﬁrst proposed\\nin [32] and is referred to as backpropagation through structure(BPTS). Backward propagation proceeds from the root to theleaves. Note that gradient contributions must be summed over\\nall the replicas of the transition network to correctly implement\\nweight sharing. In Fig. 3, we depict the coupling and unfoldingof the transition network and the output network on a forest oftwo incremental trees.\\nIV . L\\nINGUISTIC CORPUS AND NETWORK TRAINING\\nA. Linguistic Corpus\\nAll the experiments in this paper are based on the Wall Street'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='A. Linguistic Corpus\\nAll the experiments in this paper are based on the Wall Street\\nSection of the Penn-Treebank Corpus [21]. We have adopted thestandard setting widely accepted in [33]. Speci ﬁcally, sections\\n2–21 have been used to form the training set (39 832 sentences,\\n950 026 words), section 23 has been used for the test set (2416sentences, 56 683 words) and section 24 for the validation set(3 677 sentences, 85 335 words). The entire dataset used for our\\nexperiments includes therefore 45 925 sentences for a total of 1\\n092 044 words. The average sentence length is 24 in a range of1–141 (1 –67 in the test set). The labels (tags) on the nodes of\\nthe parse trees can be divided into part-of-speech (POS or preter-\\nminal) tags, and nonterminal tags: POS tags dominate a single\\nlexical item and indicate the syntactic category of the item (e.g.,a noun or a verb), while non terminal nodes dominate sequencescalled “phrases ”that can be made of preterminal and/or non-\\nterminal tags. In the Penn Treebank, the POS tags are 45, and\\nthe nonterminal tags are 26. Although the syntactic annotationschema provides a wide range of semantic and coindexing in-formation, we have used only syntactic information about the\\ndominance relation.\\n4\\n4This limitation can be a very compelling one, since most parsing models\\nachieve valuable results by including lexical statistics on word occurrence andfunctional dependencies [29], [34].'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='Authorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 964 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 4, JULY 2005\\nFig. 3. Network unfolding on a forest of two elements. (a) Syntactic tree. (b) Unfolded recursive net. (c) Utility function network.\\nB. Connection Path Analysis\\nOne of our working hypothesis is the “coverage assumption, ”\\nwhich states that we can extract, from a large corpus, the com-\\nplete set of connection paths with which to form all possible\\nincremental trees. This is likely to be only approximately true,and we perform the following experiment to get a quantita-tive result on the validity of this assumption. We build subsets\\nwith an increasing number of sentences: from 100 to the full\\n40 K sentences in steps of 100 sentences. The list of connec-tion paths with their frequencies, and the number\\nof distinct\\nconnection paths were then extracted from each subset by simu-\\nlating an incremental parse of each sentence. The simulator took\\nas input the parse tree for a sentence and, scanning each wordfrom left to right, marked the subgraph of the tree that connectedthe new word to the previous incremental tree as a connection\\npath [17]. Fitting the results [seeFig. 4(a)] with a polynomial\\nmodel we have\\nwhere\\n 0.434. This has a remark-\\nable similarity with Heaps law [35], an empirical rule whichdescribes the vocabulary growth as a function of the text size.'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='Heaps law establishes that a text of n words has a vocabulary of\\nsize\\nwith\\n where for English in ﬁrst approxima-\\ntion\\n . Considering how often certain connection paths\\nwere used in the making of the syntactic trees we observed that\\nthe frequency counts distribute accordingly to another famous\\nlinguistic law that bears the name of Zipf ’s law. Zipf ’s law ex-\\npresses a relationship between the frequency of words occurringin a large corpus and their rank. Given a corpus under examina-\\ntion, the rank of a word is de ﬁned as the position of that word\\nin the list of all the words in the corpus, ordered by frequencyof occurrence. According to Zipf ’sl a w\\n, which can\\nbe viewed as the existence of a constant\\n such that\\n .\\nWhat the law states is that there are few very common words\\nand many low frequency words. Considering connection pathsinstead of words we found that their frequency was closely de-scribed by the Zipf ’s law [Fig. 4(b)]. Moreover we found that\\nFig. 4. (a) Number of different connection path in respect of dataset size (units\\nin 100 sentences). (b) Connection paths Zip ﬁan distribution.\\nthe same distribution holds if we keep distinct the connection\\npaths whose foot node belongs to a speci ﬁc category (such as\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. COSTA et al. : AMBIGUITY RESOLUTION ANALYSIS IN INCREMENTAL PARSING 965\\nnouns, verbs, articles,'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='nouns, verbs, articles,\\n ). Finally we saw that new connection\\npaths extracted after having processed 10 000 sentences wererarely used (less than 1% of the time) in successive trees. This\\nsupported our hypothesis on the coverage approximation.\\nC. Network Training\\nFor each word\\nin a sentence, a forest of alternatives was\\ngenerated by extracting the incremental tree\\n spanning\\n, and joining it with all the legal connection paths.\\nEach sentence had an average length of 24 words and eachforest contained on average 120 trees. Considering that the\\naverage number of nodes of an incremental tree was 27, we\\nhave that the entire dataset had\\nforests,\\n trees,\\nand\\n nodes.\\nThe learning regime is online: weights are updated after the\\npresentation of each forest. A separate set of 1000 sentences\\nfrom section 24 of the treebank was used as a validation set\\nto control over ﬁtting by early stopping. Given the considerable\\namount of training data, accuracy on the validation set was mon-itored after the presentation of each group of 100 sentences. Op-\\ntimization was stopped if the validation error reached a min-\\nimum and did not decrease for the subsequent 1000 sentences.In this setting, three epochs were enough to reach the optimum.\\nThe state transition network was implemented as a single\\nlayer feedforward network with an input vector composed by1 unit for the threshold, 71 units for the one-hot encoding of\\nthe nonterminal and POS tag symbols and 25 units to represent'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='the nonterminal and POS tag symbols and 25 units to represent\\nthe state vector of each child node. We noted that the longestproduction in the dataset had 51 nonterminal symbols on itsright-hand side, and that productions with more than 15 non-\\nterminal symbols were very rare. Since each nonterminal posi-\\ntion was associated with its own set of weights in the transitionnetwork, we pruned long productions in order to avoid poor es-timates of the weights associated with positions that are infre-\\nquently ﬁlled in. Pruning beyond the 15th position resulted in a\\nreduction of only 0.3% of all the productions. The input layerhad therefore a total of 447 units. The output layer (which en-coded the state of the node being processed) was made of 25\\nunits. The state transition network had a total of 11 K free pa-\\nrameters. The nonlinearity was the standard hyperbolic tangentfunction. The utility network was a feedforward network with\\nunits in input and 1 unit in output. Once the recursive net-\\nwork was unrolled, the forward and backward phase proceeded\\nfollowing the standard backpropagation algorithm with a ﬁxed\\nlearning rate and momentum\\n . Good values for the parame-\\nters\\n and\\n have been experimentally determined on a working\\nset to be\\n and\\n 0.1. Training the system on the\\nwhole dataset took less than three days of CPU per epoch on a1 GHz Pentium III Intel processor.\\nWe evaluated the learning curve of the system. The training'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='We evaluated the learning curve of the system. The training\\nset has been partitioned into subsets of 100, 400, 1000, 4000,\\n10 000, and 40 000 sentences. A validation set of 1000 sentenceswas used for early-stopping. We report in Fig. 5 the percentage\\nof times that the correct element has been ranked by the system\\nin the ﬁrst position on a test set of 2416 sentences. On the\\naxis we report the number of training sequences and on y the\\nfraction of training trees correctly ranked in ﬁrst position. The\\nFig. 5. Learning curve for the experiment described in Section IV-C.\\nresults indicate that the difference between training with 10 000\\nor 40 000 sentences yields a 3% relative error reduction.\\nV. S TRUCTURAL ANALYSIS\\nIn this section, we will characterize the preferences learned\\nby the RNN trained on a large corpus. We will start by ana-\\nlyzing the correlation between the network ’s performance and\\nthe structural properties of the incremental trees. Then we willstudy the in ﬂuence of the frequency of the connection paths on\\nthe choices of the system. Finally, we will compare the prefer-\\nences learned by the network with some heuristics studied in\\npsycholinguistics.\\nA. Correlation Between Structural Features and Accuracy\\nThe aim of conducting the analysis reported here is to uncover\\nthose structural properties on which the network makes its deci-sions. Our approach is to test hypotheses about these structuralfeatures, and to test them via statistical analysis of the network ’s'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='performance on a sample of test items. The results will be infor-\\nmative for a general understanding of the network architecture,and can be used to re ﬁne the system to increase performance.\\nThe set of structural features under investigation is reported\\nin Table I. These features include both complexity measures ofthe trees, and statistical properties of the data set. They can begrouped into the following sets.\\n\\x7fNumber of nodes in the incremental tree (rows: nodes\\nin tree, nodes in cpath, max outdeg, anchor outdeg, rootoutdeg): a higher value implies a more dif ﬁcult error as-\\nsignment task for the network when propagating the error.\\nMoreover the tree automaton that we approximate with\\nthe connectionist system is more complex as the numberof possible con ﬁgurations increases.\\n\\x7fHeight of the incremental tree (rows: anchor depth, tree\\nheight, cpath height): a greater height implies more steps\\nin the propagation of the information and as a conse-quence a weakening of the gradient information.\\n\\x7fFrequency of the connection path (row: freq of cpath):\\nsince after all the RNN is a statistical estimator, this countis a valuable baseline for comparing the RNN perfor-mance.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 966 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 4, JULY 2005\\nTABLE I\\nSTATISTICS FOR THE 11 F EATURES USED IN THE STUDY OF THE TRAINED'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='TABLE I\\nSTATISTICS FOR THE 11 F EATURES USED IN THE STUDY OF THE TRAINED\\nNETWORK BEHA VIOR .THEFINAL COLUMN /40 /26 /41INDICATES THE SPEARMAN ’S\\nCORRELATION COEFFICIENT BETWEEN THE RELEV ANT FEATURE AND THE\\nNETWORK ’SERROR .( /3 /58 /112/60 0.05; ns: N OTSIGNIFICANT )\\nIn addition, we study the number of alternative incremental trees\\n(row: forest size), as we expect a negative correlation between\\nthe number of alternatives and the prediction performance. Fi-nally, we study the word absolute position (row: word index),since words that occur later in the sentence condition the choice\\nof the correct attachment on a more complex and variable con-\\ntext.\\nTo summarize, we hypothesize that the degree of error in the\\nnetwork ’s assignment of preference to an incremental tree (i.e.,\\n) will correlate positively with the structural complexity\\nof that tree. We also hypothesize that the error will correlatenegatively with the frequency of the connection path.\\nFor each of the features of interest we have collected basic sta-\\ntistical information (max value, mean, standard deviation, skew,\\nkurtosis) and tested for normality so to be able to use the appro-priate statistical test later on. To evaluate our hypotheses, wecomputed the Spearman ’s correlation coef ﬁcient between these\\nfeatures and the network ’s error on the correct element. The cor-'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='features and the network ’s error on the correct element. The cor-\\nrelation was run separately for each feature, over a randomlysampled subset of 200 pairs (error, feature). We report the cor-relation coef ﬁcients (column:\\n) in Table I. The test indicates\\na signi ﬁcant, if small, positive correlation between each feature\\nand the network ’s error, except the root outdegree, and a rele-\\nvant negative correlation between the frequency of the connec-tion path and the error. The most signi ﬁcant positive correlations\\nare with the size of the connection path and the forest size. Thus,\\nthe correlation results support our hypotheses.\\nB. Structured Characterization of True and False Positives\\nIn the next set of analysis, we investigate the hypothesis that\\nthe network learns to prefer simpler structures to complex struc-\\ntures, and that this preference in ﬂuences its decision, both when\\nit identi ﬁes the correct incremental tree, and when it mistakenly\\nchooses an incorrect incremental tree. To evaluate this hypoth-\\nesis, we distinguish true positive elements and false positive el-\\nements: true positives are the correct incremental trees that arepreferred by the network; false positives are the trees preferredby the network but that do not correspond to the correct trees in\\nthe treebank.\\nAtﬁrst, we will identify statistically signi ﬁcant differences in'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='the treebank.\\nAtﬁrst, we will identify statistically signi ﬁcant differences in\\nthe average values of some features. Then, analyzing the distinc-tive features, we will identify some characterizing properties ofTABLE II\\nMEANS FOR STRUCTURAL CHARACTERISTICS FOR THE CORRECT ELEMENT\\nVERSUS THE ELEMENT INCORRECTLY CHOSEN BY THE NETWORK .\\nTABLE SHOWS FEATURES WHERE PAIRWISE COMPARISONS WERE\\nSIGNIFICANT AT /112/60 0.05\\nthe set of true positive elements against the second preferred ele-\\nment, and we will do the same with the false positive against the\\ncorrect elements. For the features that do not exhibit a normaldistribution we use the Wilcoxon Matched-Pairs Signed-RanksTest on a random sample of 200 pairs from the dataset for each\\nfeature. For all the other features a paired\\n-test is used, ran-\\ndomly sampling 100 pairs from the dataset for each feature.\\nIn the ﬁrst experiment, the tests are used to determine whether\\nthere are meaningful differences in some feature of the trees\\nwhen comparing the network false positive with the correct el-\\nements. These tests are informative about the impact that thestructural features have on the network incorrect choices. We\\nreport the results that are signi ﬁcant\\n0.05\\n under the re-\\nspective statistical tests in Table II. In column “correct elem, ”we\\nreport the average values for the correct element that the RNNhas not been able to predict and in “false pos ”column we report\\nthe average values for the wrong element picked by the net. In'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='the average values for the wrong element picked by the net. In\\nthe last column, we report the size of the difference\\n, stated\\nin terms of the standard deviation of the corresponding distri-bution. The interesting result of this experiment is that in the\\ncase of wrong predictions, attachment choices predicted by the\\ntrained RNN yield, on average, oversimpli ﬁed trees. As shown\\nin Table II, the value of each of the four statistically signi ﬁcant\\nfeatures is smaller in the incremental tree obtained by following\\nthe RNNs predicted attachment than the value taken by the same\\nfeature on the correct incremental tree. This indicates that thenetwork preference for simpler trees has a measurable impact onperformance: the network has a signi ﬁcant tendency to choose\\nthe incorrect tree because it is simpler than the correct alterna-\\ntive.\\nThere was no signi ﬁcant effect of the outdegree on the RNN\\nfalse positive error. We note how the differences within the\\nwhole incremental tree are much smaller than those between\\nindividual connection paths. This indicates that connectionpaths are the key element responsible for the discrimination be-tween the correct element and the incorrectly chosen element.\\nWe will be using this ﬁnding for enhancing the performance of\\nthe system.\\nIn a second experiment, we tested differences between true\\npositives and the element ranked second by the net. This allows\\nus to determine the information on which the network bases its'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='us to determine the information on which the network bases its\\npreference when it correctly predicts the appropriate element.We report the signi ﬁcant results in Table III (to be read as the\\nprevious one). The same trend was kept for the true positives:\\nthe RNN has preferred the correct incremental trees because of\\ntheir “simplicity ”in comparison to the second ranked alterna-\\ntive, which turns out to be more complex. Note that now the rootand the anchor outdegree have become a meaningful feature in\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. COSTA et al. : AMBIGUITY RESOLUTION ANALYSIS IN INCREMENTAL PARSING 967\\nTABLE III\\nMEANS FOR STRUCTURAL CHARACTERISTICS FOR THE CORRECTLY CHOSEN\\nELEMENT VERSUS THE ELEMENT RANKED SECOND BY THE NETWORK .\\nTABLE SHOWS FEATURES WHERE PAIRWISE COMPARISONS WERE\\nSIGNIFICANT AT /112/60 0.05\\na way that is still consistent with the hypothesis that “simpler ”\\ntrees are preferred, i.e., the correct incremental trees have rootsand anchors with smaller outdegrees. This latter fact can be rep-\\nresented by a heuristic that disprefers joining those connection\\npaths that increase the number of children of the root or of theanchor, since this leads to wrong incremental trees.\\nWe suspect that the simplicity preference of the network is\\nmainly due to the combinatorial nature of the elements of this'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='mainly due to the combinatorial nature of the elements of this\\ndomain, since all the features are strongly correlated, and therecould be an underlying factor that is the direct cause of thepreference. Analyzing the Zip ﬁan distribution of connection\\npaths we ﬁnd that shorter connection paths are more frequent.\\nAs a direct consequence, most correct incremental trees arethemselves simpler because they are more frequently derivedby joining simpler elements. In order to understand the mag-\\nnitude of this effect, we have run a Pearson Correlation test\\non a sample of 10 000 pairs of connection paths number ofnodes versus log(freq). We obtain a correlation of\\n0.33\\n(statistical signi ﬁcance\\n 0.001) indicating that smaller\\nconnection paths are reliably more frequent.\\nIn the following subsection, we therefore investigate the in-\\nﬂuence of connection paths frequencies on false positives and\\ntrue positives, respectively.\\nC. In ﬂuence of Connection Paths Frequencies\\nIn Fig. 6, there are several comparison between the network\\nresults and other psycholinguistic or frequency-based pref-erences. Here we compare the RNN to the simple frequencyheuristic obtained by ranking each alternative connection path\\naccording to its corpus frequency. The test is done on the\\nstandard test set of 2416 sentences (Section 23). Each point\\nin the diagram of Fig. 6 is to be interpreted as:\\n is the\\nproportion of times that the correct element has been ranked in\\nthe position'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='is the\\nproportion of times that the correct element has been ranked in\\nthe position\\n or less. From Fig. 6, we can deduce that the RNN\\nbases its decisions on something more than the pure frequency.\\nA paired\\n -test was used to determine the in ﬂuence of the\\nlog-transformed frequency5of the connection path on the\\nnetwork accuracy. As in the previous analysis, pairwise com-parisons were conducted both for true positives and for the falsepositives. For the true positive, the mean log-frequency of the\\nconnection path was 9.2 against a mean of 5.2 for the second\\nbest ranked alternative, this difference being highly signi ﬁcant\\non the random sample of 100 pairs. For the false positivedataset there was no signi ﬁcant difference in the mean of the\\n5This is because the connection path frequency follows a Zip ﬁan (log log)\\ndistribution.\\nFig. 6. Comparison between RNN and psycholinguistic heuristics.\\nlog frequency (7.4 for the correct element versus 7.2 of the net-\\nwork ’s incorrectly predicted element,\\n 1). Notice also that\\nthe overall mean is much higher for the true positives than the\\nfalse positives. This result can be explained by observing that\\nin the case of the true positives the frequency distribution of theconnection paths is more skewed, with the correct alternativehaving a much higher frequency than the other alternatives.\\nThis seems to indicate that it is more dif ﬁcult for the RNN to'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='This seems to indicate that it is more dif ﬁcult for the RNN to\\nexpress a preference when it cannot draw information directlyfrom the frequency distribution of the alternative connectionpaths.\\nSince the network performance is better than the frequency\\nheuristic, we can conclude that the connection path frequencyis an important factor that determines accuracy, but that the de-cision strategy of the network takes other factors into account.\\nD. Filtering Out the Connection Paths Frequency Effect\\nThe following set of experiments aims at understanding what\\ninformation is exploited by the trained RNN in those cases\\nwhere it makes a correct prediction, but when the preferred\\nconnection path is notthe most frequent one. We isolate these\\ncases (which represent 10% of the correct prediction cases) andwe analyze their characteristics as we have previously done. In\\nTable IV , we report the average value of the signi ﬁcant features\\nthat discriminate between the correctly predicted element andthe most frequent element and the relative difference of thevalues. We observe how the RNN has preferred slightly more\\ncomplex alternatives in terms of heights or number of nodes,\\nbut has preferred cases characterized by anchors with a smalleroutdegree and at a higher distance from the root. This con ﬁrms\\nthe importance played by frequency and simplicity on the con-\\nnection path, but indicates a preference for deeper anchors —in'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='nection path, but indicates a preference for deeper anchors —in\\nother words, a preference for lower attachment points in thetree. We therefore try to decompose the preference task intotwo subtasks (as reported in Section II-B and Fig. 2): the ﬁrst\\none consists in ﬁnding the correct attachment point of the\\nconnection path, and the second one consists in choosing thecorrect connection path itself. Given the previous ﬁndings, we\\nhypothesize that the network employs a somewhat more com-\\nplex decision strategy to disambiguate the attachment point,\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 968 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 4, JULY 2005\\nbut that it then exploits only the connection path ’s frequency to\\nchoose the appropriate connection path to attach at that point.\\nE. Analysis of Attachment Preference\\nWe measure the accuracy of the RNN in determining the cor-\\nrect attachment position along the right frontier. We proceed by\\ngrouping all the incremental trees that share the same attach-ment node. We then rank the groups according to the highestscore given by the RNN to any member of the group. We con-\\nsider a group correctly predicted iff the correct connection path\\nbelongs to the group that is ranked highest. The prediction ac-curacy achieved is 91.5%. The baseline in this case (a randompredictor that chooses randomly a connection path) has an ac-'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='curacy of 37.6%, that is, how many connection paths are in the\\ncorrect group in respect to the total number of connection pathsin all groups averaged over all the forests. Remarkably, if weconsider how many times the RNN correctly predicts the anchor\\nattachment within the three best ranked alternatives we achieve\\nan accuracy of 98.4%.\\nF . Analysis of Connection Path Preference\\nIn this section, we study the disambiguation of alternative\\nconnection paths and we analyze the relation between fre-\\nquency-based predictions and RNN predictions.\\nIn the ﬁrst experiment, we assume that an oracle is available\\nthat chooses the correct anchor. In this setting, the network ranks\\nthe correct connection path in ﬁrst position with a 89.5% accu-\\nracy. Since the predictions made by the RNN and the most fre-quent connection path predictor are highly overlapping (91.4%),we re ﬁned the analysis considering the true positive (the correct\\nalternative is predicted) and found that in only 3.8% of these\\ncases the preferred incremental tree had a connection path thatwas not the most frequent; considering the false positives (pre-diction is not correct) we found that the RNN preferred a more\\nfrequent connection path (instead of the correct less frequent\\none) 66.3% of the time. We conclude that the RNN does exploitfrequency information once the correct anchor is given.\\nThis is not necessarily a negative ﬁnding and high error rate\\ncan also be expected for human ﬁrst-pass disambiguation de-'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='can also be expected for human ﬁrst-pass disambiguation de-\\ncisions, that are also biased by frequencies [15]. We remarkthat there are no experimental results concerning human perfor-mance on ﬁrst-pass disambiguation carried out on large corpora.\\nWrong ﬁrst-pass decisions probably do not have a dramatic im-\\npact on the overall performance of the human parser, thanks toits ability to revert to alternative, nonpreferred analysis later onin the parsing process. This ability might be realized in terms\\nof a serial backtracking strategy, or alternatively, in terms of the\\nreranking of parallel alternatives.\\nIn a second experiment, we assumed that the anchors were\\npredicted by the trained RNN. More precisely, after ranking all\\nthe network ’s prediction we extracted the anchors, removed du-\\nplicates after the ﬁrst occurrence and obtained a rank over the\\nanchors. We then collected all the connection paths that matched\\none of the ﬁrst\\nanchors and compared the rankings ob-\\ntained using either the path frequency or the preference of theRNN. In Table V , we report the results obtained for\\n,\\nand\\n .TABLE IV\\nCOMPARISON BETWEEN RNN AND FREQUENCY HEURISTIC ,COMPARING\\nMEANS FOR STRUCTURAL CHARACTERISTICS .TABLE SHOWS FEATURES\\nWHERE PAIRWISE COMPARISONS WERESIGNIFICANT AT /112/60 0.05\\nTABLE V\\nCOMPARISON BETWEEN FREQUENCY HEURISTIC AND RNN INACCURACY OF\\nCONNECTION PATH DISAMBIGUATION ,GIVEN THE ANCHOR\\nPREDICTED BY THE RNN\\nThese experiments and those reported in Section V-C con ﬁrm'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='PREDICTED BY THE RNN\\nThese experiments and those reported in Section V-C con ﬁrm\\nthe initial hypothesis: the RNN is very successful in predicting\\nthe anchor and relies mainly on frequency information to predict\\nconnection paths. However, the last experiment also shows thatthe RNN can learn more than pure corpus frequency.\\nIn order to gain a better insight on the kind of statistics that the\\nnetwork is really employing, we assume the working hypothesisthat the human parser and the RNN share some common mech-anism for ambiguity resolution. We then simulate some known\\nheuristics that have been found in psycholinguistic research, and\\ninvestigate to what extent they are matched by the network.\\nG. Comparison to Psycholinguistic Heuristics\\nAmong the purely structural preferences expressed by the\\nsyntactic module of the human parser, psycholinguistic studies\\nidentify the minimal attachment (MA) preference and the late\\nclosure (LC) preference [36]. The MA preference suggeststhat humans tend to prefer simpler and shorter analysis. In ourframework, this translates to preferring connection paths having\\nfewer nodes, which generally implies shorter connection paths.\\nFor example, choice 1 would be the preferred one in Fig. 2(b).The LC preference suggests that humans prefer to connect thecurrent word with more recently processed material. In our\\nframework, this is equivalent to preferring low attachments,'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='framework, this is equivalent to preferring low attachments,\\ni.e., deeper anchors. For example, choice 1 would be preferredin Fig. 2(a). Since a single preference scheme would lead toa number of ties, we ﬁrst apply one scheme and then break\\nties by applying the other one. Should ties still occur we resort\\nto the frequency of connection paths. There are two possiblecombinations: LC-over-MA and MA-over-LC. Results are pre-sented in Fig. 6. In order to test whether the RNN has learned\\nto express a preference that mimics that of the heuristics, we\\nmeasure the overlap between pairs of predictors. We report ourresults in Table VI. In the ﬁrst row, we count how many times\\ntwo methods rank the same element in ﬁrst position. In the\\nsecond row, we count how many times there is one common\\nelement in the ﬁrst two ranked positions.\\nThe results indicate that the network choices coincide with\\nthose of the heuristics only in roughly half of the cases. If\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. COSTA et al. : AMBIGUITY RESOLUTION ANALYSIS IN INCREMENTAL PARSING 969\\nTABLE VI\\nOVERLAPPING PREFERENCES : RNN V ERSUS HEURISTIC AND\\nHEURISTIC VERSUS HEURISTIC\\nwe allow the ﬁrst or second choice of the network to match\\neither the ﬁrst or second choice of the heuristic combination\\nweﬁnd that the preferences expressed by the RNN and by the\\nLC-over-MA heuristic agree in more than 78% of the cases.'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='LC-over-MA heuristic agree in more than 78% of the cases.\\nWe can conclude that the network uses a criterion similar to\\nLC-over-MA heuristic combination, but exploits more complex\\ninformation for those cases in which the heuristic does not apply.This accounts for the 66% error reduction when comparing theprediction accuracy of the RNN to the heuristic combination.\\nVI. E\\nNHANCEMENTS\\nA. Tree Simpli ﬁcation\\nThe experimental results reported in Section V have shown\\nhow the complexity of the incremental trees negatively affects\\nthe prediction performance. We would like to decrease this com-\\nplexity (i.e., the number of nodes) without taking the risk of dis-regarding useful features. Intuitively, not all the information ofthe incremental tree is signi ﬁcant for the disambiguation task.\\nSpeci ﬁcally, it can be argued that the knowledge of the internal\\ncomposition of “closed ”constituents, i.e., constituents that have\\nbeen fully parsed, can be summarized by the nonterminal tagthat immediately dominates the constituent. In other words, we\\nconjecture that the knowledge that a deeply nested NP is made\\nof a sequence of (DT NN) or rather a more complex (DT JJ NNNN) is not much more informative when deciding how to attacha connection path. If this hypothesis is true it should be possible\\nto eliminate a signi ﬁcant part of the nodes of the incremental'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='to eliminate a signi ﬁcant part of the nodes of the incremental\\ntree without decreasing the discriminating power of the infor-mation that is left in the remaining nodes. We propose a reducingscheme where we keep all the nodes that dominate incomplete\\ncomponents plus all their children. Because of the incremental\\nnature of the algorithm, it turns out that these nodes belong to theright frontier of the incremental tree, or to the children of suchnodes. The procedure we are adopting turns out to be consistent\\nwith the notion of c-command\\n6in theoretical linguistics. When\\nwe create\\n , we keep only those nodes that c-command the right\\nfrontier of\\n , plus the right frontier of\\n itself. Preserving\\nthe nodes that c-command the nodes that are active (those that\\nare potential anchors) is linguistically motivated in that it keeps\\nthe nodes that can exhibit a “linguistic in ﬂuence ”on each other.\\nIn Fig. 7, we show the subset of retained nodes. In order to testthe equivalence hypothesis, we have run an experiment with the\\nfollowing setting. The datasets are the standard training, vali-\\ndation, and test sets where we have applied the simpli ﬁcation\\nprocedure. We employ a recursive network having\\n20 units\\nand an output network. We report in Fig. 6 the comparison be-\\ntween the performance on the reduced dataset and the normal\\ndataset. We observe an increase of performances from 81.7%\\n6A node A c-commands a node B if B is a sister of A or descendent of a sister\\nof A [37].'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='6A node A c-commands a node B if B is a sister of A or descendent of a sister\\nof A [37].\\nFig. 7. Tree simpli ﬁcation: the shaded area shows the subset of nodes included\\nin the simpli ﬁed tree for the attachment of the ﬁnal word.\\nto 84.82% with a relative error reduction of 17%. The results\\nindicate that the simpli ﬁcation procedure preserves relevant in-\\nformation; in fact, we have helped the system by eliminatingpotential sources of noise, making the task somewhat simplerand allowing for a better generalization. To explain this behavior\\nwe can hypothesize the fact that the states that encode the infor-\\nmation relating to deeply embedded nodes (i.e., those that arestructurally more distant from the right frontier) are “noisy ”and\\nconfound less embedded states (i.e., those closer to the frontier).\\nB. Modular Networks\\nWhen the learning domain can naturally be decomposed into\\na set of disjoint subdomains, it is possible to specialize sev-eral learners on each subdomain. A special case for these spe-cialized learners is when we have informationally encapsulated\\n“modules ”[38], that is, predictors whose internal computation\\nis unaffected by the other modules. The linguistic data that weare processing present an intuitive decomposition: the knowl-edge needed to process the attachment of verb-footed connec-\\ntion paths is quite different from the knowledge used to attach'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='tion paths is quite different from the knowledge used to attach\\narticle or punctuation-footed connection paths. It seems plau-sible that the features that are relevant for discriminating thecorrect incremental trees are different when dealing with con-\\nnection paths that have different foots. If there is no signi ﬁcant\\ninformation overlap between the different cases, we can parti-tion the dataset and select a smaller subset of examples withwhich to train a predictor. The adoption of a modular approach\\nmoreover allows a tighter parameter tuning of each module.\\nThe knowledge of the domain suggests that certain at-\\ntachment decisions are harder than others. For example,prepositional phrase attachment is notoriously a hard problem,\\nespecially when lexical information is not used (which is our\\ncase). In order to determine the “hardness ”of each subtask,\\nwe setup an experiment that has the following setting. Wedivide the set of POS tags into\\n10 subsets where we\\ncollate “similar ”tags, i.e., tags that have a similar grammatical\\nfunction.7A special set contains all those tags that could not be\\nput in any other subset.8We employ a network having\\n 25\\nunits and adopt the same training/validation/test as introduced\\nin Section IV-A. The dataset has been preprocessed with the\\nsimpli ﬁcation scheme introduced in the previous section.\\n7For example, all the tags MD VB VBD VBG VBN VBP VBZ, which cor-\\nrespond to modal verbs and verbs with various different tense and agreement'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='respond to modal verbs and verbs with various different tense and agreement\\ninformation, are grouped together under the category VERB.\\n8It includes POS tags that denote foreign words, exclamations, symbols, etc.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 970 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 4, JULY 2005\\nTABLE VII\\nSPECIALIZATION IMPROVEMENT .PRECISION RESULTS AFTER TRAINING ON THE\\nORIGINAL 500-S ENTENCE TRAINING SETWITHSPECIALIZED NETWORKS\\n/40 /82\\n /41,AND AFTER TRAINING ON THE 40 k T RAINING SETWITH AN\\nUNSPECIALIZED NETWORK /40 /82\\n /41AND SPECIALIZED NETWORKS /40 /82\\n /41.\\nRELATIVE ERROR REDUCTION ISSHOWN IN THE FINAL COLUMN\\nThe prediction results are collected and partitioned into the ap-\\npropriate subsets according to which POS tag was involved inthe attachment decision. We report the results in Table VII,where column\\nshows the best accuracy obtained using the\\nmethod of Section VI-A, while column Size reports the frac-\\ntion of the total dataset represented by each subset. The resultsindicate that the problem is harder in the case of adverbs and\\nprepositions and easier for nouns, verbs, and articles.\\nWeproposetoenhancetheoverallperformancebylettingsingle\\nnetworks to concentrate on speci ﬁc ambiguities, i.e., having a\\nRNN being exposed only to attachment decisions involving, for\\nexample, adverbs, or prepositions. The network specialization'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='example, adverbs, or prepositions. The network specialization\\ncan be done in an online or batch fashion. The online schemeis realized using a single network with\\ndifferent “switching ”\\nweight sets for the recursive and output networks. Here the POS\\ntag of the current word selects the appropriate weight set. The\\nbatchschemeisrealizedbypreprocessingthetrainingandtestsetsobtaining\\ndifferent subsets according, once again, to the POS\\ntag and then employing\\n different networks each one exposed\\nonly to uniform attachment decisions. Since the latter solution\\nallows an easier parallelization of the training and testing phase,we resorted to the batch approach. We run two experiments. Intheﬁrst one, we replicated the training set of [39], applied the\\nreduction preprocessing, trained the modular network and tested\\nthe performance of the new architecture. We report in column\\nof Table VII the results. We obtained a total precision in\\nﬁrst position of 80.57% against the previous result of 74.0% [39]\\nyielding a 25% relative error reduction. On a second experiment\\nwe trained a network of 25 units on the standard training set of 40k sentences and we tested the resulting network on the standard2 k sentences test set. We report the comparison between the\\nperformance of the specialized networks (column\\n) and the\\nunspecialized network (column\\n ) on the same dataset and the\\nrelative error reduction in the last column.\\nThe results indicate that the specialization procedure results'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='The results indicate that the specialization procedure results\\nin an overall enhancement of the performance (17.79% rela-\\ntive error reduction in respect of the unspecialized network,52% relative error reduction in respect to the previous result of74.0% [39]) and that some categories greatly bene ﬁt from this\\napproach.\\n9We believe that the reason is that the resources (i.e.,\\n9Note that the result reported for the possessive case, due to the limited\\nnumber of examples, does not show any statistically signi ﬁcant difference\\nbetween the specialized and unspecialized case.areas in the state –space) allocated for discriminating the less fre-\\nquent classes (conjunctions, punctuation, adverbs) do not haveto compete against the ones allocated for the most frequent cases\\n(nouns, verbs).\\nVII. C\\nONCLUSION\\nWe have shown how the analysis of the preferences expressed\\nby the RNN gives a useful insight into the nature of the statisticalinformation used by the system. We have found that the RNNbases its preferences to disambiguate the attachment point on\\ncomplex structural information, but mainly resorts to frequency\\nin choosing the correct connection path. We have moreovershown how the system prefers to attach simple structures torecently processed material, modeling human heuristics, but that\\nthe incremental tree offers a richer context on which to condition\\nthe preferences. Taking advantage of the highly structural natureof the domain we have been able to propose a simpli ﬁcation'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='scheme and a specialized architecture that have enhanced the\\noverallpredictionaccuracyofthenetwork.Webelievethatfurther\\nimprovements are achievable introducing more information bylexicalizing the underlying grammar. Future work will focus ontheuseoftheRNNasaninformanttoguideanincrementalparser.\\nR\\nEFERENCES\\n[1] G. Altmann and M. Steedman, “Interaction with context during human\\nsentence processing, ”Cognition , vol. 30, pp. 191 –238, 1988.\\n[2] W. Marslen-Wilson, “Linguistic structure and speech shadowing at very\\nshort latencies, ”Nature , vol. 244, pp. 522 –533, 1973.\\n[3] M. J. Pickering and M. J. Traxler, “Plausibility and recovery from\\ngarden paths: An eye-tracking study, ”J. Experimental Psychol.:\\nLearning, Memory, and Cognition , vol. 24, no. 4, pp. 940 –961, 1998.\\n[4] M. Bader and I. Lasser, “German verb- ﬁnal clauses and sentence pro-\\ncessing, ”inPerspectives on Sentence Processing , C. Clifton, L. Fra-\\nzier, and K. Rayner, Eds. Mahwah, NJ: Lawrence Erlbaum Associates,\\n1994.\\n[5] Y . Kamide and D. C. Mitchell, “Incremental pre-head attachment in\\nJapanese parsing, ”Lang. Cognitive Processes , vol. 14, pp. 631 –632,\\n1999.\\n[6] E. P. Stabler, Parsing for incremental interpretation, Univ. California at\\nLos Angeles, 1994. manuscript.\\n[7] B. Roark and M. Johnson, “Efﬁcient probabilistic top-down and left-\\ncorner parsing, ”inProc. 37th Annu. Meeting Association for Computa-\\ntional Linguistics , 1999, pp. 421 –428.'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='tional Linguistics , 1999, pp. 421 –428.\\n[8] P. C. R. Lane and J. B. Henderson, “Incremental syntactic parsing of\\nnatural language corpora with simple syncrony networks, ”IEEE Trans.\\nKnowl. Data Eng. , vol. 13, no. 2, Mar.-Apr. 2001.\\n[9] J. Henderson, “Neural network probability estimation for broad cov-\\nerage parsing, ”inProc. 10th Conf. Eur. Chapter Association for\\nComputational Linguistics , EACL 2003, Budapest, Hungary, 2003, pp.\\n131–138.\\n[10] A. N. Jain, “Parsing complex sentences with structured connectionist\\nnetworks, ”Neural Computat. , vol. 3, pp. 110 –20, 1991.\\n[11] C. Kemke, “A constructive approach to parsing with neural net-\\nworks —The hybrid connectionist parsing method, ”inProc. 15th Conf.\\nCanadian Soc. Computational Studies of Intelligence , vol. 2338, AI\\n2002, Calgary, AB, Canada, 2002, pp. 310 –318.\\n[12] R. Miikkulainen, Subsymbolic Natural Language Processing: An Inte-\\ngrated Model of Scripts, Lexicon, and Memory . Cambridge, MA: MIT\\nPress, 1993.\\n[13] S. Wermter and V . Weber, “SCREEN: Learning a ﬂat syntactic and se-\\nmantic spoken language analysis using arti ﬁcial neural networks, ”J.\\nArtif. Intell. Res. , vol. 6, pp. 35 –85, 1997.\\n[14] F. Cuetos and D. C. Mitchell, “Cross-linguistic differences in parsing:\\nRestrictions on the use of the late closure strategy in Spanish, ”Cogni-\\ntion, vol. 30, pp. 72 –105, 1988.\\n[15] D. C. Mitchell, F. Cuetos, M. M. B. Corley, and M. Brysbaert, “Expo-'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='[15] D. C. Mitchell, F. Cuetos, M. M. B. Corley, and M. Brysbaert, “Expo-\\nsure-based models of human parsing: Evidence for the use of coarse-grained (nonlexical) statistical records, ”J. Psycholing. Res. , vol. 24,\\n1995.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. COSTA et al. : AMBIGUITY RESOLUTION ANALYSIS IN INCREMENTAL PARSING 971\\n[16] P. Sturt, F. Costa, V . Lombardo, and P. Frasconi, “Learning ﬁrst-pass\\nstructural attachment preferences using dynamic grammars and recur-sive neural networks, ”Cognition , vol. 88, pp. 133 –169, 2003.\\n[17] V . Lombardo and P. Sturt, “Incrementality and lexicalism: A treebank\\nstudy, ”inLexical Representations in Sentence Processing , S. Stevenson\\nand P. Merlo, Eds. Philadelphia, PA: John Benjamins, 1999.\\n[18] P. Frasconi, M. Gori, and A. Sperduti, “A general framework for adaptive\\nprocessing of data structures, ”IEEE Trans. Neural Netw. , vol. 9, no. 5,\\npp. 768 –786, Sep. 1998.\\n[19] A. Sperduti and A. Starita, “Supervised neural networks for the classi ﬁ-\\ncation of structures, ”IEEE Trans. Neural Netw. , vol. 8, no. 3, Mar. 1997.\\n[20] M. Collins and N. Duffy, “Convolution kernels for natural language, ”\\ninAdvances in Neural Information Processing Systems 14 , T. G. Di-\\netterich, S. Becker, and Z. Ghahramani, Eds. Cambridge, MA: MIT\\nPress, 2002.\\n[21] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building a large'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='Press, 2002.\\n[21] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building a large\\nannotated corpus of English: The Penn Treebank, ”Computat. Ling. , vol.\\n19, pp. 313 –330, 1993.\\n[22] S. Stevenson, “Competition and recency in a hybrid network model\\nof syntactic disambiguation, ”J. Psycholing. Res. , vol. 23, no. 4, pp.\\n295–321, 1994.\\n[23] T. V osse and G. Kempen, “Syntactic structure assembly in human\\nparsing: A computational model based on competitive inhibition and alexicalist grammar, ”Cognition , vol. 75, pp. 105 –143, 2000.\\n[24] E. Brill, “A simple rule-based part-of-speech tagger, ”inProc. ANLP-92\\n3rd Conf. Applied Natural Language Processing , Trento, IT, 1992, cite-\\nseer.nj.nec.com/brill92simple.html, pp. 152 –155.\\n[25] D. Milward, “Dynamic dependency grammar, ”Ling. Philos. , vol. 17, no.\\n6, 1994.\\n[26] V . Lombardo and P. Sturt, “Toward a dynamic version of tag, ”inProc.\\n/84/65 /71 /43 /54 Workshop , Venezia, 2002, pp. 101 –110.\\n[27] ,“Incremental processing and in ﬁnite local ambiguity, ”inProc.\\n19th Annu. Conf. Cognitive Science Soc. , Stanford, CA, 1997, pp.\\n448–453.\\n[28] H. Thompson, M. Dixon, and J. Lamping, “Compose-reduce parsing, ”\\ninProc. 29th Meeting Association Computational Linguisticsy , Berkley,\\nCA, Jun. 1991, pp. 87 –97.\\n[29] M. Collins, “Three generative, lexicalised models for statistical parsing, ”\\ninProc. 35th Annu. Meeting Association for Computational Linguistics ,\\n1997, pp. 16 –23.'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='inProc. 35th Annu. Meeting Association for Computational Linguistics ,\\n1997, pp. 16 –23.\\n[30] W. W. Cohen, R. E. Schapire, and Y . Singer, “Learning to order things, ”\\ninAdvances in Neural Information Processing Systems , M. I. Jordan,\\nM. J. Kearns, and S. A. Solla, Eds. Cambridge, MA: MIT Press, 1998,\\nvol. 10.\\n[31] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and B. Sch ölkopf,\\n“Ranking on data manifolds, ”inAdvances in Neural Informa-\\ntion Processing Systems 16 , S. Thrun, L. Saul, and B. Sch ölkopf,\\nEds. Cambridge, MA: MIT Press, 2004.\\n[32] C. Goller and A. Kuechler, “Learning task-dependent distributed struc-\\nture-representations by back-propagation through structure, ”inProc.\\nIEEE Int. Conf. Neural Networks , 1996, pp. 347 –352.\\n[33] M. J. Collins, “A new statistical parser based on bigram lexical depen-\\ndencies, ”inProc. 34th Annu. Meeting Association for Computational\\nLinguistics , Santa Cruz, CA, 1996, pp. 184 –191.\\n[34] E. Charniak, “Expected-frequency interpolation, ”Dept. Comput. Sci.,\\nBrown Univ., Boston, MA, Tech. Rep. CS96-37, 1996.\\n[35] J. Heaps, Information Retrieval-Computational and Theoretical As-\\npects . New York: Academic, 1978.\\n[36] L. Frazier, “On comprehending sentences: Syntactic parsing strategies, ”\\nPh.D. dissertation, Univ. Connecticut, Storrs, CT, 1978.\\n[37] N. Chomsky, Lectures on Government and Binding : Foris, 1981.\\n[38] A. Sharkey, On Combining Arti ﬁcial Neural Nets, 1996.'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='[38] A. Sharkey, On Combining Arti ﬁcial Neural Nets, 1996.\\n[39] F. Costa, P. Frasconi, V . Lombardo, and G. Soda, “Toward incremental\\nparsing of natural language using recursive neural networks, ”Appl. In-\\ntell., vol. 19, no. 1 –2, pp. 9 –25, 2003.\\nFabrizio Costa received the M.Sc. degree in elec-\\ntronic engineering and the Ph.D. degree in computer\\nscience, both from the University of Florence, Flo-\\nrence, Italy, in 1998 and 2002, respectively.\\nHisﬁelds of scienti ﬁc investigation include ma-\\nchine learning with connectionist and kernel basedapproaches over structured data, with emphasis on\\nproblems in the natural language and bio-informatic\\ndomain.\\nPaolo Frasconi (S’91–M’96) received the M.Sc. de-\\ngree in electronic engineering and the Ph.D. degree\\nin computer science, both from the University of Flo-\\nrence, Florence, Italy, in 1990 and 1994, respectively.\\nHe is currently an Associate Professor of Com-\\nputer Science at the University of Florence. Hepreviously held positions at the University of\\nCagliari, Cagliari, Italy, at the University of Wollon-\\ngong, Wollongong, Australia, and the Massachusetts\\nInstitute of Technology, Cambridge. His current\\nresearch interests are in the area of machine learning\\nusing connectionist models and belief networks, with particular emphasis\\non problems involving learning about sequential and structured information.\\nApplication ﬁelds of his interest include bioinformatics, natural language'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='Application ﬁelds of his interest include bioinformatics, natural language\\nprocessing, pattern recognition, and document processing. He serves as anAssociate Editor of the ACM Transactions on Internet Technology .\\nDr. Frasconi serves as an Associate Editor for the IEEE T\\nRANSACTIONS ON\\nNEURAL NETWORKS and the IEEE T RANSACTIONS ON KNOWLEDGE AND DATA\\nENGINEERING . In 2001, he co-directed the NATO Advanced Studies Institute\\n“Artiﬁcial Intelligence and Heuristic Methods for Bioinformatics. ”He is a\\nMember of the Association for Computing Machinery (ACM), the International\\nAssociation for Pattern Recognition (IAPR), and the Associazione Italiana perl’Intelligenza Arti ﬁciale (\\n/65 /73 /3 /73/65).\\nVincenzo Lombardo was born 1964. He received the\\nLaurea degree in computer science from the Univer-\\nsity of Turin, Turin, Italy, in 1987 and the Ph.D. de-\\ngree in computer science from the conjoined Turin-\\nMilan Ph.D. Programme in 1993.\\nHe is currently an Associate Professor in the\\nDepartment of Computer Science and the School of\\nMultimedia and Arts, University of Turin. His cur-\\nrent research areas are natural language processing\\n(syntax, parsing, computational psycholinguistics),artiﬁcial intelligence for art and drama, and computer\\nmusic.\\nPatrick Sturt received the M.A. degree in linguistics\\nfrom University College London, London, U.K., and\\nthe Ph.D. degree in cognitive science from the Uni-\\nversity of Edinburgh, Edinburgh, U.K.\\nHe is currently a Senior Lecturer in Psychology'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='versity of Edinburgh, Edinburgh, U.K.\\nHe is currently a Senior Lecturer in Psychology\\nat the University of Glasgow, Glasgow, U.K. His\\nresearch interests include the experimental studyof human sentence processing using eye-movement\\nrecording techniques, and computational models of\\nparsing.\\nGiovanni Soda (M’04) received the degree M.Sc. de-\\ngree in mathematics from the University of Florence,\\nFlorence, Italy, in 1969.\\nFrom 1971 to 1982, he was a Researcher at\\nthe National Council of Research (CNR), where\\nhis activity included formal systems for language\\nmanipulation, and data structures. From 1983 to\\n1991, he was Associate Professor of Programming\\nLanguages in the Faculty of Electronic Engineering,\\nUniversity of Florence, where he is presently a\\nFull Professor of arti ﬁcial intelligence. His current\\ninterests include knowledge representation systems, integration of arti ﬁcial\\nintelligence techniques with databases, neural networks and document pro-\\ncessing. He is author of about 100 papers in international journals, contributed\\nvolumes, and conference proceedings on several aspects of computer science\\nand arti ﬁcial intelligence.\\nProfe. Soda is a Member of the Association for Computing Machinery (ACM)\\nand a Member of the Steering Committee of the Associazione Italiana per l ’In-\\ntelligenza Arti ﬁciale ( /65/73 /3 /73/65). He is the Editor-in-Chief of the /65/73 /3 /73/65Notizie.'),\n",
       " Document(metadata={'title': 'Ambiguity Resolution Analysis in Incremental Parsing of Natural Language', 'year': 2005}, page_content='Authorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='An algorithm for suffix stripping \\nM.F. Porter \\nComputer Laboratory, Corn Exchange Street, Cambridge \\nABSTRACT \\nThe automatic removal of suffixes from words in English is of particular interest in the field \\nof information retrieval. An algorithm for suffix stripping is described, which has been \\nimplemented as a short, fast program in BCPL. Although simple, it performs slightly better \\nthan a much more elaborate system with which it has been compared. It effectively works \\nby treating complex suffixes as compounds made up of simple suffixes, and removing the \\nsimple suffixes in a number of steps. In each step the removal of the suffix is made to depend \\nupon the form of the remaining stem, which usually involves a measure of its syllable length. \\n1. INTRODUCTION \\nRemoving suffixes from words by automatic means is an operation which \\nis especially useful in the field of information retrieval. In a typical IR \\nenvironment, one has a collection of documents, each described by the \\nwords in the document title and possibly the words in the document \\nabstract. Ignoring the issue of precisely where the words originate, we can \\nsay that a document is represented by a vector of words, or terms. Terms \\nwith a common stem will usually have similar meanings, for example: \\nCONNECT \\nCONNECTED \\nCONNECTING \\nCONNECTION \\nCONNECTIONS \\nFrequently, the performance of an IR system will be improved if term \\ngroups such as this are conflated into a single term. This may be done by'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='groups such as this are conflated into a single term. This may be done by \\nremoval of the various suffixes -ED, -ING, -ION, -IONS to leave the \\nsingle stem CONNECT. In addition, the suffix stripping process will \\nreduce the total number of terms in the IR system, and hence reduce the \\nsize and complexity of the data in the system, which is always advantageous. \\nMany strategies for suffix stripping have been reported in the \\nliterature.(e.g. 1-6) The nature of the task will vary considerably depending \\non whether a stem dictionary is being used, whether a suffix list is being \\nused, and of course on the purpose for which the suffix stripping is being \\ndone. Assuming that one is not making use of a stem dictionary, and that \\nthe purpose of the task is to improve IR performance, the suffix stripping \\nprogram will usually be given an explicit list of suffixes, and, with each \\nsuffix, the criterion under which it may be removed from a word to leave \\n130 Program, Vol. 14. no. 3, pp 130-137, July 1980 July 1980 Suffix stripping \\na valid stem. This is the approach adopted here. The main merits of the \\npresent program are that it is small (less than 400 lines of BCPL), fast \\n(it will process a vocabulary of 10,000 different words in about 8.1 seconds \\non the IBM 370/165 at Cambridge University), and reasonably simple. \\nAt any rate, it is simple enough to be described in full as an algorithm in \\nthis paper. (The present version in BCPL is freely available from the'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content=\"this paper. (The present version in BCPL is freely available from the \\nauthor. BCPL itself is available on a wide range of different computers, \\nbut anyone wishing to use the program should have little difficulty in \\ncoding it up in other programming languages.) Given the speed of the \\nprogram, it would be quite realistic to apply it to every word in a large file \\nof continuous text, although for historical reasons we have found it \\nconvenient to apply it only to relatively small vocabulary lists derived from \\ncontinuous text files. \\nIn any suffix stripping program for IR work, two points must be borne in \\nmind. Firstly, the suffixes are being removed simply to improve IR \\nperformance, and not as a linguistic exercise. This means that it would not \\nbe at all obvious under what circumstances a suffix should be removed, \\neven if we could exactly determine the suffixes of the words by automatic \\nmeans. \\nPerhaps the best criterion for removing suffixes from two words W1 and \\nW2 to produce a single stem S, is to say that we do so if there appears to \\nbe no difference between the two statements 'a document is about W1' and \\n'a document is about W2'. So if W1 = 'CONNECTION' and \\nW2 = 'CONNECTIONS' it seems very reasonable to conflate them to a \\nsingle stem. But if W1 = 'RELATE' and W2 = 'RELATIVITY' it seems \\nperhaps unreasonable, especially if the document collection is concerned \\nwith theoretical physics. (It should perhaps be added that RELATE and\"),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='with theoretical physics. (It should perhaps be added that RELATE and \\nRELATIVITY are conflated together in the algorithm described here.) \\nBetween these two extremes there is a continuum of different cases, and \\ngiven two terms W1 and W2, there will be some variation in opinion as to \\nwhether they should be conflated, just as there is with deciding the \\nrelevance of some document to a query. The evaluation of the worth of a \\nsuffix stripping system is correspondingly difficult. \\nThe second point is that with the approach adopted here, i.e. the use of \\na suffix list with various rules, the success rate for the suffix stripping will \\nbe significantly less than 100%, irrespective of how the process is evaluated. \\nFor example, if SAND and SANDER get conflated, so most probably \\nwill WAND and WANDER. The error here is that the -ER of WANDER \\nhas been treated as a suffix when in fact it is part of the stem. Equally a \\nsuffix may completely alter the meaning of a word, in which case its \\nremoval is unhelpful. PROBE and PROBATE for example, have quite \\ndistinct meanings in modern English. (In fact these would not be conflated \\nin our present algorithm.) There comes a stage in the development of a \\nsuffix stripping program where the addition of more rules to increase the \\nperformance in one area of the vocabulary causes an equal degradation \\nof performance elsewhere. Unless this phenomenon is noticed in time, it'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='of performance elsewhere. Unless this phenomenon is noticed in time, it \\nis very easy for the program to become much more complex than is really \\nnecessary. It is also easy to give undue emphasis to cases which appear to \\n131 Program Vol. 14 no. 3 \\nbe important, but which turn out in practice to be rather rare. For example, \\ncases in which the spelling of the root of the word changes with the \\naddition of a suffix, as in DECEIVE/DECEPTION, RESUME/ \\nRESUMPTION, INDEX/INDICES, occur much more rarely in \\nreal vocabularies than one might at first suppose. In view of the error \\nrate that must in any case be expected, it did not seem worthwhile to \\ntry and cope with these cases. \\nIt is not obvious that the simplicity of the present program is any demerit. \\nIn a test on the well known Cranfield 200 collection7 it gave an improvement \\nin retrieval performance when compared with a very much more elaborate \\nprogram which has been in use in IR research at Cambridge since 1971(2, 6). \\nThe test was done as follows: the words of the titles and abstracts in the \\ndocuments were passed through the earlier suffix stripping system, and \\nthe resulting stems were used to index the documents. The words of the \\nqueries were reduced to stems in the same way, and the documents were \\nranked for each query using term coordination matching of query against \\ndocument. From these rankings, recall and precision values were obtained \\nusing the standard recall cutoff method. The entire process was then'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content=\"using the standard recall cutoff method. The entire process was then \\nrepeated using the suffix stripping system described in this paper, and the \\nresults were as follows: \\nearlier system \\nprecision \\n0 \\n10 \\n20 \\n30 \\n40 \\n50 \\n60 \\n70 \\n80 \\n90 \\n100 recall \\n57.24 \\n56.85 \\n52.85 \\n42.61 \\n42.20 \\n39.06 \\n32.86 \\n31.64 \\n27.15 \\n24.59 \\n24.59 present system \\nprecision \\n0 \\n10 \\n20 \\n30 \\n40 \\n50 \\n60 \\n70 \\n80 \\n90 \\n100 recall \\n58.60 \\n58.13 \\n53.92 \\n43.51 \\n39.39 \\n38.85 \\n33.18 \\n31.19 \\n27.52 \\n25.85 \\n25.85 \\nClearly the performance is not very different. The important point is that \\nthe earlier, more elaborate system certainly performs no better than the \\npresent, simple system. \\n(This test was done by Prof. C.J. van Rijsbergen.) \\n2. THE ALGORITHM \\nTo present the suffix stripping algorithm in its entirety we will need a few \\ndefinitions. \\nA consonant in a word is a letter other than A, E, I, O and U, and other \\nthan Y preceded by a consonant. (The fact that the term 'consonant' is \\ndefined to some extent in terms of itself does not make it ambiguous.) So \\nin TOY the consonants are T and Y, in SYZYGY they are S, Z and G. \\nIf a letter is not a consonant it is a vowel. \\n132 July 1980 Suffix stripping \\nA consonant will be denoted by c, a vowel by v. A list ccc ... of length \\ngreater than 0 will be denoted by C, and a list vvv ... of length greater \\nthan 0 will be denoted by V. Any word, or part of a word, therefore has \\none of the four forms: \\nCVCV ... C \\nCVCV ... V \\nVCVC ... C \\nVCVC ... V\"),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content=\"one of the four forms: \\nCVCV ... C \\nCVCV ... V \\nVCVC ... C \\nVCVC ... V \\nThese may all be represented by the single form \\n[C] VCVC ... [V] \\nwhere the square brackets denote arbitrary presence of their contents. \\nUsing (VC)m to denote VC repeated m times, this may again be written \\nas \\n[C] (VC)m [V]. \\nm will be called the measure of any word or word part when represented \\nin this form. The case m=0 covers the null word. Here are some examples: \\nm=0 TR, EE, TREE, Y, BY. \\nm=l TROUBLE, OATS, TREES, IVY. \\nm = 2 TROUBLES, PRIVATE, OATEN, ORRERY. \\nThe rules for removing a suffix will be given in the form \\n(condition) S1 → S2 \\nThis means that if a word ends with the suffix S1, and the stem before S1 \\nsatisfies the given condition, S1 is replaced by S2. The condition is usually \\ngiven in terms of m, e.g. \\n(m> 1) EMENT → \\nHere S1 is 'EMENT' and S2 is null. This would map REPLACEMENT \\nto REPLAC, since REPLAC is word part for which m = 2. \\nThe 'condition' part may also contain the following: \\n*S — the stem ends with S (and similarly for the other letters). \\n*v* — the stem contains a vowel. \\n*d — the stem ends with a double consonant (e.g. -TT, -SS). \\n*o — the stem ends cvc, where the second c is not W, X or Y (e.g. -WIL, \\n-HOP). \\nAnd the condition part may also contain expressions with and, or and not, \\nso that \\n(m> 1 and (*S or *T)) \\ntests for a stem with m> 1 ending in S or T, while \\n(*d and not (*L or *S or *Z)) \\ntests for a stem ending with a double consonant other than L, S or Z.\"),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content=\"tests for a stem ending with a double consonant other than L, S or Z. \\nElaborate conditions like this are required only very rarely. \\n133 Program Vol. 14 no. 3 \\nIn a set of rules written beneath each other, only one is obeyed, and this \\nwill be the one with the longest matching S1 for the given word. For \\nexample, with \\nSSES → SS \\nIES → I \\nSS → SS \\nS → \\n(here the conditions are all null) CARESSES maps to CARESS since \\nSSES is the longest match for S1. Equally CARESS maps to CARESS \\n(S1 = 'SS') and CARES to CARE (S1 = 'S'). \\nIn the rules below, examples of their application, successful or otherwise, \\nare given on the right in lower case. The algorithm now follows: \\nStep 1a \\nSSES \\nIES \\nSS \\nS → \\n→ \\n→ \\n→ SS \\nI \\nSS caresses \\nponies \\nties \\ncaress \\ncats → \\n→ \\n→ → \\n→ caress \\nponi \\nti caress \\ncat \\nStep 1b \\n(m> 0) \\n(*v*) \\n(*v*) EED \\nED \\nING → \\n→ \\n→ EE feed \\nagreed \\nplastered \\nbled \\nmotoring \\nsing → \\n→ \\n→ \\n→ \\n→ \\n→ feed \\nagree \\nplaster \\nbled \\nmotor \\nsing \\nIf the second or third of the rules in Step lb is successful, the following \\nis done: \\nAT → ATE \\nBL → BLE \\nIZ → IZE \\n(*d and not (*L or *S or *Z)) \\n→ single letter \\n(m=1 and *o) → E conflat(ed) \\ntroubl(ing) \\nsiz(ed) \\nhopp(ing) \\ntann(ed) \\nfall(ing) \\nhiss(ing) \\nfizz(ed) \\nfail(ing) \\nfil(ing) → \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ conflate \\ntrouble \\nsize \\nhop \\ntan \\nfall \\nhiss \\nfizz \\nfail \\nfile \\nThe rule to map to a single letter causes the removal of one of the double \\nletter pair. The -E is put back on -AT, -BL and -IZ, so that the suffixes\"),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='letter pair. The -E is put back on -AT, -BL and -IZ, so that the suffixes \\n-ATE, -BLE and -IZE can be recognised later. This E may be removed \\nin step 4. \\nStep 1c \\n(*v*) Y → I happy \\nsky → \\n→ happi \\nsky \\n134 July 1980 Suffix stripping \\nStep 1 deals with plurals and past participles. The subsequent steps are \\nmuch more straightforward. \\nStep 2 \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) \\n(m>0) ATIONAL \\nTIONAL \\nENCI \\nANCI \\nIZER \\nABLI \\nALLI \\nENTLI \\nELI \\nOUSLI \\nIZATION \\nATION \\nATOR \\nALISM \\nIVENESS \\nFULNESS \\nOUSNESS \\nALITI \\nIVITI \\nBILITI → ATE \\n→ TION \\n→ ENCE \\n→ ANCE \\n— IZE \\n→ ABLE \\n→ AL \\n→ ENT \\n→ E \\n→ OUS \\n→ IZE \\n→ ATE \\n→ ATE \\n→ AL \\n→ IVE \\n→ FUL \\n→ OUS \\n→ AL \\n→ IVE \\n→ BLE relational \\nconditional \\nrational \\nvalenci \\nhesitanci \\ndigitizer \\nconformabli \\nradicalli \\ndifferetli \\nvileli \\nanalogousli \\nvietnamization \\npredication \\noperator \\nfeudalism \\ndecisiveness \\nhopefulness \\ncallousness \\nformaliti \\nsensitiviti \\nsensibiliti → \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ relate \\ncondition \\nrational \\nvalence \\nhesitance \\ndigitize \\nconformable \\nradical \\ndifferent \\nvile \\nanalogous \\nvietnamize \\npredicate \\noperate \\nfeudal \\ndecisive \\nhopeful \\ncallous \\nformal \\nsensitive \\nsensible \\nThe test for the string S1 can be made fast by doing a program switch on \\nthe penultimate letter of the word being tested. This gives a fairly even'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='the penultimate letter of the word being tested. This gives a fairly even \\nbreakdown of the possible values of the string SI. It will be seen in fact \\nthat the S1-strings in step 2 are presented here in the alphabetical order \\nof their penultimate letter. Similar techniques may be applied in the other \\nsteps. \\nStep 3 \\n(m>0) ICATE → IC \\n(m>0) ATIVE → \\n(m>0) ALIZE → AL \\n(m>0) ICITI → IC \\n(m>0) ICAL → IC \\n(m>0) FUL → \\n(m>0) NESS → triplicate \\nformative \\nformalize \\nelectriciti \\nelectrical \\nhopeful \\ngoodness → \\n→ \\n→ \\n→ \\n→ \\n→ triplic \\nform \\nformal \\nelectric \\nelectric \\nhope \\ngood \\nStep 4 \\n(m>1) AL \\n(m>1) ANCE (m>1) ENCE \\n(m>1) ER \\n(m>1) IC \\n(m>1) ABLE \\n(m>1) IBLE → \\n→ → \\n→ \\n→ \\n→ \\n→ revival \\nallowance inference \\nairliner \\ngyroscopic \\nadjustable \\ndefensible → \\n→ → \\n→ \\n→ \\n→ \\n→ reviv \\nallow infer \\nairlin \\ngyroscop \\nadjust \\ndefens \\n135 Program Vol. 14 no. 3 \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) \\n(m>1) ANT \\nEMENT \\nMENT \\nENT \\nand (*S or \\n*T)) ION \\nOU \\nISM \\nATE \\nITI \\nOUS \\nIVE \\nIZE → \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ irritant \\nreplacement \\nadjustment \\ndependent \\nadoption \\nhomologou \\ncommunism \\nactivate \\nangulariti \\nhomologous \\neffective \\nbowdlerize → \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ \\n→ irrit \\nreplac \\nadjust \\ndepend \\nadopt \\nhomolog \\ncommun \\nactiv \\nangular \\nhomolog \\neffect \\nbowdler \\nThe suffixes are now removed. All that remains is a little tidying up. \\nStep 5a \\n(m>1) E \\n(m=1 and not *o) E → \\n→ probate \\nrate cease → \\n→ \\n→ probat \\nrate ceas \\nStep 5b'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='Step 5a \\n(m>1) E \\n(m=1 and not *o) E → \\n→ probate \\nrate cease → \\n→ \\n→ probat \\nrate ceas \\nStep 5b \\n(m>1 and *d and *L) → \\n→ single letter \\ncontroll \\nroll → \\n→ control \\nroll \\nThe algorithm is careful not to remove a suffix when the stem is too short, \\nthe length of the stem being given by its measure, m. There is no linguistic \\nbasis for this approach. It was merely observed that m could be used quite \\neffectively to help decide whether or not it was wise to take off a suffix. \\nFor example, in the following two lists: \\nlist A list B \\nRELATE DERIVATE \\nPROBATE ACTIVATE \\nCONFLATE DEMONSTRATE \\nPIRATE NECESSITATE \\nPRELATE RENOVATE \\n-ATE is removed from the list B words, but not from the list A words. \\nThis means that the pairs DERIVATE/DERIVE, ACTIVATE/ \\nACTIVE, DEMONSTRATE/DEMONSTRABLE, NECESSITATE/ \\nNECESSITOUS, will conflate together. The fact that no attempt is made \\nto identify prefixes can make the results look rather inconsistent. Thus \\nPRELATE does not lose the -ATE, but ARCHPRELATE becomes \\nARCHPREL. In practice this does not matter too much, because the \\npresence of the prefix decreases the probability of an erroneous conflation. \\n136 July 1980 Suffix stripping \\nComplex suffixes are removed bit by bit in the different steps. Thus \\nGENERALIZATIONS is stripped to GENERALIZATION (Step 1), \\nthen to GENERALIZE (Step 2), then to GENERAL (Step 3), and then \\nto GENER (Step 4). OSCILLATORS is stripped to OSCILLATOR'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='to GENER (Step 4). OSCILLATORS is stripped to OSCILLATOR \\n(Step 1), then to OSCILLATE (Step 2), then to OSCILL (Step 4), and \\nthen to OSCIL (Step 5). In a vocabulary of 10,000 words, the reduction \\nin size of the stem was distributed among the steps as follows: \\nSuffix stripping of a vocabulary of 10,000 words \\nNumber of words reduced in step \\n\" \\n\" \\n\" \\n\" \\nNumber of words not reduced: 1: \\n2: \\n3: \\n4: \\n5: 3597 \\n766 \\n327 \\n2424 \\n1373 \\n3650 \\nThe resulting vocabulary of stems contained 6370 distinct entries. Thus \\nthe suffix stripping process reduced the size of the vocabulary by about \\none third. \\nACKNOWLEDGEMENTS \\nThe author is grateful to the British Library R&D Department for the \\nfunds which supported this work. \\nREFERENCES \\n1. LOVINS, J.B. Development of a Stemming Algorithm. Mechanical \\nTranslation and Computational Linguistics. 11 (1) March 1968 \\np. 22-31. \\n2. ANDREWS, K. The Development of a Fast Conflation Algorithm \\nfor English. Dissertation for the Diploma in Computer Science, Com\\xad\\nputer Laboratory, University of Cambridge, 1971. \\n3. PETRARCA, A.E. and LAY W.M. Use of an automatically gen\\xad\\nerated authority list to eliminate scattering caused by some singular \\nand plural main index terms. Proceedings of the American Society for \\nInformation Science, 6 1969 p. 277-282. \\n4. DATTOLA, Robert T. FIRST: Flexible Information Retrieval Sys\\xad\\ntem for Text. Webster N.Y: Xerox Corporation, 12 Dec 1975. \\n5. COLOMBO, D.S. and NIEHOFF R.T. Final report on improved'),\n",
       " Document(metadata={'title': 'An Algorithm for Suffix Stripping', 'year': 1980}, page_content='5. COLOMBO, D.S. and NIEHOFF R.T. Final report on improved \\naccess to scientific and technical information through automated vocab\\xad\\nulary switching. NSF Grant No. SIS75-12924 to the National Science \\nFoundation. \\n6. DAWSON, J.L. Suffix Removal and Word Conflation. ALLC Bul\\xad\\nletin, Michaelmas 1974 p. 33-46. \\n7. CLEVERDON, C.W., MILLS J. and KEEN M. Factors Deter\\xad\\nmining the Performance of Indexing Systems 2 vols. College of Aero\\xad\\nnautics, Cranfield 1966. \\n137'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='1556-603x/13/$31.00©2013ieee  August 2013 | ieee Comput Ation Al intelligen Ce mAgAzine    41Abstract —The Internet of Things (IoT) is emerging as the \\nmajor trend in shaping the development of the next generation \\nof information networks. The challenges of the enormous, \\ndynamic, incredibly diverse and high complexity of the IoT \\nurgently require novel self-organization scheme because most of the existing distributed self-organization schemes cannot be directly applied to it. In this paper, we propose an intelligent self-organizing scheme (ISOS) for the IoT inspired by the \\nendocrine regulating mechanism. For each node in the net-\\nwork, an autonomous area is established, where the node can effectively interact with its peers and perform self-control according to its own status and dynamic circumstance in a \\ndecentralized infrastructure. By introducing the hormone \\nmechanism as the medium for information transmission and data sharing, the nodes can collaborate with each other and work in a cooperative way. Through adjusting the release pro-cedure of the hormones, the ability to effectively detect service \\nrandomly generated can also be guaranteed in the probabilistic \\npartially-working IoT. Simulation results verify the perfor-mance of the proposed mechanism that entitles the IoT to the ability of maintaining its status in a globally stable status, while effectively discovering the random service requests in a resource-critical configuration. The ISOS would be of great'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='significance for the practical implementation of the IoT.\\nI. Introduction \\nthe past few decades have witnessed striking develop-\\nments in communication and networking technolo-gies which have yielded many information network \\narchitectures. One prominent product of this evolu-\\ntion, the Internet of Things (IoT), is emerging as the major \\ntrend in shaping the development of the next generation of information networks [1]–[3]. As a novel giant-sized network infrastructure, the IoT is characterized by its large-scale hetero-\\ngeneous network elements, the uncertainty of the sensing \\ninformation and the dynamic external environment it usually resides in. These features raise the difficulty of realizing the IoT in its true sense [2]–[4]. Although attention from both the aca-\\ndemic and engineering fields on these issues about the IoT \\nhave been raised and are still on the rise, most of the approach-es proposed have been conceived to be applied to a single, well-defined specific application field [5]–[7], and the currently proposed models or architectures of the IoT mostly stay at the \\nDigital Object Identifier 10.1109/MCI.2013.2264251\\nDate of publication: 12 July 2013Yongsheng Ding, Yanling Jin,  \\nLihong Ren, and Kuangrong Hao \\nDonghua University, Shanghai, China\\nAn Intelligent \\nSelf-Organization \\nScheme for \\nthe Internet  \\nof Things\\n© photodisc'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='An Intelligent \\nSelf-Organization \\nScheme for \\nthe Internet  \\nof Things\\n© photodisc\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 42    ieee Comput Ation Al intelligen Ce mAgAzine | August 2013conceptual stage or provide a possible guidance for its detailed \\nresearch and implementation in the future [8]–[11].\\nSelf-organization is a great concept for building scalable sys-\\ntems consisting of a huge number of subsystems [12], [13]. The primary objectives are improving scalability and dynamic adap-tation to changing environmental conditions. The design of self-organization schemes for distributed systems is considered as the most likely solution to the challenges of the enormous, \\ndynamic, incredibly diverse and high complexity in IoT, which \\ncalls for distributed intelligence and makes smart objects (or a subset thereof) autonomously react to a wide range of different situations without human interventions [2], [13].\\nUntil now, many self-organization algorithms and protocols \\nhave been developed for communication networks, and ad hoc and sensor networks in particular [14]–[16]. In many cases, these networks are similar to IoT or are components of IoT, but there are distinguished conceptual differences among these networks \\nand the IoT, for which more complicated and enormous prob-'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='and the IoT, for which more complicated and enormous prob-\\nlems need to be solved. Clearly, according to the conventional networking paradigms which are not able to accommodate the scale, heterogeneity and complexity of such scenarios, most of the existing distributed self-organization schemes cannot be directly \\napplied to IoT and novel schemes are urgently required [2]–[4]. \\nWhile the challenges above such as scalability, heterogeneity \\nand complexity are somehow new by-products of the evolu-\\ntion in communication technologies, they have been success-fully dealt with by nature [3], [17]. Self-organized control \\ninspired by biological systems has been receiving considerable \\nattentions as a promising concept for realizing robustness, scal-ability, and adaptability [3], [16]. Varieties of such methods have been proposed and found their way in the communication and \\nnetwork research due to the analogy between biological sys-\\ntems and large networks [18], [19], such as the ant colony opti-mization [20], the particle swarm optimization [13]–[22], and other similar methods [23], [24]. Other mechanisms like the natural time synchronization, the artificial immune system [25] \\nand the intercellular information exchange as well [15]. \\nThe artificial endocrine system (AES) is another type of bio-\\ninspired mechanism but has not gained much attention due to its'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='inspired mechanism but has not gained much attention due to its \\nlimitation in practical application. A possible reason for such a sit-uation lies in the fact applying hormone-based schemes to the \\ncommunication and control of the distributed and self-organized \\nsystems is application-specific, but some exploratory achieve-ments still have been made. W . Shen et al. [26] proposed a con-cept of hormone for communication and control in modular self-reconfigurable robots. B. Atakan et al. [14] introduced a \\nhomeostasis-inspired autonomous communication protocol for \\nwireless audio sensor networks to maintain a relatively stable state with the sensor nodes’ self-organization capabilities. Y . Ding et al. [27] presented a method inspired from the neuroendocrine-\\nimmune system for the automatic composition and dynamic \\nmanagement of Web services. The implementations of these bio-inspired mechanisms actually provide implications that their fea-tures could have the potential for being applied to the research of the self-organization and the corresponding methods in IoT.In this paper, an intelligent self-organization scheme (ISOS) \\nfor the IoT inspired by the endocrine mechanism of blood glu-cose regulation in human body is proposed. Taking the character-\\nistics and requirements of the IoT into account, an autonomous'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='istics and requirements of the IoT into account, an autonomous \\narea is established for each node in the network. The node with such an area can interact with its peers and perform self-control effectively according to its own status and dynamic circumstances in a decentralized infrastructure. Also, the ability to effectively dis-\\ncover service requests randomly generated can be guaranteed by \\nadjusting the releasing procedure of the hormones in a probabilis-tic partially-working IoT. Such mechanism entitles the IoT to the ability of maintaining its status in a globally stable status, while \\nrealizing an effective detection of the random service requests in a \\nresource-critical configuration, which would be of great signifi-cance for the practical implementation of the IoT. A series of  \\nscenario-based simulations with results verifies the performance of the proposed algorithm.\\nThe remainder of this paper is organized as follows. The char-\\nacteristics of the self-organization mechanisms and the logical structure for building the self-organized IoT are provided in Sec-tion 2. In Section 3, the ISOS mechanism and the detailed deductions are given. The simulations and the analysis of the \\nresults are provided in Section 4. The research efforts are finally \\nconcluded in Section 5, together with the plan for future work.\\nII. Self-Organization Mechanisms for Internet of Things\\na. Characteristics of the Self-Organization-Featured ioT'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='a. Characteristics of the Self-Organization-Featured ioT\\nAs a specific controlling paradigm for complex systems, the \\nconcept of self-organization covers multiple properties which can be regarded as the basic requirements for enabling the \\ndesired behaviors in such systems. On account of the inherent \\ncharacteristics and the expected functionality of the IoT, a qualified implementation of a self-organization scheme should satisfy the requirements below.\\n1) Decentralized infrastructure based on autonomy. Comparing \\nthe self-organized systems with the conventional centralized \\nsystems, the main difference lies in whether the global infor-mation of the system can be collected and utilized [3], [12]. For the IoT, which is characterized by highly evolved hetero-\\ngeneity, a fixed controlling center capable and powerful \\nenough to control the status of the whole system and maneuver its behaviors is generally unreasonable. So a self-organization scheme based on the decentralized infrastruc-ture may be much more suitable for the IoT. A possible \\nimplementation of this idea is that each node in the IoT can \\nbe implemented with a certain degree of autonomy. Such nodes can therefore collect information about their respective environments and make decisions for their own behaviors. In this case, the data (including instructions and information) \\ntransmitted among nodes in a fully decentralized form can be'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='transmitted among nodes in a fully decentralized form can be \\nrestricted into a limited area, which decreases heterogeneity of the whole network of the IoT. Such decentralized archi-tecture also enables the self-organization scheme to be scal-able and adaptive to variations on the size of the network.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. August 2013 | ieee Comput Ation Al intelligen Ce mAgAzine    432) Efficient collaboration based on ubiquitous data exchang-\\ning and sharing. The ubiquitous and efficient data exchange in the IoT is not only one of its primary objec-\\ntives but also an essential prerequisite for the realization of \\nthe self-organization scheme. The IoT extends the inter-connection among the information equipment to that of all the intelligent or non-intelligent objects [4]. Such extensiveness in quantity, type and connecting mode \\nrequire a mechanism that can guarantee the compatibility \\namong network elements. Based on the data exchanging and sharing mechanism of nodes, a wide range of collabo-ration can be established. With such a feature, the member \\nnodes in the IoT consisting of tight-coupling regions \\nshould be able to collect information from their neighbors while the comprehensive and intelligent services which ask for participation of multiple nodes with different catego-ries, functions and contents can also be offered.'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='3) Energy-optimized solution based on working status swi-tchover. The minimization of the energy spent for com-munication/computing purposes is always a primary concern for the IoT -owned entities, and the solutions to optimize the energy consumption (even at the expense \\nof performance) have become more and more attractive \\n[2]. Sleep control mechanism is one of the approaches that is effective in reducing energy consumption, and many studies on the distributed sleep control techniques where the kernel purpose is to switch off redundant \\nnodes to reduce energy consumption have been actively \\ncarried out [16], [28]. Another advantage that may be brought by the sleep control strategy is that the internal throughput of the IoT can be decreased and the interfer-\\nence among nodes in a densely distributed network can \\nbe eliminated due to the limitation of the quantity of the active nodes. Special attention should be paid to the IoT is based on a 3A (“Anywhere, Anytime, Anyone”) princi-ple with full randomness [1]. As a result, a well-designed \\nsleep control strategy for the IoT should not only \\ndecrease its energy consumption but also satisfy the requirements of the reasonably working nodes to con-duct the high-quality functionalities.\\n4) Intelligent service discovery based on adaptive response to \\nthe demands. Facing the clients’ various requests, the nodes'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='the demands. Facing the clients’ various requests, the nodes \\nin an IoT have to organize autonomously and provide basic means for sharing data and performing the coordinated tasks. This calls for the ability to adaptively perform service discov-ery rather than requiring an external trigger to the current \\ncontext. The rising of the IoT provides a shift in service pro-\\nvisioning, moving from the current vision of always-on ser-vices, typical of the Web era, to always-responsive situated services, built and composed at run-time to respond to a \\nspecific need and able to account for the client’s context [2]. \\nT o achieve this goal, the node on discovering a service request should not only keep detection on the client but also inform other nodes around immediately to guarantee the persistent monitoring till all the possible demands have been satisfied in an appropriate way (e.g., satisfied or reasonably transmitted, delayed or ignored). By this means, an intelligent service discovery scheme should be established on each indi-\\nvidual self-organizing node, and thus an integrated service \\ndiscovery system can be realized.\\nB. Logical Structure of the Self-Organized ioT\\nConsidering an IoT consisting of N  self-organizing nodes ran-\\ndomly deployed and can perform the service discovery within their coverage according to the design requirements as above. \\nThe model of a member node and an object (marked with A'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='The model of a member node and an object (marked with A \\nand B, respectively) in IoT are shown in Fig. 1. Some specific ranges for different functions are defined as below.\\n1) \\n.RC The circular area with a radius of RC is the commu-\\nnication range of the node N at point A. The value of RC \\nis determined by the type of the central node. \\n2) .RSD The circular area with a radius of RSD is the range in \\nwhich a node can receive the request for service (RFS) raised around it. The RFS from a client can be “heard” if his \\nor her distance from the node is less than \\n.RSD The node \\nthat detects an RFS will record the information of its trans-\\nmitter, and the format of such record can be denoted as\\n RFSobjID, ,coordinates , t ^h  (1)\\n where objID denotes the transmitter of an RFS, t  denotes \\nthe round when this RFS is originated and coordinates denotes the location of the client. Note that the sensing scopes of the nodes can be different and assume that all the \\nnodes in the network have the same \\nRSD which does not \\nbring significant change to the algorithm.\\n3) .RSR The circular area with the radius of RSR is the \\nrange that a client at point B can spread its RFS to. The \\nRFS can be discovered by a working node only if its dis-\\ntance from the transmitter is less than .RSR\\nGenerally, the area that a node can discover the RFS should \\nbe large enough to guarantee the reliability of the discovery. \\nBesides, the value of RC cannot be too small to guarantee a'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='Besides, the value of RC cannot be too small to guarantee a \\nreasonable information transmitting process among the nodes in the IoT. Here the relationship of \\n,,RRCS D and RSR can be \\ndefined as\\n . RR RR SR SD SC# ==  (2)\\nAs described earlier, the giant scale, complex heterogeneity \\nand systematic dynamics of the IoT ask its member nodes for a \\nhigh level of autonomy. Based on the node model of the IoT, an individual autonomous area can be created for each node where it can perform self-control autonomously according to the dynamic circumstance and its real-time status rather than \\nreceiving instructions from outside. Fig. 2 shows the autono-\\nmous areas of the node A and node B, respectively. The features of such areas can be observed as below,\\n1) Every node in the IoT has an autonomous area depending on its location, communication radius \\nRC and the \\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 44    ieee Comput Ation Al intelligen Ce mAgAzine | August 2013distribution of its neighboring nodes. As illustrated in \\nFig. 2 (a), the autonomous area of node A contains the \\nnode A itself and other 13 nodes (including the node B) \\nwithin the range of .RC Note that the node A only man-\\nages itself but is not responsible for controlling the other \\nnodes in its autonomous area, namely, the controlling hier-\\narchy is flat for every node in this area.'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='archy is flat for every node in this area. \\n2) The autonomous areas of different nodes may overlap on \\neach other, and the member nodes joining in such areas are dynamically changing with their respective working status. \\nCompared with the situation in Fig. 2 (a), the autonomous \\narea of node A in Fig. 2 (b) is organized by 10 nodes with-in its communication radius \\nRC while 4 of them are sleep-\\ning which therefore cannot be taken as its members in this situation. The organization of member nodes in each \\nautonomous area affects neither the forming process of \\nsuch area nor the regular working process of each node involved. Such design makes the nodes dynamically form their respective autonomous areas, which will help them adapt to the continuously changing environment, and the \\nfollowing control scheme built in them can be modified \\nwith scalability and ease of implementation.\\nWith such a design, the nodes are able to work autono-\\nmously in a fully distributed pattern and a decentralized dis-\\ntributed control structure is therefore established.\\nIII. The ISOS for the Self-Organized IoT\\na. The Endocrine Regulating Mechanism  \\nwith its implication to the ioT\\nThe hormone in the endocrine system of human body plays a \\ncritical role in controlling the body’s physiological balance and keeping the internal environment in a steady state [26], [29]. The \\nendocrine regulation in blood glucose, which must be regulated'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='endocrine regulation in blood glucose, which must be regulated \\nto a comparatively balanced status, is a good example for demon-strating the detail mechanism of the hormone-based homeostasis. The islets of the Langerhans are the primary endocrine glands associated with the regulation of blood glucose level, and two \\ntypes of predominant metabolic hormones are applied and act \\nantagonistically during the regulation [30], [31]: 1) Insulin pro-motes anabolism and decreases the blood glucose level effectively. As the blood glucose level rises, more insulin will be released \\nimmediately to bring it back to normal range as quickly as possi-\\nble; 2) The glucagon stimulates the catabolism and leads to an increase of blood glucose. As the blood glucose level drops, the pancreas releases more glucose and halts the insulin secretion.\\nThe insulin and glucagon secreted are the main parts of a \\nfeedback system that maintain blood glucose level at a stable level. Besides the hormone-secreting mechanism above, the pancreas is also associated with the digestive system which cannot be ignored when analyzing the internal balance of the body. Some researchers find that the feeling of hunger also \\naffects glucagon secretion. The release of signals of hunger, \\nwhich is produced by the hypothalamus and usually means the body is short of fuel, can promote the secretion of gluca-gon directly without decreasing the blood glucose. This mechanism along with the hormone regulating process main-'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='tains the stability of the blood glucose level and thus guaran-\\ntees the brain’s metabolism and energy supply. It also indicates that, for the regulation of a certain metabolic activity, the col-laboration does not only happen between the glands and tis-\\nsues that have direct regulating effects on the biochemical \\nindices, but also exists among different systems related to the regulated activity. \\nRSD\\nAB\\nRCRSR\\nFigure 1  The node model with different functional ranges in the IoT.\\nABRC\\nABRC(a)\\n(b)\\nFigure 2  The autonomous area of the nodes in IoT. (a) The autono-\\nmous area of a node A contains 14 nodes. (b) The autonomous area \\nof a node A contains 10 nodes.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. August 2013 | ieee Comput Ation Al intelligen Ce mAgAzine    45For an IoT with a large quantity of heterogeneous nodes \\ndistributed in a wide area, it is a challenging task to empower \\nthe nodes with self-management and autonomic capabilities. \\nAs a highly self-organized system, the endocrine system has \\nmany intrinsic appealing characteristics such as heterogeneity, scalability and being inherently adaptive to the environment, which actually brings great similarities between it and the IoT as follows.'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='1) The member glands and organs in the endocrine system release different hormones circulating throughout the body and act simultaneously without directly affecting each other and any central biological controller. The \\norganization of the components is completely decentral-\\nized, and the function is mostly realized through various hormones, respectively. This feature can also be regarded as a significant characteristic of the expected network structure of the IoT, namely, a highly decentralized struc-\\nture and medium-based control.\\n2) The internal stability of the body, where the endocrine \\nsystem resides, is maintained through collaborative effort of heterogeneous sets of subsystems (e.g. various glands and organs) and mechanisms. Similarly, a global stability \\nof the working status of the IoT should be satisfied based \\non the extensive interconnection and interactions of the different elements. As such the synergy and collaboration among different nodes have become necessities on account of the sharply increasing complexity of the tasks \\ncarried out by the IoT. For a completely distributed net-\\nwork, the regulating mechanism of the endocrine system based on mutual cooperation can be a prominent solution for building such a scheduling scheme.'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='3) The endocrine system has an intrinsic appealing character-istic on effectively managing constrained resources with a global intelligence instead of the superposition of individu-al efforts. The hormones propagate throughout the body to achieve complex behaviors while each hormone only \\naffects specific target sites and the basic regulating rules are \\nusually simple and limited. Note that a practical IoT can-not energize all the resources to function at the same time but has to design a reasonable schedule to reach a more effective and persistent consumption level. In addition, the \\ndevices have great differences in the aspects of communica-\\ntion and computing capabilities. Taking the low energy storage and capabilities of nodes into account, all the fea-tures above should be possible to realize with simple proce-dures or algorithms. Accordingly, the endocrine yet again \\nhelp researchers by providing pointers for solution \\napproaches which address the trade-off between the high demand and limited supply of resources.\\n4) The endocrine system can perform rapid responses \\nagainst disturbances from both the internal and external'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='against disturbances from both the internal and external \\nenvironments so that the normal functionality of the body would not be harmed or can recover as soon as pos-sible. Similar to this regulating process, the expected self-organization schemes should not only be featured with the overall stability of the whole network, but also the ability of effectively detecting the service requests ran-domly generated, which lays the foundation for future \\nintelligent service provided in the IoT.\\nB. The implementation of the iSOS\\nThe ISOS proposed for the IoT is responsible for maintaining \\nthe general stability of the whole network, and can also help the nodes quickly and effectively discover the RFS emerging \\nrandomly. We assume that a normal working procedure consists \\nof several rounds and each round includes two phase noted as Phase 1 and Phase 2. The Phase 1 is the period of correspon-dence and the status judgment of the node, which is also the place where the ISOS takes effect. In Phase 2, each node \\nswitches its working status according to the result of Phase 1.\\nThe ISOS can be regarded as an instance of the sleep con-\\ntrol scheme (or called the activation strategy), in which two \\nexclusive working statuses are designed for each node of'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content=\"exclusive working statuses are designed for each node of\\n1) Waking status. A node in waking status is capable of data exchange with other nodes in its autonomous area and receiving RFS sent from the possible clients around it. The nodes in this status consume energy at a higher level compared with those in the sleeping status.\\n2) Sleeping status. A node in sleeping status stops transmitting information and cannot detect any RFS sent from the clients around it in order to acquire better energy effi -\\nciency. The node with this status requires less energy and waits until the next cycle arrives.\\nConsider an IoT consisting of N  nodes randomly deployed \\nin a predefined area, here a working status register is set to indi-cate whether the node \\nNi is in its waking or sleeping status in \\nthe round t , which can be expressed as statusFlag (,)it and sat-\\nisfy the relations as below,\\n statusFlag () ,ifi sawake .\\nstatusFla g( ), if isasleep .i,t\\ni,tN\\nN1\\n0i\\ni=\\n='  (3)\\nThe features and differences of the working statuses are \\nlisted in Table 1. Obviously, the overall energy consumption of \\nthe network will be significantly decreased if a certain portion of nodes can be led to their respective sleeping status.\\nThe ubiquitous interconnections and interoperations \\namong nodes is one of the fundamental features of the IoT that needs to build on the information transmission via cer-tain types of medium. The biological organisms in an endo-crine system have varieties of hormones circulating\"),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='throughout the body and take effect simultaneously without \\ndirectly affecting each other. This indicates that the hor-mone can be introduced to construct the information exchange network for the IoT. With the aid of hormones, it will be possible for each node to tune its own status accord-\\ning to the transmitted information rather than relying on \\ninstructions from other entities. Also, such autonomous areas combined by hormones have the potential to strengthen the connections among certain nodes to improve their coopera-tion and functions.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 46    ieee Comput Ation Al intelligen Ce mAgAzine | August 2013Based on such an idea, the ISOS can be developed for \\nIoT in which different hormones are realized and take effect \\nsimultaneously. As with the regulation model of the blood \\nglucose level, two antagonistic hormones can be designed \\nand exchanged among nodes to keep the network working properly on a waking probability predefined as P . These \\nhormones are defined as waking hormone (WH) and sleep-ing hormone (SH), respectively. Then the dynamic stability \\nof the IoT can be realized by tuning the impact of these \\ntwo hormones, which in fact simulates the blood glucose level regulating process with the glucagon and insulin involved, respectively.\\nIn the t  round, the hormones WH and SH sent from \\nNi to'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content=\"In the t  round, the hormones WH and SH sent from \\nNi to \\nNj can be expressed as WH,,ijt^h  or SH ,,, ijt^h  respectively, \\nwhich are given by\\n WH ,,,\\n,sendsWH.\\nOtherwise.ijtNt oN 10 ij= ^h '  (4)\\nand\\n SH ,,,,sendsS\\nH.\\nOtherwise.ijtNt oN 10 ij= ^h '  (5)\\nHere another register is set to express the active level of \\neach node in the IoT, which can be regarded as an indicator for \\nthe comprehensive evaluation of received hormones. The active level register owned by the node \\nNi in the round t can be \\ndenoted as AL ,it^h which is used to make the decision of the \\nworking status in the ISOS as\\n statusFlag ,,\\n,AL ,S witch_AL\\nAL , Switch_AL,itit\\nit10 <$=\\n^^\\n^hh\\nh)  (6)\\nwhere Switch_A L is the threshold for performing the status \\nswitchover. In each round, the nodes communicate with each \\nother through the hormones, and the runtime value of \\nAL ,it^h is fluctuating which is affected by WH and SH syn-\\nthetically as below:\\n1) A node ()Ni will increase its , AL it^h value by a WHD if it \\nreceives a WH from another node as\\n AL,A L, . it it WHD =+ ^^hh  (7)\\n2) A node ()Ni on receiving an SH from its neighbors will \\nchange its AL ,it^h value by a SHD as AL ,A L, . it it SHD =+ ^^hh  (8)\\nIt can be noted from (7) and (8) that the AL(i,t ) value is \\ntuned by both the received quantities of WH and SH, which actually represent the working status of the node’s neighbors to some extent. Suppose the contributions from both \\nWHD and \\nSHD on changing the AL ,it^h value have a same intensity but\"),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='WHD and \\nSHD on changing the AL ,it^h value have a same intensity but \\nin different directions, namely,\\n , WH SH DD=-  (9)\\nthen the register for AL ,it^h has an accumulating feature that \\nalways puts the alterations brought by received hormones in \\nthe current round to its previous value. In this way, the nodes \\nhave the ability to “remember” their history status.\\nFig. 3 illustrates the ISOS workflow, which consists of three \\nsequential parts as below.\\n(1) Hormone transmission and Reception\\nIn this step, the nodes in the IoT acquire information mainly \\nrepresented by the hormones being transmitted and received from the surroundings nodes. At the beginning of each round, a node firstly checks the value of its working status register \\n(that actually stores the status after the switchover in the last \\nround) to determine its status at the beginning of this round. For the case of \\nstatusFlag (, ), it 11-=  the node is awake so \\nthat both the hormone transmitting module and receiving module are activated. If \\nstatusFlag (, ), it 10-=  the node is \\nsleeping so that no RFS around it can be detected. In this case, the sleeping node would send a WH to itself to avoid being \\nfrozen in the sleeping status and being unable to awaken. This is \\na critical process to guarantee the IoT nodes can work alter-nately especially for the nodes without other nodes in their autonomous areas and cannot receive any external hormone.\\nFor an awake node \\n,Ni an indicator RFS_Indicato r is set'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content=\"For an awake node \\n,Ni an indicator RFS_Indicato r is set \\nfor recording whether any RFS around has been received, so\\n RFS_Indicato r,,\\n,ifanyRFSisreceived.\\notherwise.it10=\\n^h '  (10)\\nThese statuses bring two different cases that can be observed \\nas below.\\n1) Ni does not discover any RFS, which indicates that no cli-\\nent in need of service are located around this node. Ni trans-\\nmits both the WH and SH within its autonomous area in \\norder to maintain the general stability and functions of the \\nnetwork. (,) Pi t WH  and (,) Pi t SH  denote the probabilities for \\nNi to release WH and SH in the round t, respectively and \\nare dynamically determined by the hormones received dur-\\ning the last round t -1 and satisfy the following equation as\\n (,)( ,) ,( ,),( ,) [,]. Pi tP it Pi tP it 10 1 WH SH WH SH ! +=  (11)\\nAs shown in Fig. 3, the node also sends an SH to itself if there \\nis no RFS around it, which increases the probability of the node to enter the sleeping status in the next round hence beneficial  \\nto control its individual energy consumption. With such a  Table 1  The features of two working statuses.\\niTem Phase sleePing s TaTus Waking s TaTus\\nPhase 1 Phase 2 Phase 1 Phase 2\\nNode mANA gemeNT\\nINformATIoN  \\nTrANsmITTINg\\nINformATIoN  \\nreceIvINg\\nrfs de TecTIoN\\nTImerY\\nN\\nY\\nN\\nYNN\\nN\\nN\\nYYY\\nY\\nY\\nYYN\\nN\\nN\\nY\"),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='TrANsmITTINg\\nINformATIoN  \\nreceIvINg\\nrfs de TecTIoN\\nTImerY\\nN\\nY\\nN\\nYNN\\nN\\nN\\nYYY\\nY\\nY\\nYYN\\nN\\nN\\nY\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. August 2013 | ieee Comput Ation Al intelligen Ce mAgAzine    47probabilistic mechanism for the hormone transmission, the per-\\ncentage of the working nodes against the total number of nodes in the IoT can be maintained around the expected value.\\n2) \\nNi has received at least one RFS. This indicates that at \\nleast one client asking for service is located around the node \\n.Ni T o guarantee a persistent service discovery, Ni \\nshould be maintained in its waking status and try to wake up other nodes around the client as soon as possible.\\nThe next step is to implement the hunger signals in \\nthe ISOS and make sure that it performs just like the blood level regulation process. After \\nNi receives RFS, it \\nwill be designed to release more WH to drive the other nodes nearby to work immediately. T o ensure that all the \\nrelated nodes can switch to the working status at once, \\nthe quantity of WH released by \\nNi is set to Switch_A L in \\ncompliance with (5). The area where the nodes can be wakened is a disc with a radius \\n,Rwk which should be \\nassigned an appropriate value such that it is neither too small to wake up the nodes nor too large to lead to \\nunnecessary energy consumption.\\nRound (t - 1)'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='unnecessary energy consumption.\\nRound (t - 1)\\nRound (t + 1)Hormone Transmit ting ModuleHormone Transmit ting and R eceiving Hormone P rocessingPhase 1, R ound (t)\\nStatus Det ermination\\nand S witchingHormone R eceiving Module\\nNi Receive s Hor monesNi Releases WH t o Itself\\nNi Releases WH t o Itself\\nNi Sends Lar ge Quantit y of\\nWH t o the Neighbor Nodes\\nNi Sets the P robabilities f or Releasing Hor mones in the Ne xt Round\\nNi Calculat es the A ctive Level AL(i, t)\\nAL(i, t) $ S witch_AL?\\nNi Switches t o Working Status\\nPhase 2, R ound (t)Ni Switches t o Sleeping StatusN\\nYNi Sends WH and SH\\nin Its Autonomous R egionIs Ni Awake?N\\nY\\nN\\nYCan Nt Detect\\nAny srvReqs?\\nFigure 3  The flowchart of the Ios.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 48    ieee Comput Ation Al intelligen Ce mAgAzine | August 2013Fig. 4 depicts the different geographic relationships between \\nan RFS-raised client and a waking node who received the \\nRFS with various Rwks. Three scenarios are observed here \\nwhich include (a) , RRwk S#  (b) , RR R2 < wk SS #  and (c) \\n. RR 2>wk S The shaded fractions in Fig. 4 are the maximum \\neffective wakeup areas in these three situations which can be denoted as \\n(). Siwakeup  When , RR02 wk S ##  the () Siwakeup  can \\nbe calculated as\\n()(, )(, )\\n(, )(, )\\n[( ,)\\n(, )\\n(, )] .arccos\\narccosSi RdABRRd AB R\\nRdABRRd AB R\\nRd AB RR\\ndABR\\nRd AB R180 2\\n180 2\\n212wakeupwk\\nwk\\nwkwk\\nwk\\nwk\\nwkS\\nSS\\nS'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='RdABRRd AB R\\nRd AB RR\\ndABR\\nRd AB R180 2\\n180 2\\n212wakeupwk\\nwk\\nwkwk\\nwk\\nwk\\nwkS\\nSS\\nS\\nSS\\nS22 2 2\\n22 2 2\\n2 2 2 2\\n2 2\\n4 4 421$$$$\\n$$$$\\n$$ $$\\n$r\\nr=+-\\n++-\\n-+\\n+\\n-- -^e\\n^^e\\n^ ^\\n^\\n^ho\\nhho\\nh\\nh h\\nh \\n(12)\\nThe maximum value of () Siwakeup  can be obtained as \\nRR 2>wk S is reached, and\\n () . Si R wakeup S2$r=  (13)\\nT o make the nodes that are unnecessary to be woken up \\nbut are kept in sleeping status, we can set\\n . RR 2 wk S=  (14)\\nThis actually guarantees that the nodes possibly necessary \\n(note that the specified location of the client is unpredictable) for \\ndetecting the RFS can be available ahead of time in a limited \\nrange, and therefore improves the probability of service discovery while maintaining the general stability of the network.\\n(2) Hormone processing\\nDuring the hormone processing stage, each node calculates the \\nquantity of hormones it has received and makes sure that the fol-lowing tasks are accomplished: 1) Set the probability of sending hormones in the next round in the form of \\n(, ) Pi t1 WH+ and \\n(, ), Pi t1 SH+ and 2) Determine the active level. Note that the \\nactive level of a node is determined by the comprehensive results of the transmission of  WH and SH. Assuming the set that contains all the nodes in the network \\nas N, and the subset within the circle area with \\nNi as the cen-\\nter and RC as the radius can be expressed as\\n ,, ,, . CiRN dijR ji NN Cj Cj! #! = ^^ hh \" , (15)\\nFrom (7), (8) and (15) the active level of Ni can be \\nacquired by\\n AL ,A L(,)'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='From (7), (8) and (15) the active level of Ni can be \\nacquired by\\n AL ,A L(,)\\nSerReq_Indicator ,\\nWH ,, SH ,, ,\\nSerReq_Indicator ,\\nSwitch_A LW H, ,it it\\njt\\njit jit\\njt\\njit1\\n1\\n,\\nWH SH\\n,\\nWHjC iR\\njC iRwkC\\n$$ $\\n$$ $DD\\nD=-\\n+-\\n+\\n+!\\n!^\\n^\\n^^\\n^\\n^^\\n^h\\nh\\nhh\\nh\\nhh\\nh6\\n6@\\n@/\\n/ \\n(16)\\nfrom which Ni can calculate its own hormone level in the \\ncurrent round.\\n(3) node status Determination and switchover\\nEach node determines its own working status based on its active level. The detailed procedures of such approach including the switchover of the status can be described in the third part of Fig. 3. Each node firstly applies a range limitation for its hor-\\nmone level that includes a lower bound denoted as \\nMin_AL  \\nand an upper bound denoted as Max_AL,  and\\n AL ,Max_AL,\\nMin_AL,AL ,M ax_A L\\nAL ,M in_AL.itit\\nit>\\n<= ^^\\n^hh\\nh)  (17)\\nThe aim of this step is to prevent the node from falling \\ninto the “persistently waking” or “persistently sleeping” status. As shown in Fig. 3, the working status register can \\nbe set to indicate the current status shown in (6) with the reasonable active level AL(i,t). Note that (6) only indicates the difference between statuses rather than deciding the detailed mapping relationship between the values of the \\nflag and the statuses.\\nIV. Simulation and Results\\nIn order to verify the validity of the proposed ISOS, a series of \\nexperiments have been made to get \\nthe performance of the proposed \\nISOS on regulating and maintaining'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='the performance of the proposed \\nISOS on regulating and maintaining \\nthe general stability of the network, and the ability of effectively discov-ering the service requests randomly generated in the IoT.\\nThe parameters applied for the \\nsimulation are listed in Table 2. Based on these parameters, the density of the nodes and the average number of \\nnodes within the autonomous area  \\nof a certain node (including itself) can \\nbe expected.\\nAd (A, B) d (A, B) d (A, B)BA BA BRS RS RS RwkRwk Rwk\\n(a) (b) (c)\\nFigure 4  The location relations between the target and the service-capable nodes with various \\n.Rwk (a) RRwk S# . (b) RR R2 <wk SS # . (c) . RR 2>wk S\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. August 2013 | ieee Comput Ation Al intelligen Ce mAgAzine    49a. Verification on the ability of Regulation  \\nand Stabilization for the iSOS\\nThe following simulation and corresponding analysis are car-\\nried out to verify the performance of the ISOS on regulating and maintaining the whole network status with the predefined \\nindices (which refers to the percentage of the nodes that should \\nbe in waking status), and no RFS is involved. The distribution of the nodes is completely randomized. All the nodes are ini-tially set to the waking status, and then they are autonomously working according to the waking probability P  with the pro-\\nposed ISOS. Specifically, the value of P  changed from 0.1 to'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='posed ISOS. Specifically, the value of P  changed from 0.1 to \\n0.9 with an increment of 0.1 for each time in the simulations. For each value of P , a group of ten experiments are carried out \\nand their results are averaged to minimize the side effects \\nbrought by the randomness of the network distributions. \\nThe results with different waking probabilities are listed in \\nTable 3, and the detailed tuning processes of the waking node \\nquantities are illustrated in Fig. 5. Here only the cases with  \\nP = 0.2, 0.5 and 0.8 are selected to draw the graph (one in ten \\nexperiments for each value of P ), similar results can still be \\nacquired by changing the probability to its subsequent values. A conventional distributed algorithm named randomized algo-rithm (RA) [32] that drives the nodes to work in a random frequency is applied for comparison. Note that an identical \\nnetwork distribution should be applied with both the networks \\nwith RA and ISOS to avoid possible inconsistencies on the network structure that may lead to incomparable results.\\nSome principle features that the IoT with the proposed ISOS \\nbears can be observed from Table 3 and Fig. 5 as follows.\\n1) The percentage of the nodes in the waking status can be tuned from the initial status (P  = 1) to the expected level, \\nno matter which value is previously assigned to P  (from \\n0.1 to 0.9). After the quickly and effectively regulating \\nprocess at the beginning, the quantity of the waking nodes'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='process at the beginning, the quantity of the waking nodes \\ncan be tuned to the expected value and then the steady state of the network can be maintained continuously until new disturbance (e.g. RFS or the change of P ) appears.\\n2) The time consumed by the network for the tuning process \\nto the expected value is small. On most occasions, the  \\nquantity of nodes in wak-\\ning status can reach the expected value in only one regulating process or even \\nnone (refer to the columns \\nof oscillation number and settling time in Table 3). This indicates that the net-work can attain the final \\nstate quickly and efficiently \\nby applying the proposed ISOS, though the nodes are still working in a dis-\\ntributed form and cannot \\nacquire any overall infor-mation of the network.3) The variation (error) of the average quantity of the wak-\\ning nodes away from its expected value is slight. The \\nmaximum value of the variation reached 2.12% for the case with P = 0.4, which is much smaller than the theo-retical error range \\n%.10!  On most occasions, the error \\ncan be limited to less than 1% (which can be referred to the column of number of nodes in the waking status in Table III). Although the communication and coordination \\namong nodes may be constrained by the distribution of \\nthe network, the ISOS can still preserve the high adjust-ing accuracy and stability of the network.\\n4) The network with the proposed ISOS experiences less fluctuations after the network gets into its stable status. It'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='can be observed from Table 3 that the fluctuation range \\nof the quantity of the waking nodes is less than 2% in most cases after the network reaches a steady state, as can be seen in Fig. 5. For P = 0.2/0.5/0.8, the standard devia-\\ntions of the quantities of the waking nodes with the pro-\\nposed ISOS and RA are 1.0198/9.3390, 1.3684/11.5242, \\nand 0.9788/8.4015, respectively. This indicates that the proposed ISOS has a better ability to maintain the stabili-ty of the network and with higher control precision.\\nThe coverage ratio changes of the waking nodes with dif-\\nferent P during the simulations is shown in Fig. 6. By com-\\nparing the results in Fig. 6 and Fig. 5, it can be observed that the coverage ratios and the quantities of the waking nodes in Table 2  Parameters of the networks and the environment.\\nCaTegory iTem Value\\nAreA \\ncoNfIgur ATIoNLeNg Th\\nWIdTh400 m400 m\\nNeTWork  \\ncoNfIgur ATIoNPerceNTA ge of WorkINg Nodes (p)\\nQuANTITY of Node (n)\\ncommu NIcATIoN r AdIus\\nservINg r AdIus(0,1)\\n500\\n32 m\\n16 m\\nhormoN e  \\ncoNfIgur ATIoNmIN_h\\nmAx_h\\nsWITch_h\\nAH HH DD=-1010\\n01\\nTable 3  Tuning processes and results with different probabilities of working nodes.\\nosCilla Tion n umber seTTling Time QuanTiT y oF aW ake n odes\\nP aVerage range aVerage range aVerage/ nrange/ n aVerage error\\n0.1 0 [0] 6.1 [6, 7] 52.95 [-0.97%, 1.02%] 0.59%\\n0.2 0.8 [0, 1] 21.2 [6, 29] 100.02 [-1.18%, 1.02%] 0.004%\\n0.3 0.8 [0, 1] 21.6 [6, 35] 149.92 [-1.74%, 1.23%] -0.016%'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='0.3 0.8 [0, 1] 21.6 [6, 35] 149.92 [-1.74%, 1.23%] -0.016%\\n0.4 0.3 [0, 1] 9.3 [3, 25] 205.77 [-2.87%, 2.11%] 1.115%\\n0.5 0.3 [0, 1] 2.2 [0, 9] 249.89 [-0.94%, 1.25%] 0.022%\\n0.6 0.6 [0, 1] 12.3 [0, 32] 297.77 [-1.20%, 1.56%] -0.446%\\n0.7 0.9 [0, 1] 25.4 [4, 36] 349.79 [-2.56%, 1.47%] -0.042%\\n0.8 1 [1] 23.7 [13, 29] 399.85 [-1.37%, 1.29%] -0.03%\\n0.9 0 [0] 5.8 [5, 6] 450.43 [-1.00%, 1.00%] 0.086%\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 50    ieee Comput Ation Al intelligen Ce mAgAzine | August 2013the network are correlated to each other, and the coverage \\narea usually becomes larger as more nodes are waking. How-ever, there are still other factors that may bring influence to \\nthe coverage ratio, e.g. the distribution of the nodes. It can \\nbe noticed in Fig. 6 that the coverage ratios with both ISOS and RA increase with the raising waking probability of the IoT, and the ratio fluctuation of the ISOS is much smaller than that of RA in all of the cases. The standard deviations of \\nthe coverage ratio in the network with the proposed ISOS \\nand RA are 0.3709/2.8375, 0.0841/1.7393, and 0.0602/0.8931, for P \\xa0=\\xa00.2/0.5/0.8 respectively. The reason \\nfor such a phenomenon lies with the fact that the node can \\nobtain information about the other nodes in its autonomous'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='obtain information about the other nodes in its autonomous \\narea by the hormones it received according to the ISOS, although the nodes cannot control each other directly. If suf-ficient waking nodes around a certain node can be provided, the probability of sending WH and its own waking probabil-\\nity will be decreased. In this way, the waking nodes in the IoT can distribute as evenly and scattered as possible. It can \\nalso be noticed from Fig. 6(a) that the larger the P  is, the \\nsmaller the difference between the fluctuations of the two \\nalgorithms will be. The reason for this is that the total cover-\\nage area of the nodes is fixed provided the distribution of the network in the IoT has been given (that is, an average of 0.9033 in the simulation). As the P increases, though, more \\nwaking nodes will lead to an increment of the overlapping \\ncoverage area, but the total coverage area won’t be larger \\nthan the predefined fixed value.\\nB. Verification on the RFS Discovering ability of the iSOS\\nIn this section, the performance of detecting RFS for the nodes applying ISOS with different waking probabilities in IoT is tested and verified. It is assumed that a client located at [255, \\n310] is raising RFS during the rounds from 100 through 120 \\nin a randomly generated IoT. The distribution of the nodes around this client is shown in Fig. 7(a). There are 6 nodes \\nThe Quantit y of the W aking Nodes\\nThe Quantit y of the W aking Nodes (P = 0.5)P = 0.8Waking Nodes C ount500\\n450\\n400\\n350300'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='The Quantit y of the W aking Nodes (P = 0.5)P = 0.8Waking Nodes C ount500\\n450\\n400\\n350300\\n250\\n200\\n150\\n100\\n50\\n0Waking Nodes C ount320\\n300\\n280\\n260\\n240\\n220\\n200Loop C ount\\n(a)05 01 00 150\\nLoop C ount\\n(b)05 01 00 150P = 0.5\\nP = 0.2E\\nRA\\nISOS\\nRA\\nFigure 5  The quantity of the waking nodes with different waking \\nprobabilities. (a) P ./ ./ . 02 05 08 = . (b) P ..05=\\nThe Cov erage Ratio of the  Waking Nodes\\nThe Cov erage Ratio of the  Waking Nodes (P = 0.5)P = 0.8Coverage Ratio # 10090\\n80\\n7060\\n50\\n40\\n30\\n20Coverage Ratio # 10074\\n72\\n70\\n686664\\n64\\n60Loop C ount\\n(a)05 01 00 150\\nLoop C ount\\n(b)05 01 00 150P = 0.5\\nP = 0.2 ISOS\\nRA\\nISOS\\nRA\\nFigure 6  The coverage ratio of the waking nodes with different wak-\\ning probabilities. (a) P ./ ./ . 02 05 08 = . (b) P ..05=\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. August 2013 | ieee Comput Ation Al intelligen Ce mAgAzine    51whose distance away from the client is less than RS being \\ndeployed in the network. According to the ISOS, a waking \\nnode will try to wake up all the other nodes located less than \\nR2S away from it if it detects an RFS. This area around the cli-\\nent is also marked with the dashed lines in Fig. 7, and 11 nodes are covered within this scope.\\nTaking the case of P  = 0.5 as an example, the working state'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='Taking the case of P  = 0.5 as an example, the working state \\nchanges of the nodes around the client before and after the RFS proposes are analyzed. In the 100-th loop, since there is no RFS raised, the nodes work autonomously with the waking probability P = 0.5, and only 2 and 7 nodes are waking within \\nthe distances of \\nRS and R2S away from the client, respectively. \\nAn RBF rises in the 101-th loop, all the 11 nodes covered by the \\nR2S range switch to the waking status immediately, which \\ncan be observed in Fig. 7(c). This indicates that the nodes, whatever their previous statuses are, can be woken up right \\naway by other waking nodes around them and detect the exist-\\ning RFS. In the following continuous 20 rounds \\nLoop , 101 120 =- ^h  all the 6 nodes in the RFS-detectable \\narea are kept in their waking status. When the RFS disappears in the 121-th loop, all the waking nodes are unable to detect \\nthe RFS again and therefore send SH to themselves and their respective neighbor nodes. As a result, almost all the nodes \\nwithin the distances of \\nRS fall asleep rapidly with only 4 nodes \\nremained in the waking status within the distances of R2S (as \\nshown in Fig. 7(d)), playing the roles like sentinels. After that, all these nodes go back to their normal working status for idle \\ntime, namely, regulating themselves to maintain the general sta-\\nbility of the network working.\\nFig. 8 depicts the quantity of the waking nodes around the \\nRFS-raised client (within'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='Fig. 8 depicts the quantity of the waking nodes around the \\nRFS-raised client (within \\n/RR2SS ) and the RA are also taken \\nfor comparison in different scenarios with ././.. P 02 05 08 =  \\nActually, the same results as in the case of P  = 0.5 can also be \\nretrieved when the P  varies from 0.1 to 0.9 in the experi-\\nments. We can also observe from Fig. 8 that the response speed of the network with the application of the ISOS is so fast that \\nall the nodes in the related target area can be woken up in \\nonly one loop (namely, the time for the proposed ISOS run-ning once for each node). After that, they will stay in the wak-ing status until the client stop raising RFS. When the RFS dis-\\nappears, the nodes can return to their working status in just \\none or two loops. Clearly, such result can only be acquired in the IoT that drives all its available resources (all the nodes) without ISOS (e.g. the network with RA) to detect the RFS, \\n350\\n340\\n330\\n320\\n310\\n300\\n290\\n280\\n270\\n200 220 240 260\\n(a)\\nLoop: 101/150 Loop: 121/150Loop: 100/150\\n280 300350\\n340\\n330\\n320\\n310\\n300\\n290\\n280\\n270\\n200 220 240 260\\n(b)280 300\\n350340\\n330\\n320\\n310\\n300\\n290\\n280\\n270\\n200 220 240 260\\n(c)280 300350\\n340\\n330\\n320\\n310\\n300\\n290\\n280\\n270\\n200 220 240 260\\n(d)280 300\\nFigure 7  distribution of the waking nodes around an rfc-raised client ([255 310]) in different rounds (p = 0.5). (a) distribution of all nodes. \\n(b) Loop = 100. (c) Loop = 101. (d) Loop = 121.'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='(b) Loop = 100. (c) Loop = 101. (d) Loop = 121.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. 52    ieee Comput Ation Al intelligen Ce mAgAzine | August 2013which indicates that partial working strategy of the proposed \\nISOS can lower the requirement for resources on conducting similar tasks. The proposed ISOS realizes a combination \\nbetween the rapidness for RFS response and the high effi-\\nciency on resource usage.The quantity of all the waking nodes in the IoT with \\n././ . P 02 05 08 =  is also depicted in Fig. 9 to provide a general \\nview of the whole process. It can be observed that for the ISOS, the quantity of working nodes rapidly rises to a new level after the target appears in the monitored area in the 101-th round. As \\nthe RFS disappears in the 121-th round, the quantities of the \\nworking nodes fall back to their original levels as determined by different P. This phenomenon is more significant in the case \\nwith P = 0.2, for the nodes available to be awakened to work \\nand that occupies 80% of the total amount of the nodes in the \\nnetwork, which leaves large room for activation. The results \\nprove that the IoT with the proposed ISOS can overcome external disturbance and reach a new stable status, resuming to its original status after the disturbance is removed. Each node \\nuses the ISOS to tune its own status so that the RFS can be dis-'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='uses the ISOS to tune its own status so that the RFS can be dis-\\ncovered efficiently. But in the IoT with the RA, the appearance and disappearance of the RFS can not bring any changes to the tuning process of the nodes’ working status. Such a feature also indicates that the network has the ability of allocating sufficient \\nresources to accomplish its tasks and releasing them when nec-\\nessary which is of great help to save the limited energy held by the whole IoT.\\nIn addition, it was found that the larger the P  is set, the smaller \\nthe difference between the ISOS and RA will be, which is similar \\n10P = 0.8\\nP = 0.5\\nP = 0.2\\nLoop Count5\\n0Waking Nodes Count95 100 105 110 115 120\\nISOS: Within Rs\\nISOS: Within Rc\\nRA: Within Rs\\nRA: Within Rc125 130\\n95 100 105 110 115 120 125 130\\n95 100 105 110 115 120 125 13010\\n5\\n0\\n10\\n5\\n0\\nFigure 8  The changes of the quantity of the waking nodes around the rfs-raised client (within /RR2SS ) with different p(p = 0.2/0.5/0.8).\\nP = 0.8Waking Nodes C ount500\\n450\\n400\\n350300\\n250\\n200\\n150\\n100\\n50\\n0\\nLoop C ount05 01 00 150P = 0.5\\nP = 0.2ISOS\\nRA\\nFigure 9  The quantity of the waking nodes with different  \\np (p = 0.2/0.5/0.8) in an rfs-raised environment.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply. August 2013 | ieee Comput Ation Al intelligen Ce mAgAzine    53[7] S. Tozlu , M. Senel , W. Mao, and A.  Keshavarzian , “Wi-Fi enabled sensors for the In -'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='ternet of Things: A practical approach ,” IEEE Commun. Mag. , vol. 50, no. 6, pp. 134–143, \\nJune 2012 .\\n[8] L. Atzori , A. Iera, G. Morabito , and M.  Nitti, “The social Internet of Things (SIoT)—\\nWhen social network meet the Internet of Things: Concept, architecture and network \\ncharacterization ,” Comput. Netw. , vol. 56 , no. 16 , pp. 3594 –3608, Nov.  2012 .\\n[9] J. P. Espada , O. S. Martinez , J. M. C. Lovelle , B. C. P. G.-Bustelo , M. A. Alvarez , and \\nA. G. Garcia , “Modeling architecture for collaborative virtual objects based on services ,” \\nJ. Netw. Comput. Applicat. , vol. 34 , no. 5 , pp. 1634 –1647, Sept.  2011 .\\n[10] K. Gama , L. Touseau , and D.  Donses , “Combining heterogeneous service technolo -\\ngies for building an Internet of Things middleware ,” Comput. Commun. , vol. 35 , no. 4 , \\npp. 405 –417, Feb.  2012 .\\n[11] A. Gluhak , S. Krco, M. Nati, D. Pfisterer , N. Mitton , and T.  Razafindralambo , “A \\nsurvey on facilities for experimental Internet of Things research ,” IEEE Commun. Mag. , \\nvol. 49 , no. 11 , pp. 58 –67, Nov.  2011 .\\n[12] F. Dressler , “A study of self-organization mechanisms in ad hoc and sensor networks ,” \\nComput. Commun. , vol. 31 , no. 13 , pp. 3018 –3029, Aug.  2008 .\\n[13] M. F.  De Castro , L. B. Ribeiro , and C. H. S. Oliveira , “An autonomic bio-inspired \\nalgorithm for wireless sensor network self-organization and efficient routing ,” J. Netw. \\nComput. Applicat. , vol. 35 , no. 6 , pp. 2003 –2015, Nov.  2012 .'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='Comput. Applicat. , vol. 35 , no. 6 , pp. 2003 –2015, Nov.  2012 .\\n[14] B. Atakan  and O. B.  Akan , “Distributed audio sensing with homeostasis-inspired \\nautonomous communication ,” Ad hoc Netw. , vol. 9 , no. 4 , pp. 552 –564, June  2011 .\\n[15] F. Dressler, I. Dietrich, R. German, and B. Kruger, “ A rule-based system for pro -\\ngramming self-organized sensor and actor networks ,” Comput. Netw. , vol. 53, no. 10, pp. \\n1737–1750, July 2009.[16] A. Mutazono , M. Sugano , and M.  Murata , “Energy efficient self-organizing control \\nfor wireless sensor networks inspired by calling behavior of frogs ,” Comput. Commun. , vol. \\n35, no. 6 , pp. 661 –669, Mar.  2012 .\\n[17] Y. Jin and B.  Bernhard , “A systems approach to evolutionary multiobjective structural \\noptimization and beyond ,” IEEE Comput. Intell. Mag. , vol. 4 , no. 3 , pp. 62 –76, Aug. 2009.\\n[18] M. Meisel , V. Pappas , and L.  Zhang , “A taxonomy of biologically inspired research in \\ncomputer networking ,” Comput. Netw. , vol. 54 , no. 6 , pp. 901 –916, Apr.  2010 .\\n[19] J. C. Bezdek , S. Rajasegarar , M. Moshtaghi , C. Leckie , \\nM. Palaniswami , and T. \\nC. Havens , “Anomaly detection in environmental monitoring networks ,” IEEE Comput. \\nIntell. Mag. , vol. 6 , no. 2 , pp. 52 –58, May  2011 .\\n[20] J.-W. Lee, B.-S. Choi, and J.-J.  Lee, “Energy-efficient coverage of wireless sensor \\nnetworks using ant colony optimization with three types of pheromones ,” IEEE Trans. \\nInd. Informat. , vol. 7 , no. 3 , pp. 419 –427, 2011 .'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='Ind. Informat. , vol. 7 , no. 3 , pp. 419 –427, 2011 .\\n[21] R. V. Kulkarni  and G. K. Venayagamoorthy , “Particle swarm optimization in wire -\\nless-sensor networks: A brief survey ,” IEEE Trans. Syst. Man, Cybern. C , vol. 41 , no. 2 , \\npp. 262 –267, 2011 .\\n[22] Y. Hu, Y. Ding, and K.  Hao, “An immune cooperative particle swarm optimization \\nalgorithm for fault-tolerant routing optimization in heterogeneous wireless sensor net -\\nworks ,” Math. Problems Eng. , vol. 2012 , Article ID 743728, pp. 1 –19, 2012 .\\n[23] L. Filipe , M. Vieira , U. Lee, and M.  Gerla , “Phero-trail: A bio-inspired location \\nservice for mobile underwater sensor networks ,” IEEE J. Select. Areas Commun. , vol. 28 , \\nno. 4, pp. 553 –563, May  2010 .\\n[24] R. V.  Kulkarni  and G. K. Venayagamoorthy , “Bio-inspired algorithms for autono -\\nmous deployment and localization of sensor nodes ,” IEEE Trans. Syst. Man, Cybern. C , \\nvol. 40 , no. 6 , pp. 663 –675, Nov.  2010 .\\n[25] S. Samarah , M. Al-Hajri , and A.  Boukerche , “A predictive energy-efficient tech -\\nnique to support object-tracking sensor networks ,” IEEE Trans. Veh. Technol. , vol. 60 , \\nno. 2, pp. 656 –663, 2011 .\\n[26] W. Shen, B. Salemi , and P.  Will, “Hormone-inspired adaptive communication and \\ndistributed control for CONRO self-reconfigurable robots ,” IEEE Trans. Robot. Auto -\\nmat., vol. 18 , no. 5 , pp. 700 –712, 2002 .\\n[27] Y. Ding, H. Sun, and K.  Hao, “A bio-inspired emergent system for intelligent web'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='[27] Y. Ding, H. Sun, and K.  Hao, “A bio-inspired emergent system for intelligent web \\nservice composition and management ,” Knowl.-Based Syst. , vol. 20 , no. 5 , pp. 457 –465, \\nJune 2007 .\\n[28] J. A. Fuemmeler  and V. V.  Veeravalli , “Smart sleeping policies for energy efficient track -\\ning in sensor networks ,” IEEE Trans. Signal Processing , vol. 56 , no. 5 , pp. 2091 –2101, 2008.\\n[29] R. V.  Kulkarni , A. Forster , and G. K. Venayagamoorthy , “Computational intel -\\nligence in wireless sensor networks: A survey ,” IEEE Commun. Surveys Tuts. , vol. 13 ,  \\nno. 1, pp. 68 –96, 2011.\\n[30] M. G. Pedersen , C. D.  Man, and C.  Cobelli , “Multiscale modeling of insulin secre -\\ntion,” IEEE Trans. Biomed. Eng. , vol. 58 , no. 10,  pp. 3020 –3023, Nov.  2011 .\\n[31] K. M. Heppner , K. M. Habegger , J. Day, P. T. Pfluger , D. Perez-Tilve , B. Ward , V. \\nGelfanov , S. C. Woods , R. DiMarchi , and M.  Tschop , “Glucagon regulation of energy \\nmetabolism ,” Physiol. Behav. , vol. 100 , no. 5 , pp. 545 –548, July 2010 .\\n[32] J. Chen , K. Cao, K. Li, and Y.  Sun, “Distributed sensor activation algorithm for \\ntarget tracking with binary sensor network s,” Cluster Comput. , vol. 14 , no. 1 , pp. 55 –64, \\nJan. 2011 .\\n[33] Y. Ding and L.  Gao, “Macrodynamics analysis of migration behaviors in large-scale \\nmobile agent systems for the future internet ,” IEEE Trans. Syst. Man, Cybern A , vol. 41 , \\nno. 5, pp. 1032 –1036, Sept.  2011 .'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='no. 5, pp. 1032 –1036, Sept.  2011 .\\n to the case where the coverage ratio is concerned. The ISOS can \\nawaken all the related nodes in any case and such effect will be more obvious when the waking probability P is smaller. In the case \\nwith P = 100%, all the resources are used (meaning all nodes are \\nawakened) and the network has been downgraded to an ordinary persistent working structure. So the proposed algorithm is not expected to show any better performance.\\nV. Conclusions\\nIn this paper, an intelligent self-organization scheme inspired by the endocrine mechanism for the nodes in IoT is pro -\\nposed. Based on the predefined autonomous area, the indi-\\nvidual node in the IoT can work independently in a decen-\\ntralized infrastructure according to its own status and dynamic circumstance. The extensive interconnection and interaction among nodes by hormones constructs an infor-mation exchange network for the IoT where the nodes can \\nwork in a cooperative way to accomplish tasks and objectives \\nassigned to the IoT. Simulation results show that the pro-posed ISOS has the ability to regulate and maintain the gen-eral stability of the network to work effectively with a wak-\\ning probability that is realized based on a highly distributed \\nmedium-based data transmission and decentralized control structure. The ability to effectively detect the service requests randomly generated can also be guaranteed with the applica-tion of the bio-inspired sleep control schemes. The proposed'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='ISOS also shows more performance improvement as the \\nwaking probability is small, which indicates that the pro-posed algorithm can provide more obvious improvement in a resource-critical environment.\\nFuture research may be taken to include possible approaches \\nto provide comprehensive intelligent services where physical objects are actively involved in the serving process. It requires not only mechanisms for service discovery but also the methods to deploy and compose services during the runtime in a distributed fashion, which should be supported autonomously within all \\nphases of the service lifecycle [33]. Some interactive and collab-\\norative mechanisms should also be employed to realize some more intelligent and ubiquitous data exchange and resources allocation strategies in the IoT’s dynamic environment.\\nReferences\\n[1] L. Atzori , A. Iera, and G . Morabito , “The Internet of Things: A survey ,” Comput. \\nNetw. , vol. 54 , no. 15 , pp. 2787 –3805, Oct. 2010 .\\n[2] D. Miorandi , S. Sicari , F. De Pellegrini , and I.  Chlamtac, “ Internet of Things: Vision, \\napplications and research challenges ,” Ad hoc Networks , vol. 10 , no. 7 , pp. 1497 –1516, Sept. \\n2012.\\n[3] F. Dressler  and O. B.  Akan , “A survey on bio-inspired networking ,” Comput. Netw. , \\nvol. 54 , no. 6 , pp. 881 –900, Apr.  2010 .\\n[4] H. Ma, “Internet of Things: Objectives and scientific challenges ,” J. Comput. Sci. \\nTechnol. , vol. 26, no. 6 , pp. 919 –924, Nov.  2011 .'),\n",
       " Document(metadata={'title': 'An Intelligent Self-Organization Scheme for the Internet of Things', 'year': 2013}, page_content='Technol. , vol. 26, no. 6 , pp. 919 –924, Nov.  2011 .\\n[5] S. Li, L. Xu, and X.  Wang , “Compressed sensing signal and data acquisition in wire -\\nless sensor networks and Internet of Things ,” IEEE Trans. Ind. Informat. , to be published.\\n[6] D. Guinard , V. Trifa, S. Karnouskos , P. Spiess , and D.  Savio , “Interacting with the \\nSOA-based Internet of Things: Discovery, query, selection, and on-demand provisioning \\nof web services ,” IEEE Trans. Services Comput. , vol. 3 , no. 3 , pp. 223 –235, July–Sept.  2010.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:55:22 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='Applying regression models to query-focused multi-document\\nsummarization\\nYou Ouyanga,*, Wenjie Lia, Sujian Lib, Qin Lua\\naDepartment of Computing, The Hong Kong Polytechnic University, Hong Kong\\nbKey Laboratory of Computational Linguistics, Peking University, Ministry of Education, China\\narticle info\\nArticle history:\\nReceived 6 January 2009Received in revised form 8 March 2010Accepted 14 March 2010Available online 3 April 2010\\nKeywords:\\nQuery-focused summarizationSupport Vector RegressionTraining data constructionabstract\\nMost existing research on applying machine learning techniques to document summariza-\\ntion explores either classiﬁcation models or learning-to-rank models. This paper presentsour recent study on how to apply a different kind of learning models, namely regression\\nmodels, to query-focused multi-document summarization. We choose to use Support Vec-\\ntor Regression (SVR) to estimate the importance of a sentence in a document set to be sum-marized through a set of pre-deﬁned features. In order to learn the regression models, wepropose several methods to construct the ‘‘pseudo” training data by assigning each sen-\\ntence with a ‘‘nearly true” importance score calculated with the human summaries that\\nhave been provided for the corresponding document set. A series of evaluations on theDUC data sets are conducted to examine the efﬁciency and the robustness of the proposed\\napproaches. When compared with classiﬁcation models and ranking models, regression'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='approaches. When compared with classiﬁcation models and ranking models, regression\\nmodels are consistently preferable.\\n/C2112010 Elsevier Ltd. All rights reserved.\\n1. Introduction\\nDocument summarization techniques are one way of helping people ﬁnd information effectively and efﬁciently. There are\\ntwo main approaches to automatic summarization: abstractive approaches and extractive approaches. Abstractive ap-\\nproaches, which promise to produce summaries that are more like what a human might generate but are limited by the pro-\\ngress of natural language understanding and generation and the more widely used extractive approaches which rank\\nsentences by importance, extract salient sentences, and then compose the summary. Sentence ranking has for some time\\nnow been carried out using machine learning techniques. Early techniques usually treated sentence ranking as a binary clas-siﬁcation problem where classiﬁcation models learnt from sets of ‘‘important” and ‘‘unimportant” sentences sets to build\\nclassiﬁcation models which were used to identify ‘‘key” sentences. Subsequent learning-to-rank approaches normally\\nlearned from ordered sentence pairs or lists, which are easier to obtain than the training data for classiﬁcation models. In\\neither case, most classiﬁcation and ranking models transform binary or ordering information into continuous values which\\nare used in classiﬁcation or ranking.'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='are used in classiﬁcation or ranking.\\nAn alternative to these approaches is offered by regression models, which learn continuous functions which directly esti-\\nmate the importance of sentences, which can be better characterized as continuous than discrete. Another advantage of\\nregression models is that their training data uses continuous importance values, unlike the classiﬁcation or ranking models\\nwhich use either discrete sentence labels or ranked sentence pairs. Thus, the learned regression functions should estimate\\n0306-4573/$ - see front matter /C2112010 Elsevier Ltd. All rights reserved.\\ndoi:10.1016/j.ipm.2010.03.005*Corresponding author. Tel.: +852 6857 0255.\\nE-mail addresses: csyouyang@comp.polyu.edu.hk (Y. Ouyang), cswjli@comp.polyu.edu.hk (W. Li), lisujian@pku.edu.cn (S. Li), csluqin@comp.polyu.\\nedu.hk (Q. Lu).Information Processing and Management 47 (2011) 227–237\\nContents lists available at ScienceDirect\\nInformation Processing and Management\\njournal homepage: www.elsevier.com/locate/infoproman\\nsentence importance more accurately, depending on the quality and the quantity of the training data, whether it is obtained\\nmanually or automatically.\\nIn this paper, we study how to apply regression models to the sentence ranking problem in query-focused multi-docu-\\nment summarization. We implement the regression models using Support Vector Regression (SVR). SVR is the regression'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='type of Support Vector Machines ( Vapnik, 1995 ) and is capable of building state-of-the-art optimum approximation func-\\ntions. As data for this study, we will construct ‘‘pseudo” training data automatically from human summaries and then use\\nthese and their document sets to develop and compare several N-gram based methods that estimate ‘‘nearly true” sentence\\nimportance scores. The training data is then used to learn a mapping function from a set of pre-deﬁned sentence features to\\nthese ‘‘nearly true” sentence importance scores. The learned function is then used to predict the importance of the sentences\\nin the test data. We carry out a series of experiments to evaluate the efﬁciency and robustness of our approaches.\\nThe remainder of the paper is organized as follows. Section 2brieﬂy introduces the related work. Section 3explains the\\nproposed approach. Section 4presents experiments, evaluations and discussions. Section 5concludes the paper.\\n2. Related work\\nThe application of machine learning techniques in document summarization has a long history. Kupiec, Pedersen, and\\nChen (1995) ﬁrst proposed a trainable summarization approach which adopted word-based features and used a naïve Bayes-\\nian classiﬁer to learn feature weights according to a set of given summaries. The learning-based system that combined all the\\nfeatures performed better than any other system using only one single feature. Many early studies followed this idea and'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='extended Kupiec et al.’s work by examining more extensive feature sets and/or classiﬁcation models, such as decision trees\\nand neural networks. ( Chuang & Yang, 2000; Mani & Bloedorn, 1998; Neto, Freitas, & Celso, 2002 ).Hirao and Isozaki (2002)\\nused a set of documents in which key sentences had been annotated manually to train a Support Vector Machine (SVM) clas-\\nsiﬁcation model to learn how to extract important sentences. They reported that SVM outperformed other machine learning\\nmodels such as decision tree or boosting methods in the Japanese Text Summarization Challenge (TSC). Zhou and Hovy\\n(2003) proposed a Hidden Markov Model (HMM) based approach to estimate the extract desirability of an English sentence\\nand trained the parameters on the labeled data generated from the Yahoo Full Coverage Collection. The resulting system was\\ncomparable to the best system in the Document Understanding Conference (DUC) 2001 generic summarization data set.\\nZhao, Wu, and Huang (2005) applied the Conditional Maximum Entropy (ME) model to the DUC 2005 query-focused sum-\\nmarization task but achieved only mediocre performance. Shen, Sun, Li, Yang, and Chen (2007) presented a Conditional Ran-\\ndom Fields (CRF) based framework for generic summarization and reported that CRF performed better than many existing\\nmodels, such as HMM and SVM. A common feature of all these work is that they all relied on classiﬁcation models to rank\\nsentences.'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='sentences.\\nMore recently, learning-to-rank models have been examined. Amini, Usunier, and Gallinari (2005) investigated how to\\nuse learning-to-rank models for query-focused single-document summarization and compared the proposed ranking algo-\\nrithm to a logistic classiﬁer. The ranking algorithm outperformed the logistic classiﬁer. Fisher and Roark (2006) implemented\\na perceptron-based ranking system learned from automatically constructed training data. The system ranked eighth of 34participating systems. At DUC 2007, Toutanova (2007) proposed the PYTHY system which learned a log-linear ranking func-\\ntion to combine more than 20 features. The PYTHY system was augmented with two sophisticated post-processing pro-\\ncesses, i.e. heuristic sentence simpliﬁcation and dynamic sentence scoring. It performed very well and ranked second of\\n30 DUC participating systems. Learning-to-rank models were also applied to webpage summarization tasks and compared\\nto classiﬁcation models ( Metzler & Kanungo, 2008; Wang, Jing, Zhang, & Zhang, 2007 ).Amini and Usunier (2009) presented a\\ntransductive approach that learned the ranking function with few labeled instances. This approach outperformed classiﬁca-\\ntion models in sentence ranking.\\nAn important requirement of learning-based sentence ranking approaches is that one should have sufﬁcient training data.\\nSince document summarization tasks usually involve many factors, it takes a lot of time and effort to manually annotate the'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='training data. To reduce the expense of manual annotation, semi-automatic strategies which use other types of resources to\\ngenerate the training data have been widely adopted. The most common manually-generated resources are human summa-\\nries which are primarily provided for automatic evaluations. The human summaries have been successfully used to judge\\nsentence importance and to generate the positive and negative sentence sets for classiﬁcation models ( Chuang, 2000; Kupiec\\net al., 1995 ). They have also been used to judge the preference between sentences and to generate the training data for rank-\\ning models ( Fisher & Roark, 2006; Toutanova et al., 2007 ). The reason why human summaries can be used for generating the\\ntraining data is that these manually-written summaries are regarded as containing the most important concepts of the ori-\\nginal data set and thus can be used to judge sentence importance. Experiment done by Conroy, Schlesinger, and O’Leary\\n(2006) supported this. They deﬁned an ‘‘Oracle” score which was calculated from the probability distribution of the Uni-\\ngrams in human summaries and found that the summaries generated by directly using the ‘‘Oracle” sentence scores to ex-\\ntract sentences are even comparable to human summaries on the DUC 2006 data set under the automatic evaluation method\\nROUGE. This showed that human summaries can be used effectively in judging the importance of sentences and thus can be'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='used to generate the training data for the learning models.\\nDuring our participation in the DUC competition, we made an initial attempt at applying regression models to the query-\\nfocused multi-document summarization task ( Li, Ouyang, Wang, & Sun, 2007 ). We utilized the human summaries from DUC228 Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–2372006 to generate the training data and used it to learn a sentence scoring function with Support Vector Regression. The\\nlearned function was then applied on the DUC 2007 data set to score sentences. In this present study, we follow this regres-\\nsion-style summarization framework and present a further study on how to develop effective regression-based summariza-\\ntion approaches. First, we expand the training data construction scheme to four different methods for discovering more\\naccurate estimation of sentence importance in order to more effectively train the regression models. Second, we adjust\\nthe learning framework to enable direct comparisons between regression models, classiﬁcation models and learning-to-rank\\nmodels.\\n3. Regression models for query-focused multi-document summarization\\nOur summarization approach is built upon the typical feature-based extractive framework, which ranks and extracts sen-\\ntences according to a set of pre-deﬁned sentence features and a composite scoring function. The learning models in our fea-'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='ture-based approach thus search for an optimum composite scoring function with the ﬁxed feature set.\\n3.1. Feature design\\nSince sentences are scored according to their feature values, the features play an important role in sentence scoring and\\nranking. In this paper, we design seven features to characterize various aspects of a sentence in query-focused multi-docu-\\nment summarization, including three query-dependent and four query-independent features. The features are formulated as\\nfollows.\\n3.1.1. Word matching feature (query-dependent)\\nIn query-focused summarization tasks, queries directly reﬂect the information expected in the anticipated answers. So\\nthe words in the queries are especially informative for sentence scoring. A corresponding query-dependent word feature\\nis then deﬁned as the overlapping degree between the words in a sentence and the words in a query, i.e.,\\nfwordðsÞ¼X\\nwj2sX\\nwi2qsame ðwi;wjÞ ð1Þ\\nwhere fis the feature value, qis the query. The function same (wi,wj)=1i f wi=wj, and 0 otherwise. This feature considers\\nexact matches between words in sentences and queries.\\n3.1.2. Semantic matching feature (query-dependent)\\nWords which do not explicitly appear in the query may also contain information that is semantically related to the query.\\nTherefore, we introduce a semantic feature to capture the overlapping semantic information between a sentence and a query\\nbased on the semantic lexicon WordNet, as\\nfwordnet ðsÞ¼X\\nwj2sX\\nwi2qsimilarity ðwi;wjÞ ð2Þ'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='based on the semantic lexicon WordNet, as\\nfwordnet ðsÞ¼X\\nwj2sX\\nwi2qsimilarity ðwi;wjÞ ð2Þ\\nwhere the function similarity (wi,wj) is the lesksimilarity function introduced in ( Banerjee & Pedersen, 2002 ), which scales\\nthe semantic relation between the two words.\\n3.1.3. Named entity matching feature (query-dependent)\\nWhen the query contains a question that explicitly asks for or about some entities, the sentences containing those entities\\nare more likely to contain the direct answers. Therefore, we believe that named entities are important in scaling the infor-\\nmation contained in a sentence. The entity-based query-dependent feature is deﬁned as the number of the matched named\\nentities between a sentence and a query.\\nfentityðsÞ¼j entity ðsÞ\\\\entity ðqÞj ð3Þ\\nwhere | entity (s)\\\\entity (q)| is the number of named entities in both sand q.\\n3.1.4. Word TF-IDF feature (query-independent)\\nIt is well known that not all words in a text are of equal importance. Many criteria have been proposed for measuring\\nthe information content in the text. In this paper, we use a traditional information retrieval (IR) word weighting scheme,\\ni.e. the TF-IDF (term frequency-inverse document frequency), to scale the information richness of the words and then in turn\\nthe sentences.\\nftfidfðsÞ¼X\\nwi2stfidfðwiÞ ð4Þ\\nwhere tﬁdf(wi) is the tﬁdf score of the word wiin the data set.Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–237 2293.1.5. Named entity feature (query-independent)'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='Similarly, we include a query-independent entity-based feature which is deﬁned as the number of named entities in a\\nsentence.\\nfentityno ðsÞ¼j entity ðsÞj ð5Þ\\n3.1.6. Stop-word penalty feature (query-independent)\\nAs stop-words rarely carry useful information, sentences which contain many stop-words are unlikely to be informative.\\nWe thus deﬁne a stop-word based feature as follows.\\nfstopword ðsÞ¼j stopword ðsÞj ð6Þ\\nwhere | stopword (s)| is the number of the stop-words in s.\\n3.1.7. Sentence position feature (query-independent)\\nOn the assumption that authors introduce the main idea or brieﬂy summarize the main content at the beginning of the\\ntext, we further assume that opening sentences are more informative and more important to the document set. We thus de-\\nﬁne the position-based feature as follows:\\nfposition ðsÞ¼1/C0i/C01\\nnð7Þ\\nwhere nis the total number of the sentences, and sis the ith sentence in the document.\\n3.2. Regression-style sentence ranking method\\nA composite function uses the deﬁned features to synthesize the effects of all the features and compute the importance\\nscores for the sentences. In this paper, we adopt Support Vector Regression (SVR) to learn the scoring function with the fea-\\nture set.\\nThe regression model is trained from a set of known topics Dwhich provide the importance score of every sentence. Here\\nthe deﬁnition of a topic comes from the DUC competitions, in which each topic contains a given query and a set of relevant'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='documents. A sentence sinD(actually in the document set of D) is assigned1with a score indicating its importance score\\nscore (s) and an associated feature vector F(s). The training data is constructed by correlating the sentence’s score and its features\\ntogether, i.e., {( score (s),F(s))|seD}. The target is to predict the score of a new sentence s0in an unknown topic D0through its\\nfeature vector F(s0). This task can be cast as a typical linear regression problem, i.e., using the training data {( score (s),F(s))|seD}\\nto learn the optimum regression function f:F(s)?Rfrom a candidate function set { f(x)=w/C1x+b|weRn,beR}. For this regres-\\nsion problem, linear SVR chooses the optimum function f0(x)=w0/C1x+b0by minimizing the structure risk function\\nUðw;bÞ¼1\\n2jjwjj2þC1\\njDjX\\nsi2DLðscore ðsiÞ/C0ð w/C1FðsiÞþbÞÞ !\\nð8Þ\\nwhere L(x) indicates the loss function, Cindicates the weight to balance the factors and |D|indicates the number of sentences\\ninD.\\nOnce the regression function f0is learned, we use it to provide an estimation of the importance score of the new sentence\\ns0by\\nSc^oreðs0Þ¼f0ðFðs0ÞÞ ¼ w0/C1Fðs0Þþb0 ð9Þ\\nIn the practical tasks, the target summary is usually limited in length to a maximum number of words. To maximize the\\ninformation included in the summary, it is better to select the sentences that contain more information but fewer words.\\nTherefore, the ﬁnal scoring function is obtained with an additional normalizing process as\\nSc^ore normðs0Þ¼1\\njs0jsc^oreðs0Þ ð10Þ'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='Sc^ore normðs0Þ¼1\\njs0jsc^oreðs0Þ ð10Þ\\nwhere | s0| is the number of the words in s0.\\n3.3. Model comparison: classiﬁcation, ranking and regression\\nGenerally, a classiﬁcation problem is depicted mathematically as follows: given the input as a vector xeRnand the output\\nas a binary value ye{ +1, /C01}, the target function is a function g(x) learned from a labeled training data set D={ (x1,y1),/C1/C1/C1(x-\\nl,yl)} to approximate the true relationship between input xand output y. A basic principle for ﬁnding the best approximation\\nfunction is to minimize the total classiﬁcation errorPl\\ni¼1LðgðxiÞ;yiÞon the training data set D, where Lis a pre-deﬁned loss\\n1The issue of how to gain to assign the score to the sentence will be addressed in Section 3.4.230 Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–237function (for example, the square loss function L(a,b)=(a/C0b)2). There are many different speciﬁc classiﬁcation models such\\nas SVM, decision tree, and neutral network. Here we will not look at the speciﬁc differences between the classiﬁcation\\nmodels.\\nClassiﬁcation-based summarization often categorizes sentences as either worthy of extraction into the summary or not.\\nThe shortcoming of binary decisions is that most of the time too many or too few sentences are taken as summary sentences\\nrelative to the summary length requirement. So it is normally necessary to convert binary decisions into real-valued scores in'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='some way so that scores can be used to rank sentences and top-ranked sentences can be selected to form a summary.\\nIn contrast, a learning-to-rank problem is depicted mathematically as follows: given the input as a vector xeRn, and the\\noutput as a rank ye{1, 2, /C1/C1/C1,r}, the target function is a function g(x) learned from a training data set consisted of ordered\\noutput pairs D¼ x1\\n1;x2\\n1;r1/C0/C1\\n;x1\\n2;x2\\n2;r2/C0/C1\\n;/C1/C1/C1;x1\\nl;x2\\nl;rl/C0/C1 /C8/C9\\n, where rirepresents the relative preference between the two inputs x1\\ni\\nand x2\\ni. The best function should minimize the total ranking errorPl\\ni¼1Lgx1\\ni;x2\\ni/C0/C1\\n;ri/C0/C1\\non the training data D. When applied to\\nsummarization, most learning-to-rank models ﬁrst use real-valued scores to make a judgment between the two sentencesand then use pair-wise preference to obtain the full rank of the whole sentence set. This means that the learning-to-rank\\nmodels still rank sentences using real-valued scores.\\nA regression problem can be depicted mathematically as follows: given the input as a vector x\\neRn, the output as a real\\nvalue yeR, the target function is a function g(x) learned from a labeled training data set D={ (x1,y1),/C1/C1/C1(xl,yl)} to approxi-\\nmate the true relationship between input xand output y. The best approximation function must also minimize the total error\\nof the predicted value and the true valuePl\\ni¼1LðgðxiÞ;yiÞon the training data D. Summarization methods based upon regres-'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='i¼1LðgðxiÞ;yiÞon the training data D. Summarization methods based upon regres-\\nsion models attempt to directly construct a mapping function between the feature vectors and the importance scores.\\n3.4. Training data construction\\nLearning regression models requires a set of topics in which the sentence importance is known. However, there is no such\\nkind of training data available to us. Therefore, we should construct the training data in advance. However, it is impractical to\\nprecisely annotate sentence importance manually. In this paper, we adopt an alternative strategy of semi-automatically\\nassigning ‘‘nearly true” importance scores to the sentences by using several N-gram-based methods and with reference to\\nhuman summaries. The basic assumption is that if human summaries are good, the sentences in the documents which\\nare more similar to those in human summaries are also more likely to be good would thus be likely to be assigned higher\\nscores.\\nGiven a document set Dand a human summary set H={H1,/C1/C1/C1,Hm}(Hiis the ith human summary), each sentence sinD\\nwill be assigned an importance score score (s|H). The scores are calculated by the N-gram probabilities of sto be recognized as\\na summary sentence given the human summaries.\\nUsing the bag-of-words model, the probability of an N-gram tunder the ith human summary Hican be calculated by\\npðtjHiÞ¼freqðtÞ=jHij ð11Þ'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='pðtjHiÞ¼freqðtÞ=jHij ð11Þ\\nwhere freq(t) is the frequency of tinHiand | Hi| is the number of the words in Hi. To obtain the probability of tunder all hu-\\nman summaries, we consider two strategies, i.e. the Maximum strategy\\npMaxðtjHÞ¼Max\\nHi2HðpðtjHiÞÞ ð12Þ\\nand the Average strategy\\npAvgðtjHÞ¼1\\njHjX\\nHi2HpðtjHiÞ ð13Þ\\nwhere | H| is the total number of human summaries in H.\\nThe overall score of a sentence sis calculated using the sum of the probabilities of all the N-grams it contains, as follows ( p\\ncan be either pmaxorpavg),\\nscore ðsjHÞ¼X\\ntj2spðtjjHÞ ð14Þ\\nAnalogously, we have another two scoring methods based on N-gram statistics, i.e.\\nscore MaxðsjHÞ¼X\\ntj2SMax\\nHi2HðfreqðtjÞ=jHijÞ ð15Þ\\nand\\nscore AvgðsjHÞ¼X\\ntj2SX\\nHi2HðfreqðtjÞ=jHijÞ ð16Þ\\nMore speciﬁcally, in our experiments we use Uni-grams and Bi-grams.Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–237 2313.5. Redundancy removal\\nIf the words in two sentences are very similar, the sentences will probably have similar feature values and consequently\\nalso similar sentence scores. Thus it is inevitable for an extractive approach to select certain high scored sentences that con-\\nvey the same or quite similar information into a summary. Given the ﬁxed length of the summary, a summary that includes\\ntoo much redundant information will be missing opportunities to include more relevant information. To alleviate this prob-'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='lem, the maximum marginal relevance (MMR) approach ( Carbonell & Goldstein, 1998 ) is applied during the process of sen-\\ntence selection. First, all the sentences are ranked in descending order according to their estimated scores. Then, the\\nsummary sentences are selected iteratively, each time with the current candidate sentence of interest being compared\\nagainst the other sentences already chosen. If the sentence is not signiﬁcantly similar to any sentence already in the sum-\\nmary (the similarity value of the two sentences is smaller than a given threshold, 0.7 in this study), the sentence is added to\\nthe summary. The iteration is repeated until the length of the summary reaches the upper limit.\\n4. Experiment and evaluation\\n4.1. Experiment set-up\\nWe conduct a series of experiments on the query-focused multi-document summarization task initiated by DUC in 2005.\\nThe task requires creating a brief, well-organized, and ﬂuent summary from a set of documents related to a given query. This\\ntask has been speciﬁed as the main evaluation task over 3 years (2005–2007) and thus provides a good benchmark for\\nresearchers to exchange their ideas and experiences in this area. Each year, DUC assessors develop a total of about 50\\nDUC topics. Each topic includes 25–50 newswire documents and a topic description simulating a potential user’s query.\\nFor each topic, four human summarizers are asked to use the related documents to write a 250-word summary that is sub-'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='sequently used in automatic evaluation.\\nIn all the experiments, the documents and queries are pre-processed by removing the stop-words and stemming the\\nremaining words. Four types of named entities, including persons, organizations, locations and times, are automatically\\ntagged by GATE ( Cunningham, Maynard, Bontcheva, & Tablan, 2002 ). According to the task deﬁnitions, system-generated\\nsummaries are strictly limited to a length of 250 English words. After sentence scoring, the highest scored sentences are se-lected from the original documents into the summary until the word (actually the sentence) limitation is reached. Consid-\\nering the focus of this study, no post-processing such as sentence compression or fusion is carried out. SVM\\nlight(Joachims,\\n1999 ) is used to implement SVR and the parameters of SVMlightare set to default values.\\n4.2. Evaluation metrics\\nIn DUC, the system-generated summaries are evaluated against several manual and automatic evaluation metrics ( Dang,\\n2005 ). In this paper, we use two of the DUC automatic evaluation criteria, namely ROUGE-2 and ROUGE-SU4,2to compare our\\nsystems (implemented using the proposed approaches) with human summarizers and top-performing DUC systems. ROUGE\\n(Recall Oriented Understudy for Gisting Evaluation) ( Lin & Hovy, 2002 ) is a state-of-the-art automatic summarization evaluation\\nmethod that makes use of N-gram comparison. It evaluates the system summaries by comparing them with human summaries.'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='For example, ROUGE-2 evaluates a system summary by matching its Bi-grams against the human summaries, i.e.,\\nRnðsÞ¼Ph\\nj¼1P\\nti2SCount ðtijS;HjÞ\\nPh\\nj¼1P\\nti2SCount ðtijHjÞð17Þ\\nwhere Sis the summary to be evaluated, Hj(j=1, 2, ...,h) is the jth human summary regarded as the gold standard. tiindi-\\ncates the Bi-grams in the summary S, Count (ti|Hj) is the number of times the Bi-gram tioccurred in the jth human summary\\nHjand Count (ti|S,Hj) is the number of times tioccurred in both Sand Hj.\\nROUGE-SU4 is very similar to ROUGE-2. It matches Uni-grams and skip-Bi-grams of a summary against human summa-\\nries instead of Bi-grams. A skip-Bi-gram is a pair of words in their sentence order, allowing for gaps within a limited size. A\\nmore detailed description of ROUGE can be found in ( Lin & Hovy, 2002 ). Although ROUGE uses simple N-gram statistics, it\\nworks well in DUC. For example, in the DUC 2005, ROUGE-2 had a Spearman correlation of 0.95 and a Pearson correlation of\\n0.97 compared with human evaluation.\\n4.3. Experiment 1: comparison of feature weights automatically learned and manually assigned on the DUC 2005\\nIn the ﬁrst experiment, we use all the features introduced in Section 3.2in all the runs of the SVR-based summarization\\nsystem. Therefore, the inﬂuential factor to the results is the training data used for learning the regression functions. Recall\\nthat in Section 3.4we introduced four methods using two kinds of N-grams (including Uni-gram and Bi-gram) and two'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='2We run ROUGE-1.5.5 with the parameters ‘‘ /C0n2/C0x/C0m/C024/C0u/C0c95/C0r1000 /C0fA/C0p0.5/C0t0/C0d”.232 Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–237alternative scoring strategies (including Maximum and Average). In this experiment, we conduct a comparison of these\\nmethods for model learning. The ‘‘oracle” scoring method (Conroy and Schlesinger, 2006) is also implemented for reference.\\nHere the ‘‘oracle” score is used to learn the regression function like the proposed methods in Section 3.4. Besides the SVR-\\nbased systems, we also develop several baseline systems that use linear functions with manually assigned weights to com-\\nbine the features. Since manually assigned weights may happen to produce the worst or the best performance, four different\\nsets of weights are chosen. All the systems are evaluated on the DUC 2005 (i.e. the ﬁrst year of query-focused multi-docu-\\nment summarization) data set. The training data for the learning-based system is constructed from the DUC 2006 data set.\\nFor simplicity and consistency, we use the DUC 2006 data set to construct the training data in most of the follow-up exper-\\niments unless otherwise stated.\\nTable 1 presents the average ROUGE-2 and ROUGE-SU4 scores and the corresponding 95% conﬁdential intervals of the\\nsystems on the DUC 2005 data set. The systems with the same features but different composite functions performed differ-'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='ently. The best system outperformed the worst one by more than 30%. This means that the efﬁciency of feature combinationis indeed very important to the sentence scoring function. In general, the learned regression functions perform better than\\nthe linear combinations with manually assigned weights. This clearly demonstrates the advantages of using regression mod-\\nels to supervise the combination process. Of the regression-based systems, the one learned by the ‘‘Uni+Max” training data\\nconstruction method was the best.\\n4.4. Experiment 2: different features on the DUC 2005\\nNo matter which models are used, the features always have a direct inﬂuence on the efﬁciency of the composite scoring\\nfunction. In this second experiment, we test the systems with different feature sets. We gradually combined the seven fea-\\ntures introduced in Section 3.2to form nine different feature sets and scoring functions. The training data is all constructed\\nusing the ‘‘Uni+Max” method. Again the evaluations are carried out on the DUC 2005 data sets. Table 2 illustrates the ROUGE\\nscores of the system with different feature sets. The results suggest that SVR is effective in searching for the optimal weights.\\nWhen more appropriate features are involved, the SVR model is capable of tuning the weights of the incremental feature sets\\nto achieve better composite functions.\\n4.5. Experiment 3: comparison of different learning models'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='to achieve better composite functions.\\n4.5. Experiment 3: comparison of different learning models\\nThe purpose of the third experiment is to compare the effectiveness of different machine learning models in sentence\\nimportance estimation, including regression models, classiﬁcation models and learning-to-rank models. Here we consis-\\ntently use the Support Vector Machine to implement all the models to make the comparison more fair, i.e., Support Vector\\nClassiﬁcation ( Vapnik, 1995 ), Support Vector Regression ( Vapnik, 1995 ) and ranking SVM ( Joachims, 2002 ).\\nThe ‘‘Uni+Max” method is used to construct the training data for all these models. After scoring all the sentences in a topic\\nagainst human summaries, we normalize all the estimated scores by the maximum one among them. The training data for\\nthe regression models is constructed by pairing the normalized score and the feature vector for every sentence. The training\\ndata for the classiﬁcation models is constructed by using the sentences with scores above 0.7 as the positive sentences and\\nthe sentences with scores below 0.3 as the negative sentences. The training data for the ranking models is constructed by\\npairing any two sentences whose score gap is larger than 0.5. In our implementation, all the thresholds are obtained\\nexperimentally.\\nTable 1\\nResults of learned and manually assigned weights on DUC 2005.\\nSubmission Average ROUGE-2 (CI) Average ROUGE-SU4 (CI)\\nUni+Max 0.0757 0.1335\\n(0.0720 ,0.0791 )( 0.1300 ,0.1370 )'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='Uni+Max 0.0757 0.1335\\n(0.0720 ,0.0791 )( 0.1300 ,0.1370 )\\nUni+Avg 0.0747 0.1319\\n(0.0711, 0.0783) (0.1284, 0.1355)\\nOracle 0.0726 0.1298\\n(0.0691, 0.0759) (0.1263, 0.1331)\\nBi+Avg 0.0713 0.1268\\n(0.0681, 0.0745) (0.1237, 0.1299)\\nBi+Max 0.0712 0.1271\\n(0.0675, 0.0741) (0.1239, 0.1302)\\nHuman Assigned Weight 1 0.0657 0.1221\\n(0.0623, 0.0691) (0.1183, 0.1259)\\nHuman Assigned Weight 2 0.0645 0.1174\\n(0.0606, 0.0683) (0.1124, 0.1219)\\nHuman Assigned Weight 3 0.0620 0.1173\\n(0.0586, 0.0651) (0.1135, 0.1208)\\nHuman Assigned Weight 4 0.0572 0.1087\\n(0.0537, 0.0606) (0.1038, 0.1132)Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–237 233We ﬁrst run the systems on the DUC 2005 data set. Table 3 provides the average ROUGE-2 and ROUGE-SU4 scores and the\\ncorresponding 95% conﬁdential intervals. As expected, regression models outperform both classiﬁcation models and ranking\\nmodels. To further prove this result, we extend the experiment to the DUC 2006 and the 2007 data sets. When evaluated on\\nthe DUC 2006 data set, we use 10% of the topics to construct training data in a 10-fold cross-validation process. The results\\nare presented in Tables 4 and 5 respectively. The evaluations on DUC 2006 and 2007 conﬁrm the advantages of the regres-\\nsion-style learning approach in feature combination. Moreover, the success of utilizing more information shows that the\\n‘‘pseudo” scores are reliable estimations of the sentence importance.'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='‘‘pseudo” scores are reliable estimations of the sentence importance.\\n4.6. Experiment 4: comparison with the DUC participating systems\\nNext, we compare our summarization systems to the state-of-the-art systems. Tables 6–8 present the ROUGE results of\\nthe SVR-based systems (shaded grey) together with the top performing systems in DUC (labeled 15,17and so on), the worst\\nhuman summary (labeled as H,Aand so on), and the NIST baseline that returns the ﬁrst 250-words of the most recent doc-\\nument for each document set. As showed in the results, our ‘‘Uni+Max” system is able to outperform all the participating\\nsystems in the DUC 2005 (31 systems in all), stand at the second position in the DUC 2006 (34 systems in all) and the ﬁfth\\nposition in the DUC 2007 (30 systems in all). This shows that the proposed approaches are competitive when compared to\\nstate-of-art summarization systems. As a matter of fact, the participating systems have improved a lot over the 2 years. Thisis the reason why the rank of our system drops from 2005 to 2007.Table 2\\nResult of combining different features with Uni+Max on DUC 2005.\\nfcentroid fword fposition fstopword fentity +fentityno fwordnet Average ROUGE-2 (CI) Average ROUGE-SU4 (CI)\\np0.0603 0.1111\\n(0.0571, 0.0635) (0.1071, 0.1152)p0.0628 0.1222\\n(0.0598, 0.0656) (0.1190, 0.1252)pp0.0641 0.1171\\n(0.0612, 0.0670) (0.1142, 0.1199)pp0.0706 0.1276\\n(0.0673, 0.0738) (0.1243, 0.1310)pp p0.0709 0.1265\\n(0.0675, 0.0740) (0.1230, 0.1299)pp p0.0729 0.1321'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='(0.0675, 0.0740) (0.1230, 0.1299)pp p0.0729 0.1321\\n(0.0694, 0.0761) (0.1288, 0.1356)pp pp0.0747 0.1331\\n(0.0711, 0.0781) (0.1295, 0.1367)pp pp p0.0751 0.1331\\n(0.0715, 0.0786) (0.1296, 0.1366)pp pp p p0.0757 0.1335\\n(0.0720 ,0.0791 )( 0.1300 ,0.1370 )\\nTable 3\\nResults of different learning models on DUC2005.\\nModel Average ROUGE-2 (CI) Average ROUGE-SU4 (CI)\\nRegression 0.0757 0.1335\\n(0.0720 ,0.0791 )( 0.1300 ,0.1370 )\\nRanking 0.0715 0.1287\\n(0.0682, 0.0748) (0.1253, 0.1322)\\nClassiﬁcation 0.0641 0.1208\\n(0.0612, 0.0669) (0.1175, 0.1241)\\nTable 4Results of different learning models on DUC2006.\\nModel Average ROUGE-2 (CI) Average ROUGE-SU4 (CI)\\nRegression 0.0926 0.1485\\n(0.0883 ,0.0969 )( 0.1443 ,0.1525 )\\nRanking 0.0890 0.1443\\n(0.0852, 0.0928) (0.1403, 0.1477)\\nClassiﬁcation 0.0834 0.1387\\n(0.793, 0.0876) (0.1344, 0.1428)234 Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–2374.7. Experiment 5: comparison of training on the different DUC data sets\\nIn all the preceding experiments, the models were learned on the same training data constructed from the DUC 2006 data\\nset. In the ﬁnal experiment, we examine the performance of the SVR-based systems trained with different training data sets.\\nWe use the ‘‘Uni+Max” method to generate three sets of training data from each of the DUC 2005, 2006 and 2007 data sets.\\nThe regression functions are then trained on each training data set before being evaluated on the other data sets. When train-'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='ing and evaluation are carried out on the different data sets, we use entire data sets. When training and evaluation are car-\\nried out on the same data sets, we use 10% of the topics in the data sets are used to construct the training data and 10-fold\\ncross-validations. Tables 9 and 10 present the 3 /C23 training-evaluation result matrices. The systems trained on the DUC\\n2006 data set performs better than those trained on DUC 2005 or DUC 2007. This shows that the source data for constructing\\nthe training data also inﬂuences the efﬁciency of model learning. In future work, we would like to further investigate how to\\nadapt the learned models to a new data set with completely different topics or topic distributions, and how to choose the\\ntraining data among the known topics given the new topic to be summarized.\\n4.8. Discussion\\nExperiment 1 has shown that of the four training data construction methods, the training data generated by Uni-gram\\nmethods are in general better for training regression functions than Bi-gram methods. This may be because in the originaldocument sets, Bi-grams are spread much more sparsely than Uni-grams. For example, many more sentences that receiveTable 5\\nResults of different learning models on the DUC2007.\\nModel Average ROUGE-2 (CI) Average ROUGE-SU4 (CI)\\nRegression 0.1133 0.1652\\n(0.1084, 0.1164) (0.1608, 0.1695)\\nRanking 0.1075 0.1616\\n(0.1032, 0.1120) (0.1573, 0.1659)\\nClassiﬁcation 0.1011 0.1562\\n(0.0967, 0.1055) (0.1519, 0.1606)'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='(0.1032, 0.1120) (0.1573, 0.1659)\\nClassiﬁcation 0.1011 0.1562\\n(0.0967, 0.1055) (0.1519, 0.1606)\\nTable 6Comparing with the DUC 2005 participating systems.\\nSubmission Average ROUGE-2 (CI) Average ROUGE-SU4 (CI)\\nH 0.0880 0.1471\\n(0.0770, 0.0998) (0.1366, 0.1594)\\nUni+Max 0.0757 0.1335\\n(0.0720 ,0.0791 )( 0.1300 ,0.1370 )\\n15 0.0738 0.1326\\n(0.0711, 0.0764) (0.1300, 0.1354)\\n17 0.0726 0.1298\\n(0.0692, 0.0760) (0.1263, 0.1331)\\n8 0.0706 0.1285\\n(0.1256, 0.1313) (0.0677, 0.0735)\\nNIST baseline 0.0416 0.0885\\n(0.0386, 0.0446) (0.0842, 0.0924)\\nTable 7\\nComparing with the DUC 2006 participating systems.\\nSubmission Average ROUGE-2 (CI) Average ROUGE-SU4 (CI)\\nA 0.1001 0.1648\\n(0.0898, 0.1123) (0.1574, 0.1734)\\n24 0.0950 0.1534\\n(0.0907, 0.0992) (0.1494, 0.1574)\\nUni+Max 0.0926 0.1485\\n(0.0883 ,0.0969 )( 0.1443 ,0.1525 )\\n15 0.0900 0.1448\\n(0.0858, 0.0942) (0.1411, 0.1488)\\n12 0.0892 0.1457\\n(0.0848, 0.0938) (0.1415, 0.1499)\\nNIST baseline 0.0491 0.0962\\n(0.0451, 0.0534) (0.0918, 0.1006)Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–237 235zero scores in Bi-gram methods (which means the Bi-grams in these sentences never appear in human summaries) than in\\nUni-gram methods, at about 75% vs. 20%. In other words, the data sparseness problem is more serious in Bi-gram methods\\nthan in Uni-gram methods. This unavoidably inﬂuences the performance of machine learning approaches and accounts for'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='the better performance of Uni-gram methods. On the other hand, the ‘‘Maximum” and ‘‘Average” strategies differ very little.\\nIn Experiment 5 we also compared the effects of different training data on model learning. The results show that the qual-\\nity of the training data varies as the data resources change. This may be because the similarity scores are estimates of the\\n‘‘true” sentence importance scores. As the accuracy of the estimates varies on different topics, the quality of the generated\\ntraining data also varies. Therefore, when the source data changes, the quality of the generated training data using the same\\nconstruction method may also differ. The crucial factor here is how well the similarity measure based upon human summa-\\nries can estimate ‘‘true” importance.\\nTo sum up, a good training data set for learning regression models requires (1) a good topic set with well-written human\\nsummaries, and (2) an appropriate method for sentence importance estimation.\\n5. Conclusion and future work\\nThis paper has presented our studies of how to develop a regression-style sentence ranking scheme for query-focused\\nmulti-document summarization. We examined different methods for constructing the training data based on human sum-maries. We also presented what we have learned on how to construct good training data and compared the effectiveness ofTable 10\\nRougeSU-4 results of regression models built from different data sources.\\nTest 2005E 2006E 2007E\\nTrain\\n2005T 0.1252 0.1411 0.1597'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='Test 2005E 2006E 2007E\\nTrain\\n2005T 0.1252 0.1411 0.1597\\n(0.1221, 0.1284) (0.1371, 0.1448) (0.1555, 0.1638)\\n2006T 0.1335 0.1485 0.1652\\n(0.1300, 0.1370) (0.1443, 0.1525) (0.1608, 0.1695)\\n2007T 0.1154 0.1398 0.1595\\n(0.1122, 0.1188) (0.1355, 0.1437) (0.1553, 0.1638)Table 8Comparing with the DUC 2007 participating systems.\\nSubmission Average ROUGE-2 (CI) Average ROUGE-SU4 (CI)\\nH 0.1289 0.1840\\n(0.1154, 0.1422) (0.1737, 0.1931)\\n15 0.1239 0.1750\\n(0.1189, 0.1288) (0.1701, 0.1897)\\n29 0.1201 0.1694\\n(0.1152, 0.1249) (0.1648, 0.1740)\\n4 0.1181 0.1679\\n(0.1133, 0.1226) (0.1638,0.1718)\\n24 0.1176 0.1743\\n(0.1128, 0.1228) (0.1696, 0.1793)\\nUni+Max 0.1133 0.1652\\n(0.1084 ,0.1164 )( 0.1608 ,0.1695 )\\n13 0.1115 0.1630\\n(0.1067, 0.1164) (0.1581, 0.1678)\\nNIST baseline 0.0599 0.1036\\n(0.0561, 0.0639) (0.0995, 0.1077)\\nTable 9ROUGE-2 results of regression models built from different data sources.\\nTest 2005E 2006E 2007E\\nTrain\\n2005T 0.0684 0.0860 0.1073\\n(0.0654, 0.0714) (0.0818, 0.0904) (0.1036, 0.1112)\\n2006T 0.0757 0.0926 0.1133\\n(0.0720, 0.0791) (0.0883, 0.0969) (0.1084, 0.1164)\\n2007T 0.0612 0.0843 0.1073\\n(0.0583, 0.0643) (0.0799, 0.0886) (0.1027, 0.1117)236 Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–237different learning models for sentence ranking. Our experiments have shown that regression models are to be preferred over\\nclassiﬁcation models or learning-to-rank models for estimating the importance of the sentences. We also showed that the'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='resulting summarization system based on the proposed ranking approach is competitive on the DUC 2005–2007 data sets.\\nIn future work we will examine the effectiveness of regression models in a wider variety of summarization tasks as well as\\nconsidering a greater number of features, such as features based on document structure or sentence relations, which would\\nallow us to model documents in a more sophisticated way.\\nAcknowledgements\\nThe work described in this paper was partially supported by Hong Kong RGC Projects (PolyU5211/05E and PolyU5217/\\n07E), NSFC programs (60603093 and 60875042) and 973 National Basic Research Program of China (2004CB318102).\\nReferences\\nAmini, M. R., Usunier, N. (2009). Incorporating prior knowledge into a transductive ranking algorithm for multi-document summarization. In Proceedings of\\nthe 32nd international ACM SIGIR conference on research and development in information retrieval, poster session (pp 704–705).\\nAmini, M. R., Usunier, N., & Gallinari, P. (2005). Automatic text summarization based on word-clusters and ranking algorithms, ECIR 2005. In D. E. Los ada & J.\\nM. Fernández-Luna (Eds.). LNCS (Vol. 3408, pp. 142–156). Heidelberg: Springer.\\nBanerjee, S., & Pedersen, T. (2002). An adapted lesk algorithm for word sense disambiguation using WordNet. In Proceedings of the third international\\nconference on computational linguistics and intelligent text processing (CICLING-02) (pp. 136–145).'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='conference on computational linguistics and intelligent text processing (CICLING-02) (pp. 136–145).\\nCarbonell, G. J., & Goldstein, J. (1998). The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the\\n21st annual international ACM SIGIR conference on research and development in information retrieval (pp. 335–336). Melbourne, Australia.\\nChuang, W. T., & Yang, J. (2000). Extracting sentence segments for text summarization: a machine learning approach. In Proceedings of the 23rd annual\\ninternational ACM SIGIR conference on research and development in information retrieval (pp. 152–159).\\nConroy, J. M., Schlesinger, J. D., & O’Leary, D. P. (2006). Topic-focused multi-document summarization using an approximate oracle score. In Proceedings of\\nthe COLING/ACL 2006 main conference poster sessions (pp. 152–159).\\nCunningham, H., Maynard, D., Bontcheva, K., & Tablan, V. (2002) GATE: A framework and graphical development environment for robust NLP tools and\\napplications. In Proceedings of the 40th anniversary meeting of the association for computational linguistics (pp. 168–175).\\nDang, H. T. (2005). Overview of DUC 2005. In Document understanding conference 2005 .<http://duc.nist.gov> .\\nFisher, S., & Roark, B. (2006). Query-focused summarization by supervised sentence ranking and skewed word distributions. In Document understanding\\nconference 2006 .<http://duc.nist.gov >.'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='conference 2006 .<http://duc.nist.gov >.\\nHirao, T., & Isozaki, H. (2002). Extracting important sentences with support vector machines. In Proceedings of the 19th international conference on\\ncomputational linguistics (pp. 342–348).\\nJoachims, T. (1999). Making large-scale SVM learning practical. In B. Schölkopf, C. Burges, & A. Smola (Eds.), Advances in kernel methods – Support vector\\nlearning . MIT-Press.\\nJoachims, T. (2002). Optimizing search engines using clickthrough data. In Proceedings of the ACM conference on knowledge discovery and data mining (KDD) .\\nKupiec, J. M., Pedersen, J., & Chen, F. (1995). A trainable document summarizer. In Edward A. Fox, Peter Ingwersen, Raya Fidel (Eds.), Proceedings of the 18th\\nannual international ACM SIGIR conference on research and development in information retrieval (pp. 68–73).\\nLi, S., Ouyang, Y., Wang, W., & Sun, B. (2007). Multi-document summarization using support vector regression. In Document understanding conference 2007 .\\n<http://duc.nist.gov >.\\nLin, C. Y., & Hovy, E. (2002). Manual and automatic evaluation of summaries. In Document understanding conference 2002 .<http://duc.nist.gov >.\\nMani, I., & Bloedorn, E. (1998). Machine learning of generic and user-focused summarization. In Proceedings of the 15th national/10th conference on artiﬁcial\\nintelligence/innovative applications of artiﬁcial intelligence (pp. 820–826). Madison, WI, United States.'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='Metzler, D., Kanungo, T. (2008). Machine learned sentence selection strategies for query-biased summarization. In SIGIR 2008 workshop on learning to rank\\nfor information retrieval .\\nNeto, J. L., Freitas, A. A., & Celso, A. A. (2002). Kaestner. Automatic text summarization using a machine learning approach. In Proceedings of the 16th Brazilian\\nsymposium on artiﬁcial intelligence: Advances in artiﬁcial intelligence (pp. 205–215).\\nShen, D., Sun, J., Li, H., Yang, Q., Chen, Z. (2007). Document summarization using conditional random ﬁelds. In Proceedings of the 20th international joint\\nconference on Artiﬁcial intelligence (pp. 2862–2867).\\nToutanova, K. et al. (2007). The PYTHY summarization system: Microsoft research at DUC 2007. In Document understanding conference 2007 .<http://\\nduc.nist.gov> .\\nVapnik, V. N. (1995). The nature of statistical learning theory . Springer.\\nWang, C., Jing, F., Zhang, L., & Zhang, H. (2007). Learning query-biased web page summarization. In Proceedings of the 16th ACM conference on conference on\\ninformation and knowledge management (pp. 552–562).\\nZhao, L., Wu, L., & Huang, X. (2005). Fudan University at DUC 2005. In Document understanding conference 2005 .<http://duc.nist.gov> .\\nZhou, L., & Hovy, E. (2003). A web-trained extraction summarization system. In Proceedings of HLT-NAACL 2003 (pp. 205–211).\\nYou Ouyang is currently a Ph.D. student in the Department of Computing, the Hong Kong Polytechnic University, Hong Kong. He received the B.Sc. and M.Sc.'),\n",
       " Document(metadata={'title': 'Applying regression models to query-focused multi-document summarization', 'year': 2011}, page_content='degree in Peking University, Beijing, China, in 2004 and 2007 respectively. His main research interests include statistical natural language proce ssing, text\\nmining and data mining.\\nWenjie Li is currently an assistant professor in the Department of Computing, the Hong Kong Polytechnic University, Hong Kong. She received her Ph.D.\\ndegree from department of systems engineering and engineering management in the Chinese University of Hong Kong, Hong Kong, in 1997. Her main\\nresearch topics include natural language processing, information extraction and temporal information processing.\\nSujian Li is currently an assistant professor in the Key Laboratory of Computational Linguistics, Peking University, China. Her main research topics include\\nInformation Extraction, Automatic Indexing, Computational Linguistics.\\nQin Lu is currently a professor and associate head of the Department of Computing, the Hong Kong Polytechnic University, Hong Kong. Her research has\\nbeen on open systems especially in interoperability and internationalization, Chinese computing and natural language processing. She is currentl y the\\nrapporteur of the ISO/IEC/JTC1/SC2/WG2’s ideographic Rapporteur Group for the standardization of ideograph characters in the ISO/IEC 10646 stand ard.Y. Ouyang et al. / Information Processing and Management 47 (2011) 227–237 237'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='Applying DataMining Techniques\\nforDescriptivePhraseExtraction\\nin DigitalDocument Collections\\nHelenaAhonen OskariHeinonen MikaKlemettinen\\nA. Inkeri Verkamo\\nUniversitätTübingen UniversityofHelsinki\\nWilhelm-Schickard-InstitutfürInformatik Department ofComputerScience\\nSand 13,D–72076Tübingen P.O.Box 26,FIN–00014 UniversityofHelsinki\\nGermany Finland\\nhahonen@informatik.uni-tuebingen.de {oheinone,mklemett,verkamo}@cs.helsinki.ﬁ\\nAbstract\\nTraditionally,textshavebeenanalysedusingvariousin-\\nformationretrievalrelatedmethods,suchasfull-textanal-ysis, and natural languageprocessing. However, only few\\nexamples of data mining in text, particularly in full text,\\nareavailable.\\nIn this paper we show that general data mining meth-\\nods are applicable to text analysis tasks such as descrip-\\ntive phrase extraction. Moreover, we present a general\\nframeworkfortextmining. Theframeworkfollowsthegen-eral knowledge discovery process, thus containing steps\\nfrom preprocessing to the utilization of the results. The\\ndataminingmethodthatwe applyis basedon generalized\\nepisodesandepisoderules.\\nWe give concrete examples of how to preprocess texts\\nbasedontheintendeduseofthediscoveredresultsandwe\\nintroduceaweightingschemethathelpsinpruningoutre-\\ndundant or non-descriptive phrases. We also present re-\\nsultsfrom real-lifedataexperiments.\\n1 Introduction\\nRecently, we have seen the exuberant appearance of\\nvery large heterogeneous full-text document collections,\\navailable for any end user. The variety of users’ wishes'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='available for any end user. The variety of users’ wishes\\nis broad. The user may need an overall view of the doc-\\nument collection: what topics are covered, what kind ofdocumentsexist, are the documentssomehowrelated, and\\nso on. On theother hand, theuser may wantto ﬁnd a spe-\\nciﬁc piece of information content. At the other extreme,someusersmaybeinterestedinthelanguageitself,e.g.,in\\nwordusagesorlinguisticstructures.\\nA common feature for all the tasks mentioned is that\\nthe user does notknowexactly what he/she is lookingfor.Hence, a data mining approach should be appropriate, be-\\ncausebydeﬁnitionit isdiscoveringinterestingregularitiesor exceptionsfromthe data, possiblywithoutaprecisefo-\\ncus(see,e.g.,[13]).\\nSurprisinglyenough, only a few examplesof data min-\\ning in text, or text mining , are available. The most notable\\nare the KDT system [6] and Document Explorer [7] used\\ninminingReutersnewsarticles. Theirapproach,however,requires a substantial amount of background knowledge,\\nandisnotapplicableassuchtotextanalysisingeneral. An\\napproachmoresimilarto ourshasbeenusedin thePatent-\\nMinerSystem fordiscoveringtrendsamongpatents[8].\\nInthispaper,weshowthatgeneraldataminingmethods\\nareapplicabletotextanalysistasks;wealsopresentagen-\\neralframeworkfortextmining(Section2). Theframework\\nfollows the general knowledge discovery (KDD) process,\\nthus containingsteps from preprocessingto the utilization\\noftheresults. Wealsopresentexampleapplicationswithin'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='oftheresults. Wealsopresentexampleapplicationswithin\\ntheﬁeldsofinformationretrievalandnaturallanguagepro-cessing(Section3),andsomeresultsfromreal-lifedataex-\\nperimentsto show that our approachis applicablein prac-\\ntice(Section4). Section5is ashortconclusion.\\n2 General framework fortext mining\\nIn our approach, we consider text as sequentialdata, in\\nmany respects similar to the data collected by sensors or\\notherobservationsystems. Thegeneralknowledgediscov-\\nery process, adapted to the task of text processing, is rep-\\nresentedin Figure1. Thestarting pointis textualdata,and\\nthe end productis information describing phenomenathatarefrequentinthedata,e.g.,phrasesorco-occurringterms.\\nIn our approach, this information is presented as episodes\\nandepisode rules , two concepts which will be described\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. in more detail in Section 2.1. In additionto describingthe\\ndiscovery methods, we explain the strategic decisions of\\nthe preprocessing and postprocessing phases that are nec-\\nessaryto focusourdiscoveryprocess.\\n2.1 Episodes\\nEpisode rules andepisodes are a modiﬁcation of the\\nconcept of association rules andfrequent sets , applied to\\nsequential data. (For the basic deﬁnitions of associationrules,see,e.g.,[1],andfortheintroductionofepisodesand\\nepisode rules, see [12].) Sequential data, such as text, can\\nbeseenasasequenceofpairs (featurevector,index) where'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='beseenasasequenceofpairs (featurevector,index) where\\nfeature vector consists of an ordered set of features and\\nindexcontains information about the position of the word\\nin the sequence. It is common practice that the sequence\\nis represented in an increasing order of the indices (corre-\\nsponding to the order of the words in the originaltext). Afeaturecanbe/#0Fa word;e.g.,baseform,inﬂectedwordform,stem,/#0Fagrammaticalfeature;e.g.,partofspeech,case,num-ber,/#0Fa punctuationmarkorotherspecialcharacter,or/#0FanSGML structuretag.\\nWe deﬁne a textepisode asa pair /#0B /=/#28 V/; /#14 /#29,w he r e V\\nis a collection of feature vectors, and /#14is a partial order\\non V. Given a textsequence S, a textepisode /#0B /=/#28 V/; /#14 /#29\\noccurswithin Sif there is a way of satisfying the feature\\nvectorsin Vusing the featurevectorsin Sso that the par-\\ntial order /#14is respected. Intuitively, this means that the\\nfeaturevectorsof Vcanbefoundwithin Sinanorderthat\\nsatisﬁes thepartialorder /#14.\\nFor an occurrence of the episode to be interesting, all\\nfeature vectors of the episode must occur close enough\\nin S. What is close enough is deﬁned by giving a limit,\\nthewindow size W, within which the episode must oc-\\ncur. Hence, instead of considering all occurrences of the\\nepisode in S, we only examine occurrences within sub-\\nstrings S\\n/0of Swhere the difference of the indices of\\nthe feature vectors in S\\n/0is at most W. Moreover, since\\nthere may be several partially differing occurrencesof the'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='/0is at most W. Moreover, since\\nthere may be several partially differing occurrencesof the\\nepisodewithinthesubstring S\\n/0,werestrictourselvestothe\\ndistinctminimal occurrences of the episode; for a formal\\ndeﬁnitionofa minimaloccurrence,see [11].\\nAsanexample,letusthinkofatextsequencewherethe\\nfeaturevector containsthe base formof the word, the part\\nofspeech,andthenumberoftheword(whenappropriate).\\nThe textknowledge discovery in databases would now be\\npresentedbythesequence\\n(knowledge _N_SG,1)(discovery _N_SG,2)\\n(in_PP,3)(database _N_PL,4)Forawindowsizeof2,thissequencecontainstheepisode\\n(knowledge _N_SG,discovery _N_SG), but does not con-\\ntaintheepisode( knowledge _N_SG,database _N_PL).\\nThe most useful types of partial orders are (1) total or-\\nders,wherethefeaturevectorsofeachepisodehaveaﬁxed\\norder; such episodes are called serial; and (2) trivial par-\\ntial orders, where the order is not signiﬁcant at all; such\\nepisodes are called parallel. A typical exampleof a serial\\ntextepisodeisa phrase,consistingofasequenceofrelated\\nwords, with a speciﬁc meaning. A typical example of a\\nparallel text episode is a collection of co-occurring terms\\nwhichmaydescribethecontentsofadocumentbetterthananysingleterm.\\nThesupportof/#0Bin S(with respect to a given window\\nsize W) is deﬁned as the number of minimal occurrences\\nof /#0Bin S. Usually,weareonlyinterestedinepisodeswith\\na support exceeding a given support threshold , meaning\\nthattheyoccurinthesequencefrequentlyenoughnottobe\\nconsideredaccidental.'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='thattheyoccurinthesequencefrequentlyenoughnottobe\\nconsideredaccidental.\\nWe now move on to discuss episode rules. An episode\\nrulegivestheconditionalprobabilitythatacertainepisode\\noccurs(withinanintervalofgivenwindowsize),giventhat\\na subepisode has occurred (within the same or possibly\\nsmaller interval). Formally, an episode rule is an expres-\\nsion /#0C /#29 /#0B /#5Bwin/#0C–win/#0B\\n/#5D /;where /#0Cand /#0Bare episodes, /#0C\\nisasubepisodeof /#0B,andwin/#0Candwin/#0Barewindowsizes,\\nwithwin/#0C\\n/#14win/#0B.T h esupportof the rule is the sup-\\nport of /#0B. We also call /#0Btheepisode of the episode rule .\\nTheconﬁdence of the rule is the conditional probability\\nthat /#0Boccurs, given that /#0Coccurs, under the window size\\nconstraints speciﬁed by the rule. Since /#0Bincludes all the\\nfeaturevectorsof /#0C,weomitthefeaturevectorsof /#0Cwhen\\nrepresentingthe right-handside of the rule. In the follow-ing,theterm right-handside (ofarule)denotesthisdiffer-\\nence. Anexampleofanepisoderulecouldbe\\nknowledge,discovery,in/#29databases [4–5]\\n0.87 (33/38)\\nwhich tells us that in 87 per cent of the cases, where the\\nthree words (knowledge, discovery, in) occurred within\\n4 consequent words, also the word databases occurred\\nwithin 5 words. The supportof the rule is 33 and the sup-port of the episode on the left-hand side is 38. Usually,\\nwe arenot interestedin ruleswith a negligibleconﬁdence,\\ne.g.,lessthan20percent;henceitiscommontoselectonlythoseruleswithaconﬁdenceexceedingagiven conﬁdence\\nthreshold.'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='threshold.\\nThe method that we have used to discover frequent\\nepisodesandepisoderulesinourdataisdescribedin [11].\\nThis method allows us to discover serial and parallelepisodesofagivensupportthresholdandepisoderulesofa\\ngivenconﬁdencethresholdforacollectionofwindowsizes\\nwithaﬁxedupperlimit.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. DiscoveryEpisodes\\nRules Episode\\nRule Algorithm\\nPreprocessing PostprocessingPrune\\nGroup\\nOrder\\nAnalysis ProgramMorphologicalSelect/Filter\\nConvertConvert\\nFigure1. Knowledgediscovery fromtextualrepresentation intoepisodesand episoderules.\\n2.2 Preprocessing the data\\nAs we saw in Figure 1, the process of obtaining useful\\ninformationalsoreliesonpreprocessingthedatabeforethediscoveryphase,andonpostprocessingtheresultsafterthe\\ndiscovery phase. In particular, the preprocessing phase is\\ncrucial to the efﬁciency of the process, since according to\\nthe results in differentdomainareas and applications, pre-\\nprocessing can require as much as 80 per cent of the totaleffort[10].\\nThere are certain special aspects in the preprocessing\\nof textual data. Text consists of words, special characters,andstructuralinformation. Thepreprocessingrequiredde-\\npends heavily on the intended use of the results. Typi-\\ncally, the data is homogenized by replacing special char-\\nacters and structural information (e.g., SGML tags) with'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='acters and structural information (e.g., SGML tags) with\\nsymbols. Punctuationmarksandstructuralinformationof-ten need to be handled separately. Some of them may be\\nignored entirely, some of them may require special treat-\\nment, e.g., to ﬁnd and process the sentence breaks. Thelonger distance between words of differentsentences may\\nbetakenintoaccountbyinsertingalargergapintheindex-\\ningscheme;similarapproachhasbeenusedin[8].\\nPreprocessingmayinvolvesomeamountofnaturallan-\\nguage analysis. Morphological analysis gives us detailedinformationof the data which may be included in the fea-\\nture vector. We may also use this analysis to generalize\\nthe data, e.g., by replacing words by their parts of speech,whichallowsustoidentifyconstructssuchas (preposition,\\nnoun)insteadofcombinationsofspeciﬁc words.\\nFiltering of the data is used to focus our discovery\\nphase, to limit the number of results so that we are not\\noverwhelmed by uninteresting rules, and to decrease theprocessing effort needed in the discovery phase. Pruning\\nmay be done either before or after the discovery phase. If\\nwedonothavea clearideaofwhatkindofregularitieswearelookingforitisoftenadvisabletodeferthepruningde-\\ncisions to the postprocessingphase instead of heavy prun-\\ning in the preprocessing phase. On the other hand, with\\nlarge collections of documents, efﬁciency has to be taken\\nintoaccount. Incaseswhereweknowwhatfeaturesweare'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='intoaccount. Incaseswhereweknowwhatfeaturesweare\\nnotinterested in, preprocessingmust be preferred,to limitthesizeofthesearchspaceandthetimerequirementofthe\\ndiscoveryphase.\\nIn the preprocessingphase, pruningcan be used in two\\ndistinct ways. We may prune entire feature vectors, i.e.,\\ndropuninterestingitemssuchasarticles,prepositions,non-\\ninformativeverbs(e.g., be),orpunctuationmarks,orselect\\nonlysomeclassofwords(e.g.,nouns)tobeexamined. We\\nmay also focus on some features of each word, e.g., byleavingonly thebaseformof the word,or only partof the\\nmorphologicalinformation.\\nHence,thesentence\\nDocumentsareaninterestingapplicationﬁeld\\nfordataminingtechniques.\\nisﬁrstpreprocessedintothefollowingtaggedform:\\n(document _N_PL,1)(be_V_PRES_PL ,2)\\n(an_DET,3)(interesting _A_POS,4)\\n(application _N_SG,5)(ﬁeld_N_SG,6)\\n(for_PP,7)(data_N_SG,8)(mining_N_SG,9)\\n(technique _N_PL,10)(STOP,11)\\nInthefollowingstep,uninterestingpartsarepruned,result-\\ningin\\n(document _N_PL,1)(interesting _A_POS,4)\\n(application _N_SG,5)(ﬁeld_N_SG,6)\\n(data_N_SG,8)(mining_N_SG,9)\\n(technique _N_PL,10)\\nThetextmayalso beprunedto contain,e.g.,onlynouns,\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. (document _N_PL,1)(application _N_SG,5)\\n(ﬁeld_N_SG,6)(data_N_SG,8)\\n(mining_N_SG,9)(technique _N_PL,10)\\nor, depending on the goal of the research, the word itself\\nmay be discarded and only the morfological informationretained,'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='may be discarded and only the morfological informationretained,\\n(N_PL,1)(A_POS,4)(N_SG,5)(N_SG,6)\\n(N_SG,8)(N_SG,9)(N_PL,10)\\n2.3 Postprocessing theresults\\nTheepisodediscoverymethodtypically produceslarge\\namounts of episodes and episode rules. Hence, postpro-\\ncessing is essential to permit focusing our study. The cru-\\ncial problem is to deﬁne which episodes are sensible and\\nthuscanbeconsideredasuseful. Indatamining,typically,therelevanceofepisodesisverystronglydependentonthe\\napplication. One of our hypotheses is that there are mea-\\nsures of relevance which are common to all documents,independent of the semantic content of the texts. How-\\never, different usage needs also affect applying the mea-\\nsures. Postprocessing involves pruning, grouping and or-\\ndering the results. The usability of the results may be en-\\nhanced by using knowledge on the rules and episodes ofasingledocumentandcomparingit to similarinformation\\nontheentiredocumentcollection.\\nAsanexamplewe considerthefollowingsetting where\\nwewanttoﬁndasetofcharacteristicphrasesandkeywords\\nforeachdocument.\\nThemeasuresbasedontherulesofonedocumentare:/#0FLength: The(absolute)length lenofanepisodeisthe\\nnumberoffeaturevectors(usuallywords)itcontains.\\nThe length of a rule is the length of its episode. To\\nscalethelengthwecompareitto thehighestpossible\\nvalue which is the maximal window size winmax, i.e.,\\nwegetthe(relative) lengthl /=len\\nwinmax\\n/:/#0FTightness: Let len/1andlen/2bethe(absolute)lengths'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='wegetthe(relative) lengthl /=len\\nwinmax\\n/:/#0FTightness: Let len/1andlen/2bethe(absolute)lengths\\noftheleft-handsideoftheruleandtheentirerule,re-\\nspectively,andlet win/1andwin/2bethewindowsizes\\nof the left-hand side and the entire rule, respectively.Furthermore,let diffbethemeanofthedifferencesof\\nthesemeasures,i.e.,\\ndiff/=\\n/#28win/1\\n/,len/1\\n/#29/+/#28win/2\\n/,len/2\\n/#29/2\\n/:\\nThetightness toftheruleiscomputedast /=/1 /,diff\\nwinmax\\n/:\\n/#0FMutualconﬁdence: Let s/1, s/2,a nd s/3bethesupports\\nof the left-hand side, the right-hand side, and the en-\\ntirerule,respectively. The mutualconﬁdence mofthe\\nruleiscalculatedasm /=\\n/#28 s/3\\n/=s/1\\n/#29/+ /#28 s/3\\n/=s/2\\n/#29/2\\n/:\\nThe length of a phrase should be taken into account,\\nsince longer phrases should be preferred: they are more\\ndescriptive and it is always possible to reconstruct shorterphrases from longer ones. As longer phrases usually are\\nless frequent, there may be need to compensate them in\\nweighting.\\nTheupperlimitforthewindowsizealsogivestheupper\\nlimitfor the lengthof a phrase, i.e., if wewant to see longphrases,also thislimithasto be set higher. This, however,\\nmay result in short phrases spreading out too much, since\\nmoreslackisallowedwithinthephrase. Tightnessreducesthis effect and gives a way to decrease the weight of these\\npossiblyundesiredphrases.\\nMutual conﬁdence attempts to reveal ties between the\\nleft-handside and the right-handside of the rule. The for-'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='left-handside and the right-handside of the rule. The for-\\nmula above prefers cases in which words appear often to-gether and seldom with some other words. Other alter-\\nnatives are possible, e.g., we might want to get phrases,\\nin which one part is very frequentand appears with manyother words. That is, we might want to ﬁnd descriptive\\nattributestospecifyafrequentword.\\nTocreatearankingofruleswithinadocumentwecalcu-\\nlatea weightfor each rulecombiningthe abovemeasures.\\nItcanbedoneinseveralways,dependingontheemphasiswe want to give to each measure. We consider the sup-\\nportofaruletobeacentralfactor,andhence,calculatethe\\nweightwby multiplying the support by a combination of\\ntheothermeasures.\\nThemeasuresabovegiveaweighttoeachphrasebased\\non the knowledgewe have about the document. Since we\\nare interested in characteristic phrases that can also dis-\\ncriminate documents, we have to consider the distribution\\nof the phrases in the collection. For instance, in the legal\\ntexts we have used, many legal terms appear in almost alldocuments. Hence, for each rule in a document we com-\\npute itsinverse document frequency (IDF) that describes\\nhow common the rule is in the collection. The value iscomputedas\\nidf/= /, log\\n/#12numberofdocumentscontainingtherule\\nnumberofdocumentsinthecollection\\n/#13/:\\nFinally,theweightincludingtheIDFiscomputedasw /= s /#02idf /#02\\nt /+ m /+ l/3\\n/:'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='/#13/:\\nFinally,theweightincludingtheIDFiscomputedasw /= s /#02idf /#02\\nt /+ m /+ l/3\\n/:\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. After each rule has a weight, it is possible to select the\\nbestrulesandpruneawaytheruleswithaweightlessthan\\nagiventhreshold.\\nThediscoveryprocessoftenproducesanumberofrules\\nor episodes which may seem redundant. To prune out re-dundantrulesofdifferenttypes,somerulesofthumbcould\\nbeasfollows. (Key: kemikaali “chemicals”, käsittely“pro-\\ncessing”, varastointi “storage”,and teollinen“industrial”.)\\n1. For several occurrencesof the same rule, with differ-\\nent window sizes, e.g., (conﬁdence and weightwof\\ntheruleareincluded)\\nkemikaali _Nkäsittely_N /#29varastointi _N\\n[6–6] 0.667 23.00\\nkemikaali _Nkäsittely_N /#29varastointi _N\\n[5–6] 0.692 24.23\\nkemikaali _Nkäsittely_N /#29varastointi _N\\n[3–5] 0.708 26.03\\ntake the average of the weights or the largest weight.\\n(Wehaveusedthelatteroneofthese.)\\n2. Forrulescontainingsamewordswithdifferentdistri-\\nbution between the left-hand side and the right-handside,e.g.,\\nkemikaali _Nteollinen_A/#29\\nkäsittely_Nvarastointi _N\\n[2–5] 0.994 31.71\\nkemikaali _Nteollinen_Akäsittely_N /#29\\nvarastointi _N\\n[5–6] 0.947 29.36\\ntaketherulewith thelargestweight.\\n3. For subepisodes (subepisode rules) of other rules,\\ne.g., (support for entire rule and left-hand side, and\\nweight woftheruleareincluded)\\nkemikaali _Nteollinen_A /#29'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='weight woftheruleareincluded)\\nkemikaali _Nteollinen_A /#29\\nkäsittely_Nvarastointi _N\\n(17/18) 31.71\\nteollinen_Akäsittely_N /#29varastointi _N\\n(33/38) 45.27\\nteollinen_A /#29käsittely_N\\n(37/38) 73.04\\nteollinen_A /#29varastointi _N\\n(33/38) 56.87\\nprune away the subepisode unless its support is\\n(signiﬁcantly) higher than the support of its su-\\nperepisode(s).3 Applications\\nOur text mining approach is motivated by the needs of\\ninformation retrieval as explained in Section 3.1. The ap-\\nproach—withminormodiﬁcations—could,however,beapplied to a multitude of varioustasks. In Section 3.2, we\\npresent some applications in the area of natural language\\nprocessing.\\n3.1 Information retrieval tasks\\nIn information retrieval — or more speciﬁcally text re-\\ntrieval — key words and key phrases are commonly usedto boostquery processing[15, 9]. Consider a commonin-\\nformation retrieval task: The user expresses his/her infor-\\nmation needs, e.g., by giving a query, and the system exe-\\ncutesthesearchbymatchingthequerywiththedocuments.\\nWith large collections simply scanning the text is not fea-sible. Hence, a set of representative key words must be\\nselected and attached to the documents. For this purpose,\\nsingle-term key words may be too broad to be used alone.Phrases consisting of sequences of related words carry a\\nmore speciﬁc meaning than the single terms included in\\nthephrases.\\nA set of phrases can be regarded as a content descrip-\\ntor that should distinguish the documentfrom other docu-'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='tor that should distinguish the documentfrom other docu-\\nmentsinthecollection. Inadditionto simplequeries,con-\\ntent descriptors can be used for various text classiﬁcationtasks. For instance, documents can be clustered accord-\\ning to their similarity, e.g., to visualize a large document\\ncollection[5].\\nAlthough indexing and selecting key words are well-\\nstudied within information retrieval, new challenges havebeen recently set by the sudden appearance of very large\\nheterogeneous full text document collections. Lewis and\\nSpärck Jones [9] consider compoundkey terms as one es-\\nsentialpossibilitytoimprovethequalityoftextretrievalin\\nthis new situation. They also emphasize the need of ex-haustiveexperimentingtodeterminewhatthepreciseform\\nof these compound terms should be, and how they should\\nbeselectedandweightedrelativetotheirconstituents.\\nThepreprocessingrequiredfordiscoveringkeyphrases\\nis fairly straightforward. The grammatical features of thewords are not used, and typically we are interested either\\nin the original word or in its base form (e.g., processing\\norprocess). The structural information and punctuation\\nmarksareusuallydropped,buttheymayaffectthegapsin\\ntheindexingscheme,e.g.,itisoftendesiredthatthewordsin a phraseoccurin the same sentence. Commonfunction\\nwords(prepositions,articles,etc.) arepruned.\\nAfter the discovery phase, postprocessing includes\\nweighting and pruning of the resulting episode rules, as\\ndescribedinSection2.3anddemonstratedinSection4.3.'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='describedinSection2.3anddemonstratedinSection4.3.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. 3.2 Natural language processing\\nAnotherapplicationareais analysingthelinguisticfea-\\ntures of the text. We have considered three natural lan-\\nguageprocessingapplications:\\n1. discoveringgrammaticalrules,\\n2. discoveringcollocations,and\\n3. constructinggeneralizedconcordances.\\nThegrammatical rules that we consider here are any\\nrules describing dependencies between linguistic features\\nattached to words. For instance, we may want to studythe structure of sentences by discovering the ordered se-\\nquences of parts of speech. The preprocessing requires\\nleavingonlytheselected morphologicalfeatureswhiletheactual word forms are pruned. Depending on the focus of\\nthe study, entire feature vectors of some words and punc-\\ntuationmarksmay beprunedas well. Postprocessingmay\\ninclude sorting and grouping of rules according to some\\nfeatures.\\nCollocations arerecurrentcombinationsofwordscorre-\\nsponding to arbitrary word usages. Unlike typical phrasesused in information retrieval, collocations often contain\\nprepositions and inﬂected words. Smadja [16] distincts\\nthreetypesofcollocations:\\n1. predicative relations; e.g., frequent predicate–object\\npairslike make–decision ,\\n2. rigidnounphrases;e.g., TheDowJonesaverageof30\\nindustrialstocks ,a nd\\n3. phrasal templates that may contain empty slots; e.g.,'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='industrialstocks ,a nd\\n3. phrasal templates that may contain empty slots; e.g.,\\nThe average ﬁnished the week with a net loss of/#5BNUMBER /#5D.\\nIn addition to the linguistic interest, collocations may be\\nuseful in retrieval tasks. It has been shown [14] that sometypes of collocations are domain-dependent and, hence,\\ngood indicators of the topics covered by the document.\\nWhat kind of collocations are considered interesting de-\\npendsontheintendedapplication. Iftheyareusedas con-\\ntent descriptors in information retrieval, their discriminat-ingabilityisonecriterion.\\nAwidely used toolfor examiningword usagesin some\\ncollection of texts is constructing concordances : all the\\noccurrences of a given word in the collection are listed\\ntogether with the context, i.e., the words appearing im-mediately before and after the word. If the collection is\\nlarge, however, concordances can provide too much data.\\nOne way to group different word uses or to rank them in\\norder of importance is to sort concordance lines accord-\\ning to the collocations surroundingthe word [4]. We con-sider an even more advancedapproach, so-called general-\\nizedconcordances :frequentpatternsthatmaycontainboth\\nwords and grammatical features. Preprocessing may dropall sentences that do not contain the given word. Another\\npossibility is to remove the episodes or episode rules not\\ncontaining the word in the postprocessing phase. As the'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='containing the word in the postprocessing phase. As the\\nfeatures of a feature vector are handled as one entity, thecurrent discovery method does not produce concordances\\nincluding both word forms and grammatical features. For\\ninstance,studyingtheword discovery ,thepattern\\nknowledge /discovery/ in /#5BN /#5D\\ncannotbeproduced. However,theinstancesofthispattern,\\ne.g.,\\nknowledge /discovery/ in databases\\ncanbefoundusingsome grep-liketool,giventhatoriginal\\nwordformsand partsof speechare includedin the featurevector.\\n4 Experiments\\nTosurveytheusefulnessofknowledgediscoverymeth-\\nods and the discovered knowledge in the context of textdocuments,wehavemadeexperimentswith realdatasets.\\nWe ﬁrst brieﬂy describe the data sets and the conversion\\nof the original data into a suitable format for the analysis.\\nThen,wepresenttheexperimentsandtheirresults.\\n4.1 Data sets andpreprocessing\\nIntheexperimentsweusedarandomsampleof14doc-\\numentstakenfromalargecollectionofFinnishlegaltexts,originally in SGML format. The sizes of the documents\\nvariedbetween2,500and60,000words(see\\nTable 1). The\\nnumber of words includes punctuation marks; the num-ber of real words is about 20% lower and the number\\nof words and their morphological interpretations is about\\n20%higher,respectively. TheSGMLtagsarenotincluded\\ninthewordcounts.\\nAfter cleaning the data, the statutes were fed to a mor-\\nphological analyser program called Fintwol (a product of\\nLingsoft,Inc.), which givesus the base formand themor-'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='Lingsoft,Inc.), which givesus the base formand themor-\\nphological analysis of each word. Note that Fintwol only\\nlooks at one word at the time and does not try to disam-biguateusing the word context. An exampleof the output\\nis\\nrikoslain rikoslaki N GEN SG\\nwhich tells us that the word which occurred in the text isrikoslain, its base form is rikoslaki (in English, Criminal\\nAct), and the word is noun (N), genitive (GEN) and sin-gular (SG). However, a word may have several interpreta-\\ntions,beingeveninﬂectionsofseparatebaseforms.\\nAfter the preprocessing phase, the data format consists\\nof one word, its morphologicalinformation and index perline. Multipleinterpretationsofthewordsarepresentedon\\nconsecutive lines. The selected features are concatenated\\nasoneentity(seeexamplesinSection2).\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. Statute Phrases Pruned Phrases Co-occ.terms PrunedCo-occ.terms\\n#Words Epis. Size Rules Epis. Size Rules Epis. Size Epis. Size Epis. Size Epis. Size\\none one (1) one (2) one (1) one (2) one\\n1 4 273 39 36 335 32 137 30 118 54 35 28 81 31\\n2 38 970 472 310 103 408 285 43473 213 2 362 425 443 197 1670 321\\n3 2 622 29 24 726 21 428 19 61 32 25 17 48 27\\n4 19 682 321 204 116 279 184 35345 138 1 532 271 323 125 1025 164\\n5 6 427 90 78 14 86 74 774 59 194 100 72 57 174 86\\n6 61 559 962 491 327 839 449 1081090 349 4 911 618 1025 315 3801 488'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='6 61 559 962 491 327 839 449 1081090 349 4 911 618 1025 315 3801 488\\n7 5 815 87 66 17 78 58 890 46 311 89 78 38 238 59\\n8 20 756 369 186 259 262 169 41343 136 1 601 241 270 122 850 189\\n9 3 997 48 34 26 42 32 947 28 113 45 44 25 95 35\\n10 5 491 59 48 14 55 46 657 38 186 64 54 35 135 52\\n11 7 169 160 70 175 122 60 43114 53 376 89 99 45 255 67\\n12 3 445 32 28 430 26 235 23 124 43 31 19 90 31\\n13 4 576 68 50 27 62 44 15 70 39 191 69 59 31 149 48\\n14 5 239 48 40 12 42 36 461 33 110 53 47 29 84 42\\nTable 1. The test data and results. The number of co-occurring terms is taken from the test with a gap\\nbetweensentences;withoutagaptheﬁguresareabout20%higher. Withco-occurringterms,ﬁgures(1)\\nand (2)refer tothetwodifferent experimentsdescribed in Table2.\\n4.2 Phrases and co-occurring terms\\nIn our preliminary experiments (see [3]), we selected\\ncertaininterestingpartsofspeechandconsideredtwosim-\\nple test cases, the discovery of phrasesandco-occurring\\nterms. The latter case is particularly important in Finnish,\\nwheretheword orderis, in general, veryﬂexible. We also\\nwanted to compare the results between the analysis of the\\nwords (a) with and (b) without separating successive sen-tences(see\\nTable 2).\\nFor phrase discovery, we selected only nouns, proper\\nnouns, and adjectives. The search for co-occurringterms,\\nontheotherhand,wasﬁrstcarriedthroughwithnounsand\\nproper nouns, and then with nouns, proper nouns, verbs,andadjectives. In theexperiments,thewordsnotincluded'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='in the selected set were bypassed. We also used different\\nwindow sizes for episodes and produced both parallel andserial episodes. See\\nTable 2for the exact parameters used\\nintheexperiments.\\nThe results from discovering phrases and co-occurring\\nterms were rather similar. Both approachesproduced rea-\\nsonable results; in fact the main difference was some in-\\ncreaseordecreaseintheterm/phrasefrequencies. Withco-\\noccurring terms, the same effect occurred between the re-\\nsultsobtainedwithorwithoutagapbetweenthesentences.Examples of the phrases we found are the terms teolli-\\nnen käsittely (industrialprocessing) and vesioikeus päätös\\n(WaterRightsCourtjudgement)in\\nFigure2. (Notethatthe\\nFinnish phrases are given without inﬂections and are not\\nalwaysproperphrasesassuch.)\\nThe episodes and episode rules can be studied sepa-\\nrately, but also in parallel: we can ﬁrst search for frequent\\nepisodes and then study them more carefully by lookingat the rules. For instance, consider the examples in\\nFig-\\nure 2. If we take the episode (a) that occurs in the Decree\\non Chemicals (statute #9) rather frequently then by look-(a) 37: teollinen_Akäsittely_N\\n(b)teollinen_A /#29käsittely_N\\n[1– 2] 0.9737 (37/38)[1– 3] 0.9737 (37/38)\\n(c) 44: vesioikeus _Npäätös_N\\n(d)vesioikeus _N/#29päätös_N\\n[1– 2] 0.0681 (38/558)\\n[1– 3] 0.0735 (41/558)\\nFigure2. Exampleresultsfromphrasediscovery:\\nepisodes and episode rules from the Decree on\\nChemicals (a and b; statute #9) and Water RightsAct(cand d;statute#6).'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='Chemicals (a and b; statute #9) and Water RightsAct(cand d;statute#6).\\ningattherule(b)wecanconcludethatthephrase teollinen\\nkäsittelyis not only a common phrase, but in practice the\\ntermteollinen always implies an immediate occurrenceof\\nthe term käsittely. On the contrary, with an equally fre-\\nquentepisode (c) in the Water Rights Act (statute #6), therule (d) tells us that vesioikeus is actually quite rarely im-\\nmediatelyfollowedby päätös. Thiskindofanalysiscanbe\\ncompletedbylookingatallrulesthathaveeither vesioikeus\\nontheleft-handsideor päätösontheright-handside.\\nTo complement the experiments with phrases and co-\\noccurringterms, weexpandedourapproachto covermore\\ndetailedtextandlanguageanalysis. Weconsideredthefull\\nmorphologicalfeatures of the words and the structural in-formationoftheSGMLdocuments;fordetails, see[2].\\n4.3 Postprocessing bypruningandweighting\\nAfter thediscoveryphasewe appliedpostprocessingas\\ndescribed in Section 2.3, using the pruning schemes as\\ndeﬁned in Section 2.3. In the case of episode rules, we\\nusedallthethreeschemes,whereastheepisodeswereonly\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. Discovery of Episode Selectedparts Distinct Episode Episode Gap Window\\ntype ofspeech symbols/ support ruleconf. between sizes\\ndocument threshold threshold sentences\\nPhrases serial N,PROP,A 365..2693 10 0.2 5(+1) 1..6'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='document threshold threshold sentences\\nPhrases serial N,PROP,A 365..2693 10 0.2 5(+1) 1..6\\nCo-occurring parallel (1) N, PROP 293..1904 10 — (a) 10(+1) 1..11\\nTerms (2) N, PROP, V,A 457..3109 (b) 1\\nTable 2. The test parameter values. Note the abbreviations for nouns (N), proper nouns (PROP), adjec-\\ntives(A),andverbs (V).\\nprunedbycomparingtheirsupportstothesupportsoftheir\\nsuperepisodes.\\nThe overview of the results can be seen in Table 1as\\nnumbersof prunedrules andepisodes. Notethat thenum-\\nbersofepisodesgivencontaintheepisodesoflengthoneaswell, whereas all the rules contain at least two words. For\\ncomparison, we have included the number of all episodes\\nas well as the number of episodes of size one (denoted\\n“Sizeone”in\\nTable 1). Itisinterestingtoseethatalthough\\nall these documentsare legal text there are signiﬁcant dif-ferences: onecannotpredictthenumberofrules,episodes,\\nand frequentsingle wordsfrom thesize of the documents.\\nEven the pruning has varying effects. This fact is not aweaknessofthemethod,butonemorewaytocharacterize\\nvariousdocuments.\\nTo study the different schemes of weighting, we cal-\\nculated for each rule its lengthl, tightness t, mutual con-\\nﬁdence m, and inversed document frequency idf.U s i n g\\nthesemeasures,wecalculatedtheweight woftherule(see\\nSection2.3).\\nTheeffectsofweightingcanbestudiedintheAppendix,\\nwhichpresentsthewholesetofweightedrulesforonedoc-ument (Chemicals Act, statute #11). All the rules seem to'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='be sensible, and in most cases the ordering given by the\\nweight follows the support of the rule as is reasonable.However, an example where this is not the case, see the\\nsecond last rule momentti _N/#29tarkoitettu _Ain the Ap-\\npendix. This is a strong rule, but it is not descriptive nor\\ndiscriminating with respect to the other documents in the\\ncollection.\\nDueto thesmall test collection, theIDF valuedoesnot\\nalwaysdiscriminateasmuchaswewouldliketosee: evensome very general legal phrases appear in few documents\\nonly. Hence, in this set of 43 rules there are 16 rules (8 of\\nthem within the ﬁrst 20 rules) that can be considered toogeneral.\\n5 Conclusion\\nInthispaperweshowedthatgeneraldataminingmeth-\\nodsareapplicabletotextanalysistasks. Moreover,wepre-\\nsented a general framework for text mining. The frame-\\nwork follows the general KDD process, thus containingstepsfrompreprocessingtotheutilizationoftheresults.\\nWe gave concrete examples of how to pre- and post-process texts based on the intended use of the discovered\\nresults. We also presented example applications from in-\\nformationretrievalandnaturallanguageprocessing.\\nThe applicability of our approach was demonstrated\\nwith experiments on real-life data, showing that episodes\\nand episode rules produced discriminate between docu-\\nments. Both pre- and postprocessing have essential roles\\ninpruningandweightingtheresults.\\nAcknowledgments\\nWe thank Hannu Toivonen, Ph.D., for the episode rule'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='Acknowledgments\\nWe thank Hannu Toivonen, Ph.D., for the episode rule\\nalgorithm implementation. This work has been partiallysupported by the Academy of Finland post-doctoral grant\\nand the EC/TMR Marie Curie research training grant\\n#ERBFMBICT972821 (Helena Ahonen), and grants bythe 350th Anniversary Foundation of the University of\\nHelsinki (Oskari Heinonen) and the Nokia Foundation\\n(MikaKlemettinen).\\nReferences\\n[1] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and\\nA. I. Verkamo. Fast discovery of association rules. In\\nU.M.Fayyad,G.Piatetsky-Shapiro,P.Smyth,andR.Uthu-rusamy, editors, Advances in Knowledge Discovery and\\nData Mining , pages 307–328. AAAI Press, Menlo Park,\\nCalifornia,USA,1996.\\n[2] H. Ahonen, O. Heinonen, M. Klemettinen, and A. I.\\nVerkamo. Applying Data Mining Techniques in Text Anal-\\nysis. Technical Report C-1997-23, University of Helsinki,Department of Computer Science,Mar. 1997.\\n[3] H. Ahonen, O. Heinonen, M. Klemettinen, and A. I.\\nVerkamo. Mining in the phrasal frontier. In J.Komorowski\\nand J. Zytkow, editors, Proceedings of the First European\\nSymposium on Principles of Data Mining and KnowledgeDiscovery (PKDD’97) , number 1263 in Lecture Notes in\\nArtiﬁcialIntelligence,pages343–350, Trondheim,Norway,\\nJune 1997. Springer-Verlag.\\n[4] D. Biber. Co-occurrence patterns among collocations: a\\ntool for corpus-based lexical knowledge. Computational\\nLinguistics , 19(3):531–538, 1993.\\n[5] D. R. Cutting, D. Karger, J. Pedersen, and J. W. Tukey.'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='Linguistics , 19(3):531–538, 1993.\\n[5] D. R. Cutting, D. Karger, J. Pedersen, and J. W. Tukey.\\nScatter/Gather: A cluster-based approach to browsing large\\ndocument collections. In N. Belkin, P. Ingwersen, andA. Mark Pejtersen, editors, Proceedings of the 15th Annual\\nInternational ACM/SIGIR Conference (SIGIR’92) , pages\\n318–329, Copenhagen, Denmark, June 1992.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. [6] R. Feldman, I. Dagan, and W. Klösgen. Efﬁcient algo-\\nrithms for mining and manipulating associations in texts.\\nInCybernetics and Systems, Volume II, The Thirteenth Eu-\\nropean Meeting on Cybernetics and Systems Research ,V i -\\nenna, Austria, Apr.1996.\\n[7] R. Feldman, W. Kloesgen, and A. Zilberstein. Document\\nexplorer: Discovering knowledge in document collections.\\nInZ.W.RasandA.Skowron, editors, Proceedings ofTenth\\nInternational Symposium on Methodologies for IntelligentSystems (ISMIS’97) , number 1325 in Lecture Notes in Ar-\\ntiﬁcial Intelligence, pages 137–146, Charlotte, North Car-\\nolina, USA,Oct.1997. Springer-Verlag.\\n[8] B. Lent, R. Agrawal, and R. Srikant. Discovering trends in\\ntext databases. In D. Heckerman, H. Mannila, D. Pregibon,\\nand R. Uthrysamy, editors, Proceedings of the Third In-\\nternational Conference on Knowledge Discovery and Data\\nMining (KDD’97) , pages 227–230, Newport Beach, Cali-\\nfornia, USA,Aug. 1997. AAAI Press.'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='Mining (KDD’97) , pages 227–230, Newport Beach, Cali-\\nfornia, USA,Aug. 1997. AAAI Press.\\n[9] D. D. Lewis and K. Spärck Jones. Natural language pro-\\ncessing for information retrieval. Communications of the\\nACM, 39(1):92–101, 1996.\\n[10] H. Mannila. Data mining: machine learning, statistics, and\\ndatabases. In Proceedings of the 8th International Con-\\nferenceonScientiﬁc andStatisticalDatabase Management ,\\npages 1–6, Stockholm, Sweden, 1996.[11] H. Mannila and H. Toivonen. Discovering generalized\\nepisodes using minimal occurrences. In E. Simoudis,\\nJ. Han, and U. Fayyad, editors, Proceedings of the Sec-\\nondInternationalConference onKnowledge DiscoveryandData Mining (KDD’96) , pages 146–151, Portland, Oregon,\\nUSA,Aug. 1996. AAAI Press.\\n[12] H. Mannila, H. Toivonen, and A. I. Verkamo. Discov-\\nering frequent episodes in sequences. In Proceedings of\\nthe First International Conference on Knowledge Discov-ery and Data Mining (KDD’95) , pages 210–215, Montreal,\\nCanada, Aug. 1995.\\n[13] G.Piatetsky-ShapiroandW.J.Frawley,editors. Knowledge\\nDiscovery inDatabases . AAAIPress,MenloPark,Califor-\\nnia,USA,1991.\\n[14] E. Riloff. Little words can make a big difference for text\\nclassiﬁcation. In E. A. Fox, P. Ingwersen, and R. Fidel,\\neditors,Proceedings of the 18th International ACM SIGIR\\nConference on Research and Development in InformationRetrieval (SIGIR’95) , pages 130–136, Seattle, Washington,\\nUSA,July 1995.\\n[15] G. Salton. Automatic Text Processing: The Transfor-'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='USA,July 1995.\\n[15] G. Salton. Automatic Text Processing: The Transfor-\\nmation, Analysis, and Retrieval of Information by Com-\\nputer. Addison-Wesley Publishing Company, Reading,\\nMassachusetts, USA,1988.\\n[16] F.Smadja. Retrieving collocations fromtext: Xtract. Com-\\nputational Linguistics , 19(1):143–177, 1993.\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply. Appendix —Weightedepisode rules\\nWeightedepisoderulesfromtheChemicalsAct(statute\\n#11). Columns: ssupport of the rule, ttightness of therule, mmutual conﬁdence of the rule, lrelative length of\\nthe rule,idfinverse documentfrequency, and wweight of\\ntherule.\\nepisode rule stml i d f w\\nteollinen_A => käsittely_N 37 1.00 0.81 0.33 2.64 69.35\\nkäsittely_N => varastointi_N 35 0.75 0.72 0.33 2.64 55.44teollinen_A käsittely_N => varastointi_N 33 0.42 0.84 0.50 2.64 51.40vaarallinen_A => kemikaali_N 32 0.83 0.58 0.33 1.95 36.19\\nkemikaali_N teollinen_A käsittely_N => varastointi_N 18 0.67 0.66 0.67 2.64 31.84\\nterveys_N => vaarallinen_A 18 0.83 0.62 0.33 2.64 28.51\\nlaki_N => noja_N 26 0.67 0.47 0.33 1.95 24.84terveys_N => ympäristö_N 17 0.67 0.65 0.33 2.64 24.68noja_N annettu_A säännös_N => määräys_N 13 0.83 0.64 0.67 2.64 24.37ympäristö_N => vaarallinen_A 15 1.00 0.50 0.33 2.64 24.16\\nsäännös_N => määräys_N 16 0.92 0.47 0.33 2.64 24.08'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='säännös_N => määräys_N 16 0.92 0.47 0.33 2.64 24.08\\nannettu_A => säännös_N 17 0.83 0.43 0.33 2.64 23.79sosiaali-_A => terveydenhuolto_N 17 0.92 0.85 0.33 1.95 23.20noja_N => annettu_A 19 1.00 0.50 0.33 1.95 22.60terveys_N => kemikaali_N 17 0.75 0.40 0.33 2.64 21.99\\nnoja_N => määräys_N 16 0.75 0.48 0.33 2.64 21.96\\nlaki_N noja_N annettu_A => säännös_N 13 0.58 0.61 0.67 2.64 21.28\\nturvatekniikka_N => keskus_N 10 1.00 1.00 0.33 2.64 20.59\\nlaki_N annettu_A => säännös_N 15 0.42 0.64 0.50 2.64 20.59suomi_N suomi_PROP => ympäristökeskus_N 18 0.42 0.83 0.50 1.95 20.36\\nvaarallinen_A kemikaali_N teollinen_A käsittely_N => varastointi_N\\n10 0.92 0.57 0.83 2.64 20.33\\nnoja_N annettu_A => säännös_N 15 0.42 0.60 0.50 2.64 19.80\\nvaarallinen_A kemikaali_N teollinen_A => käsittely_N 11 0.75 0.55 0.67 2.64 19.17\\nsosiaali-_A terveydenhuolto_N => tuotevalvontakeskus_N 15 0.42 0.94 0.50 1.95 18.14ympäristö_N => kemikaali_N 14 0.83 0.31 0.33 2.64 18.11\\nkäsittely_N => koskeva_A 14 0.75 0.29 0.33 2.64 17.00\\nsäännös_N => noudattaminen_N 11 0.83 0.54 0.33 2.64 16.55ympäristö_N vaarallinen_A => kemikaali_N 13 0.42 0.48 0.50 2.64 16.13laki_N noja_N => annettu_A 15 0.67 0.48 0.50 1.95 16.09\\nvaarallinen_A kemikaali_N => teollinen_A 12 0.67 0.35 0.50 2.64 15.84\\nannettu_A => asetus_N 11 0.92 0.24 0.33 2.64 14.52'),\n",
       " Document(metadata={'title': 'Applying Data Mining Techniques for Descriptive Phrase Extraction in Digital Document Collections', 'year': 1997}, page_content='annettu_A => asetus_N 11 0.92 0.24 0.33 2.64 14.52\\nvaarallinen_A kemikaali_N => varastointi_N 11 0.67 0.30 0.50 2.64 14.23noja_N annettu_A => määräys_N 14 0.42 0.59 0.50 1.95 13.65terveys_N ympäristö_N => vaarallinen_A 10 0.58 0.44 0.50 2.64 13.46teollinen_A käsittely_N varastointi_N => koskeva_A 10 0.58 0.27 0.67 2.64 13.46\\ntarkoitettu_A teollinen_A => käsittely_N 10 0.42 0.59 0.50 2.64 13.20\\nteollinen_A käsittely_N => koskeva_A 12 0.42 0.31 0.50 2.64 12.99terveys_N vaarallinen_A => kemikaali_N 8 0.75 0.53 0.50 2.64 12.46varastointi_N => koskeva_A 11 0.67 0.26 0.33 2.64 12.20varastointi_N => ilmoitus_N 10 0.75 0.24 0.33 2.64 11.62\\nlaki_N voima_N => päivä_N 11 0.75 0.79 0.50 1.03 7.70\\nmomentti_N => tarkoitettu_A 28 1.00 0.49 0.33 0.44 7.52\\nannettu_A => laki_N 10 1.00 0.16 0.33 1.25 6.25\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 05:00:47 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/341427491\\nA Stacking-based Ensemble Learning Method for Outlier Detection\\nArticle \\xa0\\xa0 in\\xa0\\xa0Balk an Journal of Electric al and Comput er Engineering  · April 2020\\nDOI: 10.17694/b ajec e.679662\\nCITATIONS\\n17READS\\n336\\n3 author s:\\nAbdul Ahad\\nIqra Univ ersity\\n34 PUBLICA TIONS \\xa0\\xa0\\xa0200 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nErdal T asci\\nNational Canc er Instit ute (U SA), National Instit utes of He alth\\n37 PUBLICA TIONS \\xa0\\xa0\\xa0494 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAybars Ugur\\nEge Univ ersity\\n55 PUBLICA TIONS \\xa0\\xa0\\xa0813 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Abdul Ahad  on 09 June 2020.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.BALKAN JOURNAL OF ELE CTRICAL & COMPUTER ENGINEERING,     V ol. 8, No. 2 , April  2020                                                  \\n \\nCopyright © BAJECE                                                                 ISSN: 2147 -284X                                                     http://dergipark.gov.tr/bajece          \\nAbstract —Outlier detection is considered as one of the crucial \\nresearch areas for data mining. Many methods have been studied \\nwidely and utilized for achieving better results in outlier \\ndetection from existing literature; however, the effects  of these \\nfew ways are inadequate. In this paper, a stacking -based \\nensemble classifier has been proposed along with four base'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='ensemble classifier has been proposed along with four base \\nlearners (namely, Rotation Forest, Random Forest, Bagging and \\nBoosting) and a Meta -learner (namely, Logistic Regression) to \\nprogre ss the outlier detection performance. The proposed \\nmechanism is evaluated on five datasets from the ODDS library \\nby adopting five performance criteria. The experimental \\noutcomes demonstrate that the proposed method outperforms \\nthan the conventional ensembl e approaches concerning the \\naccuracy, AUC (Area Under Curve), precision, recall and F -\\nmeasure values. This method can be used for image recognition \\nand machine learning problems, such as binary classification.  \\n \\nIndex Terms — Outlier detection, Ensemble lear ning, Machine \\nLearning, Classification, Data Mining.  \\nI. INTRODUCTION  \\nUTLIER IS defined as an observation that deviates from \\nother observations or suspicious events that are generated \\nby different mechanisms. Outliers are anomalous, irregular, or \\noutlying re flections, the distortion of estimations in statistical \\nmodels [1].  \\n \\nThis is one of the best approaches of data analysis to deal with \\nobservations having numerous datasets, as automated tools are \\nbeing used in it to find patterns and relationships. In rec ent \\nyears, outlier detection has been widely used in several \\nindustries, such as medical, to detect credit card frauds and \\nsensors (IoT).   \\n \\n \\n \\nABDUL AHAD ABRO , is with the Department of Computer Engineering,'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='sensors (IoT).   \\n \\n \\n \\nABDUL AHAD ABRO , is with the Department of Computer Engineering, \\nEge U niversity, Izmir, Turkey, (e -mail: abdulahadabro1@gmail.com ). \\n https://orcid.org/0000 -0002 -3591 -9231  \\n \\nERDAL TAŞCI , is with the Department of Computer Engineering, Ege \\nUniversity, Izmir, Turkey, (e -mail: arif.erdal.tasci@ege.edu.tr ). \\nhttps://orcid.org/0000 -0001 -6754 -2187  \\n \\nAYBARS U ĞUR, is with the Department of Computer Engineering, Ege \\nUniversity, Izmir, Turkey, (e -mail:  aybars.ugur@ege.edu.tr ). \\n https://orcid.org/0000 -0003 -3622 -7672 \\n \\nManuscript received January  24, 2020 ; accepted April  14, 2020 .  \\nDOI: 10.17694/ bajece.679662  Ensemble Learning is a machine learning technique that \\naggregates various base models to generate a single predict ive \\nmodel. Numerous methods are used in Ensemble Learning to \\nreduce bias (boosting), variance (bagging), or to progress \\npredictions (stacking) [2]. It also means that the concept \\nprovides a promissory field of future research.  \\nWhile Random Forest was deve loped approximately two \\ndecades ago, it gives a powerful performance, simplicity in \\nimplementation and interpretability [3].  \\nOn the other hand, Rotation Forest, which is proposed by \\nPardo [4-5], provides favourable outcomes when compared to \\nAdaBoost, Rando m Subspaces, Bagging and Iterated Bagging.  \\nThe principal contribution of this paper is a) A stacking -based \\nensemble learning method which improves the outlier'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='ensemble learning method which improves the outlier \\ndetection performance ii) A comparative analysis of four base \\nlearners and one Meta -ensemble lear ner on five datasets from \\nthe ODDS library in terms of five evaluation criteria; accuracy \\n(Acc), AUC, precision, recall and F -measure.  \\nThis paper is structured with several different sections. In \\nsection II, related work presents ideas about ensemble \\nmeth ods. Section III discusses the proposed method in detail. \\nSection IV, provides experimental work, detail of datasets and \\noutcomes. Section V, is related to the evaluation of \\nperformance and results. Lastly, conclusion and future work \\nare suggested in Secti on VI.  \\nII. RELATED WORK \\nOutliers are mainly se gregated into three main areas: \\nCollective outliers, global outliers and contextual outliers [3]. \\nGlobal Outliers consider that outliers are associated with all \\nthe available data points. Contextual Outliers consid er that \\ndata separated from other data points in the context. However, \\nCollective Outlier values are different data groups that are \\ninaccurate according to a complete dataset. Outlier values are \\nalso known as abnormal as they examine the change to \\nidentify unexpected behaviour [4].  \\nA static ensemble shows the base learner and the fusion rule is \\nfixed for each single test point [5]. Generally, Bagging and \\nRandom subspace methods are employed in these processes. \\nFor instance, methods used to generate num erous diverse'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='For instance, methods used to generate num erous diverse \\ntraining subsets for base learners are combined in bagging and  \\nrandom subspace. Many ensemble  approach are also applied \\nover clustering algorithms. Therefore, the aggregation and \\nstructure standard of the ensemble is set for each single test \\npoint in this form of outlier detection strategy [6]. \\nIn other studies, Rotboost is a classifier of an ensemble, \\ninferred by combining the AdaBoost and Rotation forest. \\nThere are various datasets from the UCI ML repository, A Stacking -based Ensemble Learning Method \\nfor Outlier Detection  \\nAA. ABRO,  E. TAŞ CI,  A. UĞUR\\uf020 \\nO \\n181BALKAN JOURNAL OF ELE CTRICAL & COMPUTER ENGINEERING,     V ol. 8, No. 2 , April  2020                                                  \\n \\nCopyright © BAJECE                                                                 ISSN: 2147 -284X                                                     http://dergipark.gov.tr/bajece         among which a classification tre e that is being utilized  as the \\nbase learning algorithm. It has been shown by their results that \\nRotboost could generate a lower prediction error in an \\nensemble classifier in comparison to Rotation Forest or \\nAdaBoost [5]. The ensemble learning approaches s uch as \\nbagging mainly emphasis to get an ensemble model with less \\nvariance than its components; whereas, boosting and stacking \\ngenerally try to generate strong models less biased than their'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='generally try to generate strong models less biased than their  \\ncomponents even if variance can also be condensed. Random \\nForest ( a subprocess of the Meta -ensemble method) is used as \\na base learner in the rotation forest. This approach has \\nenhanced performances [7]. \\nIn [8], polarized images have been classified using Random \\nForest and Rotation Forest and it is concluded that Rotation  \\nForest provides more accurate results than SVM and Random \\nForest; however, Random Forest provides faster results than \\nRotation Forest.  \\nIt is examined whether Rotation Forest is the best classifier \\nthat assists in resolving problems with continuity or not.  [6]. \\nIn [9], A-Stacking and A -Bagging, the adaptive versions of \\nensemble learning approaches are proposed . A-Bagging \\nmethod has been applied by using the same base learners over \\nnumerous subsets of data and the predictions are aggregated \\nby using weighted  majority voting.  \\nIn [10], it is shown that ML algorithms provide satisfactory \\nperformance for the prediction of the outcomes in comparison \\nwith logistic regression.  \\nIII. DETAILS  OF THE  PROPOSED  METHOD   \\nIn this paper, we have proposed a framework of a Stacking -\\nbased ensemble learning method, including rotation forest, \\nrandom forest, bagging, boosting and logistic regression. \\nThere are numerous phases of the system, such as related with \\ndatasets, base and stacking -based ensemble learners. In order'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='datasets, base and stacking -based ensemble learners. In order \\nto obtain the g eneralization performance of the system, 10 -\\nfold cross -validation is used for all learners and datasets. The \\nranges of the values in data pre -processing may be high when \\ncompared to non-outlier  datasets. In this scenario, \\nclassification algorithms could be  affected significantly or \\nnegatively by some features.  \\nIn this work, four base learners and one Meta -learner are \\nemployed with one Stacking -based Meta classifier. Rotation \\nForest classifier depends upon feature extraction for \\nensembles. Typically, it prov ides more authentic results than \\nAdaBoost and Random Forest. The Random Forest classifier \\nis based on several collections of tree classifiers and randomly \\nselected sub -spaces of data are being used to create each \\nclassifier independently.  \\nEnsemble Learnin g such as bagging and boosting assist in \\ndiminishing various influences such as classification error. \\nFurthermore, combinations of many classifiers drop variance, \\nparticularly in the case of unstable classifiers, which may \\ngenerate a more reliable classifi cation than a single classifier.  \\nThe main idea of this study is to establish and provide data \\ncomprised of detecting outliers to present new methods related \\nto outlier detection in classification with logistic regression.  \\nWhereas, logistic regression pred icts to analyze, explain and \\nindicate the interrelation between one nominal and a'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='indicate the interrelation between one nominal and a \\ndependent binary variable, ratio -level independent or interval variables. Weka (Waikato Environment for Knowledge \\nAnalysis)[11], could examine and test multiple outliers [12], \\nwithout losing the impact of swamping and masking. We \\ndemonstrated the behaviour of our method through simulation \\nwith different percentages of outliers and sample sizes. In this \\nprocess, the different datasets have been utilized referred to \\nfrom the ODDS  library.  \\nIV. EXPERIMENTAL  WORK  \\nIn the  experimental process, five datasets have been used from \\nthe ODDS library for classifications [13].  \\nThe characteristics of datasets are analyzed concerning the \\nattributes and the number of instances. These datasets are \\ngenerally used to solve issues related to machine learning. \\nThere are no missing values in these datasets and there are \\nvarious numerical attribute  description s, which are illustrated \\nin Table I. As it can be observed from Table I, various \\ndatasets, the numb er of samples, dims and outliers are \\npresented for each dataset. Datasets are  chosen according to \\ntheir distinct parameters from the ODDS library source. It is \\ndetermined by investigating the appropriate data or datasets \\nwhich are being utilized in the fin dings of outliers. The \\nproposed stacking -based ensemble learning method has been \\nintroduced for this process. This method utilized the \\nimbalanced classification problems of binary (two -class)'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='imbalanced classification problems of binary (two -class) \\nwhere the positive case such as (class 1) is taken as an outlier  \\nand negative case (class 0) is taken as normal.  \\nTABLE I  \\nDATASETS DESCRIPTIONS  \\n \\nDatasets  Samples  Dims  Outliers  \\nGlass  214 9 9 (4.2%)  \\nLetter Recognition  1600  32 100(6.25%)  \\nShut tle 49097  9 3511 (7%)  \\nForest Cover  286048  10 2747(0.9%)  \\nVertebral  240 6 30 (12.5%)  \\n \\nIn this work, four different ensemble learning approaches have \\nbeen carried out along with the ensemble learning method, \\nwhich is considered suitable for the detection o f outliers. \\nHowever, the performance metrics are calculated based on \\noutlier detection according to binary classification problems. \\nIn this method, a technique has been used, which is known as \\nlogistic regression from the field of statistics and it is bein g \\nused to solve binary classification issues. A stacking -based \\nensemble method, along with logistic regression and four \\ndifferent baseline methods have been presented in Fig. 1.   \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\nFig.1. Ensemble Learning baseline methods  \\nBagging is a modest and  very influential ensemble process. It \\nis considered as the Bootstrap procedure to a high -variance \\nML algorithm. Simultaneously, Boosting denotes a group of \\nOutlier Detection  \\n \\n182BALKAN JOURNAL OF ELE CTRICAL & COMPUTER ENGINEERING,     V ol. 8, No. 2 , April  2020'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='Copyright © BAJECE                                                                 ISSN: 2147 -284X                                                     http://dergipark.gov.tr/bajece         algorithms that employ weighted averages to interchange the \\nweak learners into stronger learners. T he random forest \\nconsists of multiple random decision trees [14]. Rotation \\nforest is a tree -based ensemble that performs and transforms \\non subsets of attributes before constructing each tree.  \\nV. PERFORMANCE  EVALUATION  \\nA. Evaluation Measures  \\nThis section describ es the five performance evaluation \\nmeasures of the proposed method, consisting of accuracy, \\nAUC, precision, recall and F -measure.  \\n \\nAccuracy represents how near a measurement is to an \\nidentified or accepted figure. It is further defined in Eq.1.  \\n \\n                       (1) \\n \\nIn equation 1, TN, FN, FP and TP show the number of True \\nNegatives, False Negatives, False Positives and True \\nPositives.  \\n \\nAUC represents the Area under the ROC Curve. AUC \\ncalculates the whole two -dimensional area beneath the whole \\nROC cu rve from (0,0) to (1,1).  \\n \\nPrecision is a positive analytical value [15]. Precision defines \\nhow reliable measurements are, although they are farther from \\nthe accepted value.  \\nThe equation of precision is shown in Eq.2.  \\n \\n            \\n            (2) \\n \\nThe Reca ll is the hit rate  [15]. The recall is the reverse of \\nprecision; it calculates false negatives against true positives.'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='precision; it calculates false negatives against true positives. \\nThe equation is illustrated in Eq. 3.  \\n \\n            (3) \\n \\nF-measure can be defined as the weighted average [16] of \\nprecision and recall. This rating considers both false positives \\nand false negatives. The equation is illustrated in Eq. 4.  \\n \\n         \\n              (4) \\n \\nTables II -VII present accuracy, AUC, precision, recall and F -\\nmeasure individual values with ensemble methods for all \\ndatasets .  \\nTo sum up, Tables II -VI, have been designed according to the \\ndiverse data sets concerning the numerous approaches of \\nensemble learn ing in terms of different specifications. In Table \\nII, logistic regression has better outcomes, which provides \\n99.5327% Acc in comparison to others. Likely, in Table III, \\nrotation forest indicates 95.1875% Acc adequate \\nconsequences. Similarly, in Table IV,  the random forest \\npresents 99.9939% Acc effective results. Likewise, in Table V, the random forest illustrates the 99.9857% Acc productive \\noutcomes. However, in the end, logistic regression shows a \\n92.5% Acc result in Table VI.  \\n \\nTABLE II  \\nRESULTS OF ENSEMBLE LEARNING METHODS BY UTILIZING THE \\nGLASS DATASET  \\n \\n \\nTABLE III  \\nRESULTS OF ENSEMBLE LEARNING METHODS BY UTILIZING THE \\nLETTER RECOGNITION DATASET  \\n \\n \\nTABLE IV  \\n RESULTS OF ENSEMBLE LEARNING METHODS BY UTILIZING THE \\nSHUTTLE DATASET  \\nShuttle  \\nMethods  Acc \\n(%) AUC  Precision  Recall  F-Measure  \\nBagging  99.9919  0.999  1.000  1.000  1.000'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='(%) AUC  Precision  Recall  F-Measure  \\nBagging  99.9919  0.999  1.000  1.000  1.000  \\nAdaBoost  99.8330   1.000  0.998  0.998  0.998  \\nRandom \\nForest  99.9939  1.000  1.000  1.000  1.000  \\nRotation \\nForest  99.9817  1.000  1.000  1.000  1.000  \\nLogistic \\nRegression  99.6497  0.988  0.997  0.996  0.996  \\n \\n \\nTABLE V  \\nRESULTS OF ENSEMBLE LEARNING METHODS BY UTILIZING THE \\nFOREST COVER DATASET  \\nForest Cover  \\nMethods  Acc \\n (%) AUC  Precision  Recall F-Measure  \\nBagging  99.9790   1.000  1.000  1.000  1.000  \\nAdaBoost  99.8133  0.999  0.998  0.998  0.998  \\nRandom \\nForest  99.9857  1.000  1.000  1.000  1.000  \\nRotation \\nForest  99.9773  1.000  1.000  1.000  1.000  \\nLogistic \\nRegression  99.8941  1.000  0.999  0.999  0.999  \\n \\n \\n \\n Glass  \\nMethods  Acc \\n(%) AUC  Precision  Recall  F-Measure  \\nBagging  96.2617  0.988  0.954  0.963  0.954  \\nAdaBoost  99.0654  0.996  0.991  0.991  0.990  \\nRandom \\nForest  97.6636  0.997  0.975 0.977  0.974  \\nRotation \\nForest  97.1963  0.993  0.969  0.972  0.968  \\nLogistic \\nRegression  99.5327  0.999  0.996  0.995  0.995  \\nLetter Recognition  \\nMethods  Acc \\n(%) AUC  Precis ion Recall  F- \\nMeasure  \\nBagging  94.8750   0.944  0.949  0.949  0.932  \\nAdaBoost  93.7500   0.744 0.938  0.938  0.968  \\nRandom \\nForest  95.0000  0.987  0.953  0.950  0.934  \\nRotation \\nForest  95.1875  0.930  0.952  0.952  0.938  \\nLogistic \\nRegression  94.0625  0.813  0.925  0.941  0.926'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='Logistic \\nRegression  94.0625  0.813  0.925  0.941  0.926 \\n183BALKAN JOURNAL OF ELE CTRICAL & COMPUTER ENGINEERING,     V ol. 8, No. 2 , April  2020                                                  \\n \\nCopyright © BAJECE                                                                 ISSN: 2147 -284X                                                     http://dergipark.gov.tr/bajece         TABLE VI  \\n RESULTS OF ENSEMBLE LEARNING METHODS BY UTILIZING THE \\nVERTEBRAL DATASET  \\nVertebral  \\nMethods  Acc \\n(%) AUC  Precision  Recall  F-\\nMeasure  \\nBagging  91.6667  0.887  0.912  0.917  0.903  \\nAdaBoost  87.0833  0.879  0.844  0.871  0.852  \\nRandom \\nForest  91.6667  0.889 0.910  0.917  0.905  \\nRotation \\nForest  91.6667  0.929  0.909  0.917  0.909  \\nLogistic \\nRegression  92.5000 0.930  0.919  0.925  0.920  \\n \\nIn general, bagging has mo re successive consequences than \\nboosting, whereas, the random forest provides more effective \\noutputs than rotation forest in most of the datasets. On the \\nother hand, logistic regression has also provided satisfactory \\nresults to some extent, which is illust rated in Tables II and VI.  \\nIn Table VII, a stacking -based ensemble learning method has \\nbeen applied, in which the model is trained with the combined \\nprediction preceding model. The logistic regression has been \\nset as a Meta classifier and experienced the d iverse datasets \\nwith numerous methods like rotation forest, random forest, \\nboosting and bagging in the given order.'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='boosting and bagging in the given order.  \\nThe letter recognition, forest cover and vertebral datasets have \\nsignificant outputs concerning the accuracy, AUC, precision, \\nrecall and F -measure parameters in Table VII; however, glass \\nand shuttle datasets show similar outcomes for Tables II and \\nIV.  \\nTable VII demonstrates the comparison of all datasets results, \\nwith respect to our proposed  stacking -based meta -ensemble \\nlearning method. As it  is clearly shown in Table VII, a Meta -\\nensemble classifier, stacking with four base learners (namely, \\nRotation Forest, Random Forest, Bagging and Boosting) and \\none Meta -learner (namely, Logistic Regression) provide \\nhighly accurate outcomes as compare to ot hers.  \\n \\nTABLE VII  \\n  OUR STACKING -BASED ENSEMBLE LEARNING METHOD  \\n \\nProposed Stacking  \\nMeta Classifier Logistic Regression  \\nClassifier  Acc \\n (%) Impr.  \\n(%) AUC  Precision  Recall  F-\\nMeasure  \\n \\nGlass   \\n*99.5327   \\n0.0000   \\n0.997   \\n*0.996   \\n*0.995   \\n*0.995  \\nLetter \\nRecogniti on 97.4375  2.2500  *0.987  0.973  0.974  0.974  \\nShuttle  *99.9939  0.0000  *1.000  *1.000  *1.000  *1.000  \\nForest Cover  99.9860  0.0003  *1.000  *1.000  *1.000  *1.000  \\nVertebral  93.3333  0.8333  0.903  0.931  0.933  0.926  \\n  \\n-      * Indicates the  similar performance results c oncerning base \\nlearner.  \\n-        High Acc, AUC, Precision, Recall and F - measure is shown       \\nin Bold, wh ile the greyed shows insufficient results.'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='in Bold, wh ile the greyed shows insufficient results.  \\n-       Impr. represents improvement  according to best results of \\nTable s II-VI. \\n \\nMoreover, in Table VII, it is  analyzed that when stacking \\nbased ensemble learning method combines with logistic \\nregression, it provides more accurate outcomes than logistic regression; whereas, logistic regression does not provide better \\noutcomes when applied individually.  \\nVI. CONCLUSION  AND  FUTURE  WORK  \\nThis research work has proposed an approach of ensemble \\nclassifiers in multiple datasets efficiently for outlier detection. \\nHowever, these different methods such as Random forest, \\nRotation forest, Bagging and Boosting (base learners) and \\nMeta-learner logistic regression under stacking classifiers \\noccupy more space and consume more time for computations.  \\nThis method enables us to provide more productive and \\neffective outputs by using the advantages of these algorithms.  \\nWe believe that this s cenario is suitable for both research and \\ncommercial applications. The performance of the classifier \\nmodels can be different depending on the datasets that will be \\nchosen. In the future, other hybridization of ensemble learning \\nmethods will be employed for  performance improvement.  \\nREFERENCES  \\n[1] Ö. G. Alma, S. Kurt and U. Aybars, “Genetic algorithms for outlier \\ndetection in multiple regression with different information criteria,” \\nvol. 9655, 2011.'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='detection in multiple regression with different information criteria,” \\nvol. 9655, 2011.  \\n[2] C. Pardo, J. F. Diez -Pastor, C. Garcí a-Osorio and J. J. Rodríguez, \\n“Rotation Forests for regression,” Appl. Math. Comput., vol. 219, \\nno. 19, pp. 9914 –9924, 2013.  \\n[3] L. Chen, S. Gao and X. Cao, “Research on real -time outlier \\ndetection over big data streams,” Int. J. Comput. Appl., vol. 7074, \\npp. 1 –9, 2017.  \\n[4] N. Simidjievski, “Predicting long -term population dynamics with \\nbagging and boosting of process -based models,” vol. 42, pp. 8484 –\\n8496, 2015.  \\n[5] C. Zhang and J. Zhang, “RotBoost\\u202f: A technique for combining \\nRotation Forest and AdaBoost,” vol. 29, pp. 1524 –1536, 2008.  \\n[6] A. Bagnall, M. Flynn, J. Large, J. Line, A. Bostrom and G. Cawley, \\n“Is rotation forest the best classifier for problems with continuous \\nfeatures?,” 2018.  \\n[7] E. Taşcı, “A Meta -Ensemble Classifier Approach: Random Rotation \\nForest,” Balk. J. Electr. Comput. Eng., vol. 7, no. 2, pp. 182 –187, \\n2019.  \\n[8] P. Du, A. Samat, B. Waske, S. Liu and Z. Li, “Random Forest and \\nRotation Forest for fully polarized SAR image classification using \\npolarimetric and spatial features,” ISPRS J. Ph otogramm. Remote \\nSens., vol. 105, pp. 38 –53, 2015.  \\n[9] S. Agarwal and C. R. Chowdary, “A -Stacking and A -Bagging: \\nAdaptive versions of ensemble learning algorithms for spoof \\nfingerprint detection,” Expert Syst. Appl., vol. 146, p. 113160, \\n2020.'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='fingerprint detection,” Expert Syst. Appl., vol. 146, p. 113160, \\n2020.  \\n[10] J. zhou  Feng, Y. Wang, J. Peng, M. wei Sun, J. Zeng and H. Jiang, \\n“Comparison between logistic regression and machine learning \\nalgorithms on survival prediction of traumatic brain injuries,” J. \\nCrit. Care, vol. 54, pp. 110 –116, 2019.  \\n[11] Eibe Frank, Mark A. Hall  and Ian H. Witten (2016). The WEKA \\nWorkbench. Online Appendix for \"Data Mining: Practical Machine \\nLearning Tools and Techniques\", Morgan Kaufmann, Fourth \\nEdition, 2016.  \\n[12] T. A. Engel, A. S. Charão, M. Kirsch -Pinheiro and L. A. Steffenel, \\n“Performance i mprovement of data mining in weka through GPU \\nacceleration,” Procedia Comput. Sci., vol. 32, pp. 93 –100, 2014.  \\n[13] Shebuti Rayana (2016).  ODDS Library \\n[http://odds.cs.stonybrook.edu ]. Stony Brook, NY: Stony Brook \\nUniversity, Department of Computer Science.  \\n[14] Y. Zhou and G. Qiu, “Random forest for label ranking,” Expert \\nSyst. Appl., vol. 112, pp. 99 –109, 2018.  \\n[15] T. Fawcett, “An introduction to ROC analysis,” Pattern Recognit. \\nLett., vol. 27, no. 8, pp. 8 61–874, 2006.  \\n[16] L. A. Bull, K. Worden, R. Fuentes, G. Manson, E. J. Cross, and N. \\nDervilis, “Outlier ensembles: A robust method for damage detection \\nand unsupervised feature extraction from high -dimensional data,” J. \\nSound Vib., vol. 453, pp. 126 –150, 2 019.'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='Sound Vib., vol. 453, pp. 126 –150, 2 019. \\n184BALKAN JOURNAL OF ELE CTRICAL & COMPUTER ENGINEERING,     V ol. 8, No. 2 , April  2020                                                  \\n \\nCopyright © BAJECE                                                                 ISSN: 2147 -284X                                                     http://dergipark.gov.tr/bajece         ABDUL AHAD ABRO was born in \\n1988. He received the BS degree \\nfrom the University of Sindh, MSc \\ndegree from Mohammad Ali Jinnah \\nUniversity and enrolled in a PhD \\ndegree at the Ege University, \\nComputer Engineering Department, \\nIzmir , Turkey. From 2014 to 2015, \\nhe was an ad -hoc lecturer in Sindh Madressatul Islam \\nUniversity. He also serves as a reviewer for \\nnumerous  indexed journals & conferences. His \\nresearch interests are machine learning, data mining \\nand ensemble learning.  \\n \\n \\nERDAL  TAŞCI was born in 1989. \\nHe received the BS, MSc and PhD \\ndegrees from the Ege University, \\nComputer Engineering Department, \\nIzmir, Turkey, in 2011, 2013 and \\n2018, respectively. He has been \\nworking as a teaching assistant at \\nthe Department of Computer Engine ering of Ege \\nUniversity, Izmir, Turkey. He has been reviewing for \\nvarious journals including IEEE Transactions on \\nSystems, Man and Cybernetics: Systems, Computer \\nMethods and Programs in Biomedicine and IET \\nImage Processing. His research interests are patte rn \\nrecognition, machine learning, image processing and \\ndata mining.'),\n",
       " Document(metadata={'title': 'A Stacking-based Ensemble Learning Method for Outlier Detection', 'year': 2020}, page_content='recognition, machine learning, image processing and \\ndata mining.  \\n \\nAYBARS UĞUR is a full -time \\nprofessor in the Department of \\nComputer Engineering at Ege \\nUniversity, Izmir, Turkey. He \\nreceived his BS, MSc and Ph.D. \\ndegrees in computer engineering \\nfrom Ege University, Izmir, Turkey, \\nin 1993, 1996, 2001, respectively. His rese arch \\ninterests are artificial intelligence, machine learning, \\ndeep learning, swarm intelligence, computer vision, \\noptimization, intelligent systems and computer \\ngraphics.  \\n \\n \\n185\\nView publication stats'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='are used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2Figure 1: The Transformer - model architecture.'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='2Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='the matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='dff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='consider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='and decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='architectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='models have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='Transformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='13Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the'),\n",
       " Document(metadata={'title': 'Attention Is All You Need', 'year': 2017}, page_content='sentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021 4291\\nAttention in Natural Language Processing\\nAndrea Galassi , Marco Lippi , and Paolo Torroni\\nAbstract — Attention is an increasingly popular mechanism\\nused in a wide range of neural architectures. The mechanismitself has been realized in a variety of formats. However,because of the fast-paced advances in this domain, a systematicoverview of attention is still missing. In this article, we deﬁnea uniﬁed model for attention architectures in natural languageprocessing, with a focus on those designed to work with vectorrepresentations of the textual data. We propose a taxonomy ofattention models according to four dimensions: the representationof the input, the compatibility function, the distribution function,and the multiplicity of the input and/or output. We present theexamples of how prior information can be exploited in attentionmodels and discuss ongoing research efforts and open challengesin the area, providing the ﬁrst extensive categorization of thevast body of literature in this exciting domain.\\nIndex Terms — Natural language processing (NLP), neural\\nattention, neural networks, review, survey.\\nI. I NTRODUCTION\\nIN MANY problems that involve the processing of natural\\nlanguage, the elements composing the source text are\\ncharacterized by having each a different relevance to the task\\nat hand. For instance, in aspect-b ased sentiment analysis, cue'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='at hand. For instance, in aspect-b ased sentiment analysis, cue\\nwords, such as “good” or “bad,” could be relevant to some\\naspects under consideration, but not to others. In machinetranslation, some words in the source text could be irrelevant in\\nthe translation of the next word. In a visual question-answering\\ntask, background pixels could be irrelevant in answering aquestion regarding an object in the foreground but relevant to\\nquestions regarding the scenery.\\nArguably, effective solutions to such problems should factor\\nin a notion of relevance, so as to focus the computational\\nresources on a restricted set of important elements. Onepossible approach would be to tailor solutions to the speciﬁc\\ngenre at hand, in order to better exploit known regularities\\nof the input, by feature engineering. For example, in theargumentative analysis of persuasive essays, one could decide\\nto give special emphasis to the ﬁnal sentence. However, such\\nan approach is not always viable, especially if the input is\\nlong or very information-rich, such as in text summarization,\\nManuscript received July 5, 2019; revised December 17, 2019 and April 17,\\n2020; accepted August 20, 2020. Date of publication September 10, 2020; dateof current version October 6, 2021. This work was supported by Horizon\\n2020, project AI4EU, under Grant 825619. (Corresponding author: Andrea\\nGalassi.)\\nAndrea Galassi and Paolo Torroni are with the Department of Computer'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='Galassi.)\\nAndrea Galassi and Paolo Torroni are with the Department of Computer\\nScience and Engineering (DISI), Univ ersity of Bologna, 40126 Bologna, Italy\\n(e-mail: a.galassi@unibo.it; paolo.torroni@unibo.it).\\nMarco Lippi is with the Department o f Sciences and Methods for Engineer-\\ning (DISMI), University of Modena and Reggio Emilia, 41121 Modena, Italy\\n(e-mail: marco.lippi@unimore.it).\\nColor versions of one or more of the ﬁgures in this article are available\\nonline at https://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TNNLS.2020.3019893where the output is the condensed version of a possibly lengthy\\ntext sequence. Another approach of increasing popularity\\namounts to machine learning the relevance of input elements.\\nIn that way, neural architectures could automatically weigh the\\nrelevance of any region of the input and take such a weight\\ninto account while performing the main task. The commonestsolution to this problem is a mechanism known as attention.\\nAttention was ﬁrst introduced in natural language process-\\ning (NLP) for machine translation tasks by Bahdanau et al. [2].\\nHowever, the idea of glimpses had already been proposed in\\ncomputer vision by Larochelle and Hinton [3], following the\\nobservation that biological retin as ﬁxate on relevant parts of\\nthe optic array, while resolution f alls off rapidly with eccentric-\\nity. The term visual attention became especially popular afterMnih et al. [4] signiﬁcantly outperformed the state of the art in'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='several image classiﬁcation tasks as well as in dynamic visual\\ncontrol problems such as object tracking due to an architecture\\nthat could adaptively select and then process a sequence of\\nregions or locations at high resolution and use a progressively\\nlower resolution for further pixels.\\nBesides offering a performan ce gain, the attention mecha-\\nnism can also be used as a tool for interpreting the behavior ofneural architectures, which are notoriously difﬁcult to under-\\nstand. Indeed, neural networks a re subsymbolic architectures;\\ntherefore, the knowledge they gather is stored in numericelements that do not provide a ny means of interpretation\\nby themselves. It then becomes hard if not impossible to\\npinpoint the reasons behind the wrong output of a neural\\narchitecture. Interestingly, attention could provide a key to\\npartially interpret and explain neural network behavior [5]–[9],even if it cannot be considered a reliable means of explana-\\ntion [10], [11]. For instance, the weights computed by attention\\ncould point us to relevant information discarded by the neuralnetwork or to irrelevant elements of the input source that have\\nbeen factored in and could explain a surprising output of the\\nneural network.\\nTherefore, visual highlights of attention weights could be\\ninstrumental in analyzing the outcome of neural networks,and a number of speciﬁc tools have been devised for such a\\nvisualization [12], [13]. Fig. 1 shows an example of attention'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='visualization [12], [13]. Fig. 1 shows an example of attention\\nvisualization in the context of aspect-based sentiment analysis.\\nFor all these reasons, attentio n has become an increasingly\\ncommon ingredient of neural architectures for NLP [14], [15].\\nTable I presents a nonexhaustive list of neural architectures\\nwhere the introduction of an attention mechanism has brought\\nabout a signiﬁcant gain. Works are grouped by the NLPtasks they address. The spectrum of tasks involved is remark-\\nably broad. Besides NLP and computer vision [16]–[18],\\nattentive models have been successfully adopted in manyother different ﬁelds, such as speech recognition [19]–[21],\\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/4292 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\nFig. 1. Example of attention visualization for an aspect-based sentiment a nalysis task, from [1, Fig. 6]. Words are highlighted according to attenti on scores.\\nPhrases in bold are the words considered relevant for the task or human rationales.\\nTABLE I\\nNONEXHAUSTIVE LIST OF WORKS THAT EXPLOIT ATTENTION ,GROUPED\\nBY THE TASK(S)ADDRESSED\\nrecommendation [22], [23], time-series analysis [24], [25],\\ngames [26], and mathematical problems [27], [28].\\nIn NLP, after an initial exploration by a number of seminal\\npapers [2], [59], a fast-paced development of new attention'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='papers [2], [59], a fast-paced development of new attention\\nmodels and attentive architectures ensued, resulting in a highly\\ndiversiﬁed architectural landscape. Because of, and adding to,the overall complexity, it is not unheard of different authors\\nwho have been independently following similar intuitions lead-ing to the development of almost identical attention models.\\nFor instance, the concepts of inner attention [68] and word\\nattention [41] are arguably one and the same. Unsurprisingly,\\nthe same terms have been introduced by different authors to\\ndeﬁne different concepts, thus further adding to the ambiguityin the literature. For example, the term context vector is used\\nwith different meanings by Bahdanau et al. [2], Yang et al.\\n[52], and Wang et al. [129].\\nIn this article, we offer a systematic overview of attention\\nmodels developed for NLP. To this end, we provide a general\\nmodel of attention for NLP tasks and use it to chart the\\nmajor research activities in this area. We also introduce a\\ntaxonomy that describes the existing approaches along fourdimensions: input representati on, compatibility function, dis-\\ntribution function, and input/output multiplicity. To the best\\nof our knowledge, this is the ﬁrst taxonomy of attentionmodels. Accordingly, we provid e a succinct description of each\\nattention model, compare the models with one another, and\\noffer insights on their use. Moreover, we present the examples\\nregarding the use of prior information in unison with attention,'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='regarding the use of prior information in unison with attention,\\ndebate about the possible future uses of attention, and describesome interesting open challenges.\\nWe restrict our analysis to attentive architectures designed\\nto work with vector representation of data, as it typically isthe case in NLP. Readers inter ested in attention models for\\ntasks where data have a graphical representation may refer to\\nLee et al. [130].\\nWhat this survey does not offer is a comprehensive account\\nof all the neural architectures for NLP (for an excellentoverview, see [131]) or of all the neural architectures for NLP\\nthat uses an attention mechanis m. This would be impossible\\nand would rapidly become obs olete because of the sheer\\nvolume of new articles featuring architectures that increasingly\\nrely on such a mechanism. Moreover, our purpose is to\\nproduce a synthesis and a critical outlook rather than a ﬂat\\nlisting of research activities. For the same reason, we do not\\noffer a quantitative evaluation of different types of attentionmechanisms since such mechan isms are generally embedded\\nin larger neural network architectures devised to addressGALASSI et al. : ATTENTION IN NLP 4293\\nspeciﬁc tasks, and it would be pointless in many cases to\\nattempt comparisons using different standards. Even for a\\nsingle speciﬁc NLP task, a fair evaluation of different attention\\nmodels would require experimentation with multiple neuralarchitectures, extensive hyperparameter tuning, and validation'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='over a variety of benchmarks. However, attention can be\\napplied to a multiplicity of tasks, and there are no datasets that would meaningfully cover such a variety of tasks.\\nAn empirical evaluation is thus beyond the scope of this\\narticle. There are, however, a number of experimental studies\\nfocused on particular NLP tasks, including machine transla-\\ntion [37], [42], [48], [132], argumentation mining [125], textsummarization [58], and sentiment analysis [7]. It is worth-\\nwhile remarking that, on several occasions, attention-based\\napproaches enabled a dramatic development of entire research\\nlines. In some cases, such a development has produced an\\nimmediate performance boost. This was the case, for example,\\nwith the transformer [36] for sequence-to-sequence annotation,\\nas well as with BERT [60], currently among the most popular\\narchitectures for the creation of embeddings. In other cases,the impact of attention-based models was even greater, paving\\nthe way to radically new approaches for some tasks. This\\nwas the inﬂuence of Bahdanau et al. ’s work [2] to the ﬁeld\\nof machine translation. Likewise, the expressive power of\\nmemory networks [59] signiﬁcantly contributed to the idea\\nof using deep networks for reasoning tasks.\\nThis survey is structured as follows. In Section II, we deﬁne\\na general model of attention and describe its components.We use a well-known machine-translation architecture intro-\\nduced by Bahdanau et al. [2] as an illustration and an instance'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='duced by Bahdanau et al. [2] as an illustration and an instance\\nof the general model. In Section III, we elaborate on theuses of attention in various NLP tasks. Section IV presents\\nour taxonomy of attention models. Section V discusses how\\nattention can be combined with knowledge about the task or\\nthe data. Section VI is devoted to open challenges, current\\ntrends, and future directions. Section VII concludes this article.\\nII. A\\nTTENTION FUNCTION\\nThe attention mechanism is a part of a neural architecture\\nthat enables to dynamically highlight relevant features of the\\ninput data, which, in NLP, is typically a sequence of textualelements. It can be applied directly to the raw input or to its\\nhigher level representation. The core idea behind attention is to\\ncompute a weight distribution on the input sequence, assigning\\nhigher values to more relevant elements.\\nTo illustrate, we brieﬂy describe a classic attention archi-\\ntecture, called RNNsearch [2]. We chose RNNsearch because\\nof its historical signiﬁcance and for its simplicity with respect\\nto other structures that we will describe further on.\\nA. Example for Machine Translation and Alignment\\nRNNsearch uses attention for machine translation. The\\nobjective is to compute an output sequence ythat is a\\ntranslation of an input sequence x. The architecture consists\\nof an encoder followed by a decoder, as shown in Fig. 2.\\nThe encoder is a bidirectional recurrent neural network\\n(BiRNN) [133] that computes an annotation term h\\nifor every'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='(BiRNN) [133] that computes an annotation term h\\nifor every\\nFig. 2. Architecture of RNNsearch [2] ( left) and its attention model (right).\\ninput term xiofx\\n(h1,..., hT)=BiRNN (x1,..., xT). (1)\\nThe decoder consists of two cascading elements: the atten-\\ntion function and an RNN. At each time step t, the attention\\nfunction produces an embedding ctof the input sequence,\\ncalled a context vector. The s ubsequent RNN, characterized\\nby a hidden state st, computes from such an embedding\\na probability distribution over all possible output symbols,\\npointing to the most probable symbol yt\\np(yt|y1,..., yt−1,x)=RNN (ct). (2)\\nThe context vector is obtained as follows. At each time\\nstep t, the attention function takes as input the previous hidden\\nstate of the RNN st−1and the annotations h1,..., hT.S u c h\\ninputs are processed through an alignment model [see (3)]\\nto obtain a set of scalar values etithat score the matching\\nbetween the inputs around position iand the outputs around\\nposition t. These scores are then normalized through a softmax\\nfunction, so as to obtain a set of weights ati[see (4)]\\neti=f(st−1,hi) (3)\\nati=exp(eti)\\n/summationtextT\\nj=1exp(etj). (4)\\nFinally, the context vector ctis computed as a weighted\\nsum of the annotations hibased on their weights ati\\nct=/summationdisplay\\niatihi. (5)\\nQuoting Bahdanau et al. [2], the use of attention “relieve[s]\\nthe encoder from the burden of having to encode all informa-\\ntion in the source sentence into a ﬁxed-length vector. With this'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='tion in the source sentence into a ﬁxed-length vector. With this\\nnew approach, the information can be spread throughout the\\nsequence of annotations, which can be selectively retrieved bythe decoder accordingly.”\\nB. Uniﬁed Attention Model\\nThe characteristics of an attention model depend on the\\nstructure of the data whereupon they operate and on the desired\\noutput structure. The uniﬁed model we propose is based on4294 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\nTABLE II\\nNOTATION\\nFig. 3. Core attention model.\\nand extends the models proposed by Vaswani et al. [36] and\\nDaniluk et al. [61]. It comprises a core part shared by almost\\nthe totality of the models found in the surveyed literature,\\nas well as some additional components that, although not\\nuniversally present, are still found in most literature models.\\nFig. 3 shows the core attention model, which is part of the\\ngeneral model shown in Fig. 4. Table II lists the key termsand symbols. The core of the attention mechanism maps a\\nsequence Kofd\\nkvectors ki, the keys, to a distribution a\\nofdkweights ai.Kencodes the data features whereupon\\nattention is computed. For instance, Km a yb ew o r do r\\ncharacter embeddings of a document, or the internal states\\nof a recurrent architecture, as it happens with the annotation\\nhiin RNNsearch. In some cases, Kcould include multiple\\nfeatures or representations of the same object (e.g., bothone-hot encoding and embedding of a word) or even—if the'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='task calls for it—representations of entire documents.\\nMore often than not, another input element q, called query,\\n1\\nis used as a reference when com puting the attention distribu-\\ntion. In that case, the attention mechanism will give emphasis\\nto the input elements releva nt to the task according to q.I fn o\\nquery is deﬁned, attention will give emphasis to the elements\\ninherently relevant to the task at hand. In RNNsearch, forinstance, qis a single element, namely, the RNN hidden state\\ns\\nt−1. In other architectures, qmay represent different entities:\\nembeddings of actual textual queries, contextual information,\\n1The concept of “query” in attention models should not be confused with\\nthat used in tasks like question answering or information retrieval. In ourmodel, the “query” is part of a general a rchitecture and is task-independent.background knowledge, and so on. It can also take the form of\\na matrix rather than a vector. For example, in their document\\nattentive reader, Sordoni et al. [67] made use of two query\\nvectors.\\nFrom the keys and query, a vector eofdkenergy scores ei\\nis computed through a c ompatibility function f\\ne=f(q,K). (6)\\nFunction fcorresponds to RNNsearch’s alignment model\\nand to what other authors call energy function [43]. Energy\\nscores are then transformed into attention weights using what\\nwe call a distribution function g\\na=g(e). (7)\\nSuch weights are the outcome of the core attention mech-\\nanism. The commonest distribution function is the softmax'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='anism. The commonest distribution function is the softmax\\nfunction, as in RNNsearch, which normalizes all the scores to\\na probability distribution. Weights represent the relevance of\\neach element to the given task, with respect to qandK.\\nThe computation of these weights may already be sufﬁcient\\nfor some tasks, such as the classiﬁcation task addressed by Cui\\net al. [70]. Nevertheless, many tasks require the computation\\nof new representation of the keys. In such cases, it is commonto have another input element; a sequence Vofd\\nkvectors\\nvi, the values, representing the data whereupon the attention\\ncomputed from Kandqis to be applied. Each element of V\\ncorresponds to one and only one element of K, and the two can\\nbe seen as different representations of the same data. Indeed,many architectures, including RNNsearch, do not distinguish\\nbetween KandV. The distinction between keys and values\\nwas introduced by Daniluk et al. [61], who use different repre-\\nsentations of the input for computing the attention distribution\\nand the contextual information.\\nVandaare thus combined to obtain a new set Zof\\nweighted representations of V[see (8)], which are then\\nmerged together so as to produce a compact representationofZusually called the context vector c[see (9)].\\n2The\\n2Although most authors use this terminology, we shall remark that Yang\\net al. [52], Wang et al. [129], and other authors use d the term context vector'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='et al. [52], Wang et al. [129], and other authors use d the term context vector\\nto refer to other elements of the attention architecture.GALASSI et al. : ATTENTION IN NLP 4295\\nFig. 4. General attention model.\\ncommonest way of obtaining cfrom Zis by summation.\\nHowever, alternatives have been proposed, including gating\\nfunctions [93]. Nevertheless, cwill be mainly determined by\\nvalues associated with higher attention weights\\nzi=aivi (8)\\nc=dk/summationdisplay\\ni=1zi. (9)\\nWhat we described so far was a synthesis of the most\\nfrequent architectural choices made in the design of attentivearchitectures. Other options w ill be explored in Section IV-D.\\nC. Deterministic Versus Probabilistic Attention\\nBefore we proceed, a brief remark about some relevant\\nnaming conventions is in order. The attention model described\\nso far is sometimes described in the literature as a mapping\\nwith a probabilistic interpretation since the use of a softmax\\nnormalization allows one to interpret the attention weights\\nas a probability distribution function (see [91]). Accordingly,some recent literature deﬁnes as deterministic attention mod-\\nels [134], [135] and those models where context words,\\nwhereupon attention is focused, are deterministically selected,for example, by using the constituency parse tree of the\\ninput sentence. However, other authors described the ﬁrst\\nmodel as deterministic (soft) attention, to contrast it with\\nstochastic (hard) attention, where the probability distribution'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='stochastic (hard) attention, where the probability distribution\\nover weights is used to hardly sample a single input as contextvector c, in particular following a reinforcement learning\\nstrategy [16]. If the weights aencode a probability distrib-\\nution, the stochastic attention model differs from our generalmodel for the absence of (8) and (9) that are replaced by the\\nfollowing equations:\\n˜s∼Multinoulli ({a\\ni}) (10)\\nc=dk/summationdisplay\\ni=1˜siviwith˜si∈{0,1}. (11)\\nTo avoid any potential confusion, in the remainder of this arti-\\ncle, we will abstain from characterizing the attention models\\nas deterministic, probabilistic, or stochastic.TABLE III\\nPOSSIBLE USES OF ATTENTION AND EXAMPLES OF RELEV ANT TASK\\nIII. U SES OF ATTENTION\\nAttention enables us to estimate the relevance of the input\\nelements as well as to combin e said elements into a com-\\npact representation—the context vector—that condenses the\\ncharacteristics of the most re levant elements. Because the\\ncontext vector is smaller than the original input, it requires\\nfewer computational resources to be processed at later stages,yielding a computational gain.\\nWe summarize possible uses of attention and the tasks in\\nwhich they are relevant in Table III.\\nFor tasks such as document classiﬁcation, where usually\\nthere is only Kin input and no query, the attention mechanism\\ncan be seen as an instrument to encode the input into a compact\\nform. The computation of such an embedding can be seen as'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='form. The computation of such an embedding can be seen as\\na form of feature selection, and as such, it can be appliedto any set of features sharing the same representation. This\\napplies to cases where features come from different domains\\nas in multimodal tasks [78] or from different levels of a neuralarchitecture [38] or where they simply represent different\\naspects of the input document [136]. Similarly, attention can\\nalso be exploited as an auxiliary task during training so\\nthat speciﬁc features can be modeled via a multitask setting.\\nThis holds for several scenarios, such as visual questionanswering [137], domain classiﬁcation for natural language\\nunderstanding [138], and semantic role labeling [97].4296 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\nWhen the generation of a text sequence is required, as in\\nmachine translation, attention enables us to make use of a\\ndynamic representation of the input sequence, whereby the\\nwhole input does not have to be encoded into a single vector.At each time step, the encoding is tailored according to the\\ntask, and in particular, qrepresents an embedding of the\\nprevious state of the decoder. More generally, the possibility toperform attention with respect to a query qallows us to create\\nrepresentations of the input that depend on the task context ,\\ncreating specialized embeddings. This is particularly useful in\\ntasks, such as sentiment analysis and information extraction.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='tasks, such as sentiment analysis and information extraction.\\nSince attention can create contextual representations of an\\nelement, it can also be used to build sequence-to-sequence\\nannotators, without resorting to RNNs or convolutional neural\\nnetworks (CNNs), as suggested by Vaswani et al. [36],\\nwho rely on an attention mechanism to obtain a whole\\nencoder/decoder architecture.\\nAttention can also be used as a tool for selecting speciﬁc\\nwords. This could be the case, for example, in dependence\\nparsing [97] and in cloze question-answering tasks [66], [70].In the former case, attention can be applied to a sentence in\\norder to predict dependences. In the latter, attention can be\\napplied to a textual document or to a vocabulary to perform aclassiﬁcation among the words.\\nFinally, attention can come in handy when multiple inter-\\nacting input sequences have to be considered in combination.\\nIn tasks such as question answering, where the input consists\\nof two textual sequences—for instance, the question andthe document or the question and the possible answers—an\\ninput encoding can be obtained by considering the mutual\\ninteractions between the elements of such sequences, ratherthan by applying a more rigid ap r i o r i deﬁned model.\\nIV . T\\nAXONOMY FOR ATTENTION MODELS\\nAttention models can be described on the basis of the\\nfollowing orthogonal dimensions: the nature of inputs (see\\nSection IV-A), the compatibility function (see Section IV-B),'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='Section IV-A), the compatibility function (see Section IV-B),\\nthe distribution function (see Section IV-C), and the number\\nof distinct inputs/outputs, which we refer to as “multiplic-ity” (see Section IV-D). Moreover, attention modules can\\nthemselves be used inside larger attention models to obtain\\ncomplex architectures such as hierarchical-input models (seeSection IV-A2) or in some mu ltiple-input coattention models\\n(see Section IV-D2).\\nA. Input Representation\\nIn NLP-related tasks, generally, KandVare representations\\nof parts of documents, such as sequences of characters, words,or sentences. These components are usually embedded into\\ncontinuous vector representations and then processed through\\nkey/value annotation functions (called kaf and vaf in Fig. 4),\\nso as to obtain a hidden representation resulting in KandV.\\nTypical annotation functions are RNN layers such as gatedrecurrent units (GRUs), long short-term memory networks\\n(LSTMs), and CNNs. In this way, k\\niandvirepresent an input\\nelement relative to its local cont ext. If the layers in charge ofannotation are trained together with the attention model, they\\ncan learn to encode information useful to the attention model.\\nAlternatively, ki/vican be taken to represent each input\\nelement in isolation, rather than in context. For instance, theycould be a one-hot encoding of words or characters or a\\npretrained word embedding. This results in an application of'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='pretrained word embedding. This results in an application of\\nthe attention mechanism directly to the raw inputs, whichis a model known as inner attention [68]. Such a model\\nhas proven to be effective by several authors, who have\\nexploited it in different fashions [36], [41], [54], [116]. The\\nresulting architecture has a smaller number of layers and\\nhyperparameters, which reduces the computational resourcesneeded for training.\\nKcan also represent a single element of the input sequence.\\nThis is the case, for example, in the work by Bapna et al. [38],\\nwhose attention architecture operates on different encodings\\nof the same element, obtained by the subsequent application\\nof RNN layers. The context embeddings obtained for all\\ncomponents individually can then be concatenated, producing\\na new representation of the document that encodes the mostrelevant representation of each component for the given task.\\nIt would also be possible to aggregate each key or value with\\nits neighbors, by computing their average or sum [128].\\nWe have so far considered the input to be a sequence of\\ncharacters, words, or sentences, which is usually the case.\\nHowever, the input can also be other things, such as a\\njuxtaposition of features or relevant aspects of the same textual\\nelement. For instance, Li et al. [56] and Zadeh et al. [78]\\nconsidered the inputs composed of different sources, and in\\n[136] and [139], the input represents different aspects of the'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='[136] and [139], the input represents different aspects of the\\nsame document. In that case, embeddings of the input can becollated together and fed into an attention model as multiple\\nkeys, as long as the embeddings share the same representation.\\nThis allows us to highlight the most relevant elements of the\\ninputs and operate a feature selection, leading to a possible\\nreduction of the dimensionality of the representation via thecontext embedding cproduced by the attention mechanism.\\nInterestingly, Li et al. [120] proposed a model in which the\\ntextual input sequence is mixed with the output of the attentionmodel. Their truncated history-attention model iterates the\\ncomputation of attention on t op of a bi-LSTM. At each step,\\nthe bi-LSTM hidden states are used as keys and values, while\\nthe context vectors computed in the previous steps act as a\\nquery.\\nWe shall now explain in more detail two successful struc-\\ntures, which have become well-established building blocks\\nof neural approaches for NLP, namely, self-attention andhierarchical-input architectures.\\n1) Self-Attention: We made a distinction between two input\\nsources: the input sequence, represented by KandV,a n d\\nthe query, represented by q. However, some architectures\\ncompute attention only based on the input sequence. These\\narchitectures are known as self-a ttentive or intraattentive mod-\\nels. We shall remark, however, that these terms are used'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='els. We shall remark, however, that these terms are used\\nto indicate many different approaches. The commonest oneamounts to the application of m ultiple steps o f attention to\\na vector K, using the elements k\\ntof the same vector as\\nquery at each step [18], [36]. At each step, the weights at\\niGALASSI et al. : ATTENTION IN NLP 4297\\nFig. 5. Example of use of attentio n in a sequence-to-sequence model.\\nrepresent the relevance of kiwith respect to kt, yielding\\ndKseparate context embeddings, ct, one per key. Attention\\ncould thus be used as a sequence-to-sequence model, as an\\nalternative to CNNs or RNNs (see Fig. 5). In this way, each\\nelement of the new sequence may be inﬂuenced by elements of\\nthe whole input, incorporating contextual information withoutany locality bounda ries. This is especially interesting since\\nit could overcome a well-known shortcoming of RNNs: their\\nlimited ability of modeling long -range dependences [140]. For\\neach element k\\nt, the resulting distribution of the weights at\\nshould give more emphasis to words that strongly relate to\\nkt. The analysis of these distributions will, therefore, provide\\ninformation regarding the relationship between the elements\\ninside the sequence. Modern text-sequence generation systemsoften rely on this approach. Another possibility is to construct\\na single query element qfrom the keys through a pooling\\noperation. Furthermore, the same input sequence could beused both as keys Kand query Q, applying a technique'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='we will describe in Section IV-D2, known as coattention.\\nOther self-attentive approaches, such as [52] and [100], are\\ncharacterized by the complete absence of any query term\\nq, which results in simpliﬁed compatibility functions (see\\nSection IV-B).\\n2) Hierarchical-Input Architectures: In some tasks, portions\\nof input data can be meaningfully grouped together into higher\\nlevel structures, where hierarchical-input attention models can\\nbe exploited to subsequently a pply multiple attention modules\\nat different levels of the composition, as shown in Fig. 6.\\nConsider, for instance, data naturally associated with a\\ntwo-level semantic structure, such as characters (the “micro”\\nelements) forming words (th e “macro” elements) or words\\nforming sentences. Attention can be ﬁrst applied to the rep-resentations of micro elements k\\ni,s oa st ob u i l da g g r e g a t e\\nrepresentations kjof the macro elements, such as context\\nvectors. Attention could then be applied again to the sequenceof macroelement embeddings, in order to compute an embed-\\nding for the whole document D. With this model, attention\\nﬁrst highlights the most relevant micro elements within each\\nmacro element and, then, the most relevant macro elements in\\nthe document. For instance, Yang et al. [52] applied attention\\nﬁrst at word level, for each sentence in turn, to compute\\nsentence embeddings. Then, they applied attention again on'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='sentence embeddings. Then, they applied attention again on\\nthe sentence embeddings to obtain a document representation.With reference to the model introduced in Section II, embed-\\ndings are computed for each sentence in D, and then, all\\nsuch embeddings are used together as keys Kto compute the\\ndocument-level weights aand eventually D’s context vector c.\\nThe hierarchy can be extended further. For instance, Wu et al.\\n[141] added another layer on top, applying attention also at\\nthe document level.\\nIf representations for both micro- and macro-level elements\\nare available, one can compute attention on one level and\\nthen exploit the result as a key or query to compute atten-\\ntion on the other, yielding two different microrepresenta-\\ntion/macrorepresentation of D. In this way, attention enables\\nus to identify the most relevant elements for the task at both\\nlevels. The attention-via-attention model by Zhao and Zhang\\n[43] deﬁnes a hierarchy with characters at the micro level andwords at the macro level. Both characters and words act as\\nkeys. Attention is ﬁrst computed on word embeddings K\\nW,\\nthus obtaining a document representation in the form of a\\ncontext vector cW, which in turn acts as a query qto guide the\\napplication of character-level attention to the keys (characterembeddings) K\\nC, yielding a context vector cforD.\\nMaet al. [113] identiﬁed a single “target” macro-object T'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='C, yielding a context vector cforD.\\nMaet al. [113] identiﬁed a single “target” macro-object T\\nas a set of words, which do not necessarily have to form asequence in D, and then used such a macro-object as keys,\\nK\\nT. The context vector cTproduced by a ﬁrst application of\\nthe attention mechanism on KTis then used as query qin\\na second application of the attention mechanism over D, with\\nthe keys being the document’s word embeddings KW.\\nB. Compatibility Functions\\nThe compatibility function is a crucial part of the attention\\narchitecture because it deﬁnes how keys and queries are\\nmatched or combined. In our presentation of compatibility\\nfunctions, we will consider a data model where qandki\\nare monodimensional vectors. For example, if Krepresents\\na document, each kimay be the embedding of a sentence,\\na word, or a character. In such a model, qandkimay have\\nthe same structure and, thus, the same size, although this isnot always necessary. However, in some architectures, qcan\\nconsist of a sequence of vectors or a matrix, a possibility we\\nexplore in Section IV-D2.\\nSome common compatibility functions are listed\\nin Table IV. Two main approaches can be identiﬁed.The ﬁrst one is to match and compare Kand q.F o r\\ninstance, the idea behind the similarity attention proposed by\\nGraves et al. [142] is that the most relevant keys are the\\nmost similar to the query. Accordingly, the authors present a\\nmodel that relies on a similarity function (sim in Table IV) to'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='model that relies on a similarity function (sim in Table IV) to\\ncompute the energy scores. For example, they rely on cosinesimilarity, a choice that suits cases where the query and the\\nkeys share the same semantic representation. A similar idea\\nis followed by the widely used multiplicative or dot attention,\\nwhere the dot product between qandKis computed. The\\ncomplexity of this computation is O(n\\nkdk). A variation of\\nthis model is scaled multiplicative attention, where a scaling\\nfactor is introduced to improve performance with large\\nkeys [36]. General attention, proposed by Luong et al. [29],4298 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\nFig. 6. Hierarchical input attention models deﬁned by Zhao and Zhang [43] (center), Yang et al. [52] (left), and Ma et al. [113] (right). The number inside\\nthe attention shapes indicates the order of application. Di fferent colors highlight different parts of the inputs.\\nTABLE IV\\nSUMMARY OF COMPATIBILITY FUNCTIONS FOUND IN THE LITERATURE .\\nW,W0,W1,...,AND bARELEARNABLE PARAMETERS\\nextends this concept in order to accommodate keys and queries\\nwith different representations. To that end, it introduces a\\nlearnable matrix parameter W. This parameter represents the\\ntransformation matrix that maps the query onto the vectorspace of keys. This transformation increases the complexity of\\nthe operation to O(nqnkdk). In what could be called a biased'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='the operation to O(nqnkdk). In what could be called a biased\\ngeneral attention, Sordoni et al. [67] introduced a learnable\\nbias, so as to consider some keys as relevant independently\\nof the input. Activated general attention [111] employs a\\nnonlinear activation function. In Table IV, actis a placeholder\\nfor a nonlinear activation function, such as hyperbolic tangent,\\ntanh, rectiﬁer linear unit, ReLU [143], or scaled exponentiallinear unit, SELU [144]. These approaches are particularly\\nsuitable in tasks where the concept of relevance of a key is\\nknown to be closely related to that of similarity to a queryelement. These include, for instance, tasks where speciﬁc\\nkeywords can be used as a query, such as abusive speech\\nrecognition and sentiment analysis.\\nA different approach amounts to combining rather than\\ncomparing Kandq, using them together to compute a joint\\nrepresentation, which is then multiplied by an importance\\nvector\\n3wimp, which has to adhere to the same semantic\\nof the new representation. Such a vector deﬁnes, in a way,relevance and could be an additional query element, as offered\\nby Ma et al. [113] or a learnable parameter. In that case,\\nwe speculate that the analysis of a machine-learned importance\\nvector could provide additional information on the model. One\\nof the simplest models that follows this approach is the concatattention by Luong et al. [29], where a joint representation\\nis given by juxtaposing keys and queries. Additive attention'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='is given by juxtaposing keys and queries. Additive attention\\nworks similarly, except that the contribution of qandKcan\\nbe computed separately. For example, Bahdanau et al. [2]\\nprecomputed the contribution of Kin order to reduce the\\ncomputational footprint. The complexity of the computation,\\nignoring the application of the nonlinear function, is thus\\nO(n\\nwnkdk),w h e r e nwindicates the size of wimp.M o r e o v e r ,\\nadditive attention in princ iple could accommodate queries\\nof different size. In additive attention and concat attention,\\nthe keys and the queries are fed into a single neural layer.We speak instead of deep attention if multiple layers are\\n3Part of our terminology. As previously noted, wimp is termed context\\nvector by Yang et al. [52] and other authors.GALASSI et al. : ATTENTION IN NLP 4299\\nemployed [54]. Table IV shows a deep attention function with\\nLlevels of depth, 1 <l<L. With an equal number of\\nneurons for all levels and reasonably assuming Lto be much\\nsmaller than any other paramet er, the complexity becomes\\nO(nL\\nwnkdk). These approaches are especially suitable when\\na representation of “relevant” elements is unavailable or it is\\navailable but encoded in a signiﬁcantly different way from theway that keys are encoded. This may be the case, for instance,\\nwith tasks such as document classiﬁcation and summarization.\\nA similar rationale lead to convolution-based atten-\\ntion [119]. It draws inspiration from biological models,'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='tion [119]. It draws inspiration from biological models,\\nwhereby biological attention is produced through search tem-plates and pattern matchi ng. Here, the learned vector w\\nimp\\nrepresents a convolutional ﬁlter, which embodies a speciﬁc\\nrelevance template. The ﬁlter i s used in a pattern-matching\\nprocess obtained by applying the convolution operation on key\\nsubsequences. Applying multiple convolutions with different\\nﬁlters enables a contextual selection of keys that match speciﬁc\\nrelevance templates, obtaining a speciﬁc energy score for\\neach ﬁlter. Since each key belongs to multiple subsequences,each key yields multiple energy scores. Such scores are\\nthen aggregated in order to obtain a single value per key.\\nAggregation could be achieved by sum or average. Table IVillustrates convolution-based attention for a single ﬁlter of\\nsize l. The complexity of such an operation is O(l\\n2nkdk).\\nThese approaches are especially suitable when a representation\\nof “relevant” elements is unavailable or it is available but\\nencoded in a signiﬁcantly different way from the way thatkeys are encoded. This may be the case, for instance, with\\ntasks such as document classiﬁcation and summarization.\\nFinally, in some models, the attention distribution ignores\\nKand only depends on q. We then speak of location-\\nbased attention. The energy associated with each key is thus\\ncomputed as a function of the key’s position, independently of\\nits content [29]. Conversely, self-attention may be computed'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='its content [29]. Conversely, self-attention may be computed\\nonly based on K, without any q. The compatibility functions\\nfor self-attention, which are a special case of the more general\\nfunctions, are omitted from Table IV.\\nFor an empirical comparison between some compatibility\\nfunctions (namely, additive and multiplicative attention) in the\\ndomain of machine translation, we suggest the reader to refer\\nto Britz et al. [37].\\nC. Distribution Functions\\nAttention distribution maps energy scores to attention\\nweights. The choice of the distribution function depends onthe properties that the distribution is required to have—for\\ninstance, whether it is required to be a probability distribution,\\na set of probability scores, or a set of Boolean scores—on theneed to enforce sparsity, and on the need to account for the\\nkeys’ positions.\\nOne possible distribution function gis the logistic sigmoid,\\nas proposed by Kim et al. [47]. In this way, each weight a\\ni\\nis constrained between 0 and 1, thus ensuring that the values\\nViand their corresponding weighted counterparts Zishare\\nthe same boundaries. Such weights can thus be interpreted as\\nprobabilities that an element is relevant. The same range canalso be forced on the context vector’s elements ci,b yu s i n g\\na softmax function, as it is commonly done. In that case, the\\nattention mechanism is called soft attention. Each attention\\nweight can be interpreted as the probability that the corre-sponding element is the most relevant.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='weight can be interpreted as the probability that the corre-sponding element is the most relevant.\\nWith sigmoid or softmax alike, all the key/value elements\\nhave a relevance, small as it may be. Yet, it can be arguedthat in some cases, some parts of the input are completely\\nirrelevant, and if they were to be considered, they would likely\\nintroduce noise rather than contribute with useful information.\\nIn such cases, one could exploit attention distributions that\\naltogether ignore some of the keys, thereby reducing thecomputational footprint. One option is the sparsemax dis-\\ntribution [91], which truncates to zero the scores under a\\ncertain threshold by exploiting the geometric properties ofthe probability simplex. This approach could be especially\\nuseful in those settings where a large number of elements\\nare irrelevant, such as in document summarization or cloze\\nquestion-answering tasks.\\nIt is also possible to model structural dependences between\\nthe outputs. For example, structured attention networks [47]\\nexploit neural conditional random ﬁelds to model the (con-\\nditional) attention distribution. The attention weights are thuscomputed as (normalized) marginals from the energy scores,\\nwhich are treated as potentials.\\nIn some tasks, such as machine translation or image cap-\\ntioning, the relevant features are found in a neighborhood of\\na certain position. In those cases, it could be helpful to focusthe attention only on a speciﬁc portion of the input. If the'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='position is known in advance, one can apply a positional mask,\\nby adding or subtracting a given value from the energy scoresbefore the application of the softmax [93]. Since the location\\nmay not be known in advance, the hard attention model by\\nXuet al. [16] considers the keys in a dynamically determined\\nlocation. Such a solution is les s expensive at inference time,\\nbut it is not differentiable. For that reason, it requires moreadvanced training techniques, such as reinforcement learning\\nor variance reduction. Local attention [29] extends this idea\\nwhile preserving differentiability. Guided by the intuition thatin machine translation at each time step, only a small segment\\nof the input can be considered relevant, and local attention\\nconsiders only a small window of the keys at a time. The\\nwindow has a ﬁxed size, and the attention can be better\\nfocused on a precise location by combining the softmaxdistribution with a Gaussian distribution. The mean of the\\nGaussian distribution is dynamic, whereas its variance can\\neither be ﬁxed, as done by Luong et al. [29] or dynamic,\\nas done by Yang et al. [39]. Selective attention [17] follows\\nthe same idea; using a grid of Gaussian ﬁlters, it considers\\nonly a patch of the keys, whose position, size, and resolutiondepend on dynamic parameters.\\nShen et al. [94] combined soft and hard attention, by apply-\\ning the former only on the elements ﬁltered by the latter.\\nMore precisely, softmax is only applied among a subset of'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='More precisely, softmax is only applied among a subset of\\nselected energy scores, whereas for the others, the weight isset to zero. The subset is determined according to a set of\\nrandom variables, with each variable corresponding to a key.\\nThe probability associated with each variable is determined4300 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\nthrough soft attention applied to the same set of keys. The\\nproper “softness” of the distribution could depend not only\\non the task but also on the query. Lin et al. [44] deﬁned a\\nmodel whose distribution is controlled by a learnable, adaptivetemperature parameter. When a “softer” attention is required,\\nthe temperature increases, producing a smoother distribution\\nof weights. The opposite happens when a “harder” attentionis needed.\\nFinally, the concept of locality can also be deﬁned according\\nto semantic rules, rather than the temporal position. This\\npossibility will be further discussed in Section V.\\nD. Multiplicity\\nWe shall now present variations of the general uniﬁed model\\nwhere the attention mechanis m is extended to accommodate\\nmultiple, possibly heter ogeneous, input s or outputs.\\n1) Multiple Outputs: Some applications suggest that the\\ndata could, and should, be interpreted in multiple ways.\\nThis can be the case when there is ambiguity in the data,stemming, for example, from words having multiple meanings\\nor when addressing a multitask problem. For this reason,'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='or when addressing a multitask problem. For this reason,\\nmodels have been deﬁned that jointly compute not only onebut multiple attention distributions over the same data. One\\npossibility presented by Lin et al. [100] and also exploited\\nby Du et al. [145] is to use additive attention (seen in\\nSection IV-B) with an importance matrix, instead of a vector,\\nW\\nimp∈Rnk×no, yielding an energy scores matrix where\\nmultiple scores are associated with each key. Such scores\\ncan be regarded as different models of relevance for the\\nsame values and can be used to create a context matrix\\nC∈Rnv×no. Such embeddings can be c oncatenated together,\\ncreating a richer and more expressive representation of the\\nvalues. Regularization penalties can be applied so as to enforce\\nthe differentiation between models of relevance (for example,\\nthe Frobenius norm). In multidimensional attention [93],where the importance matrix is a square matrix, attention\\ncan be computed featurewise. To that end, each weight a\\ni,j\\nis paired with the jth feature of the ith value vi,j,a n da\\nfeaturewise product yields the new value zi. Convolution-\\nbased attention [119] always produces multiple energy scores\\ndistributions according to the number of convolutional ﬁlters\\nand the size of those ﬁlters. Another possibility explored\\nby Vaswani et al. [36] is multihead attention. There, multiple\\nlinear projections of all the inputs ( K,V,a n d q) are performed\\naccording to learnable parameters, and multiple attention func-'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='according to learnable parameters, and multiple attention func-\\ntions are computed in parallel. Usually, the processed contextvectors are then merged together into a single embedding.\\nA suitable regularization term is sometimes imposed so as to\\nguarantee sufﬁcient dissimilarity between attention elements.Liet al. [34] proposed three possib ilities: regularization on the\\nsubspaces (the linear projections of V), the attended positions\\n(the sets of weights), or on the outputs (the context vectors).\\nMultihead attention can be especially helpful when combined\\nwith nonsoft attention distribution since different heads cancapture local and global cont exts at the same time [39].\\nFinally, labelwise attention [126] computes a separate attention\\ndistribution for each class. This may improve the performanceas well as lead to a better interpretation of the data because it\\ncould help isolate data points that better describe each class.\\nThese techniques are not mutually exclusive. For example,\\nmultihead and multidimensional attention can be combinedwith one another [95].\\n2) Multiple Inputs: Coattention: Some architectures con-\\nsider the query to be a sequence of d\\nqmultidimensional\\nelements, represented by a matrix Q∈Rnq×dq, rather than\\nby a plain vector. Examples of this setup are common in\\narchitectures designed for tasks where the query is a sentence,\\nas in question answering, or a set of keywords, as in abusive'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='as in question answering, or a set of keywords, as in abusive\\nspeech detection. In those cases, it could be useful to ﬁndthe most relevant query elem ents according to the task and\\nthe keys. A straightforward way of doing that would be to\\napply the attention mechanism to the query elements, thustreating Qas keys and each k\\nias query, yielding two inde-\\npendent representations for KandQ. However, in that way,\\nwe would miss the information contained in the interactions\\nbetween elements of KandQ. Alternatively, one could apply\\nattention jointly on Kand Q, which become the “inputs”\\nof a coattention architecture [80]. Coattention models can be\\ncoarse-grained or ﬁne-grained [112]. Coarse-grained models\\ncompute attention on each input, using an embedding of theother input as a query. Fine-grained models consider each\\nelement of an input with respect to each element of the other\\ninput. Furthermore, coattention can be performed sequentially\\nor in parallel. In parallel models, the procedures to compute\\nattention on KandQsymmetric, and thus, the two inputs are\\ntreated identically.\\na) Coarse-grained coattention: Coarse-grained models\\nuse a compact representation of one input to compute attentionon the other. In such models, the role of the inputs as\\nkeys and queries is no longer focal, and thus, a compact\\nrepresentation of Kmay play as a query in parts of the\\narchitecture and vice versa. A sequential coarse-grained model\\nproposed by Lu et al. [80] is alternating coattention, as shown'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='proposed by Lu et al. [80] is alternating coattention, as shown\\nin Fig. 7 (left), whereby attention is computed three times\\nto obtain embeddings for Kand Q. First, self-attention is\\ncomputed on Q. The resulting context vector is then used\\nas a query to perform attention on K. The result is another\\ncontext vector C\\nK, which is further used as a query as attention\\nis again applied to Q. This produces a ﬁnal context vector,\\nCQ. The architecture proposed by Sordoni et al. [67] can\\nalso be described using this model with a few adaptations.In particular, Sordoni et al. [67] omitted the last step and\\nfactor in an additional query element qin the ﬁrst two attention\\nsteps. An almost identical seque ntial architecture is used by\\nZhang et al. [86], who use qonly in the ﬁrst attention step.\\nA parallel coarse-grained model is shown in Fig. 7 (right).\\nIn such a model, proposed by Ma et al. [111], an average ( avg)\\nis initially computed on each input and then used as a query\\nin the application of attention to generate the embedding of\\nthe other input. Sequential coattention offers a more elaborate\\ncomputation of the ﬁnal results, potentially allowing to discard\\nall the irrelevant elements of Qand K, at the cost of a\\ngreater computational footprint. Parallel coattention can be\\noptimized for better performan ce, at the expense of a “simpler”\\nelaboration of the outputs. It is worthwhile noticing that theGALASSI et al. : ATTENTION IN NLP 4301'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='averaging step in Ma et al. [111]’s model could be replaced\\nby self-attention, in order to ﬁlter out irrelevant elements from\\nQand Kat an early stage.\\nb) Fine-grained coattention: In ﬁne-grained coattention\\nmodels, the relevance (energy scores) associated with each\\nkey/query element pair \\x04ki/qj\\x05is represented by the elements\\nEj,iof a coattention matrix E∈Rdq×dkcomputed by a\\ncocompatibility function.\\nCocompatibility functions can be straightforward adapta-\\ntions of any of the compatibility functions listed in Table IV.\\nAlternatively, new functions can be deﬁned. For example,Fan et al. [112] deﬁned cocompatib ility as a linear transfor-\\nmation of the concatenation bet ween the elements and their\\nproduct [see (12)]. In decomposable attention [89], the inputsare fed into neural networks, whose outputs are then multiplied\\n[see (13)]. Delaying the product to after the processing by\\nthe neural networks reduces the number of inputs of such\\nnetworks, yielding a reduction in the computational footprint.\\nAn alternative proposed by Tay et al. [75] exploits the Her-\\nmitian inner product. The elements of Kand Qare ﬁrst\\nprojected to a complex domain, then, the Hermitian product\\nbetween them is computed, and ﬁnally, only the real part of theresult is kept. Being the Hermitian product noncommutative,\\nEwill depend on the roles played by the inputs as keys and\\nqueries\\nE\\nj,i=W([ki;qj;kiqj]) (12)\\nEj,i=(act(W1Q+b1))/intercal(act(W2K+b2)). (13)\\nBecause Ej,irepresent energy scores associated with'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='Ej,i=(act(W1Q+b1))/intercal(act(W2K+b2)). (13)\\nBecause Ej,irepresent energy scores associated with\\n\\x04ki/qj\\x05pairs, computing the relevance of Kwith respect to\\nspeciﬁc query elements, or, similarly, the relevance of Qwith\\nrespect to speciﬁc key elements, requires extracting informa-\\ntion from Eusing what we call an aggregation function. The\\noutput of such a function is a pair aK/aQof weight vectors.\\nThe commonest aggregation functions are listed in Table V.\\nA simple idea is the attention pooling parallel model adopted\\nby dos Santos et al. [69]. It amounts to considering the highest\\nscore in each row or column of E. By attention pooling, a key\\nkiwill be attributed a high attention weight if and only if it\\nhas a high coattention score with respect to at least one query\\nelement qj. Key attention scores are obtained through rowwise\\nmax pooling, whereas query attention scores are obtainedthrough columnwise max pooling, as shown in Fig. 8 (left).\\nOnly considering the highest attention scores may be regarded\\nas a conservative approach. Indeed, low-energy scores canonly be obtained by keys whose coattention scores are all\\nlow and thus likely to be irrelevant to all query elements.\\nFurthermore, the keys that are only relevant to a single queryelement may obtain the same or even higher energy score\\nthan keys that are relevant t o all the query elements. The\\nsame reasoning applies to query attention scores. This is a\\nsuitable approach, for example, in tasks where the queries are'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='suitable approach, for example, in tasks where the queries are\\nkeywords in disjunction, such as in abusive speech recognition.Conversely, the approaches follow to compute the energy score\\nof an element by accounting for al l the related attention scores,\\nat the cost of a heavier computational footprint.TABLE V\\nAGGREGATION FUNCTIONS .INMOSTCASES ,aKAND aQAREOBTAINED\\nBYAPPLYING A DISTRIBUTION FUNCTION ,SUCH AS THOSE SEEN IN\\nSECTION IV-C, TOeKAND eQ,AND ARETHUS OMITTED FROM\\nTHISTABLE IN THE INTEREST OF BREVITY .ASCUSTOMARY ,\\nActISAPLACEHOLDER FOR A GENERIC NONLINEAR\\nACTIV A TION FUNCTION ,WHEREAS Dist INDICATES\\nADISTRIBUTION FUNCTION SUCH AS SOFTMAX\\nLuet al. [80] used a multilayer perceptron in order to learn\\nthe mappings from EtoeKandeQ. In [127], the computation\\nis even simpler since the ﬁnal energy scores are a lineartransformation of E.C u i et al. [70] instead applied the nested\\nmodel shown in Fig. 8 (right). First, two matrices M\\n1and\\nM2are computed by separately applying a rowwise and a\\ncolumnwise softmax on E. The idea is that each row of\\nM1represents the attention distribution over the document\\naccording to a speciﬁc query elem ent—and it could already be\\nused as such. Then, a rowwise average over M2is computed so\\nas to produce an attention distribution aQover query elements.\\nFinally, a weighted sum of M1according to the relevance of\\nquery elements is computed through the dot product between\\nM1andaQ, obtaining the document’s attention distribution'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='M1andaQ, obtaining the document’s attention distribution\\nover the keys, aK. An alternative nested attention model is\\nproposed by Nie et al. [88], whereby M1andM2are fed to\\na multilayer perceptron, as is done by Lu et al. [80]. Further\\nimprovements may be obtained by combining the results of\\nmultiple coattention models. Fan et al. [112], for instance,\\ncomputed coarse-grained and ﬁne -grained attention in parallel\\nand combined the results into a single embedding.\\nV. C OMBINING ATTENTION AND KNOWLEDGE\\nAccording to LeCun et al. [146], a major open challenge\\nin artiﬁcial intelligence (AI) is combining connectionist (or\\nsubsymbolic) models, such as deep networks, with approaches\\nbased on symbolic knowledge representation, in order to4302 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\nFig. 7. Coarse-grained coattention by Lu et al. [80] (left) and Ma et al. [111] (right). The number inside the atte ntion shapes (and besides the average\\noperator) indicates the order in which they are app lied. Different colors highlight different inputs.\\nFig. 8. Fine-grained coattentio n models presented by dos Santos et al. [69] (left) and by Cui et al. [70] (right). Dashed lines show how max-pooling/distribution\\nfunctions are applied (columnwise or rowwise).\\nperform complex reasoning tasks. Throughout the last decade,\\nﬁlling the gap between these two families of AI methodologieshas represented a major research avenue. Popular approaches'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='include statistical relational learning [147], neural-symbolic\\nlearning [148], and the application of various deep learningarchitectures [149], such as memory networks [59], neural\\nTuring machines [142], and several others.\\nFrom this perspective, attention can be seen both as an\\nattempt to improve the interpretability of neural networks\\nand as an opportunity to plug external knowledge into them.As a matter of fact, since the weights assigned by attention\\nrepresent the relevance of the input with respect to the given\\ntask, in some contexts, it could be possible to exploit thisinformation to isolate the most signiﬁcant features that allow\\nthe deep network to make its predictions. On the other\\nhand, any prior knowledge regarding the data, the domain,\\nor the speciﬁc task, whenever available, could be exploited to\\ngenerate information about the desired attention distribution,which could be encoded within the neural architecture.\\nIn this section, we overview different techniques that can\\nbe used to inject this kind of knowledge in a neural network.We leave to Section VI further discussions on the open chal-\\nlenges regarding the combination of knowledge and attention.\\nA. Supervised Attention\\nIn most of the works we surveyed, the attention model is\\ntrained with the rest of the neural architecture to perform aspeciﬁc task. Although trained alongside a supervised proce-\\ndure, the attention model per se is trained in an unsupervisedfashion'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='dure, the attention model per se is trained in an unsupervisedfashion\\n4to select useful information for the rest of the\\narchitecture. Nevertheless, in some cases, knowledge about the\\ndesired weight distribution could be available. Whether it ispresent in the data as a label or it is obtained as additional\\ninformation through external tools, it can be exploited to\\nperform a supervised training of the attention model.\\n1) Preliminary Training: One possibility is to use an exter-\\nnal classiﬁer. The weights learned by such a classiﬁer are\\nsubsequently plugged into the attention model of a different\\narchitecture. We name this procedure as preliminary training.For example, Zhang et al. [53] ﬁrst trained an attention\\nmodel to represent the probability that a sentence contains\\nrelevant information. The relevance of a sentence is givenby rationales [150], which are snippets of text that sup-\\nports the corresponding document categorizations. In work by\\nLong et al. [118], a model is preliminarily trained on\\neye-tracking data sets to estim ate the reading time of words.\\nThen, the reading time predicted by the model is used as anenergy score in a neural model for sentiment analysis.\\n2) Auxiliary Training: Another possibility is to train the\\nattention model without preliminary training, but by treating\\nattention learning as an auxiliary task that is performed jointly\\nwith the main task. This procedure has led to good results'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='with the main task. This procedure has led to good results\\n4Meaning that there is no target distribution for the attention model.GALASSI et al. : ATTENTION IN NLP 4303\\nin many scenarios, including machine translation [30], [35],\\nvisual question answering [137], and domain classiﬁcation for\\nnatural language understanding [138].\\nIn some cases, this mechanism can also be exploited to\\nhave attention model-speciﬁc features. For example, since the\\nlinguistic information is useful for semantic role labeling,\\nattention can be trained in a multitask setting to represent thesyntactic structure of a sentence. Indeed, in LISA [97], a mul-\\ntilayer multiheaded architecture for semantic role labeling, one\\nof the attention heads is trained to perform dependence parsing\\nas an auxiliary task.\\n3) Transfer Learning: Furthermore, it is possible to perform\\ntransfer learning across different domains [1] or tasks [115].\\nBy performing a preliminary training of an attentive architec-\\nture on a source domain to perform a source task, a mapping\\nbetween the inputs and the distribution of weights will belearned. Then, when another attentive architecture is trained\\non the target domain to perform the target task, the pretrained\\nmodel can be exploited. Indeed, the desired distribution can beobtained through the ﬁrst architecture. Attention learning can,\\ntherefore, be treated as an auxiliary task as in the previously\\nmentioned cases. The difference is that the distribution of the'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='mentioned cases. The difference is that the distribution of the\\npretrained model is used as ground truth, instead of using data\\nlabels.\\nB. Attention Tracking\\nWhen attention is applied multiple times on the same\\ndata, as in sequence-to-sequence models, a useful piece of\\ninformation could be how much relevance has been given to\\nthe input along different model iterations. Indeed, one mayneed to keep track of the weights that the attention model\\nassigns to each input. For example, in machine translation, it is\\ndesirable to ensure that all the words of the original phrase\\nare considered. One possibility to maintain this information\\nis to use a suitable structure and provide it as an additionalinput to the attention model. Tu et al. [33] exploited a piece\\nof symbolic information called coverage to keep track of the\\nweight associated with the inputs. Every time attention iscomputed, such information is fed to the attention model as\\na query element, and it is updated according to the output of\\nthe attention itself. In [31], the representation is enhanced by\\nmaking use of a subsymbolic representation for the coverage.\\nC. Modeling the Distribution Function by\\nExploiting Prior Knowledge\\nAnother component of the attention model where prior\\nknowledge can be exploited is the distribution function. For\\nexample, constraints can be applied to the computation of the\\nnew weights to enforce the boundaries on the weights assignedto the inputs. In [46] and [51], the coverage information is'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='exploited by a constrained distribution function, regulating the\\namount of attention that the same word receives over time.\\nPrior knowledge could also be exploited to deﬁne or to\\ninfer a distance between the elements in the domain. Suchdomain-speciﬁc distance could then be considered in any\\nposition-based distribution function, instead of the positional\\ndistance. An example of distance could be derived by thesyntactical information. Chen et al. [40] and He et al. [109]\\nused distribution functions that consider the distance between\\ntwo words along the dependence graph of a sentence.\\nVI. C\\nHALLENGES AND FUTURE DIRECTIONS\\nIn this section, we discuss open challenges and possible\\napplications of the attention mechanism in the analysis of\\nneural networks, as a support of the training process and as an\\nenabling tool for the integration of symbolic representationswithin neural architectures.\\nA. Attention for Deep Networks Investigation\\nWhether attention may or may not be considered as a\\nmean to explain neural networks is currently an open debate.Some recent studies [10], [11] suggest that attention cannot be\\nconsidered a reliable mean to explain or even interpret neural\\nnetworks. Nonetheless, other works [6]–[9] advocate the use\\nof attention weights as an analytic tool. Speciﬁcally, Jain\\nand Wallace [10] proved that attention is not consistent withother explainability metrics and that it is easy to create local\\nadversarial distributions (distributions that are similar to the'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='adversarial distributions (distributions that are similar to the\\ntrained model but produce a different outcome). Wiegreffe andPinter [9] pushed the discussion further, providing experiments\\nthat demonstrate that creating an effective global adversarial\\nattention model is much more difﬁcult than creating a local one\\nand that attention weights may contain information regarding\\nfeature importance. Their conclusion is that attention mayindeed provide an explanation of a model, if by explanation,\\nwe mean a plausible, but not necessarily faithful, recon-\\nstruction of the decision-making process, as suggested byRudin [151] and Riedl [152].\\nIn the context of a multilayer neural architecture, it is fair to\\nassume that the deepest levels w ill compute the most abstract\\nfeatures [146], [153]. Therefore, the application of attention\\nto deep networks could enable the selection of higher levelfeatures, thus providing hints to understand which complex\\nfeatures are relevant for a giv en task. Following this line of\\ninquiry in the computer vision domain, Zhang et al. [18]\\nshowed that the application of attention to middle-to-high level\\nfeature sets leads to better pe rformance in image generation.\\nThe visualization of the self-attention weights has revealed that\\nhigher weights are not attributed to proximate image regions,\\nbut rather to those regions whose color or texture is mostsimilar to that of the query image point. Moreover, the spatial'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='distribution does not follow a speciﬁc pattern, but instead,\\nit changes, modeling a region that corresponds to the objectdepicted in the image. Identifying abstract features in an input\\ntext might be less immediate than in an image, where the\\nanalysis process is greatly aided by visual intuition. Yet, it maybe interesting to test the effects of the application of attention\\nat different levels and to assess whether its weights correspond\\nto speciﬁc high-level features. For example, Vaswani et al.\\n[36] analyzed the possible rela tion between attention weights\\nand syntactic predictions, V oita et al. [49] did the same with\\nanaphora resolutions, and Clark et al. [6] investigated the\\ncorrelation with many linguistic features. V oita et al. [50]\\nanalyzed the behavior of the heads of a multihead model,4304 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\ndiscovering that different heads d evelop different behaviors,\\nwhich can be related to speciﬁc position or speciﬁc syntactical\\nelement. Yang et al. [39] seemed to conﬁrm that the deeper\\nlevels of neural architectures capture nonlocal aspects ofthe textual input. They studied the application of locality at\\ndifferent depths of an attentive deep architecture and showed\\nthat its introduction is especially beneﬁcial when it is appliedto the layers that are closer to the inputs. Moreover, when\\nthe application of locality is based on a variable-size window,'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='the application of locality is based on a variable-size window,\\nhigher layers tend to have a broader window.\\nA popular way of investigating whether an architecture\\nhas learned high-level features amounts to using the samearchitecture to perform other tasks, as it happens with transfer\\nlearning. This setting has been adopted outside the context of\\nattention, for example, by Shi et al. [154], who perform syn-\\ntactic predictions by using the hidden representations learned\\nwith machine translation. In a similar way, attention weights\\ncould be used as input features in a different model, so as\\nto assess whether they can select relevant information for a\\ndifferent learning task. This is what happens, for example,in attention distillation, where a student network is trained\\nto penalize the most confusin g features according to a teacher\\nnetwork, producing an efﬁcient and robust model in the task ofmachine reading comprehension [155]. Similarly, in a transfer\\nlearning setting, attentional heterogeneous transfer [156] has\\nbeen exploited in heterolingual text classiﬁcation to selectively\\nﬁlter input features from heterogeneous sources.\\nB. Attention for Outlier Detection and Sample Weighing\\nAnother possible use of attention may be for outlier detec-\\ntion. In tasks such as classiﬁcation or the creation of a\\nrepresentative embedding of a speciﬁc class, attention could be'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='representative embedding of a speciﬁc class, attention could be\\napplied over all the samples belonging to that task. In doing so,the samples associated with sm all weights could be regarded\\nas outliers with respect to their class. The same principle could\\nbe potentially applied to each data point in a training set,independently of its class. The computation of a weight for\\neach sample could be interpreted as assessing the relevance\\nof that speciﬁc data point for a speciﬁc task. In principle,\\nassigning such samples a low weight and excluding them\\nfrom the learning could improve a model’s robustness to noisyinput. Moreover, a dynamic computation of these weights\\nduring training would result in a dynamic selection of different\\ntraining data in different training phases. While attention-lessadaptive data selection strategies have already proven to be\\nuseful for efﬁciently obtaining more effective models [117],\\nto the best of our knowledge, no attention-based approach has\\nbeen experimented to date.\\nC. Attention Analysis for Model Evaluation\\nThe impact of attention is greatest when all the irrelevant\\nelements are excluded from the input sequence, and the\\nimportance of the relevant elements is properly balanced.\\nA seemingly uniform distribution of the attention weightscould be interpreted as a sign that the attention model has been\\nunable to identify the more useful elements. This, in turn, may'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='unable to identify the more useful elements. This, in turn, may\\nbe due to the data that do not contain useful information forthe task at hand or it may be as cribed to the poor ability of the\\nmodel to discriminate information. Nevertheless, the attention\\nmodel would be unable to ﬁnd relevant information in the\\nspeciﬁc input sequence, which may lead to errors. The analysisof the distribution of the attention weights may, therefore, be a\\ntool for measuring an architecture’s conﬁdence in performing\\na task on a given input. We speculate that a high entropy inthe distribution or the presence of weights above a certain\\nthreshold may be correlated with a higher probability of\\nsuccess of the neural model. These may, therefore, be used as\\nindicators, to assess the uncertainty of the architecture, as well\\nas to improve its interpretability. Clearly, this informationwould be useful to the user, to better understand the model\\nand the data, but it may also be exploited by more complex\\nsystems. For example, Heo et al. [157] proposed to exploit\\nthe uncertainty of their stochastic predictive model to avoid\\nmaking risky predictions in healthcare tasks.\\nIn the context of an architecture that relies on multiple\\nstrategies to perform its task, such as a hybrid model that relies\\non both symbolic and subsymbolic information, the uncer-tainty of the neural model can be used as a parameter in the\\nmerging strategy. Other contexts in which this information may'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='merging strategy. Other contexts in which this information may\\nbe relevant are multitask learning and reinforcement learning.Examples of exploitation of the uncertainty of the model,\\nalthough in contexts other than attention and NLP, can be\\nfound in works by Poggi and Mattoccia [158], Kendall et al.\\n[159], and Blundell et al. [160].\\nD. Unsupervised Learning With Attention\\nTo properly exploit unsupervised learning is widely recog-\\nnized as one of the most important long-term challenges of\\nAI [146]. As already mentioned in Section V, attention is\\ntypically trained in a supervised architecture, although without\\na direct supervision on the attention weights. Nevertheless,\\na few works have recently attemp ted to exploit attention within\\npurely unsupervised models. We believe this to be a promising\\nresearch direction, as the learning process of humans is indeed\\nlargely unsupervised.\\nFor example, in work by He et al. [161], attention is\\nexploited in a model for aspect extraction in sentiment analy-\\nsis, with the aim to remove words that are irrelevant for\\nthe sentiment and to ensure more coherence of the predicted\\naspects. In work by Zhang and Wu [162], attention is usedwithin autoencoders in a question-retrieval task. The main\\nidea is to generate semantic representations of questions, and\\nself-attention is exploited during the encoding and decoding\\nphases, with the objective to reconstruct the input sequences,\\nas in traditional autoencoders. Following a similar idea, Zhang'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='as in traditional autoencoders. Following a similar idea, Zhang\\net al. [163] exploited bidimensional attention-based recursive\\nautoencoders for bilingual phrase embeddings, whereas Tian\\nand Fang [164] exploited attentive autoencoders to build sen-\\ntence representations and performed topic modeling on short\\ntexts. Yang et al. [165] instead adopted an attention-driven\\napproach to unsupervised sentiment modiﬁcation in order toexplicitly separate sentiment words from content words.\\nIn computer vision, attention alignment has been proposed\\nfor unsupervised domain adaptation, with the aim to align theGALASSI et al. : ATTENTION IN NLP 4305\\nattention patterns of networks trained on the source and target\\ndomain, respectively [166]. We believe that this could be an\\ninteresting scenario also for NLP.\\nE. Neural-Symbolic Learning and Reasoning\\nRecently, attention m echanisms started to be integrated\\nwithin some neural-symbolic models, whose application to\\nNLP scenarios is still at an early stage. For instance, in the\\ncontext of neural logic programming [167], they have been\\nexploited for reasoning over knowledge graphs, in orderto combine parameter and struc ture learning of ﬁrst-order\\nlogic rules. They have also been used in logic attention\\nnetworks [168] to aggregate information coming from graphneighbors with both rule- and network-based attention\\nweights. Moreover, prior knowledge has also been exploited\\nby Shen et al. [169] to enable the attention mechanism to'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='by Shen et al. [169] to enable the attention mechanism to\\nlearn the knowledge representation of entities for ranking\\nquestion–answer pairs.\\nNeural architectures exploiting attention performed well\\nalso in reasoning tasks that are also addressed with symbolic\\napproaches, such as textual en tailment [99]. For instance, Hud-\\nson and Manning [170] recently proposed a new architecture\\nfor complex reasoning problems, with NLP usually being\\none of the target sub-tasks, as in the case of visual question\\nanswering. In such an architect ure, attention is used within\\nseveral parts of the model, for example, over question wordsor to capture long-range dependences with self-attention.\\nAn attempt to introduce constraints in the form of logical\\nstatements within neural networks has been proposed in [171]where rules governing attention are used to enforce word align-\\nment in tasks, such as machine comprehension and natural\\nlanguage inference.\\nVII. C\\nONCLUSION\\nAttention models have become ubiquitous in NLP applica-\\ntions. Attention can be applied to different input parts, different\\nrepresentations of the same data, or different features, to obtain\\na compact representation of the data as well as to highlight\\nthe relevant information. The selection is performed through a\\ndistribution function, which may consider locality in different\\ndimensions, such as space, time, or even semantics. Attention'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='dimensions, such as space, time, or even semantics. Attention\\ncan be used to compare the input data with a query elementbased on measures of similarity or signiﬁcance. It can also\\nautonomously learn what is to be considered relevant, by cre-\\nating a representation encoding what the important data shouldbe similar to. Integrating attention in neural architectures may\\nthus yield a signiﬁcant performance gain. Moreover, attention\\ncan be used as a tool for investigating the behavior of thenetwork.\\nIn this article, we have introduced a taxonomy of attention\\nmodels, which enabled us to systematically chart a vast portion\\nof the approaches in the literature and compare them to one\\nanother. To the best of our knowledge, this is the ﬁrst system-atic, comprehensive taxonomy of attention models for NLP.\\nWe have also discussed the possible role of attention in\\naddressing fundamental AI challenges. In particular, we haveshown how attention can be instrumental in injecting knowl-\\nedge into the neural model, so as to represent speciﬁc features,\\nor to exploit previously acquired knowledge, as in transfer\\nlearning settings. We speculate that this could pave the wayto new challenging research avenues, where attention could be\\nexploited to enforce the combination of subsymbolic models\\nwith symbolic knowledge representations to perform reasoningtasks or to address natural language understanding. Recent\\nresults also suggest that attention could be a key ingredient of'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='results also suggest that attention could be a key ingredient of\\nunsupervised learning architectures, by guiding and focusing\\nthe training process where no supervision is given in advance.\\nR\\nEFERENCES\\n[1] Y . Bao, S. Chang, M. Yu, and R. Barzilay, “Deriving machine attention\\nfrom human rationales,” in Proc. EMNLP , 2018, pp. 1903–1913.\\n[2] D. Bahdanau, K. Cho, and Y . Bengi o, “Neural machine translation by\\njointly learning to align and translate,” in Proc. ICLR , 2015, pp. 1–15.\\n[3] H. Larochelle and G. E. Hinton, “Learning to combine foveal\\nglimpses with a third-order Boltzmann machine,” in Proc. NIPS , 2010,\\npp. 1243–1251.\\n[4] V . Mnih, N. Heess, A. Graves, a nd K. Kavukcuoglu, “Recurrent models\\nof visual attention,” in Proc. NIPS . Red Hook, NY , USA: Curran\\nAssociates, 2014, pp. 2204–2212.\\n[5] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and\\nD. Pedreschi, “A survey of methods for explaining black box models,”\\nACM Comput. Surv. , vol. 51, no. 5, pp. 93:1–93:42, Aug. 2018.\\n[6] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning, “What\\ndoes BERT look at? An analysis of BERT’s attention,” in Black-\\nboxNLP@ACL . Florence, Italy: ACL, Aug. 2019, pp. 276–286.\\n[7] G. Letarte, F. Paradis, P. Giguère , and F. Laviolette, “Importance of\\nself-attention for sentiment analysis,” in Proc. BlackboxNLP@EMNLP ,\\n2018, pp. 267–275.\\n[8] S. Vashishth, S. Upadhyay, G. S. Tomar, and M. Faruqui, “Attention\\ninterpretability across NLP tasks,” 2019, arXiv:1909.11218 . [Online].'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='interpretability across NLP tasks,” 2019, arXiv:1909.11218 . [Online].\\nAvailable: http://arxiv.org/abs/1909.11218\\n[9] S. Wiegreffe and Y . Pinter, “Attention is not not explanation,” in Proc.\\nEMNLP/IJCNLP , 2019, pp. 11–20.\\n[10] S. Jain and B. C. Wallace, “Attention is not explanation,” in Proc.\\nNAACL-HLT , 2019, pp. 3543–3556.\\n[11] S. Serrano and N. A. Smith, “Is attention interpretable?” CoRR ,\\nvol. abs/1906.03731, 2019.\\n[12] S. Liu, T. Li, Z. Li, V . Srikumar, V . Pascucci, and P.-T. Bremer, “Visual\\ninterrogation of attention-based mode ls for natural language inference\\nand machine comprehension,” in Proc. EMNLP Syst. Demonstrations ,\\n2018, pp. 36–41.\\n[13] J. Lee, J.-H. Shin, and J.-S. Kim, “I nteractive visualization and manip-\\nulation of attention-based neural machine translation,” in Proc. EMNLP\\nSyst. Demonstrations , 2017, pp. 121–126.\\n[14] A. Gatt and E. Krahmer, “Survey of the state of the art in natural\\nlanguage generation: Core tasks, a pplications and evaluation,” J. Artif.\\nIntell. Res. , vol. 61, pp. 65–170, Jan. 2018.\\n[15] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural langua ge processing [review article],” IEEE\\nComput. Intell. Mag. , vol. 13, no. 3, pp. 55–75, Aug. 2018.\\n[16] K. Xu et al. , “Show, attend and tell: Neural image caption generation\\nwith visual attention,” in Proc. ICML , vol. 37, 2015, pp. 2048–2057.\\n[17] K. Gregor, I. Danihelka, A. Gra ves, D. Rezende, and D. Wierstra,'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='[17] K. Gregor, I. Danihelka, A. Gra ves, D. Rezende, and D. Wierstra,\\n“Draw: A recurrent neural network for image generation,” in Proc.\\nICML , in Proceedings of Machine Learning Research, vol. 37, 2015,\\npp. 1462–1471.\\n[18] H. Zhang, I. Goodfellow, D. Meta xas, and A. Odena, “Self-attention\\ngenerative adversarial networks,” in Proc. ICML , vol. 97, 2019,\\npp. 7354–7363.\\n[19] J. Chorowski, D. Bahdanau, K. Cho, and Y . Bengio, “End-to-end\\ncontinuous speech recognition using attention-based recurrent NN:\\nFirst results,” CoRR , vol. abs/1412.1602, 2014.\\n[20] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and\\nspell: A neural network for large vocabulary conversational speech\\nrecognition,” in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.\\n(ICASSP) , Mar. 2016, pp. 4960–4964.\\n[21] M. Sperber, J. Niehues, G. Ne ubig, S. Stüker, and A. Waibel,\\n“Self-attentional acoustic models,” in Proc. Interspeech , Sep. 2018,\\npp. 3723–3727.4306 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\n[22] S. Wang, L. Hu, L. Cao, X. Huang, D. Lian, and W. Liu, “Attention-\\nbased transactional context embe dding for next-item recommendation,”\\ninProc. AAAI , 2018, pp. 2532–2539.\\n[23] H. Ying et al. , “Sequential recommender system based on hierarchical\\nattention networks,” in Proc. IJCAI , Jul. 2018, pp. 3926–3932.\\n[24] D. T. Tran, A. Iosiﬁdis, J. Ka nniainen, and M. Gabbouj, “Temporal'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='[24] D. T. Tran, A. Iosiﬁdis, J. Ka nniainen, and M. Gabbouj, “Temporal\\nattention-augmented bilinear network for ﬁnancial time-series dataanalysis,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 30, no. 5,\\npp. 1407–1418, May 2019.\\n[25] H. Song, D. Rajan, J. J. Thiagara jan, and A. Spanias, “Attend and\\ndiagnose: Clinical time series analysis using attention models,” in Proc.\\nAAAI , 2018, pp. 4091–4098.\\n[26] J. Choi, B.-J. Lee, and B.-T. Zhang, “Multi-focus attention network\\nfor efﬁcient deep reinforcement learning,” CoRR , vol. abs/1712.04603,\\n2017.\\n[27] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” in Proc.\\nNIPS . Red Hook, NY , USA: Curran Associates, 2015, pp. 2692–2700.\\n[28] W. Kool, H. van Hoof, and M. Welling, “Attention, learn to solve\\nrouting problems!” in Proc. ICLR , 2019, pp. 1–25.\\n[29] M. Luong, H. Pham, and C. D. Manning, “Effective\\napproaches to attention-based neural machine translation,” CoRR ,\\nvol. abs/1508.04025, 2015.\\n[30] H. Mi, Z. Wang, and A. Ittycheriah, “Supervised attentions for neural\\nmachine translation,” in Proc. EMNLP , 2016, pp. 2283–2288.\\n[31] H. Mi, B. Sankaran, Z. Wang, an d A. Ittycheriah, “Coverage embed-\\nding models for neural machine translation,” in Proc. EMNLP , 2016,\\npp. 955–960.\\n[32] F. Meng, Z. Lu, H. Li, and Q. Liu, “Interactive attention for neural\\nmachine translation,” in Proc. COLING , 2016, pp. 2174–2185.\\n[33] Z. Tu, Z. Lu, Y . Liu, X. Liu, and H. Li, “Modeling coverage for neural'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='[33] Z. Tu, Z. Lu, Y . Liu, X. Liu, and H. Li, “Modeling coverage for neural\\nmachine translation,” in Proc. ACL , 2016, pp. 76–85.\\n[34] J. Li, Z. Tu, B. Yang, M. R. Lyu, and T. Zhang, “Multi-head\\nattention with disagreement regularization,” in Proc. EMNLP , 2018,\\npp. 2897–2903.\\n[35] L. Liu, M. Utiyama, A. Finch, and E. Sumita, “Neural machine\\ntranslation with supervised attention,” in Proc. COLING , 2016,\\npp. 3093–3102.\\n[36] A. Vaswani et al. , “Attention is all you need,” in Proc. NIPS . Red Hook,\\nNY , USA: Curran Associates, 2017, pp. 5998–6008.\\n[37] D. Britz, A. Goldie, M.-T. Luong, and Q. Le, “Massive exploration\\nof neural machine translation architectures,” in Proc. EMNLP , 2017,\\npp. 1442–1451.\\n[38] A. Bapna, M. Chen, O. Firat, Y . Cao, and Y . Wu, “Training deeper\\nneural machine translation models with transparent attention,” in Proc.\\nEMNLP , 2018, pp. 3028–3033.\\n[39] B. Yang, Z. Tu, D. F. Wong, F. Meng, L. S. Chao, and T. Zhang,\\n“Modeling localness for self-attention networks,” in Proc. EMNLP ,\\n2018, pp. 4449–4458.\\n[40] K. Chen, R. Wang, M. Utiyama, E. Sumita, and T. Zhao, “Syntax-\\ndirected attention for neural machine translation,” in Proc. AAAI , 2018,\\npp. 4792–4799.\\n[41] L. Wu, F. Tian, L. Zhao, J. Lai, and T. Liu, “Word attention for\\nsequence to sequence t ext understanding,” in\\nProc. AAAI , 2018,\\npp. 5578–5585.\\n[42] T. Domhan, “How much attention do you need? A granular analysis\\nof neural machine translation architectures,” in Proc. ACL , 2018,\\npp. 1799–1808.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='of neural machine translation architectures,” in Proc. ACL , 2018,\\npp. 1799–1808.\\n[43] S. Zhao and Z. Zhang, “Attention-via-attention neural machine trans-\\nlation,” in Proc. AAAI , 2018, pp. 563–570.\\n[44] J. Lin, X. Sun, X. Ren, M. Li, and Q. Su, “Learning when to\\nconcentrate or divert attention: Self-adaptive attention temperature for\\nneural machine translation,” in Proc. EMNLP , 2018, pp. 2985–2990.\\n[45] J. Lin, S. Ma, Q. Su, and X. Sun, “Decoding-history-based adap-\\ntive control of attention for neural machine translation,” CoRR ,\\nvol. abs/1802.01812, 2018.\\n[46] C. Malaviya, P. Ferreira, and A. F. T. Martins, “Sparse and con-\\nstrained attention for neural machine translation,” in Proc. ACL , 2018,\\npp. 370–376.\\n[47] Y . Kim, C. Denton, L. Hoang, and A . M. Rush, “Structured attention\\nnetworks,” in Proc. ICLR , 2017, pp. 1–21.\\n[48] P. Michel, O. Levy, and G. Neubig, “Are sixteen heads really better\\nthan one?” in Proc. NeurIPS , Vancouver, BC, Canada, Dec. 2019,\\npp. 14014–14024.\\n[49] E. V oita, P. Serdyukov, R. Sennrich, and I. Titov, “Context-aware neural\\nmachine translation learns anaphora resolution,” in Proc. ACL , 2018,\\npp. 1264–1274.\\n[50] E. V oita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, “Analyzing\\nmulti-head self-attention: Specialized heads do the heavy lifting, therest can be pruned,” in Proc. ACL , 2019, pp. 5797–5808.[51] A. F. T. Martins and J. Kreutzer, “Learning what’s easy: Fully differen-'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='tiable neural easy-ﬁrst taggers,” in Proc. EMNLP , 2017, pp. 349–362.\\n[52] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierar-\\nchical attention networks for document classiﬁcation,” in Proc. HLT-\\nNAACL , 2016, pp. 1480–1489.\\n[53] Y . Zhang, I. Marshall, and B. C. Wallace, “Rationale-augmented\\nconvolutional neural networks for text classiﬁcation,” in Proc. EMNLP ,\\n2016, pp. 795–804.\\n[54] J. Pavlopoulos, P. Malakasiotis, and I. Androutsopoulos, “Deeper\\nattention to abusive user content moderation,” in Proc. EMNLP , 2017,\\npp. 1125–1135.\\n[55] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model\\nfor abstractive sentence summarization,” in Proc. EMNLP , 2015,\\npp. 379–389.\\n[56] H. Li, J. Zhu, T. Liu, J. Zhang, and C. Zong, “Multi-modal sentence\\nsummarization with modality attention and image ﬁltering,” in Proc.\\nIJCAI , 2018, pp. 4152–4158.\\n[57] A. Lauscher, G. Glavaš, S. P. Ponzetto, and K. Eckert, “Investigating the\\nrole of argumentation in the rhetorical analysis of scientiﬁc publications\\nwith neural multi-task learning models,” in Proc. EMNLP , 2018,\\npp. 3326–3338.\\n[58] I. C. T. Ngoko, A. Mukherjee, and B. Kabaso, “Abstractive text\\nsummarization using recurrent neural networks: Systematic literaturereview,” in Proc. ICICKM , vol. 13, 2018, pp. 435–439.\\n[59] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “End-to-end\\nmemory networks,” in Proc. NIPS . Red Hook, NY , USA: Curran\\nAssociates, 2015, pp. 2440–2448.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='memory networks,” in Proc. NIPS . Red Hook, NY , USA: Curran\\nAssociates, 2015, pp. 2440–2448.\\n[60] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of\\ndeep bidirectional transform ers for language understanding,” in Proc.\\nNAACL-HLT , 2019, pp. 4171–4186.\\n[61] M. Daniluk, T. Rocktäschel, J. Welbl, and S. Riedel, “Frustratingly\\nshort attention spans in neural language modeling,” in Proc. ICLR\\n,\\n2017, pp. 1–10.\\n[62] K. M. Hermann et al. , “Teaching machines to read and compre-\\nhend,” in Proc. NIPS . Red Hook, NY , USA: Curran Associates, 2015,\\npp. 1693–1701.\\n[63] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks\\nfor machine reading,” in Proc. EMNLP , 2016, pp. 551–561.\\n[64] F. Hill, A. Bordes, S. Chopra, and J. Weston, “The goldilocks principle:\\nReading children’s books with explicit memory representations,” in\\nProc. ICLR , 2016, pp. 1–13.\\n[65] Y . Cui, T. Liu, Z. Chen, S. Wang, and G. Hu, “Consensus attention-\\nbased neural networks for Chinese reading comprehension,” in Proc.\\nCOLING , 2016, pp. 1777–1786.\\n[66] R. Kadlec, M. Schmid, O. Bajga r, and J. Kleindienst, “Text under-\\nstanding with the attention sum reader network,” in Proc. ACL , 2016,\\npp. 908–918.\\n[67] A. Sordoni, P. Bachman, and Y . Bengi o, “Iterative alternating neural\\nattention for machine reading,” CoRR , vol. abs/1606.02245, 2016.\\n[68] B. Wang, K. Liu, and J. Zhao, “Inner attention based recurrent neural\\nnetworks for answer selection,” in Proc. ACL , 2016, pp. 1288–1297.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='networks for answer selection,” in Proc. ACL , 2016, pp. 1288–1297.\\n[69] C. N. dos Santos, M. Tan, B. Xiang, and B. Zhou, “Attentive pooling\\nnetworks,” CoRR , vol. abs/1602.03609, 2016.\\n[70] Y . Cui, Z. Chen, S. Wei, S. Wang, T. Liu, and G. Hu, “Attention-over-\\nattention neural networks for reading comprehension,” in Proc. ACL ,\\n2017, pp. 593–602.\\n[71] B. Dhingra, H. Liu, Z. Yang, W. Cohen, and R. Salakhutdinov,\\n“Gated-attention readers for text comprehension,” in Proc. ACL , 2017,\\npp. 1832–1846.\\n[72] Y . Shen, P.-S. Huang, J. Gao, and W. Chen, “ReasoNet: Learning\\nto stop reading in machine comprehension,” in Proc. KDD , 2017,\\npp. 1047–1055.\\n[73] S. Kundu and H. T. Ng, “A question-focused multi-factor attention\\nnetwork for question answering,” in Proc. AAAI , 2018, pp. 5828–5835.\\n[74] H. Zhu, F. Wei, B. Qin, and T. Liu, “Hierarchical attention ﬂow\\nfor multiple-choice reading comprehension,” in Proc. AAAI , 2018,\\npp. 6077–6085.\\n[75] Y . Tay, A. T. Luu, and S. C. Hui, “Hermitian co-attention networks\\nfor text matching in asymmetrical domains,” in Proc. IJCAI , Jul. 2018,\\npp. 4425–4431.\\n[76] Y . Hao et al. , “An end-to-end model for question answering over\\nknowledge base with cross-attentio n combining global knowledge,” in\\nProc. ACL , 2017, pp. 221–231.\\n[77] Y . Diao et al. , “WECA: A WordNet-encoded collocation-attention\\nnetwork for homographic pun recognition,” in Proc. EMNLP , 2018,\\npp. 2507–2516.GALASSI et al. : ATTENTION IN NLP 4307'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='pp. 2507–2516.GALASSI et al. : ATTENTION IN NLP 4307\\n[78] A. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria, and L. Morency,\\n“Multi-attention recurrent network for human communication compre-hension,” in Proc. AAAI , 2018, pp. 5642–5649.\\n[79] H. Chen, G. Ding, Z. Lin, Y . Guo, and J. Han, “Attend to knowl-\\nedge: Memory-enhanced attentio n network for image captioning,” in\\nAdvances in Brain Inspired Cognitive Systems Cham, Switzerland:\\nSpringer, 2018, pp. 161–171.\\n[80] J. Lu, J. Yang, D. Batra, and D. Par ikh, “Hierarchical question-image\\nco-attention for visual question answering,” in Proc. NIPS . Red Hook,\\nNY , USA: Curran Associates, 2016, pp. 289–297.\\n[81] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention\\nnetworks for image question answering,” in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 21–29.\\n[82] J. Song, P. Zeng, L. Gao, and H. T. Shen, “From pixels to objects:\\nCubic visual attention for visual question answering,” in Proc. IJCAI ,\\nJul. 2018, pp. 906–912.\\n[83] D. S. Chaplot, K. M. Sathyendr a, R. K. Pasumarthi, D. Rajagopal,\\nand R. Salakhutdinov, “Gated-attentio n architectures for task-oriented\\nlanguage grounding,” in Proc. AAAI , 2018, pp. 2819–2826.\\n[84] R. Zhang, C. N. dos Santos, M. Yasunaga, B. Xiang, and D. Radev,\\n“Neural coreference resolution with deep biafﬁne attention by joint\\nmention detection and mention clustering,” in Proc. ACL , 2018,\\npp. 102–107.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='mention detection and mention clustering,” in Proc. ACL , 2018,\\npp. 102–107.\\n[85] G. Kundu, A. Sil, R. Florian, and W. Hamza, “Neural cross-lingual\\ncoreference resolution and its application to entity linking,” in Proc.\\nACL, 2018, pp. 395–400.\\n[86] Q. Zhang, J. Fu, X. Liu, and X. Huang, “Adaptive co-attention\\nnetwork for named entity recognition in tweets,” in Proc. AAAI , 2018,\\npp. 5674–5681.\\n[87] R. Dong and D. Smith, “Multi-input attention for unsupervised OCR\\ncorrection,” in Proc. ACL , 2018, pp. 2363–2372.\\n[88] F. Nie, Y . Cao, J. Wang, C. Lin, and R. Pan, “Mention and entity\\ndescription co-attention for entity disambiguation,” in Proc. AAAI ,\\n2018, pp. 5908–5915.\\n[89] A. Parikh, O. Täckström, D. Das, and J. Uszkoreit, “A decomposable\\nattention model for natura l language inference,” in Proc. EMNLP , 2016,\\npp. 2249–2255.\\n[90] Y . Tay, A. T. Luu, and S. C. Hui, “Compare, compress and propagate:\\nEnhancing neural architectures with alignment factorization for natural\\nlanguage inference,” in Proc. EMNLP , 2018, pp. 1565–1575.\\n[91] A. Martins and R. Astudillo, “From softmax to sparsemax: A sparse\\nmodel of attention and multi-label classiﬁcation,” in Proc. ICML ,\\nvol. 48, 2016, pp. 1614–1623.\\n[92] S. Wang and J. Jiang, “Learning natural language inference with\\nLSTM,” in Proc. HLT-NAACL , 2016, pp. 1442–1451.\\n[93] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, “Disan:\\nDirectional self-attention netwo rk for RNN/CNN-fre e language under-'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='Directional self-attention netwo rk for RNN/CNN-fre e language under-\\nstanding,” in Proc. AAAI , 2018, pp. 5446–5455.\\n[94] T. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, and C. Zhang,\\n“Reinforced self-attention network : A hybrid of hard and soft attention\\nfor sequence modeling,” in Proc. IJCAI , Jul. 2018, pp. 4345–4352.\\n[95] Q. Chen, Z.-H. Ling, and X. Zhu, “Enhancing sentence embedding\\nwith generalized pooling,” in Proc. COLING , 2018, pp. 1815–1826.\\n[96] I. Lopez-Gazpio, M. Maritxalar, M. Lapata, and E. Agirre, “Word n-\\ngram attention models for sentence similarity and inference,” Expert\\nSyst. Appl. , vol. 132, pp. 1–11, Oct. 2019. [Online]. Available:\\nhttp://www.sciencedirect.com/science/article/pii/S0957417419302842\\n[97] E. Strubell, P. Verga, D. Andor, D. Weiss, and A. McCallum,\\n“Linguistically-informed self-atten tion for semantic role labeling,” in\\nProc. EMNLP , 2018, pp. 5027–5038.\\n[98] Z. Tan, M. Wang, J. Xie, Y . Chen, and X. Shi, “Deep semantic role\\nlabeling with self-attention,” in Proc. AAAI , 2018, pp. 4929–4936.\\n[99] T. Rocktäschel, E. Grefenstette, K. M. Hermann, T. Kociský, and\\nP. Blunsom, “Reasoning about entailment with neural attention,” inProc. ICLR , 2016, pp. 1–9.\\n[100] Z. Lin et al. , “A structured self-attentiv e sentence embedding,” in Proc.\\nICLR , 2017, pp. 1–15.\\n[101] F. Luo, T. Liu, Q. Xia, B. Chang, and Z. Sui, “Incorporating\\nglosses into neural word sense disambiguation,” in Proc. ACL , 2018,\\npp. 2473–2482.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='glosses into neural word sense disambiguation,” in Proc. ACL , 2018,\\npp. 2473–2482.\\n[102] O. Vinyals, L. U. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton,\\n“Grammar as a foreign language,” in Proc. NIPS . Red Hook, NY , USA:\\nCurran Associates, 2015, pp. 2773–2781.\\n[103] N. Kitaev and D. Klein, “Constituency parsing with a self-attentive\\nencoder,” in Proc. ACL , 2018, pp. 2676–2686.[104] R. Kohita, H. Noji, and Y . Matsu moto, “Dynamic feature selection\\nwith attention in incremental parsing,” in Proc. COLING , 2018,\\npp. 785–794.\\n[105] T. Dozat and C. D. Manning, “Deep biafﬁne attention for neural\\ndependency parsing,” in Proc. ICLR , 2017, pp. 1–8.\\n[106] D. Tang, B. Qin, and T. Liu, “Aspect level sentiment classiﬁcation with\\ndeep memory network,” in Proc. EMNLP , 2016, pp. 214–224.\\n[107] J. Cheng, S. Zhao, J. Zhang, I. King, X. Zhang, and H. Wang,\\n“Aspect-level sentiment classiﬁcation with heat (hierarchical attention)\\nnetwork,” in Proc. CIKM , 2017, pp. 97–106.\\n[108] Y . Wang, M. Huang, X. Zhu, and L. Zhao, “Attention-based LSTM\\nfor aspect-level sentiment classiﬁcation,” in Proc. EMNLP , 2016,\\npp. 606–615.\\n[109] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, “Effective attention\\nmodeling for aspect-level sentiment classiﬁcation,” in Proc. COLING ,\\n2018, pp. 1121–1131.\\n[110] P. Zhu and T. Qian, “Enhanced as pect level sentiment classiﬁcation\\nwith auxiliary memory,” in Proc. COLING , 2018, pp. 1077–1087.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='with auxiliary memory,” in Proc. COLING , 2018, pp. 1077–1087.\\n[111] D. Ma, S. Li, X. Zhang, and H. Wa ng, “Interactive attention net-\\nworks for aspect-level sentiment classiﬁcation,” in Proc. IJCAI , 2017,\\npp. 4068–4074.\\n[112] F. Fan, Y . Feng, and D. Zhao, “Multi-grained attention network\\nfor aspect-level sentiment classiﬁcation,” in Proc. EMNLP , 2018,\\npp. 3433–3442.\\n[113] Y . Ma, H. Peng, and E. Cambria, “Targeted aspect-based sentiment\\nanalysis via embedding commonsense knowledge into an attentive\\nLSTM,” in Proc. AAAI , 2018, pp. 5876–5883.\\n[114] Y . Tay, A. T. Luu, S. C. Hui, and J. Su, “Attentive gated lexicon reader\\nwith contrastive contextual co-attention for sentiment classiﬁcation,” in\\nProc. EMNLP , 2018, pp. 3443–3453.\\n[115] J. Yu, L. Marujo, J. Jiang, P. Ka ruturi, and W. Brendel, “Improving\\nmulti-label emotion classiﬁcation via sentiment classiﬁcation with dualattention transfer network,” in Proc. EMNLP , 2018, pp. 1097–1102.\\n[116] Z. Li, Y . Wei, Y . Zhang, and Q. Yang, “Hierarchical attention transfer\\nnetwork for cross-domain sentiment classiﬁcation,” in Proc. AAAI ,\\n2018, pp. 5852–5859.\\n[117] Y . Fan, F. Tian, T. Qin, J. Bian, and T.-Y . Liu, “Learning what data to\\nlearn,” CoRR , 2017, pp. 1–7.\\n[118] Y . Long, R. Xiang, Q. Lu, C.-R. Huang, and M. Li, “Improving\\nattention model based on cognition grounded data for sentiment analy-\\nsis,” IEEE Trans. Affect. Comput. , early access, Mar. 4, 2019, doi:\\n10.1109/TAFFC.2019.2903056.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='sis,” IEEE Trans. Affect. Comput. , early access, Mar. 4, 2019, doi:\\n10.1109/TAFFC.2019.2903056.\\n[119] J. Du, L. Gui, Y . He, R. Xu, and X. Wang, “Convolution-based neural\\nattention with applications to sentiment classiﬁcation,” IEEE Access ,\\nvol. 7, pp. 27983–27992, 2019.\\n[120] X. Li, L. Bing, P. Li, W. Lam, and Z. Yang, “Aspect term extraction\\nwith history attention and selective transformation,” in Proc. IJCAI ,\\n2018, pp. 4194–4200.\\n[121] D. Chen, J. Du, L. Bing, and R. Xu, “Hybrid neural attention for\\nagreement/disagreement inf erence in online debates,” in Proc. EMNLP ,\\n2018, pp. 665–670.\\n[122] I. Habernal and I. Gurevych, “What makes a convincing argument?\\nEmpirical analysis and detecting attributes of convincingness in Web\\nargumentation,” in Proc. EMNLP , 2016, pp. 1214–1223.\\n[123] C. Stab, T. Miller, B. Schiller, P. Rai, and I. Gurevych, “Cross-topic\\nargument mining from heterogeneous sources,” in Proc. EMNLP , 2018,\\npp. 3664–3674.\\n[124] M. Spliethöver, J. Klaff, and H. Heuer, “Is it worth the attention?\\nA comparative evaluation of attention layers for argument unit seg-\\nmentation,” in ArgMining@ACL . Florence, Italy: ACL, Aug. 2019,\\npp. 74–82.\\n[125] M. Spliethöver, J. Klaff, and H. Heuer, “Is it worth the attention?\\nA comparative evaluation of attention layers for argument unit seg-\\nmentation,” in Proc. 6th Workshop Argument Mining . Florence, Italy:\\nAssociation for Computationa l Linguistics, Aug. 2019, pp. 74–82.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='Association for Computationa l Linguistics, Aug. 2019, pp. 74–82.\\n[Online]. Available: https://www.aclweb.org/anthology/W19-4509\\n[126] F. Barbieri, L. E. Anke, J. Camacho-Collados, S. Schockaert, and\\nH. Saggion, “Interpretable emoji pre diction via label-wise attention\\nLSTMs,” in Proc. EMNLP , 2018, pp. 4766–4771.\\n[127] X. Li, K. Song, S. Feng, D. Wang, and Y . Zhang, “A co-attention neural\\nnetwork model for emotion cause analysis with emotional contextawareness,” in Proc. EMNLP , 2018, pp. 4752–4757.\\n[128] L. Gui, J. Hu, Y . He, R. Xu, Q. Lu, and J. Du, “A question answering\\napproach for emotion cause extraction,” in Proc. EMNLP , 2017,\\npp. 1593–1602.4308 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 32, NO. 10, OCTOBER 2021\\n[129] X. Wang, Y . Hua, E. Kodirov, G. Hu, and N. M. Robertson, “Deep\\nmetric learning by online soft mining and class-aware attention,” inProc. AAAI , 2019, pp. 5361–5368, doi: 10.1609/aaai.v33i01.33015361.\\n[130] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, “Attention\\nmodels in graphs: A survey,” ACM Trans. Knowl. Discovery Data ,\\nvol. 13, no. 6, pp. 62:1–62:25, Nov. 2019.\\n[131] Y . Goldberg, Neural Network Methods for Natural Language Process-\\ning(Synthesis Lectures on Human L anguage Technologies), vol. 10,\\nno. 1. San Rafael, CA, USA: Morgan & Claypool, 2017.\\n[132] G. Tang, M. Müller, A. Rios, and R. Sennrich, “Why self-attention?'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='[132] G. Tang, M. Müller, A. Rios, and R. Sennrich, “Why self-attention?\\nA targeted evaluation of neural machine translation architectures,” inProc. EMNLP , 2018, pp. 4263–4272.\\n[133] M. Schuster and K. K. Paliwal, “Bi directional recurrent neural net-\\nworks,” IEEE Trans. Signal Process. , vol. 45, no. 11, pp. 2673–2681,\\nNov. 1997.\\n[134] C. Ma, L. Liu, A. Tamura, T. Zhao, and E. Sumita, “Deterministic\\nattention for sequence-to-sequence constituent parsing,” in Proc. AAAI ,\\n2017, pp. 3237–3243.\\n[135] L. Liu, M. Zhu, and S. Shi, “Impr oving sequence-to-sequence con-\\nstituency parsing,” in Proc. AAAI , 2018, pp. 4873–4880.\\n[136] S. Maharjan, M. Montes, F. A. González, and T. Solorio, “A genre-\\naware attention model to improve the likability prediction of books,”\\ninProc. EMNLP , 2018, pp. 3381–3391.\\n[137] T. Qiao, J. Dong, and D. Xu, “Exploring human-like attention\\nsupervision in visual question answering,” in Proc. AAAI , 2018,\\npp. 7300–7307.\\n[138] J.-K. Kim and Y .-B. Kim, “Supervised domain enablement atten-\\ntion for personalized domain classiﬁcation,” in Proc. EMNLP , 2018,\\npp. 894–899.\\n[139] D. Kiela, C. Wang, and K. Cho, “Dynamic meta-embeddings\\nfor improved sentence representations,” in Proc. EMNLP , 2018,\\npp. 1466–1477.\\n[140] Y . Bengio, P. Simard, and P. Fras coni, “Learning long-term depen-\\ndencies with gradient descent is difﬁcult,” IEEE Trans. Neural Netw. ,\\nvol. 5, no. 2, pp. 157–166, Mar. 1994.\\n[141] C. Wu, F. Wu, J. Liu, and Y . Huang, “Hierarchical user and item'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='[141] C. Wu, F. Wu, J. Liu, and Y . Huang, “Hierarchical user and item\\nrepresentation with three-tier attention for recommendation,” in Proc.\\nNAACL-HLT , 2019, pp. 1818–1826.\\n[142] A. Graves, G. Wayne, and I. Danihelka, “Neural turing machines,”\\nCoRR , vol. abs/1410.5401, 2014.\\n[143] X. Glorot, A. Bordes, and Y . Be ngio, “Deep sparse rectiﬁer neural\\nnetworks,” in Proc. AISTATS , vol. 15, 2011, pp. 315–323.\\n[144] G. Klambauer, T. Un terthiner, A. Mayr, and S. Hochreiter, “Self-\\nnormalizing neural networks,” in Proc. NIPS . Red Hook, NY , USA:\\nCurran Associates, 2017, pp. 971–980.\\n[145] J. Du, J. Han, A. Way, and D. Wan, “Multi-level structured self-\\nattentions for distantly supervised relation extraction,” in Proc.\\nEMNLP , 2018, pp. 2216–2225.\\n[146] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature , vol. 521,\\nno. 7553, p. 436, 2015.\\n[147] L. Getoor and B. Taskar, Introduction to Statistical Relational Learn-\\ning, vol. 1. Cambridge, MA, USA: MIT Press, 2007.\\n[148] A. S. D. Garcez, K. B. Broda, and D. M. Gabbay, Neural-Symbolic\\nLearning Systems: Foundations and Applications .B e r l i n ,G e r m a n y :\\nSpringer, 2012.\\n[149] M. Lippi, “Reasoning with d eep learning: An open challenge,” in Proc.\\nCEUR Workshop , vol. 1802, 2017, pp. 38–43.\\n[150] O. Zaidan, J. Eisner, and C. Pia tko, “Using ‘annotator rationales’\\nto improve machine learning for text categorization,” in Proc. HLT-\\nNAACL , 2007, pp. 260–267.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='to improve machine learning for text categorization,” in Proc. HLT-\\nNAACL , 2007, pp. 260–267.\\n[151] C. Rudin, “Stop explaining black box machine learning models for high\\nstakes decisions and use interpretable models instead,” Nature Mach.\\nIntell. , vol. 1, pp. 206–215, May 2019.\\n[152] M. O. Riedl, “Human-centered a rtiﬁcial intellig ence and machine\\nlearning,” Hum. Behav. Emerg. Technol. , vol. 1, no. 1, pp. 33–36, 2019.\\n[153] Q. V . Le, “Building high-level features using large scale unsupervised\\nlearning,” in Proc. IEEE ICASSP , May 2013, pp. 8595–8598.\\n[154] X. Shi, I. Padhi, and K. Knight, “ Does string-based neural MT learn\\nsource syntax?” in Proc. EMNLP , 2016, pp. 1526–1534.\\n[155] M. Hu et al. , “Attention-guided answer distillation for machine reading\\ncomprehension,” in Proc. EMNLP , 2018, pp. 2077–2086.\\n[156] S. Moon and J. G. Carbonell, “Completely heterogeneous transfer\\nlearning with attention—What and what not to transfer,” in Proc. IJCAI ,\\n2017, pp. 2508–2514.\\n[157] J. Heo et al. , “Uncertainty-aware attention for reliable interpretation\\nand prediction,” in Proc. NIPS . Red Hook, NY , USA: Curran Asso-\\nciates, 2018, pp. 917–926.[158] M. Poggi and S. Mattoccia, “Lear ning a general-purpose conﬁdence\\nmeasure based on O(1) features and a smarter aggregation strategy forsemi global matching,” in Proc. 3DV , 2016, pp. 509–518.\\n[159] R. Cipolla, Y . Gal, and A. Kendall, “Multi-task learning using\\nuncertainty to weigh losses for scene geometry and semantics,” in'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='uncertainty to weigh losses for scene geometry and semantics,” in\\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018,\\npp. 7482–7491.\\n[160] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight\\nuncertainty in neural networks,” in Proc. ICML , 2015, pp. 1613–1622.\\n[161] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, “An unsupervised\\nneural attention model for aspect extraction,” in Proc. ACL ,v o l .1 ,\\n2017, pp. 388–397.\\n[162] M. Zhang and Y . Wu, “An unsupervised model with attention autoen-\\ncoders for question retrieval,” in Proc. AAAI , 2018, pp. 4978–4986.\\n[163] B. Zhang, D. Xiong, and J. Su, “Battr ae: Bidimensional attention-based\\nrecursive autoencoders for learni ng bilingual phrase embeddings,” in\\nProc. AAAI , 2017, pp. 3372–3378.\\n[164] T. Tian and Z. F. Fang, “Atten tion-based autoencoder topic model\\nfor short texts,” Procedia Comput. Sci. , vol. 151, pp. 1134–1139,\\nJan. 2019.\\n[165] P. Yang, J. Lin, J. Xu, J. Xie, Q. Su, and X. Sun, “Speciﬁcity-driven\\ncascading approach for unsupervis ed sentiment modiﬁcation,” in Proc.\\nEMNLP-IJCNLP , 2019, pp. 5511–5520.\\n[166] G. Kang, L. Zheng, Y . Yan, and Y . Yang, “Deep adversarial attention\\nalignment for unsupervised domain a daptation: The beneﬁt of target\\nexpectation maximization,” in Proc. ECCV , 2018, pp. 401–416.\\n[167] F. Yang, Z. Yang, and W. W. Cohen, “Differentiable learning of\\nlogical rules for knowledge base reasoning,” in Proc. NIPS , 2017,\\npp. 2319–2328.'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='logical rules for knowledge base reasoning,” in Proc. NIPS , 2017,\\npp. 2319–2328.\\n[168] P. Wang, J. Han, C. Li, and R. Pan, “Logic attention based neighbor-\\nhood aggregation for inductive kno wledge graph embedding,” in Proc.\\nAAAI , vol. 33, 2019, pp. 7152–7159.\\n[169] Y . Shen et al. , “Knowledge-aware attentive neural network for ranking\\nquestion answer pairs,” in Proc. SIGIR , 2018, pp. 901–904.\\n[170] D. A. Hudson and C. D. Manning, “Compositional attention networks\\nfor machine reasoning,” in Proc. ICLR , 2018, pp. 1–20.\\n[171] T. Li and V . Srikumar, “Augmenting neural networks with ﬁrst-order\\nlogic,” in Proc. ACL , 2019, pp. 292–302.\\nAndrea Galassi received the master’s degree in\\ncomputer engineering from the Department ofComputer Science and Engineering, University ofBologna, Bologna, Italy, in 2017, where he is cur-\\nrently pursuing the Ph.D. degree.\\nHis research activity concerns artiﬁcial intelligence\\nand machine learning, focusing on argumenta-tion mining and related natural language process-\\ning (NLP) tasks. Other research interests involve\\ndeep learning applications to games and constraintsatisfaction problems (CSPs).\\nMarco Lippi held positions at the University of\\nFlorence, Florence, Italy, University of Siena, Siena,\\nItaly, and the University of Bologna, Bologna, Italy.\\nHe was a Visiting Scholar with Université Pierreet Marie Curie (UPMC), Paris, France. He is cur-rently an Associate Professor with the Department\\nof Sciences and Methods for Engineering, Univer-'),\n",
       " Document(metadata={'title': 'Attention in Natural Language Processing', 'year': 2021}, page_content='of Sciences and Methods for Engineering, Univer-\\nsity of Modena and Reggio Emilia, Modena, Italy.His work focuses on machine learning and artiﬁ-\\ncial intelligence, with applications to several areas,\\nincluding argumentation mining, legal informatics,\\nand medicine.\\nProf. Lippi received the “E. Caianiello” Prize for the Best Italian Ph.D.\\nThesis in the ﬁeld of neural networks in 2012.\\nPaolo Torroni has been an Associate Professor\\nwith the University of Bologna, Bologna, Italy, since\\n2015. He has edited over 20 books and specialissues and authored over 150 articles in compu-tational logics, multiagent systems, argumentation,\\nand natural language processing. His main research\\ninterest includes artiﬁcial intelligence.\\nProf. Torroni serves as an Associate Edi-\\ntor for Fundamenta Informaticae and Intelligenza\\nArtiﬁciale .'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='Data and Information Management, 2021; 5(3): 1–12\\nResearch Article Open Access\\nPei Pan, Yijin Chen*\\nAutomatic Subject Classification of Public \\nMessages in E-government Affairs\\nhttps:/ /doi.org/10.2478/dim-2021-0004\\nreceived October 30, 2020; accepted February 10, 2021.\\nAbstract:  Public messages on the Internet political \\ninquiry platform rely on manual classification, which \\nhas the problems of heavy workload, low efficiency, and \\nhigh error rate. A Bi-directional long short-term memory \\n(Bi-LSTM) network model based on attention mechanism \\nwas proposed in this paper to realize the automatic \\nclassification of public messages. Considering the network \\npolitical inquiry data set provided by the BdRace platform \\nas samples, the Bi-LSTM algorithm is used to strengthen \\nthe correlation between the messages before and after the \\ntraining process, and the semantic attention to important \\ntext features is strengthened in combination with the \\ncharacteristics of attention mechanism. Feature weights \\nare integrated through the full connection layer to carry \\nout classification calculations. The experimental results \\nshow that the F1 value of the message classification model \\nproposed here reaches 0.886 and 0.862, respectively, in the \\ndata set of long text and short text. Compared with three \\nalgorithms of long short-term memory (LSTM), logistic \\nregression, and naive Bayesian, the Bi-LSTM model can \\nachieve better results in the automatic classification of \\npublic message subjects.'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='achieve better results in the automatic classification of \\npublic message subjects.\\nKeywords: Internet politics inquiry, public message, \\nsubject classification, Bi-LSTM model, attention \\nmechanism\\n1  Introduction\\nSince the beginning of the 21st century, the use of \\ninformation and communication technologies (ICT) in government has been called “e-government” (Cegarra-\\nNavarro, Pachon, & Cegarra, 2012). Durrant (2002) defined it \\nas the government’s long-term commitment to improving the \\nrelationship between citizens and the public sector through \\nthe provision of cost-effective and efficient services and \\ninformation. Holmes (2001) thinks that e-government is to \\ndevelop a citizen-centered government environment, which \\nuses ICT to serve citizens whenever and wherever possible. \\nBenefiting from the rise of new technologies such as the \\nmobile network, the communication and dissemination of \\npublic affairs can be easily realized through the network \\nin China. The public message refers to the opinions and \\nsuggestions of the public to participate in social affairs \\nthrough microblogs, WeChat, e-mail, government websites, \\nand other network political platforms. With more and more \\nchannels available for expressing opinions and suggestions, \\nnow people have more opportunities to participate in \\nsocial construction. Because of this feature, government \\ndepartments also need to understand the opinions and \\nsuggestions of the public, and feedback of the public on the'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='suggestions of the public, and feedback of the public on the \\nonline political inquiry platforms. The example of public \\nmessages is shown in Fig. 1.\\nMost of the public messages are saved or converted \\ninto written text, and then archived in digital format. On \\nthis basis, the government staff needs to classify the public \\nmessages and send those messages to the concerned \\nfunctional departments for reply messages (Jeong, Lee, \\n& Hong, 2017). Public message classification is a key step \\nin the above process, however, when dealing with the \\npublic messages on the network political platform, the \\nstaff should classify the messages according to a certain \\nclassification standard, and rely on manual processing \\naccording to experience, which has the problems of heavy \\nworkload, low efficiency, and high error rate (Kim & Hong, \\n2021). The sharp increase of text data makes manual \\nclassification more and more difficult, and the high cost \\nand low efficiency of information processing hinder the \\nimprovement of government governance.\\nText classification is a process of labeling text \\ndocuments into one or more pre-specified categories, \\nand natural language processing (NLP) technology *Corresponding author: Yijin Chen,  School of Economics & \\nManagement, South China Normal University, Guangzhou, China, \\nEmail: cyj@scnu.edu.cn  \\nPei Pan, School of Information Management, Wuhan University, \\nWuhan, China\\n Open Access. © 2021 Pei Pan, Yijin Chen, published by Sciendo.'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='Wuhan, China\\n Open Access. © 2021 Pei Pan, Yijin Chen, published by Sciendo. \\n  This work is licensed under the Creative Commons Attribution-\\nNonCommercial-NoDerivatives 3.0 License.2\\u2003 \\u2003 Pei Pan, Yijin Chen\\nprovides the possibility for automatic text classification \\nin e-government affairs (Knutsson, Sneiders, & Alfalahi, \\n2012). In view of the above problems, this study hopes to \\napply a bi-directional long short-term memory (Bi-LSTM) \\nnetwork model based on attention mechanism to the \\nscene of e-government text classification, which will bring \\nsome help for the related work in the future. First, the use \\nof a deep learning algorithm for automatic classification \\ncan effectively save the time of manual classification. \\nSecond, the deep learning model can automatically \\nlearn some internal relations between data and tags. \\nApart from tagging work, it does not need any manual \\nintervention and also saves the complicated process of \\nfeature definition (Minaee et al., 2020). At the same time, \\nthe public message involves a wide range of fields, and the \\ngeneralization ability of deep learning in this classification \\nproblem is stronger than machine learning, which is \\nmore conducive to further expand the application. Third, \\nBi-LSTM fully considers the meaning of words in context, \\nwhich overcomes the disadvantage that long short-term \\nmemory (LSTM) network can only consider one-way \\ninformation of words. Finally, the research introduces the'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='information of words. Finally, the research introduces the \\nattention mechanism into the model. Different weights are \\ngiven to different words, which can effectively improve the \\nimportance of words expressing the theme, and find out \\nthe key information in the message.\\nThe rest of our paper is structured as follows: Section \\n2 reviews the related work; Section 3 focuses on the model \\npresented in this paper; Section 4 presents the comparative \\nexperiments performed and also the description and \\nanalysis of the experimental results; Section 5 discusses the advantages and limitations of the model; and Section \\n6 is the summary and future prospects.\\n2  Literature Review\\n2.1  Application of Recurrent Neural Network \\nin Chinese Text Classification\\nA recurrent neural network (RNN) is a common neural \\nnetwork in deep learning, which is mainly used in text \\nclassification, speech recognition, machine translation, \\nemotion classification, video behavior recognition, and \\nother fields. LSTM networks, a type of RNN, were proposed \\nby Hochreiter and Schmidhuber (1997). Different from \\ngeneral RNN, as an improved cyclic neural algorithm, LSTM \\ncan not only solve the common problems, such as gradient \\nexplosion or gradient disappearance in neural networks, \\nbut also solve the long-distance dependence problem that \\nthe general RNN cannot handle. RNN was named with \\nshort-term memory. When the input text is long, the RNN \\nnetwork tends to forget the previous input text, and the'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='network tends to forget the previous input text, and the \\ninformation cannot be effectively transmitted, which has \\na negative impact on the classification in the later stage. \\nThe LSTM algorithm introduces the “gate” structure to \\nrealize the forgetting or retention of text information and \\nthe long-term memory of important information (Palangi \\net al., 2016).\\nAt present, many scholars have introduced LSTM \\nmodel into the study of Chinese text classification. Tao, Li, \\nFigure 1. Example of public messages. Automatic Subject Classification of Public Messages in E-government Affairs\\u2003 \\u20033\\nLiu and Liu (2019) proposed an end-to-end classification \\nmodel of short essays based on bidirectional LSTM to \\nsolve the problems of short essays and sparse features. \\nGood results have been achieved in the classification of \\nChinese news and other data sets. Li et al. (2020) proposed \\na sentiment analysis model based on deep learning, \\nlexicon-integrated dual-channel CNN-LSTM family \\nmodel, which combined CNN and LSTM/BiLSTM branches \\nin parallel, and when tested found to be superior to many \\nbaseline methods in experiments on Chinese comment \\ntext data sets. Xie et al. (2020) proposed the LSTM \\nChinese text classification algorithm based on attention \\nmechanism and feature enhancement fusion, which not \\nonly increased the weight of important text features but \\nalso enhanced the difference between them and other text \\nfeatures, significantly improving the recognition ability of'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='features, significantly improving the recognition ability of \\nChinese text feature.\\nThe successful application of LSTM algorithm in \\nChinese text classification provides strong support for \\nrelevant researches in the field of e-government.\\n2.2  Application of Text Classification in \\nE-government\\nText classification refers to the automatic and efficient \\ntext information classification technology by using \\ncomputer technology. Currently, scholars have carried out \\nrelated researches in the field of e-government. Zablith \\nand Osman (2019) used a neural network to realize text \\nclassification and text emotion analysis of e-government \\nplatforms. The F-value of classification reached 85.16%, \\nand emotion analysis and artificial emotion assessment \\nhad a high correlation (71.44%). Zhang, Wang and Zhu \\n(2020) used the LSTM algorithm to conduct emotion \\nanalysis and discussed the influencing mechanism of \\ngovernment information release strategy on the evolution \\nof negative emotional contagion. Ku and Leroy (2014) \\ndeveloped a government decision support system that \\ncombines NLP techniques, similarity measures, and Naive \\nBayes classifiers to classify electronic reports of different \\ntypes of crimes with an accuracy rate of 94.82%. Kano, \\nFujita and Tsuda (2019) applied text mining technology \\nto classify citizen reports based on text content, and the \\nresults showed that the automatic classification based on \\nthe content of citizen reports was more accurate than the'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='the content of citizen reports was more accurate than the \\nclassification based on the default category.\\nResearches indicated that text classification is widely \\napplied in e-government. However, there are still few \\nresearches on the application of text mining technology \\nin e-government with Chinese as the main text content. There is no relevant research on the application of Bi-LSTM \\nnetwork algorithm based on attention mechanism, an \\nimproved network algorithm for long-term and short-term \\nmemory.\\n3  Classification Model of Public \\nMessage Data of E-government\\n3.1  Technical Route of Model\\nThe technical route of this model is shown in Fig. 2, which \\nmainly includes three parts: data preprocessing, model \\ntraining and testing, and model evaluation and result \\nanalysis.\\n3.2  The Usage of Word2vec in Classification \\nof Public Message\\nText representation is the basic work in NLP . The quality \\nof text representation directly affects the performance of \\nthe whole NLP system. Text vectorization is the main way \\nof text representation. This study adopts the Word2vec \\nmodel proposed by Mikolov, Chen, Corrado and Dean \\n(2013). Word2vec model is essentially a three-layer BP \\nneural network and there is only one hidden layer. Among \\nthe commonly used Word2vec models, skip-gram model \\nperforms well in a large corpus so that the author will use \\nthis model to realize the Word2vec training.\\n3.3  Bi-LSTM Based on Attention Mechanism\\n3.3.1  Long Short-term Memory Network'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='3.3  Bi-LSTM Based on Attention Mechanism\\n3.3.1  Long Short-term Memory Network\\nThe LSTM model is shown in Fig. 3. The box represents \\nan identical timing module and the LSTM model consists \\nof a series of identical timing modules. The LSTM model \\nrealizes the forgetting or retention of text information \\nthrough the “gate” structure which consists of three gates: \\nforgetting gate, input gate, and output gate.\\n① Forgetting Gate: The input of the forgetting gate is the \\noutput hi-1 of the front layer and the public message text xi, \\nwhich are inputted in this layer. To make information pass \\nselectively, the gates use sigmoid as the activation function \\nand will output a calculation result Fi between 0 and 1. Fi \\nrepresents the probability that the LSTM model state of the \\nfront layer is forgotten. 1 represents “completely reserved” 4\\u2003 \\u2003 Pei Pan, Yijin Chen\\nand 0 represents “completely abandoned”. The relevant \\nformulas are as follows:\\nσ=sigmod (x) =1\\n1+e−x   (1)  \\nFi=σ(Wf[xi, hi−1]+bf)   (2)  \\nIi=σ(Wl[xi, hi−1])+bl  (3) \\nC�i=tanh (Wc[xi, hi−1])+bc  (4)(1)\\nσ=sigmod (x) =1\\n1+e−x   (1)  \\nFi=σ(Wf[xi, hi−1]+bf)   (2)  \\nIi=σ(Wl[xi, hi−1])+bl  (3) \\nC�i=tanh (Wc[xi, hi−1])+bc  (4)(2)\\n② Input Gate: The input gate consists of two parts. The \\nfirst part uses sigmoid and the output is Ii. The second part \\nuses tanh as the activation function and the output iC\\uf025 is \\nthe output of this layer. The value of Ii is between 0 and 1, \\nand Ii represents the extent to which the public message'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='and Ii represents the extent to which the public message \\ninformation in this layer is retained. Based on this, we can update the cell state of this layer with the new information \\nfrom the public message data of e-government. The \\nrelevant formulas are stated as follows:σ=sigmod (x) =1\\n1+e−x   (1)  \\nFi=σ(Wf[xi, hi−1]+bf)   (2)  \\nIi=σ(Wl[xi, hi−1])+bl  (3) \\nC�i=tanh (Wc[xi, hi−1])+bc  (4)(3)σ=sigmod (x) =1\\n1+e−x   (1)  \\nFi=σ(Wf[xi, hi−1]+bf)   (2)  \\nIi=σ(Wl[xi, hi−1])+bl  (3) \\nC�i=tanh (Wc[xi, hi−1])+bc  (4) (4)\\nCi=Fi×Ci−1+Ii×C�i   (5) \\nOi = σ(W O[xi, hi−1]) + bO   (6)  \\nHi = Oi × tanh(C i)   (7) (5)\\n③ Output Gate: The output gate is used to control how \\nmuch the status of the unit is output to the current output \\nvalue of LSTM. First, the SIGMOD function is used to get \\na value Oi between 0 and 1. Then the LSTM model state \\nCi is processed by tanh activation function and multiplied \\nby Oi. The result Hi is the output of this layer. After Hi of \\nevery layer were weighted and summed, the type of public \\nmessage will be obtained. The relevant formulas are \\nstated as follows:Ci=Fi×Ci−1+Ii×C�i   (5) \\nOi = σ(W O[xi, hi−1]) + bO   (6)  \\nHi = Oi × tanh(C i)   (7) (6)Ci=Fi×Ci−1+Ii×C�i   (5) \\nOi = σ(W O[xi, hi−1]) + bO   (6)  \\nHi = Oi × tanh(C i)   (7) (7)\\nIn the classification of the public message data of \\ne-government, we need to analyze the long text. LSTM \\nFigure 2.  Technical route of model.\\nFigure 3. Long short-term memory network module (Zhang, Zheng,'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='Figure 3. Long short-term memory network module (Zhang, Zheng, \\nHu, & Yang, 2015). Automatic Subject Classification of Public Messages in E-government Affairs\\u2003 \\u20035\\nsolves the problem of gradient vanishing in the traditional \\nRNN by introducing the memory cell and gate mechanism, \\nwhich makes it finally realize the protection of important \\ninformation of the message. These gates determine the \\ninformation that flows in and out of the current time steps.\\n3.3.2  Bi-LSTM Based on Attention Mechanism\\nDuring the process of using the LSTM model to classify, \\nthis study found that the relationship in the context of the \\nmessage is bidirectional. Generally, LSTM can understand \\nthe following text according to the previous text, but \\ncannot understand the previous content by the following \\nmessage. At the same time, the content that has nothing \\nto do with the topic in the message will affect the training \\neffect. To improve the training effect of the model, the \\nmodel should be focused on the critical information. \\nBased on the above two reasons, this study adopts the \\nBi-LSTM model based on attention mechanism to get \\nbetter classification effect in the public message data of \\ne-government (Zhou et al., 2016).\\n(1)Bi-LSTM\\nBi-LSTM is a combination of forward LSTM and backward \\nLSTM. The forward LSTM can obtain the past data \\ninformation of the input sequence and the backward \\nLSTM can obtain the future data information of the input'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='LSTM can obtain the future data information of the input \\nsequence. In this way, the semantic relationship between \\nthe front and the back of a sentence is enhanced, and the \\naccuracy of the model is improved.\\n(2) Attention Mechanism\\nEncoder-decoder is a common model framework in deep \\nlearning. When predicting each output, its corresponding \\nsemantic code is the same, i.e., each word in the input \\ntext has the same influence on each word in the output. \\nThere are two disadvantages: one is that the semantic \\nvector cannot completely represent the whole sequence of \\ninformation. The other is that the information carried by \\nthe first input content will be covered by the later input \\ninformation.\\nThe attention mechanism in deep learning \\nsimulates the attention model of the human brain. It \\nadds an “attention area” to the classification process. It \\nrepresents the parts of the input sequence that should be \\nfocused on, so that the machine can pay attention to the \\nimportant information and then generate the next output \\naccording to the “attention area”. In the above encoder-\\ndecoder framework, the combination of attention layer \\ncan effectively solve the above problems. The details are \\nshown in Fig. 4.In the attention layer, the formulas of semantic coding \\nare as follows:\\nxT\\ni ij j\\nj1cαh\\n==∑  (8) (8)\\n()\\n()xij\\nij T\\nikk1exp e\\nα\\nexp e==\\n∑(9) \\n() ij i 1 je a s ,h−= (10) (9)\\n() ij i 1 je a s ,h−= (10)\\nTake the sentence “ 洪山公园施工吵 ” as an example.'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='() ij i 1 je a s ,h−= (10)\\nTake the sentence “ 洪山公园施工吵 ” as an example. \\nIn the above formulas, Tx is the length of the sentence, \\nhj is the output of the hidden layer of the Encoder Layer \\nat time j, and si-1 is the output of the hidden layer of \\nthe Decoder Layer at time i-1. In the text classification \\nof public message data of e-government, the model \\nlearns different weight coefficients aij at different times \\nthrough the attention mechanism, and the corresponding \\ninput feature is weighted by introducing the attention \\nmechanism into the neural network. For the sentence \\n“洪山公园施工吵 ,” we give the following probability \\ndistribution value: ( 洪山公园 , 0.2), (施工 , 0.5), and ( 吵, \\n0.3). The probability of each word represents the size of \\nattention assigned to different words by the attention \\nmechanism. As the word “ 施工 ” has the greatest impact \\non the type of message, it is given greater weight, thus \\nimproving the accuracy of classification. In this way, the \\nattention mechanism identifies the main part to express \\nthe theme of each message and the input of the model is \\nweighted to highlight the effective characteristics through \\nthe attention mechanism.\\nFigure 4. Encoder-decoder framework with attention mechanism.6\\u2003 \\u2003 Pei Pan, Yijin Chen\\nTo sum up, the Bi-LSTM model based on the attention \\nmechanism is shown in Fig. 5.\\n3.3.3  Calculation of Classification Result\\nTo carry out the text classification, this study transfers'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='To carry out the text classification, this study transfers \\nthe output vector v of the full connection layer which \\nis combined with the Attention Layer to the softmax \\nactivation function of seven categories in the output layer \\nto predict the results of text classification. The formula \\nof the prediction result of classification is explained as \\nfollows. Here, the cross-entropy loss function is used as the \\nobjective function, and the backpropagation optimization \\nis carried out by the Adam algorithm. The parameters in \\nthe text classification model are trained and updated to \\nminimize the cross-entropy of known message categories \\nand predicted message categories.\\n()c Y softmax W v b= + (11)\\nWhere Y is the prediction result matrix of message \\nclassification, while Wc is the same as Wc in the Input Gate, \\nand b is the offset term.\\n3.4  Evaluation Index\\nThis study focuses on the classification problem of seven \\ncategories with a single label. After the precision (p), recall (r) and F-score of the seven categories are calculated \\nand the macro average value is calculated, which is the \\narithmetic mean value of each classification index. In \\nthis study, these three macro average values are used to \\nevaluate the performance of the classification model.\\nn\\nii11Macro _ precision Pn==∑(12)\\nn\\nii11Macro _ recall rn==∑ (13)\\nn\\n1ii11Macro _ F _ score Fn==∑(14)\\nFigure 5. Bi-LSTM based on attention mechanism (Zhou et al., 2016). Bi-LSTM, Bi-directional long short-term memory.\\nTable 1'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='Table 1\\nData Sets\\nCategory Amounts of data\\nUrban and rural construction 2009\\nEnvironmental protection 938\\nTransportation 613\\nEducation and sports 1589\\nLabor and social security 1969\\nBusiness and tourism 1215\\nHealth and birth control 877 Automatic Subject Classification of Public Messages in E-government Affairs\\u2003 \\u20037\\nTable 2\\nSpecific Examples of Public Messages*\\nMessage \\nnumberUser ID Message topics Time Message Details Category\\n24 A00074011 Xihu Construction Group \\nin a city occupies the \\nroad for construction, \\nwhich has potential \\nsafety hazards.\\n(Original Text: A 市的西\\n湖建筑集团霸占道路进\\n行施工有安全隐患。 )2020/1/6\\n12:09:38On the road to the west of A3 Avenue, the \\nsidewalk and street lamp are included in \\nthe construction enclosure of Yanzishan \\nresettlement housing project of Xihu \\nconstruction group. Every day, especially \\nduring the commuting period, there are a lot \\nof people and traffic on this road, which has \\na great potential safety hazard. I strongly \\nrequest civilized City A to rectify this extremely \\nuncivilized road section as soon as possible. \\n(Original Text: 在A3区大道向西方向的道路，\\n西湖建筑集团燕子山安置房项目将人行道和路\\n灯纳入在施工围墙内。每天尤其上下班期间，\\n这条路上人流车流极多，安全隐患非常大。\\n强烈请求文明城市 A市，尽快整改这个极不文\\n明的路段。 )Urban and Rural \\nConstruction\\n161600 A00015711 Shengchang glass \\ncompany in Xidi \\nprovince has been \\nemitting high sulfur \\nfor many years, \\nseriously polluting the \\natmosphere.\\n(Original Text: 西地省盛\\n常玻璃公司多年高硫排\\n放，严重污染大气层。 )2014/8/14\\n11:53:40There is a Shengchang glass company in'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='常玻璃公司多年高硫排\\n放，严重污染大气层。 )2014/8/14\\n11:53:40There is a Shengchang glass company in \\nHekou Town, G5 County, which produces more \\nthan 190 tons of glass every day. Since it was \\nput into operation in June 2010, it has been \\nusing high sulfur petroleum coke as fuel, with \\nsulfur content as high as 3.5%. It melts 195 \\ntons of glass every day, uses 53 tons of coke, \\nand emits 1.8 tons of sulfur dioxide every \\nday. So far, it has emitted more than 2600 \\ntons of sulfur dioxide, seriously polluting the \\natmosphere. Since the use of petroleum coke, \\nthe company has always directly discharged \\nthe flue gas into the atmosphere without \\nany desulfurization equipment and facilities. \\n(Original Text: G5 县合口镇有个盛常玻璃公\\n司，每天产玻璃 190吨以上，自 2010年6月投\\n产以来，一直用高硫石油焦为燃料，含硫高达\\n3.5%,每天熔化玻璃 195吨，用焦量 53吨，每\\n天排放二氧化硫 1.8吨，至今累计排放 2600吨\\n以上，严重污染大气层。该公司自用石油焦以\\n来始终将烟气直接排放到大气中，无任何脱硫\\n设备和设施。 )Environmental \\nProtection\\n160660 A00039781 In City A, many fishing \\nboats stop in the middle \\nof the channel on the \\nChujiang River, affecting \\nthe passage of cargo \\nships.\\n(Original Text: 在A市，\\n楚江上有很多渔船停在\\n航道中间影响货船通\\n行。 )2017/9/12\\n16:47:13Every night between the first bridge and the \\nsecond bridge of the Chujiang River in City \\nA, there are many small boats fishing in the \\nmiddle of the river, and some fishing boats \\neven stop in the middle of the channel, which \\nseriously affects the passage of the incoming \\nand outgoing cargo ships. I hope the relevant \\ndepartments will take charge of it. (Original \\nText: 每天晚间在楚江 A市的楚江一桥至二桥之'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='departments will take charge of it. (Original \\nText: 每天晚间在楚江 A市的楚江一桥至二桥之\\n间有多小船在河中间钓鱼，有的渔船甚至停在\\n航道中间，严重影响来往的货船通行，希望有\\n关部门管管。 )Transportation8\\u2003 \\u2003 Pei Pan, Yijin Chen\\nMessage \\nnumberUser ID Message topics Time Message Details Category\\n1957 A00012120 Is the art training \\ninstitution for the \\ncollege entrance \\nexamination in Yujiang \\nvillage, Hanpu Town, \\nzone A3 legal?\\n(Original Text: A3 区含浦\\n镇玉江村高考美术培训\\n机构是否合法 ?)2019/6/20\\n10:37:18In Yujiang village, Hanpu Town, A3 Area, there \\nis a college entrance examination art training \\ninstitution called „Tongxing Tianyi”. It runs \\nschools without legal qualifications. I hope \\nthe relevant departments can crackdown on \\nthese illegal training institutions and protect \\nthe legitimate rights and interests of law-\\nabiding citizens. (Original Text: A3 区含浦镇玉\\n江村有一个叫 “同行添艺 ”的高考美术培训机\\n构，在没有取得合法办学资质的情况下非法办\\n学，我希望有关部门能够打击这些非法培训机\\n构，保护守法公民的合法权益。 )Education and \\nSports\\n116962 A000111166 Consultation on \\nmaternity leave \\npayment.\\n(Original Text: 咨询产假\\n工资发放问题。 )2019/7/16\\n10:18:22How does maternity leave pay? What materials \\nneed to be submitted to apply for maternity \\nleave salary? Where can I do it? Is it a one-time \\npayment? Is it paid before maternity leave \\nor monthly? How to calculate the amount of \\nmaternity leave salary? (Original Text: 产假工\\n资怎么发放？申请产假工资需要提交哪些材\\n料？在哪里办理？是一次性发放吗 ?是在休产\\n假之前发放还是按月发放 ?产假工资的金额是\\n怎么计算的？ )Labor and Social \\nSecurity\\n78300 A00068226 The gasoline quality \\nof the petrochemical \\ncompany in F7 county is \\ntoo poor.\\n(Original Text: F7 县的'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='of the petrochemical \\ncompany in F7 county is \\ntoo poor.\\n(Original Text: F7 县的\\n石化公司的汽油质量太\\n差。 )2017/5/25\\n16:50:55Hello! The quality of the gasoline in F7 county \\nis too poor. Every time I refuel, the engine \\nfault light will be on immediately. When I go to \\nthe place where I repair the car, it shows that \\nthe quality of the fuel is poor. After filling oil \\nin other places, the fault light will disappear \\nnaturally. Dear leader, for the benefit of \\nthousands of car owners, please send \\nsomeone to check. (Original Text: 您好 !我们 F7\\n县的汽油质量太差，我每次加油过后发动机故\\n障灯立马就亮，去修车的地方检查显示燃油质\\n量差。在其他地方加完油后，故障灯会自然消\\n失。尊敬的领导，为了千千万万车主的利益，\\n请您派人来查查。 )Business and \\nTourism\\n81060 A1046074052 The family planning \\noffice has yet to deal \\nwith the internal \\nbleeding caused by a \\nligation operation in F8 \\nCity.\\n(Original Text: F8 市的\\n一位市民因做了结扎手\\n术，导致内出血，计生\\n办还没对这起事故做个\\n处理。 )2012/7/24\\n22:51:04Hello, mayor. I’m an ordinary citizen in F8 city. \\nMy wife had a ligation operation in s hospital \\nlast year. On the same day, she had internal \\nbleeding. After arriving at the people’s \\nHospital of F8 City, she was transferred to \\nChuya Hospital of a city. After the rescue, she \\nsaved her life. But the Family Planning Office \\nhas not dealt with the accident up to now, so \\nI have no choice but to ask for help. (Original \\nText: 市长您好，我是 F8市的一个普通老百\\n姓，我老婆去年在 s医院做了个结扎手术，当\\n天就内出血，到了 F8市人民医院后又转到 A市\\n楚雅医院，经过抢救，保住了生命。但是计生\\n办到现在还没有对这起事故做个处理，我无\\n奈求帮助。 )Health and Birth \\nControlContinuedTable 2'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='楚雅医院，经过抢救，保住了生命。但是计生\\n办到现在还没有对这起事故做个处理，我无\\n奈求帮助。 )Health and Birth \\nControlContinuedTable 2\\nSpecific Examples of Public Messages* Automatic Subject Classification of Public Messages in E-government Affairs\\u2003 \\u20039\\n4  Empirical Analysis\\n4.1  Data Sources\\nThe experimental data sets are from the BdRace Data \\nMining Competition Platform of China University Big Data \\nEducation Innovation Alliance. The platform collects the \\nonline government affairs public message data for the \\nperiod November 2010 to January 2020 from the Internet. \\nIt selects and generates the Chinese text classification data \\nsets, and the corpus uses UTF-8 coding. Data sets have \\nseven categories including urban and rural construction, \\nenvironmental protection, transportation, education and \\nsports, labor and social security, business tourism, and \\nhealth and birth control are selected. The amounts of data \\nof each category are shown in Table 1 and there are 9210 \\ntexts in total. Specific examples of public messages are \\nshown in Table 2.\\n4.2  Text Preprocessing\\nIn this stage, the following preprocessing work is carried \\nout for text data.\\n①  Data Cleaning: There are a lot of spaces before and \\nafter the message details column of the data sets, \\nand the non-text information such as punctuation in \\nthe text is not needed to establish the classification \\nmodel. Therefore, first, non-text information such as \\npunctuation is removed;\\n②  Word Segmentation: There is no space between'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='punctuation is removed;\\n②  Word Segmentation: There is no space between \\nChinese words and more than one Chinese word \\ncan be expressed as words with clear meaning. The \\npurpose of Chinese word segmentation is to divide a coherent sentence into words with independent \\nmeaning according to certain segmentation criteria. \\nIn this study, Jieba Chinese word segmentation tool in \\nPython is used to segment the text data sets;\\n③  Remove Stop Words: Modal auxiliary words, \\nconjunctions, and other words have high frequency \\nand low information content, which have no impact on \\nthe subsequent analysis. They belong to noise data. In \\nthis study, we use the stop words list of Harbin Institute \\nof technology to remove stop words from the text data;\\n④  Text Vectorization: To extract the semantic information \\nof words in the vectorization stage, Word2vec method \\nis selected for vectorization. Word2vec uses vectors \\nwith the same dimension to represent the semantic \\ninformation of text, and the distance between \\nvectors can effectively represent the similarity of \\nthe meaning between words, which is an important \\nstep in the digitization of text data. In the Word2vec \\ntraining module provided by gensim database, it is \\nnecessary to adjust the parameters repeatedly to carry \\nout different training to find the best parameters for \\nthe corpus. After repeated training, the study set 8 \\nas the value of training window, set 5 as the value of \\nthe minimum word frequency threshold, and set 250'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='the minimum word frequency threshold, and set 250 \\nas the value of the word vector dimension. The skip-\\ngram model is used for word2vec training.\\n⑤  Oversampling: We can find that the sample is \\nimbalances in the data set from the amount of text \\ndata of each category. Here, the sample balance \\nis achieved by increasing the number of minority \\nsamples in the classification. The smote algorithm \\nis called for oversampling, and some random noise \\nand interference data are added at the same time to \\nprevent the model from overfitting.Table 3\\nComprehensive Analysis of Classification Model Results\\nClassification model Types of message text Accuracy Recall (r) F-score\\nBi-LSTM based on attention \\nmechanismLong text 0.884 0.887 0.886\\nShort text 0.863 0.859 0.862\\nLSTM network Long text 0.833 0.826 0.829\\nShort text 0.842 0.850 0.845\\nLogistic regression Long text 0.839 0.841 0.840\\nShort text 0.772 0.789 0.780\\nNaive bayesian Long text 0.812 0.803 0.807\\nShort text 0.731 0.756 0.743\\nBi-LSTM, Bi-directional long short-term memory; LSTM, Long short-term memory.10\\u2003 \\u2003 Pei Pan, Yijin Chen\\n4.3  Model Parameter Setting\\nThe choice of model structure parameters is critical to \\na good training model. The parameters obtained after \\nadjustment and comparison in the process are the best \\nparameters. The specific workflow of the Bi-LSTM network \\nalgorithm based on the attention mechanism is illustrated \\nas in following Algorithm 1.\\nAlgorithm 1\\nInput: the text data that has been preprocessed'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='as in following Algorithm 1.\\nAlgorithm 1\\nInput: the text data that has been preprocessed\\nOutput: accuracy, recall, and F-score\\nBegin\\nAt the ratio of 4:1, the text data sets are divided into \\ntraining data set and test data set:\\nTraining data set: x_train, Tag: y_train\\nTest data set: x_test, Tag: y_test\\nBi-directional long short-term memory network model \\nbased on attention mechanism was established:\\n①\\t Set 1200 as the value of input nodes, set 64 as the \\nvalue of hidden layer nodes and set 7 as the value of \\noutput layers which means seven categories will be \\noutputted.\\n② 20% dropout was used.\\n③  Set softmax as the activation function and set \\ncategorical_crossentropy as the loss function. The \\noptimizer of training is Adam.\\n④ Use Bi-directional long short-term memory network \\nto calculate the word vector to get the higher level \\nsentence vector.\\n⑤  Attention Layer: the results of bi-directional long \\nshort-term memory network were weighted by \\nattention layer while ATT_SIZE\\xa0=\\xa0100.\\n⑥  Call the fit function, set the iteration number of the \\nmodel to epochs\\xa0 =\\xa0 15, and specify that each batch \\ncontains 128 samples during gradient descent.\\n⑦  Input x_train to the model for training and x_test to \\nthe model for predicting. Accuracy, recall, and F-score \\nare used as indicators to reflect the effect of the model.\\nEnd\\n4.4  Analysis of Experimental Results\\nThe data sets provide two types of message text: “message \\ndetails” and “message topic”, in which “message details” is'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='details” and “message topic”, in which “message details” is \\nlong text and “message topic” is a short text. After training \\nand testing the two types of text by Bi-LSTM network \\nclassifier based on attention mechanism, the accuracy, recall, and F-score results are shown in Table 3. The \\naccuracy, recall, and F-score of LSTM algorithm, logistic \\nregression algorithm, and naive Bayesian algorithm on the \\nlong text and short text data sets are also shown in Table 3.\\nIt can be seen from Table 3 that the accuracy, recall, \\nand F-score of the models established by the four \\nalgorithms are all above 0.8, which has a good effect. \\nAmong them, the Bi-LSTM network algorithm based on \\nattention mechanism performs best, which is 6.88% \\nhigher than the F-score of an ordinary LSTM algorithm \\nand reaches 0.886. In short text classification, logistic \\nregression algorithm and naive Bayesian algorithm \\nhave poor classification effect, while Bi-LSTM network \\nalgorithm based on attention mechanism performs best, \\nwhich is 2.01% higher than the F-score of ordinary LSTM \\nalgorithm and reaches 0.862. In conclusion, the Bi-LSTM \\nnetwork algorithm based on attention mechanism has the \\nbest training effect and it is the most suitable one for the \\nclassification of public messages of e-government.\\n5  Discussion\\n5.1  The Advantages of Bi-LSTM based \\non Attention Mechanism in the Task of \\nClassification of Public Message\\nFirst, the Bi-LSTM network algorithm based on the'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='Classification of Public Message\\nFirst, the Bi-LSTM network algorithm based on the \\nattention mechanism belongs to the deep learning \\nalgorithm. Compared with the normal machine learning \\nalgorithm, it does not need the complicated feature \\ndefinition process (Li, Cao, Wang, & Xiao, 2017). At the \\nsame time, the government message involves a wide range \\nof fields. Due to the limitation of feature engineering, \\nmachine learning has weak generalization ability in this \\nclassification problem, while deep learning can better \\nachieve text classification in large text data, and the model \\nhas better generalization ability. What’s more, this model \\ncan better capture the long-distance dependence because \\nit can learn which information to remember and which \\ninformation to forget through the training process, which \\nis the advantage that machine learning does not have.\\nSecond, on the one hand, Bi-LSTM solves the problem \\nof gradient disappearance or gradient explosion in \\ntraditional RNN. On the other hand, the Bi-LSTM network \\nalgorithm based on attention mechanism can process \\ninput data from front to back and from back to front, i.e., \\nit can better capture bidirectional semantic dependency \\nwhich overcomes the disadvantage that LSTM can’t \\nencode the information from back to front. Automatic Subject Classification of Public Messages in E-government Affairs\\u2003 \\u200311\\nFinally, the introduction of attention mechanism \\nenables the model to give higher weight to important'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='enables the model to give higher weight to important \\ninformation, instead of giving the same weight to all input \\nword vectors as usual, thus improving the accuracy of \\nclassification. In previous studies, the effectiveness of \\nattention mechanism has also been verified. Zhao and \\nWu (2016) proposed a neural network with attention \\nmechanism to improve the performance of sentence \\nclassification, which can capture the information of each \\nword without any external features. Sun (2019) introduced \\nthe attention mechanism based on Gated Recurrent Unit \\n(GRU) model, which improved the classification effect to a \\ncertain extent. It can be seen that the attention mechanism \\nused in this study can play a good role in classification \\nproblems by weighting important information, and it is \\nsuitable for text classification of the public message of \\ne-government.\\n5.2  The Limitations of Bi-LSTM Based \\non Attention Mechanism in the Task of \\nClassification of Public Message\\nFirst, according to previous studies, Bi-LSTM is more \\naccurate for sentences containing multiple aspects, \\nwhile LSTM performs better for sentences with only one \\naspect (Park, Song, & Shin, 2020). In this study, there is \\nno special distinction between sentences with different \\nlevels of meaning, which may lead to some deviation in \\nthe results.\\nSecond, the high dimension of the word vector will \\naffect the performance of Bi-LSTM. The representation of \\ntext is usually a high-dimensional vector. If it is used as'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='text is usually a high-dimensional vector. If it is used as \\nthe input of Bi-LSTM, the parameters of the network will \\nincrease sharply and it is difficult to optimize the network. \\nTo solve this problem, Liu and Guo (2019) proposed a \\nmodel called attention-based Bi-LSTM with convolution \\nlayer, because they believe that convolution can be used \\nto extract the features of text vectors and reduce the \\ndimension of vectors. The same method is also used in the \\nresearch of Guo, Zhao and Cui  (2020). They use CNN to \\nreduce the dimension of the word vector matrix formed \\nby the original data, and then fuse the Bi-LSTM model for \\nemotional analysis, so as to further improve the operation \\nefficiency and prediction accuracy of the model. However, \\nthis study did not use this way to optimize the model. \\nBecause the word vector dimension of the data set in this \\npaper is less, this method is not used to improve the model. \\nHowever, in the future, when facing a larger sample size of \\nthe public message text, the above method may be needed \\nto improve the model.Finally, another problem of the Bi-LSTM network \\nalgorithm based on attention mechanism is that it requires \\na lot of hardware for training, and the linear layer needs \\na lot of memory bandwidth to calculate, which makes the \\nprocess of model training very time-consuming.\\n6  Conclusion\\nIn this study, the Bi-LSTM network algorithm based \\non attention mechanism is used to establish a multi-'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='on attention mechanism is used to establish a multi-\\nclassification model of text based on deep learning, \\naiming to realize the classification of various messages \\nin e-government more accurately. First, this study uses \\nthe Bi-LSTM algorithm to strengthen the relevance of \\nmessages before and after the training process, combines \\nthe characteristics of the attention mechanism, and \\nstrengthens the semantic attention to important text \\nfeatures. Finally, through the full-connection layer \\nfusion feature weight, the classification calculation is \\ncarried out. The experimental results on the Internet \\ngovernment message corpus show that the message \\nclassification model proposed here can achieve a higher \\nF-score and accurate precision, which has certain \\nadvantages compared with three algorithms of LSTM and \\nlogistic regression and naive Bayesian. In addition, the \\nperformance of the four algorithms on the long text and \\nshort text data sets is analyzed and clarified in detail.\\nAt present, further research can be done based on \\ntwo aspects. The first one is to recognize the number of \\naspects that sentences contain. Bi-LSTM can be used for \\nthe classification of sentences with multiple aspects, and \\nLSTM can be used for the classification of sentences with \\na single aspect, so as to further improve the classification \\neffect; second, the granularity of text can be further \\noptimized, and the attention mechanism can be used at'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='optimized, and the attention mechanism can be used at \\ndifferent levels, such as words, words and sentences.\\nReferences\\nCegarra-Navarro, J. G., Pachon, J. R. C., & Cegarra, J. L. M. (2012). \\nE-government and citizen’s engagement with local affairs \\nthrough e-websites: The case of Spanish municipalities. \\nInternational Journal of Information Management, 32 (5), 469–\\n478. doi: 10.1016/j.ijinfomgt.2012.02.008 \\nDurrant, F. (2002). e-Government and the Internet in the Caribbean: \\nAn initial assessment. In R. Traunmüller & K. Lenk (Eds.), Lecture \\nNotes in Computer Science: Vol. 2456. Electronic Government \\n(pp. 101–104). Berlin, Heidelberg: Springer. doi:10.1007/978-3-\\n540-46138-8_1512\\u2003 \\u2003 Pei Pan, Yijin Chen\\nGuo, X. D., Zhao, N., & Cui, S. Z. (2020). Consumer reviews sentiment \\nanalysis based on CNN-BiLSTM. Systems Engineering-Theory \\n& Practice, 40 (03), 653–663. doi:10.12011/1000-6788-2018-\\n1890-11 \\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term \\nmemory. Neural Computation,  9(8), 1735–1780. doi: 10.1162/\\nneco.1997.9.8.1735\\nHolmes, D. (2001). eGov: eBusiness strategies for government.  \\nBoston, MA: Nicholas Brealey Publishing.\\nJeong, H. Y., Lee, T. H., & Hong, S. G. (2017). A corpus analysis of \\nelectronic petitions for improving the responsiveness of public \\nservices: Focusing on Busan petition. The Korean Journal of \\nLocal Government Studies, 21 (1), 423–436. doi: 10.20484/\\nklog.21.1.17\\nKano, E., Fujita, Y., & Tsuda, K. (2019). A method of extracting and'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='klog.21.1.17\\nKano, E., Fujita, Y., & Tsuda, K. (2019). A method of extracting and \\nclassifying local community problems from citizen-report data \\nusing text mining. Procedia Computer Science, 159 , 1347–1356. \\ndoi: 10.1016/j.procs.2019.09.305\\nKim, N., & Hong, S. (2021). Automatic classification of citizen \\nrequests for transportation using deep learning: Case study \\nfrom Boston city. Information Processing & Management, 58 (1), \\n1–13. doi: 10.1016/j.ipm.2020.102410\\nKnutsson, O., Sneiders, E., & Alfalahi, A. (2012, October). \\nOpportunities for improving egovernment: Using language \\ntechnology in workflow management. Proceedings of the 6th \\nInternational Conference on Theory and Practice of Electronic \\nGovernance , 495–496. doi:10.1145/2463728.2463833\\nKu, C. H., & Leroy, G. (2014). A decision support system: Automated \\ncrime report analysis and classification for e-government. \\nGovernment Information Quarterly, 31 (4), 534–544. doi: \\n10.1016/j.giq.2014.08.003\\nLi, J., Cao, Y., Wang, Y., & Xiao, H. (2017). Online learning algorithms \\nfor double-weighted least squares twin bounded support vector \\nmachines. Neural Processing Letters, 45 (1), 319–339. doi: \\n10.1007/s11063-016-9527-9\\nLi, W., Zhu, L., Shi, Y., Guo, K., & Cambria, E. (2020). User reviews: \\nSentiment analysis using lexicon integrated two-channel CNN-\\nLSTM family models. Applied Soft Computing, 94 , 1–11. doi: \\n10.1016/j.asoc.2020.106435 \\nLiu, G., & Guo, J. (2019). Bidirectional LSTM with attention mechanism'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='10.1016/j.asoc.2020.106435 \\nLiu, G., & Guo, J. (2019). Bidirectional LSTM with attention mechanism \\nand convolutional layer for text classification. Neurocomputing, \\n337(APR.14), 325–338. doi: 10.1016/j.neucom.2019.01.078 \\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient \\nestimation of word representations in vector space. ArXiv \\nPreprint. arXiv:1301.3781.\\nMinaee, S., Kalchbrenner, N., Cambria, E., Nikzad, N., Chenaghlu, \\nM., & Gao, J. (2020). Deep learning based text classification: A \\ncomprehensive review. ArXiv Preprint. arXiv:2004.03705.\\nPalangi, H., Deng, L., Shen, Y., Gao, J., He, X., Chen, J ., . . .\\xa0Ward, R. \\n(2016). Deep sentence embedding using long short-term memory \\nnetworks: Analysis and application to information retrieval. \\nIEEE/ACM Transactions on Audio, Speech, and Language \\nProcessing, 24 (4), 694–707. doi:10.1109/TASLP .2016.2520371 \\nPark, H. J., Song, M., & Shin, K. S. (2020). Deep learning models and \\ndatasets for aspect term sentiment classification: Implementing \\nholistic recurrent attention on target-dependent memories. \\nKnowledge-Based Systems, 187 , 104825. doi: 10.1016/j.\\nknosys.2019.06.033\\nSun, M. M. (2019). Chinese text classification based on gru-attention. \\nModern Information Technology, 3 (03), 10–12.Tao, Z., Li, X., Liu, Y., & Liu, X. (2020). Classifying short texts with \\nimproved-attention based bidirectional long memory network. \\nData Analysis and Knowledge Discovery , 3(12), 21–29. doi:'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='Data Analysis and Knowledge Discovery , 3(12), 21–29. doi: \\n10.11925/infotech.2096-3467.2019.0267\\nXie, J., Hou, Y., Wang, Y., Wang, Q., Li, B., Lv, S., & Vorotnitsky, \\nY. I. (2020). Chinese text classification based on attention \\nmechanism and feature-enhanced fusion neural network. \\nComputing, 102 (3), 683–700. doi: 10.1007/s00607-019-\\n00766-9\\nZablith, F., & Osman, I. H.  (2019). Reviewmodus: Text classification \\nand sentiment prediction of unstructured reviews using a hybrid \\ncombination of machine learning and evaluation models. \\nApplied Mathematical Modelling, 71 , 569–583. doi: 10.1016/j.\\napm.2019.02.032\\nZhang, S., Zheng, D., Hu, X., & Yang, M. (2015, October). Bidirectional \\nlong short-term memory networks for relation classification. \\nProceedings of the 29th Pacific Asia conference on language, \\ninformation and computation , 73–78.\\nZhang, W., Wang, M., & Zhu, Y. C. (2020). Does government \\ninformation release really matter in regulating contagion-\\nevolution of negative emotion during public emergencies? From \\nthe perspective of cognitive big data analytics.  International \\nJournal of Information Management, 50 , 498–514. doi: \\n10.1016/j.ijinfomgt.2019.04.001\\nZhao, Z., & Wu, Y. (2016). Attention-based convolutional neural \\nnetworks for sentence classification. INTERSPEECH , 705–709. \\ndoi:10.21437/Interspeech.2016-354\\nZhou, P ., Shi, W., Tian, J., Qi, Z., Li, B., Hao, H., & Xu, B. (2016). \\nAttention-based bidirectional long short-term memory'),\n",
       " Document(metadata={'title': 'Automatic Subject Classification of Public Messages in E-government Affairs', 'year': 2021}, page_content='Attention-based bidirectional long short-term memory \\nnetworks for relation classification. Proceedings of the 54th \\nAnnual Meeting of the Association for Computational Linguistics  \\n(Volume 2: Short Papers), 207–212. doi: 10.18653/v1/P16-2034'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\nfjacobdevlin,mingweichang,kentonl,kristout g@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='be effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based andﬁne-tuning . The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning allpre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='power of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019word based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='train a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level andtoken-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert .\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='words has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiﬁcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).These approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='the left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciﬁc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nﬁne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='viously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\net al., 2018a). Left-to-right language model-BERT BERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nQuestion Paragraph Start/End Span \\nBERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nMasked Sentence A Masked Sentence B \\nPre-training Fine-Tuning NSP Mask LM Mask LM \\nUnlabeled Sentence A and B Pair SQuAD \\nQuestion Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to ﬁne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and ﬁne-tuning . Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel Architecture BERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthetensor2tensor library.1Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='we will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERT BASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERT LARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERT BASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorﬂow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\\n4We note that in the literature the bidirectional Trans-Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g.,hQuestion, Answeri) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='linguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The ﬁrst\\ntoken of every sequence is always a special clas-\\nsiﬁcation token ( [CLS] ). The ﬁnal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiﬁcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ( [SEP] ). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence Aor sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the ﬁnal hidden\\nvector of the special [CLS] token asC2RH,\\nand the ﬁnal hidden vector for the ithinput token\\nasTi2RH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='Task #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right orright-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly “see itself”, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.In order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a “masked\\nLM” (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the ﬁnal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.\\nAlthough this allows us to obtain a bidirec-\\ntional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='tional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nﬁne-tuning, since the [MASK] token does not ap-\\npear during ﬁne-tuning. To mitigate this, we do\\nnot always replace “masked” words with the ac-\\ntual[MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthei-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTiwill be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2: Next Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques-\\ntion Answering (QA) and Natural Language Infer-\\nence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-\\nrectly captured by language modeling. In order\\nto train a model that understands sentence rela-\\ntionships, we pre-train for a binarized next sen-\\ntence prediction task that can be trivially gener-\\nated from any monolingual corpus. Speciﬁcally,\\nwhen choosing the sentences AandBfor each pre-\\ntraining example, 50% of the time Bis the actual\\nnext sentence that follows A(labeled as IsNext ),\\nand 50% of the time it is a random sentence from\\nthe corpus (labeled as NotNext ). As we show\\nin Figure 1, Cis used for next sentence predic-\\ntion (NSP).5Despite its simplicity, we demon-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='in Figure 1, Cis used for next sentence predic-\\ntion (NSP).5Despite its simplicity, we demon-\\nstrate in Section 5.1 that pre-training towards this\\ntask is very beneﬁcial to both QA and NLI.6\\n5The ﬁnal model achieves 97%-98% accuracy on NSP.\\n6The vector Cis not a meaningful sentence representation\\nwithout ﬁne-tuning, since it was trained with NSP.[CLS] helikesplay## ing[SEP] mydogiscute[SEP]Input \\nE[CLS] Ehe Elikes Eplay E## ing E[SEP] Emy Edog Eis Ecute E[SEP] Token \\nEmbeddings \\nEA EB EB EB EB EB EA EA EA EA EASegment \\nEmbeddings \\nE0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5Position \\nEmbeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshufﬂed sentence-level corpus such as the Billion'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='cal to use a document-level corpus rather than a\\nshufﬂed sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks—\\nwhether they involve single text or text pairs—by\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspeciﬁc inputs and outputs into BERT and ﬁne-\\ntune all the parameters end-to-end. At the in-\\nput, sentence Aand sentence Bfrom pre-training\\nare analogous to (1) sentence pairs in paraphras-\\ning, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and(4) a degenerate text- ?pair in text classiﬁcation\\nor sequence tagging. At the output, the token rep-\\nresentations are fed into an output layer for token-\\nlevel tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiﬁcation, such as en-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='answering, and the [CLS] representation is fed\\ninto an output layer for classiﬁcation, such as en-\\ntailment or sentiment analysis.\\nCompared to pre-training, ﬁne-tuning is rela-\\ntively inexpensive. All of the results in the pa-\\nper can be replicated in at most 1 hour on a sin-\\ngle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model.7We de-\\nscribe the task-speciﬁc details in the correspond-\\ning subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT ﬁne-tuning re-\\nsults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col-\\nlection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo ﬁne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the ﬁnal hid-\\nden vectorC2RHcorresponding to the ﬁrst\\ninput token ( [CLS] ) as the aggregate representa-\\ntion. The only new parameters introduced during\\nﬁne-tuning are classiﬁcation layer weights W2\\nRK\\x02H, whereKis the number of labels. We com-\\npute a standard classiﬁcation loss with CandW,\\ni.e.,log(softmax( CWT)).\\n7For example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8See (10) in https://gluebenchmark.com/faq .System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\\nWe use a batch size of 32 and ﬁne-tune for 3\\nepochs over the data for all GLUE tasks. For each\\ntask, we selected the best ﬁne-tuning learning rate\\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\\nAdditionally, for BERT LARGE we found that ﬁne-\\ntuning was sometimes unstable on small datasets,\\nso we ran several random restarts and selected the\\nbest model on the Dev set. With random restarts,\\nwe use the same pre-trained checkpoint but per-\\nform different ﬁne-tuning data shufﬂing and clas-\\nsiﬁer layer initialization.9\\nResults are presented in Table 1. Both'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='siﬁer layer initialization.9\\nResults are presented in Table 1. Both\\nBERT BASE and BERT LARGE outperform all sys-\\ntems on all tasks by a substantial margin, obtaining\\n4.5% and 7.0% respective average accuracy im-\\nprovement over the prior state of the art. Note that\\nBERT BASE and OpenAI GPT are nearly identical\\nin terms of model architecture apart from the at-\\ntention masking. For the largest and most widely\\nreported GLUE task, MNLI, BERT obtains a 4.6%\\nabsolute accuracy improvement. On the ofﬁcial\\nGLUE leaderboard10, BERT LARGE obtains a score\\nof 80.5, compared to OpenAI GPT, which obtains\\n72.8 as of the date of writing.\\nWe ﬁnd that BERT LARGE signiﬁcantly outper-\\nforms BERT BASE across all tasks, especially those\\nwith very little training data. The effect of model\\nsize is explored more thoroughly in Section 5.2.\\n4.2 SQuAD v1.1\\nThe Stanford Question Answering Dataset\\n(SQuAD v1.1) is a collection of 100k crowd-\\nsourced question/answer pairs (Rajpurkar et al.,\\n2016). Given a question and a passage from\\n9The GLUE data set distribution does not include the Test\\nlabels, and we only made a single GLUE evaluation server\\nsubmission for each of BERT BASE and BERT LARGE .\\n10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to\\npredict the answer text span in the passage.\\nAs shown in Figure 1, in the question answer-\\ning task, we represent the input question and pas-\\nsage as a single packed sequence, with the ques-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='ing task, we represent the input question and pas-\\nsage as a single packed sequence, with the ques-\\ntion using the Aembedding and the passage using\\ntheBembedding. We only introduce a start vec-\\ntorS2RHand an end vector E2RHduring\\nﬁne-tuning. The probability of word ibeing the\\nstart of the answer span is computed as a dot prod-\\nuct between TiandSfollowed by a softmax over\\nall of the words in the paragraph: Pi=eS\\x01TiP\\njeS\\x01Tj.\\nThe analogous formula is used for the end of the\\nanswer span. The score of a candidate span from\\npositionito positionjis deﬁned as S\\x01Ti+E\\x01Tj,\\nand the maximum scoring span where j\\x15iis\\nused as a prediction. The training objective is the\\nsum of the log-likelihoods of the correct start and\\nend positions. We ﬁne-tune for 3 epochs with a\\nlearning rate of 5e-5 and a batch size of 32.\\nTable 2 shows top leaderboard entries as well\\nas results from top published systems (Seo et al.,\\n2017; Clark and Gardner, 2018; Peters et al.,\\n2018a; Hu et al., 2018). The top results from the\\nSQuAD leaderboard do not have up-to-date public\\nsystem descriptions available,11and are allowed to\\nuse any public data when training their systems.\\nWe therefore use modest data augmentation in\\nour system by ﬁrst ﬁne-tuning on TriviaQA (Joshi\\net al., 2017) befor ﬁne-tuning on SQuAD.\\nOur best performing system outperforms the top\\nleaderboard system by +1.5 F1 in ensembling and\\n+1.3 F1 as a single system. In fact, our single\\nBERT model outperforms the top ensemble sys-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='+1.3 F1 as a single system. In fact, our single\\nBERT model outperforms the top ensemble sys-\\ntem in terms of F1 score. Without TriviaQA ﬁne-\\n11QANet is described in Yu et al. (2018), but the system\\nhas improved substantially after publication.System Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman - - 82.3 91.2\\n#1 Ensemble - nlnet - - 86.0 91.7\\n#2 Ensemble - QANet - - 84.5 90.5\\nPublished\\nBiDAF+ELMo (Single) - 85.6 - 85.8\\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\\nOurs\\nBERT BASE (Single) 80.8 88.5 - -\\nBERT LARGE (Single) 84.1 90.9 - -\\nBERT LARGE (Ensemble) 85.8 91.8 - -\\nBERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\\nBERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\\nTable 2: SQuAD 1.1 results. The BERT ensemble\\nis 7x systems which use different pre-training check-\\npoints and ﬁne-tuning seeds.\\nSystem Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman 86.3 89.0 86.9 89.5\\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\\n#2 Single - nlnet - - 74.2 77.1\\nPublished\\nunet (Ensemble) - - 71.4 74.9\\nSLQA+ (Single) - 71.4 74.4\\nOurs\\nBERT LARGE (Single) 78.7 81.9 80.0 83.1\\nTable 3: SQuAD 2.0 results. We exclude entries that\\nuse BERT as one of their components.\\ntuning data, we only lose 0.1-0.4 F1, still outper-\\nforming all existing systems by a wide margin.12\\n4.3 SQuAD v2.0\\nThe SQuAD 2.0 task extends the SQuAD 1.1\\nproblem deﬁnition by allowing for the possibility\\nthat no short answer exists in the provided para-\\ngraph, making the problem more realistic.'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='that no short answer exists in the provided para-\\ngraph, making the problem more realistic.\\nWe use a simple approach to extend the SQuAD\\nv1.1 BERT model for this task. We treat ques-\\ntions that do not have an answer as having an an-\\nswer span with start and end at the [CLS] to-\\nken. The probability space for the start and end\\nanswer span positions is extended to include the\\nposition of the [CLS] token. For prediction, we\\ncompare the score of the no-answer span: snull=\\nS\\x01C+E\\x01Cto the score of the best non-null span\\n12The TriviaQA data we used consists of paragraphs from\\nTriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,\\nthat contain at least one of the provided possible answers.System Dev Test\\nESIM+GloVe 51.9 52.7\\nESIM+ELMo 59.1 59.2\\nOpenAI GPT - 78.0\\nBERT BASE 81.6 -\\nBERT LARGE 86.6 86.3\\nHuman (expert)y- 85.0\\nHuman (5 annotations)y- 88.0\\nTable 4: SWAG Dev and Test accuracies.yHuman per-\\nformance is measured with 100 samples, as reported in\\nthe SWAG paper.\\n^si;j=maxj\\x15iS\\x01Ti+E\\x01Tj. We predict a non-null\\nanswer when ^si;j> s null+\\x1c, where the thresh-\\nold\\x1cis selected on the dev set to maximize F1.\\nWe did not use TriviaQA data for this model. We\\nﬁne-tuned for 2 epochs with a learning rate of 5e-5\\nand a batch size of 48.\\nThe results compared to prior leaderboard en-\\ntries and top published work (Sun et al., 2018;\\nWang et al., 2018b) are shown in Table 3, exclud-\\ning systems that use BERT as one of their com-\\nponents. We observe a +5.1 F1 improvement over'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='ing systems that use BERT as one of their com-\\nponents. We observe a +5.1 F1 improvement over\\nthe previous best system.\\n4.4 SWAG\\nThe Situations With Adversarial Generations\\n(SWAG) dataset contains 113k sentence-pair com-\\npletion examples that evaluate grounded common-\\nsense inference (Zellers et al., 2018). Given a sen-\\ntence, the task is to choose the most plausible con-\\ntinuation among four choices.\\nWhen ﬁne-tuning on the SWAG dataset, we\\nconstruct four input sequences, each containing\\nthe concatenation of the given sentence (sentence\\nA) and a possible continuation (sentence B). The\\nonly task-speciﬁc parameters introduced is a vec-\\ntor whose dot product with the [CLS] token rep-\\nresentation Cdenotes a score for each choice\\nwhich is normalized with a softmax layer.\\nWe ﬁne-tune the model for 3 epochs with a\\nlearning rate of 2e-5 and a batch size of 16. Re-\\nsults are presented in Table 4. BERT LARGE out-\\nperforms the authors’ baseline ESIM+ELMo sys-\\ntem by +27.1% and OpenAI GPT by 8.3%.\\n5 Ablation Studies\\nIn this section, we perform ablation experiments\\nover a number of facets of BERT in order to better\\nunderstand their relative importance. AdditionalDev Set\\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\\n(Acc) (Acc) (Acc) (Acc) (F1)\\nBERT BASE 84.4 88.4 86.7 92.7 88.5\\nNo NSP 83.9 84.9 86.5 92.6 87.9\\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\\nTable 5: Ablation over the pre-training tasks using the\\nBERT BASE architecture. “No NSP” is trained without'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='BERT BASE architecture. “No NSP” is trained without\\nthe next sentence prediction task. “LTR & No NSP” is\\ntrained as a left-to-right LM without the next sentence\\nprediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\\ndomly initialized BiLSTM on top of the “LTR + No\\nNSP” model during ﬁne-tuning.\\nablation studies can be found in Appendix C.\\n5.1 Effect of Pre-training Tasks\\nWe demonstrate the importance of the deep bidi-\\nrectionality of BERT by evaluating two pre-\\ntraining objectives using exactly the same pre-\\ntraining data, ﬁne-tuning scheme, and hyperpa-\\nrameters as BERT BASE :\\nNo NSP : A bidirectional model which is trained\\nusing the “masked LM” (MLM) but without the\\n“next sentence prediction” (NSP) task.\\nLTR & No NSP : A left-context-only model which\\nis trained using a standard Left-to-Right (LTR)\\nLM, rather than an MLM. The left-only constraint\\nwas also applied at ﬁne-tuning, because removing\\nit introduced a pre-train/ﬁne-tune mismatch that\\ndegraded downstream performance. Additionally,\\nthis model was pre-trained without the NSP task.\\nThis is directly comparable to OpenAI GPT, but\\nusing our larger training dataset, our input repre-\\nsentation, and our ﬁne-tuning scheme.\\nWe ﬁrst examine the impact brought by the NSP\\ntask. In Table 5, we show that removing NSP\\nhurts performance signiﬁcantly on QNLI, MNLI,\\nand SQuAD 1.1. Next, we evaluate the impact\\nof training bidirectional representations by com-\\nparing “No NSP” to “LTR & No NSP”. The LTR'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='of training bidirectional representations by com-\\nparing “No NSP” to “LTR & No NSP”. The LTR\\nmodel performs worse than the MLM model on all\\ntasks, with large drops on MRPC and SQuAD.\\nFor SQuAD it is intuitively clear that a LTR\\nmodel will perform poorly at token predictions,\\nsince the token-level hidden states have no right-\\nside context. In order to make a good faith at-\\ntempt at strengthening the LTR system, we added\\na randomly initialized BiLSTM on top. This does\\nsigniﬁcantly improve results on SQuAD, but theresults are still far worse than those of the pre-\\ntrained bidirectional models. The BiLSTM hurts\\nperformance on the GLUE tasks.\\nWe recognize that it would also be possible to\\ntrain separate LTR and RTL models and represent\\neach token as the concatenation of the two mod-\\nels, as ELMo does. However: (a) this is twice as\\nexpensive as a single bidirectional model; (b) this\\nis non-intuitive for tasks like QA, since the RTL\\nmodel would not be able to condition the answer\\non the question; (c) this it is strictly less powerful\\nthan a deep bidirectional model, since it can use\\nboth left and right context at every layer.\\n5.2 Effect of Model Size\\nIn this section, we explore the effect of model size\\non ﬁne-tuning task accuracy. We trained a number\\nof BERT models with a differing number of layers,\\nhidden units, and attention heads, while otherwise\\nusing the same hyperparameters and training pro-\\ncedure as described previously.\\nResults on selected GLUE tasks are shown in'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='cedure as described previously.\\nResults on selected GLUE tasks are shown in\\nTable 6. In this table, we report the average Dev\\nSet accuracy from 5 random restarts of ﬁne-tuning.\\nWe can see that larger models lead to a strict ac-\\ncuracy improvement across all four datasets, even\\nfor MRPC which only has 3,600 labeled train-\\ning examples, and is substantially different from\\nthe pre-training tasks. It is also perhaps surpris-\\ning that we are able to achieve such signiﬁcant\\nimprovements on top of models which are al-\\nready quite large relative to the existing literature.\\nFor example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018). By contrast, BERT BASE\\ncontains 110M parameters and BERT LARGE con-\\ntains 340M parameters.\\nIt has long been known that increasing the\\nmodel size will lead to continual improvements\\non large-scale tasks such as machine translation\\nand language modeling, which is demonstrated\\nby the LM perplexity of held-out training data\\nshown in Table 6. However, we believe that\\nthis is the ﬁrst work to demonstrate convinc-\\ningly that scaling to extreme model sizes also\\nleads to large improvements on very small scale\\ntasks, provided that the model has been sufﬁ-\\nciently pre-trained. Peters et al. (2018b) presentedmixed results on the downstream task impact of'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='ciently pre-trained. Peters et al. (2018b) presentedmixed results on the downstream task impact of\\nincreasing the pre-trained bi-LM size from two\\nto four layers and Melamud et al. (2016) men-\\ntioned in passing that increasing hidden dimen-\\nsion size from 200 to 600 helped, but increasing\\nfurther to 1,000 did not bring further improve-\\nments. Both of these prior works used a feature-\\nbased approach — we hypothesize that when the\\nmodel is ﬁne-tuned directly on the downstream\\ntasks and uses only a very small number of ran-\\ndomly initialized additional parameters, the task-\\nspeciﬁc models can beneﬁt from the larger, more\\nexpressive pre-trained representations even when\\ndownstream task data is very small.\\n5.3 Feature-based Approach with BERT\\nAll of the BERT results presented so far have used\\nthe ﬁne-tuning approach, where a simple classiﬁ-\\ncation layer is added to the pre-trained model, and\\nall parameters are jointly ﬁne-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere ﬁxed features are extracted from the pre-\\ntrained model, has certain advantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore require\\na task-speciﬁc model architecture to be added.\\nSecond, there are major computational beneﬁts\\nto pre-compute an expensive representation of the\\ntraining data once and then run many experiments\\nwith cheaper models on top of this representation.\\nIn this section, we compare the two approaches'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='with cheaper models on top of this representation.\\nIn this section, we compare the two approaches\\nby applying BERT to the CoNLL-2003 Named\\nEntity Recognition (NER) task (Tjong Kim Sang\\nand De Meulder, 2003). In the input to BERT, we\\nuse a case-preserving WordPiece model, and we\\ninclude the maximal document context provided\\nby the data. Following standard practice, we for-\\nmulate this as a tagging task but do not use a CRF\\nHyperparams Dev Set Accuracy\\n#L #H #A LM (ppl) MNLI-m MRPC SST-2\\n3 768 12 5.84 77.9 79.8 88.4\\n6 768 3 5.24 80.6 82.2 90.7\\n6 768 12 4.68 81.9 84.8 91.3\\n12 768 12 3.99 84.4 86.7 92.9\\n12 1024 16 3.54 85.7 86.9 93.3\\n24 1024 16 3.23 86.6 87.8 93.7\\nTable 6: Ablation over BERT model size. #L = the\\nnumber of layers; #H = hidden size; #A = number of at-\\ntention heads. “LM (ppl)” is the masked LM perplexity\\nof held-out training data.System Dev F1 Test F1\\nELMo (Peters et al., 2018a) 95.7 92.2\\nCVT (Clark et al., 2018) - 92.6\\nCSE (Akbik et al., 2018) - 93.1\\nFine-tuning approach\\nBERT LARGE 96.6 92.8\\nBERT BASE 96.4 92.4\\nFeature-based approach (BERT BASE)\\nEmbeddings 91.0 -\\nSecond-to-Last Hidden 95.6 -\\nLast Hidden 94.9 -\\nWeighted Sum Last Four Hidden 95.9 -\\nConcat Last Four Hidden 96.1 -\\nWeighted Sum All 12 Layers 95.5 -\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\nlayer in the output. We use the representation of'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='5 random restarts using those hyperparameters.\\nlayer in the output. We use the representation of\\nthe ﬁrst sub-token as the input to the token-level\\nclassiﬁer over the NER label set.\\nTo ablate the ﬁne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without ﬁne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classiﬁcation layer.\\nResults are presented in Table 7. BERT LARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind ﬁne-tuning the entire model. This\\ndemonstrates that BERT is effective for both ﬁne-\\ntuning and feature-based approaches.\\n6 Conclusion\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to beneﬁt from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these ﬁndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.References\\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\\n2018. Contextual string embeddings for sequence'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='Alan Akbik, Duncan Blythe, and Roland V ollgraf.\\n2018. Contextual string embeddings for sequence\\nlabeling. In Proceedings of the 27th International\\nConference on Computational Linguistics , pages\\n1638–1649.\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444 .\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch , 6(Nov):1817–1853.\\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe ﬁfth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing , pages 120–128. Association for Computa-\\ntional Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nInEMNLP . Association for Computational Linguis-\\ntics.\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural language.\\nComputational linguistics , 18(4):467–479.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\ncrosslingual focused evaluation. In Proceedings\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017) , pages 1–14, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005 .\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\nQuora question pairs.\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.Kevin Clark, Minh-Thang Luong, Christopher D Man-\\nning, and Quoc Le. 2018. Semi-supervised se-\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing , pages 1914–\\n1925.\\nRonan Collobert and Jason Weston. 2008. A uniﬁed\\narchitecture for natural language processing: Deep\\nneural networks with multitask learning. In Pro-\\nceedings of the 25th international conference on\\nMachine learning , pages 160–167. ACM.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='natural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing , pages 670–680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems , pages 3079–3087.\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. 2009. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPR09 .\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nInProceedings of the Third International Workshop\\non Paraphrasing (IWP2005) .\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via ﬁlling in\\nthe.arXiv preprint arXiv:1801.07736 .\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR , abs/1606.08415.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nfrom unlabelled data. In Proceedings of the 2016\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies . Association for Computa-\\ntional Linguistics.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model ﬁne-tuning for text classiﬁcation. In\\nACL. Association for Computational Linguistics.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='ACL. Association for Computational Linguistics.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nFuru Wei, and Ming Zhou. 2018. Reinforced\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI .\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR ,\\nabs/1705.00557.Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors. In\\nAdvances in neural information processing systems ,\\npages 3294–3302.\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning , pages\\n1188–1196.\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge. In\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning , volume 46, page 47.\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nefﬁcient framework for learning sentence represen-\\ntations. In International Conference on Learning\\nRepresentations .\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS .\\nOren Melamud, Jacob Goldberger, and Ido Dagan.'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='textualized word vectors. In NIPS .\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL .\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26 , pages 3111–3119. Curran Associates,\\nInc.\\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\\nable hierarchical distributed language model. In\\nD. Koller, D. Schuurmans, Y . Bengio, and L. Bot-\\ntou, editors, Advances in Neural Information Pro-\\ncessing Systems 21 , pages 1081–1088. Curran As-\\nsociates, Inc.\\nAnkur P Parikh, Oscar T ¨ackstr ¨om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In EMNLP .\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. Glove: Global vectors for\\nword representation. In Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 1532–\\n1543.\\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\\nula, and Russell Power. 2017. Semi-supervised se-\\nquence tagging with bidirectional language models.\\nInACL.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018a. Deep contextualized word rep-\\nresentations. In NAACL .Matthew Peters, Mark Neumann, Luke Zettlemoyer,\\nand Wen-tau Yih. 2018b. Dissecting contextual'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='and Wen-tau Yih. 2018b. Dissecting contextual\\nword embeddings: Architecture and representation.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n1499–1509.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018. Improving language under-\\nstanding with unsupervised learning. Technical re-\\nport, OpenAI.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Nat-\\nural Language Processing , pages 2383–2392.\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017. Bidirectional attention\\nﬂow for machine comprehension. In ICLR .\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Proceedings of the 2013 conference on\\nempirical methods in natural language processing ,\\npages 1631–1642.\\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\\n2018. U-net: Machine reading comprehension\\nwith unanswerable questions. arXiv preprint\\narXiv:1810.06638 .\\nWilson L Taylor. 1953. Cloze procedure: A new\\ntool for measuring readability. Journalism Bulletin ,\\n30(4):415–433.\\nErik F Tjong Kim Sang and Fien De Meulder.\\n2003. Introduction to the conll-2003 shared task:\\nLanguage-independent named entity recognition. In\\nCoNLL .'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='Language-independent named entity recognition. In\\nCoNLL .\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\\nWord representations: A simple and general method\\nfor semi-supervised learning. In Proceedings of the\\n48th Annual Meeting of the Association for Compu-\\ntational Linguistics , ACL ’10, pages 384–394.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems , pages 6000–6010.\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\\nPierre-Antoine Manzagol. 2008. Extracting and\\ncomposing robust features with denoising autoen-\\ncoders. In Proceedings of the 25th international\\nconference on Machine learning , pages 1096–1103.\\nACM.\\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\\nGlue: A multi-task benchmark and analysis platformfor natural language understanding. In Proceedings\\nof the 2018 EMNLP Workshop BlackboxNLP: An-\\nalyzing and Interpreting Neural Networks for NLP ,\\npages 353–355.\\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\\ngranularity hierarchical attention fusion networks\\nfor reading comprehension and question answering.\\nInProceedings of the 56th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers) . Association for Computational Lin-\\nguistics.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='guistics.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\\nman. 2018. Neural network acceptability judg-\\nments. arXiv preprint arXiv:1805.12471 .\\nAdina Williams, Nikita Nangia, and Samuel R Bow-\\nman. 2018. A broad-coverage challenge corpus\\nfor sentence understanding through inference. In\\nNAACL .\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google’s neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint\\narXiv:1609.08144 .\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\\nLipson. 2014. How transferable are features in deep\\nneural networks? In Advances in neural information\\nprocessing systems , pages 3320–3328.\\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\\nLe. 2018. QANet: Combining local convolution\\nwith global self-attention for reading comprehen-\\nsion. In ICLR .\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\\nChoi. 2018. Swag: A large-scale adversarial dataset\\nfor grounded commonsense inference. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP) .\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\\nFidler. 2015. Aligning books and movies: Towards\\nstory-like visual explanations by watching movies\\nand reading books. In Proceedings of the IEEE'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='story-like visual explanations by watching movies\\nand reading books. In Proceedings of the IEEE\\ninternational conference on computer vision , pages\\n19–27.\\nAppendix for “BERT: Pre-training of\\nDeep Bidirectional Transformers for\\nLanguage Understanding”\\nWe organize the appendix into three sections:\\n• Additional implementation details for BERT\\nare presented in Appendix A;• Additional details for our experiments are\\npresented in Appendix B; and\\n• Additional ablation studies are presented in\\nAppendix C.\\nWe present additional ablation studies for\\nBERT including:\\n–Effect of Number of Training Steps; and\\n–Ablation for Different Masking Proce-\\ndures.\\nA Additional Details for BERT\\nA.1 Illustration of the Pre-training Tasks\\nWe provide examples of the pre-training tasks in\\nthe following.\\nMasked LM and the Masking Procedure As-\\nsuming the unlabeled sentence is my dog is\\nhairy , and during the random masking procedure\\nwe chose the 4-th token (which corresponding to\\nhairy ), our masking procedure can be further il-\\nlustrated by\\n• 80% of the time: Replace the word with the\\n[MASK] token, e.g., my dog is hairy !\\nmy dog is [MASK]\\n• 10% of the time: Replace the word with a\\nrandom word, e.g., my dog is hairy !my\\ndog is apple\\n• 10% of the time: Keep the word un-\\nchanged, e.g., my dog is hairy !my dog\\nis hairy . The purpose of this is to bias the\\nrepresentation towards the actual observed\\nword.\\nThe advantage of this procedure is that the\\nTransformer encoder does not know which words'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='word.\\nThe advantage of this procedure is that the\\nTransformer encoder does not know which words\\nit will be asked to predict or which have been re-\\nplaced by random words, so it is forced to keep\\na distributional contextual representation of ev-\\neryinput token. Additionally, because random\\nreplacement only occurs for 1.5% of all tokens\\n(i.e., 10% of 15%), this does not seem to harm\\nthe model’s language understanding capability. In\\nSection C.2, we evaluate the impact this proce-\\ndure.\\nCompared to standard langauge model training,\\nthe masked LM only make predictions on 15% of\\ntokens in each batch, which suggests that more\\npre-training steps may be required for the modelBERT (Ours) \\nTrm Trm Trm\\nTrm Trm Trm...\\n...Trm Trm Trm\\nTrm Trm Trm...\\n...OpenAI GPT \\nLstm ELMo \\nLstm Lstm \\nLstm Lstm Lstm Lstm Lstm Lstm \\nLstm Lstm Lstm  T1 T2 TN...\\n...\\n......\\n...\\n E1 E2 EN... T1 T2TN...\\n E1 E2 EN ... T1 T2 TN...\\n E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\\nOpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach.\\nto converge. In Section C.1 we demonstrate that'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='to converge. In Section C.1 we demonstrate that\\nMLM does converge marginally slower than a left-\\nto-right model (which predicts every token), but\\nthe empirical improvements of the MLM model\\nfar outweigh the increased training cost.\\nNext Sentence Prediction The next sentence\\nprediction task can be illustrated in the following\\nexamples.\\nInput =[CLS] the man went to [MASK] store [SEP]\\nhe bought a gallon [MASK] milk [SEP]\\nLabel =IsNext\\nInput =[CLS] the man [MASK] to the store [SEP]\\npenguin [MASK] are flight ##less birds [SEP]\\nLabel =NotNext\\nA.2 Pre-training Procedure\\nTo generate each training input sequence, we sam-\\nple two spans of text from the corpus, which we\\nrefer to as “sentences” even though they are typ-\\nically much longer than single sentences (but can\\nbe shorter also). The ﬁrst sentence receives the A\\nembedding and the second receives the Bembed-\\nding. 50% of the time Bis the actual next sentence\\nthat follows Aand 50% of the time it is a random\\nsentence, which is done for the “next sentence pre-\\ndiction” task. They are sampled such that the com-\\nbined length is\\x14512 tokens. The LM masking is\\napplied after WordPiece tokenization with a uni-\\nform masking rate of 15%, and no special consid-\\neration given to partial word pieces.\\nWe train with batch size of 256 sequences (256\\nsequences * 512 tokens = 128,000 tokens/batch)\\nfor 1,000,000 steps, which is approximately 40epochs over the 3.3 billion word corpus. We\\nuse Adam with learning rate of 1e-4, \\x0c1= 0:9,'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='use Adam with learning rate of 1e-4, \\x0c1= 0:9,\\n\\x0c2= 0:999, L2 weight decay of 0:01, learning\\nrate warmup over the ﬁrst 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob-\\nability of 0.1 on all layers. We use a gelu acti-\\nvation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu , following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERT BASE was performed on 4\\nCloud TPUs in Pod conﬁguration (16 TPU chips\\ntotal).13Training of BERT LARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre-\\ntraining took 4 days to complete.\\nLonger sequences are disproportionately expen-\\nsive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor ﬁne-tuning, most model hyperparameters are\\nthe same as in pre-training, with the exception of\\nthe batch size, learning rate, and number of train-\\ning epochs. The dropout probability was always\\nkept at 0.1. The optimal hyperparameter values\\nare task-speciﬁc, but we found the following range\\nof possible values to work well across all tasks:\\n•Batch size : 16, 32\\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\\nTPU-now-offers-preemptible-pricing-and-global-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='TPU-now-offers-preemptible-pricing-and-global-\\navailability.html•Learning rate (Adam) : 5e-5, 3e-5, 2e-5\\n•Number of epochs : 2, 3, 4\\nWe also observed that large data sets (e.g.,\\n100k+ labeled training examples) were far less\\nsensitive to hyperparameter choice than small data\\nsets. Fine-tuning is typically very fast, so it is rea-\\nsonable to simply run an exhaustive search over\\nthe above parameters and choose the model that\\nperforms best on the development set.\\nA.4 Comparison of BERT, ELMo ,and\\nOpenAI GPT\\nHere we studies the differences in recent popular\\nrepresentation learning models including ELMo,\\nOpenAI GPT and BERT. The comparisons be-\\ntween the model architectures are shown visually\\nin Figure 3. Note that in addition to the architec-\\nture differences, BERT and OpenAI GPT are ﬁne-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='between how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n• GPT uses a sentence separator ( [SEP] ) and\\nclassiﬁer token ( [CLS] ) which are only in-\\ntroduced at ﬁne-tuning time; BERT learns\\n[SEP] ,[CLS] and sentence A/Bembed-\\ndings during pre-training.\\n• GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n• GPT used the same learning rate of 5e-5 for\\nall ﬁne-tuning experiments; BERT chooses a\\ntask-speciﬁc ﬁne-tuning learning rate which\\nperforms the best on the development set.To isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\nA.5 Illustrations of Fine-tuning on Different\\nTasks\\nThe illustration of ﬁne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speciﬁc\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks. In\\nthe ﬁgure,Erepresents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classiﬁcation out-\\nput, and [SEP] is the special symbol to separate'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='[CLS] is the special symbol for classiﬁcation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\nB Detailed Experimental Setup\\nB.1 Detailed Descriptions for the GLUE\\nBenchmark Experiments.\\nOur GLUE results in Table1 are obtained\\nfrom https://gluebenchmark.com/\\nleaderboard and https://blog.\\nopenai.com/language-unsupervised .\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiﬁ-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment ,contradiction , or\\nneutral with respect to the ﬁrst one.\\nQQP Quora Question Pairs is a binary classiﬁ-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classiﬁcation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.BERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok \\n1 [SEP] ...Tok \\nNTok \\n1...Tok\\nM\\nQuestion Paragraph BERT'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='C\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok \\n1 [SEP] ...Tok \\nNTok \\n1...Tok\\nM\\nQuestion Paragraph BERT \\nE[CLS] E1 E2 EN\\nC\\nT1\\n T2\\n TN\\nSingle Sentence ...\\n...BERT \\nTok 1 Tok 2 Tok N ... [CLS]E[CLS] E1 E2 EN\\nC\\nT1\\n T2\\n TN\\nSingle Sentence \\nB-PER O O...\\n... E[CLS] E1 E[SEP] Class \\nLabel \\n... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\nStart/End Span Class \\nLabel \\nBERT \\nTok 1 Tok 2 Tok N ... [CLS]Tok 1[CLS] [CLS] Tok \\n1 [SEP] ...Tok \\nNTok \\n1...Tok\\nM\\nSentence 1 \\n...Sentence 2 \\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classiﬁcation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classiﬁcation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically “acceptable” or not (Warstadt\\net al., 2018).\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotationsfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\nRTE Recognizing Textual Entailment is a bi-'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='cally equivalent (Dolan and Brockett, 2005).\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset,15and every\\ntrained system that’s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n14Note that we only report single-task ﬁne-tuning results\\nin this paper. A multitask ﬁne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n15https://gluebenchmark.com/faqjority class.\\nC Additional Ablation Studies\\nC.1 Effect of Number of Training Steps\\nFigure 5 presents MNLI Dev accuracy after ﬁne-\\ntuning from a checkpoint that has been pre-trained\\nforksteps. This allows us to answer the following\\nquestions:\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh ﬁne-tuning accuracy?\\nAnswer: Yes, BERT BASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n2. Question: Does MLM pre-training converge'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='trained on 1M steps compared to 500k steps.\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\nC.2 Ablation for Different Masking\\nProcedures\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n200 400 600 800 1;0007678808284\\nPre-training Steps (Thousands)MNLI Dev Accuracy\\nBERT BASE (Masked LM)\\nBERT BASE (Left-to-Right)\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after ﬁne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand ﬁne-tuning, as the [MASK] symbol never ap-\\npears during the ﬁne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both ﬁne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npliﬁed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\nMasking Rates Dev Set Results\\nMASK SAME RND MNLI NER'),\n",
       " Document(metadata={'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'year': 2018}, page_content='tions.\\nMasking Rates Dev Set Results\\nMASK SAME RND MNLI NER\\nFine-tune Fine-tune Feature-based\\n80% 10% 10% 84.2 95.4 94.9\\n100% 0% 0% 84.3 94.9 94.0\\n80% 0% 20% 84.1 95.2 94.6\\n80% 20% 0% 84.4 95.2 94.7\\n0% 20% 80% 83.7 94.8 94.6\\n0% 0% 100% 83.6 94.9 94.6\\nTable 8: Ablation over different masking strategies.\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe[MASK] symbol for MLM; S AME means that\\nwe keep the target token as is; R NDmeans that\\nwe replace the target token with another random\\ntoken.\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speciﬁc strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\nFrom the table it can be seen that ﬁne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the M ASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe R NDstrategy performs much worse than our\\nstrategy as well.'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='CCDR • June 4, 2020 • Vol. 46 No. 6 Page 161 OVERVIEW\\nChallenges and opportunities for public health \\nmade possible by advances in natural language processing\\nOliver Baclic1*, Matthew Tunis1, Kelsey Young1, Coraline Doan2, Howard Swerdfeger2, \\nJustin\\xa0Schonfeld3*\\nAffiliations\\n1 Centre for Immunization and \\nRespiratory Infectious Disease, Public Health Agency of Canada, Ottawa, ON\\n2 Data, Partnerships and \\nInnovation Hub, Public Health Agency of Canada, Ottawa, ON\\n3 National Microbiology \\nLaboratory, Public Health Agency of Canada, Winnipeg, MB\\n*Correspondence:\\noliver.baclic@canada.ca\\njustin.schonfeld@canada.ca\\nThis work is licensed under a Creative \\nCommons Attribution 4.0 International \\nLicense.\\nIntroduction\\nThere is a growing interest in deploying artificial intelligence \\n(AI) strategies to achieve public health outcomes, particularly \\nin response to the global coronavirus disease 2019 (COVID-19) \\npandemic where novel datasets, surveillance tools and models are emerging very quickly. \\nThe objective of this manuscript is to provide a framework for \\nconsidering natural language processing (NLP) approaches \\nto public health based on historical applications. This \\noverview includes a brief introduction to AI and NLP , suggests opportunities where NLP can be applied to public health \\nproblems and describes the challenges of applying NLP in \\na public health context. Particular articles were chosen to emphasize the breadth of potential applications for NLP in public'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='health as well as the not inconsiderable challenges and risks \\ninherent in incorporating AI/NLP in public health analysis and decision support.Artificial intelligence and natural \\nlanguage processing\\nAI research has produced models that can interpret a radiograph \\n(1,2), detect irregular heartbeats using a smartwatch (3), \\nautomatically identify reports of infectious disease in the \\nmedia\\xa0(4), ascertain cardiovascular risk factors from retinal images (5) and find new targets for existing medications (6,7). \\nThe success of these models is built from training on hundreds, \\nthousands and sometimes millions of controlled, labelled and structured data points (8). The capacity of AI to provide constant, \\ntireless and rapid analyses of data offers the potential to \\ntransform society’s approach to promoting health and preventing and managing diseases. AI systems have the potential to “read” \\nand triage all of the approximately 1.3 million research articles \\nindexed by PubMed each year (9); “examine” comments from 1.5 billion Facebook users or “monitor” 500 million tweets of \\npeople struggling with mental illness on a daily basis, foodborne \\nillness or the flu (10,11); and simultaneously interact with each and every person seeking answers to their health questions, \\nconcerns, problems and challenges (12).Abstract\\nNatural language processing (NLP) is a subfield of artificial intelligence devoted to \\nunderstanding and generation of language. The recent advances in NLP technologies are'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='understanding and generation of language. The recent advances in NLP technologies are \\nenabling rapid analysis of vast amounts of text, thereby creating opportunities for health \\nresearch and evidence-informed decision making. The analysis and data extraction from scientific literature, technical reports, health records, social media, surveys, registries and other \\ndocuments can support core public health functions including the enhancement of existing \\nsurveillance systems (e.g. through faster identification of diseases and risk factors/at-risk populations), disease prevention strategies (e.g. through more efficient evaluation of the safety \\nand effectiveness of interventions) and health promotion efforts (e.g. by providing the ability to \\nobtain expert-level answers to any health related question). NLP is emerging as an important tool that can assist public health authorities in decreasing the burden of health inequality/\\ninequity in the population. The purpose of this paper is to provide some notable examples of \\nboth the potential applications and challenges of NLP use in public health.\\nSuggested citation: Baclic O, Tunis M, Young K, Doan C, Swerdfeger H, Schonfeld J. Challenges and \\nopportunities for public health made possible by advances in natural language processing. Can Commun Dis Rep 2020;46(6):161–8. https://doi.org/10.14745/ccdr.v46i06a02Keywords: natural language processing, NLP , artificial intelligence, machine learning, public healthOVERVIEW'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='CCDR • June 4, 2020 • Vol. 46 No. 6 Page 162 \\nNLP is a subfield of AI that is devoted to developing algorithms \\nand building models capable of using language in the same \\nway humans do (13). It is routinely used in virtual assistants \\nlike “Siri” and “Alexa” or in Google searches and translations. NLP provides the ability to analyze and extract information \\nfrom unstructured sources, automate question answering and \\nconduct sentiment analysis and text summarization (8). With natural language (communication) being the primary means \\nof knowledge collection and exchange in public health and \\nmedicine, NLP is the key to unlocking the potential of AI in biomedical sciences.\\nMost modern NLP platforms are built on models refined \\nthrough machine learning techniques (14,15). Machine learning \\ntechniques are based on four components: a model; data; a loss \\nfunction, which is a measure of how well the model fits the data; and an algorithm for training (improving) the model (16). Recent \\nbreakthroughs in these areas have led to vastly improved NLP \\nmodels that are powered by deep learning, a subfield of machine learning (17). \\nInnovation in the different types of models, such as recurrent \\nneural network-based models (RNN), convolutional neural \\nnetwork-based models (CNN) and attention-based models, \\nhas allowed modern NLP systems to capture and model more complex linguistic relationships and concepts than simple \\nword presence (i.e. keyword search) (18). This effort has been'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='word presence (i.e. keyword search) (18). This effort has been \\naided by vector-embedding approaches to preprocess the data that encode words before feeding them into a model. These \\napproaches recognize that words exist in context (e.g. the \\nmeanings of “patient,” “shot” and “virus” vary depending on context) and treat them as points in a conceptual space rather \\nthan isolated entities. The performance of the models has also \\nbeen improved by the advent of transfer learning, that is, taking a model trained to perform one task and using it as the starting \\nmodel for training on a related task. Hardware advancements \\nand increases in freely available annotated datasets have also boosted the performance of NLP models. New evaluation tools \\nand benchmarks, such as GLUE, superglue and BioASQ, are \\nhelping to broaden our understanding of the type and scope of information these new models can capture (19–21).\\nOpportunities\\nPublic health aims to achieve optimal health outcomes within and across different populations, primarily by developing and \\nimplementing interventions that target modifiable causes \\nof poor health (22–26). Success depends on the ability to effectively quantify the burden of disease or disease risk factors \\nin the population and subsequently identify groups that are \\ndisproportionately affected or at-risk; identify best practices (i.e. optimal prevention or therapeutic strategies); and measure \\noutcomes (27). This evidence-informed model of decision'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='outcomes (27). This evidence-informed model of decision \\nmaking is best represented by the PICO concept (patient/problem, intervention/exposure, comparison, outcome). PICO provides an optimal knowledge identification strategy to frame and answer specific clinical or public health questions (28). \\nEvidence-informed decision making is typically founded on the \\ncomprehensive and systematic review and synthesis of data in accordance with the PICO framework elements.\\nToday, information is being produced and published (e.g. \\nscientific literature, technical reports, health records, \\nsocial media, surveys, registries and other documents) at \\nunprecedented rates. By providing the ability to rapidly analyze large amounts of unstructured or semistructured text, NLP has \\nopened up immense opportunities for text-based research and \\nevidence-informed decision making (29–34). NLP is emerging as a potentially powerful tool for supporting the rapid identification \\nof populations, interventions and outcomes of interest that \\nare required for disease surveillance, disease prevention and health promotion. For example, the use of NLP platforms that \\nare able to detect particular features of individuals (population/\\nproblem, e.g. a medical condition or a predisposing biological, behavioural, environmental or socioeconomic risk factor) in \\nunstructured medical records or social media text can be used to'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='unstructured medical records or social media text can be used to \\nenhance existing surveillance systems with real-world evidence. One recent study demonstrated the ability of NLP methods to \\npredict the presence of depression prior to its appearance in \\nthe medical record (35). The ability to conduct real-time text mining of scientific publications for a particular PICO concept \\nprovides opportunities for decision makers to rapidly provide \\nrecommendations on disease prevention or management that are informed by the most current body of evidence when timely \\nguidance is essential, such as during an outbreak. NLP-powered \\nquestion-answering platforms and chatbots also carry the potential to improve health promotion activities by engaging \\nindividuals and providing personalized support or advice. Table\\xa01 \\nprovides examples of potential applications of NLP in public health that have demonstrated at least some success.\\nChallenges\\nDespite the recent advances, barriers to widespread use of NLP technologies remain.\\nSimilar to other AI techniques, NLP is highly dependent on the \\navailability, quality and nature of the training data (72). Access \\nand availability of appropriately annotated datasets (to make \\neffective use of supervised or semi-supervised learning) are fundamental for training and implementing robust NLP models. \\nFor example, the development and use of algorithms that are'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='For example, the development and use of algorithms that are \\nable to conduct a systematic synthesis of published research on a particular topic or an analysis and data extraction from electronic \\nhealth records requires unrestricted access to publisher or \\nprimary care/hospital databases. While the number of freely accessible biomedical datasets and pre-trained models has been \\nincreasing in recent years, the availability of those dealing with \\npublic health concepts remains limited (73).CCDR • June 4, 2020 • Vol. 46 No. 6 Page 163 OVERVIEW\\nThe ability to de-bias data (i.e. by providing the ability to inspect, \\nexplain and ethically adjust data) represents another major \\nconsideration for the training and use of NLP models in public \\nhealth settings. Failing to account for biases in the development (e.g. data annotation), deployment (e.g. use of pre-trained \\nplatforms) and evaluation of NLP models could compromise \\nthe model outputs and reinforce existing health inequity (74). However, it is important to note that even when datasets and \\nevaluations are adjusted for biases, this does not guarantee an \\nequal impact across morally relevant strata. For example, use of health data available through social media platforms must take \\ninto account the specific age and socioeconomic groups that \\nuse them. A monitoring system trained on data from Facebook is likely to be biased towards health data and linguistic quirks \\nspecific to a population older than one trained on data from'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='specific to a population older than one trained on data from \\nSnapchat (75). Recently many model agnostic tools have been developed to assess and correct unfairness in machine learning \\nmodels in accordance with the efforts by the government and \\nacademic communities to define unacceptable AI development (76–81).\\nCurrently, one of the biggest hurdles for further development \\nof NLP systems in public health is limited data access (82,83). \\nWithin Canada, health data are generally controlled regionally \\nand, due to security and confidentiality concerns, there is reluctance to provide unhindered access to these systems and \\ntheir integration with other datasets (e.g. data linkage). There \\nhave also been challenges with public perception of privacy and data access. A recent survey of social media users found that the \\nmajority considered analysis of their social media data to identify \\nmental health issues “intrusive and exposing” and they would not consent to this (84). \\nBefore key NLP public health activities can be realized at \\nscale, such as the real-time analysis of national disease trends, \\njurisdictions will need to jointly determine a reasonable scope \\nand access to public health–relevant data sources (e.g. health record and administrative data). In order to prevent privacy \\nviolations and data misuse, future applications of NLP in the \\nanalysis of personal health data are contingent on the ability to embed differential privacy into models (85), both during training'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='and postdeployment. Access to important data is also limited \\nthrough the current methods for accessing full text publications. Realization of fully automated PICO-specific knowledge \\nextraction and synthesis will require unrestricted access to journal \\ndatabases or new models of data storage (86).\\nFinally, as with any new technology, consideration must be given \\nto assessment and evaluation of NLP models to ensure that they are working as intended and keeping in pace with society’s \\nchanging ethical views. These NLP technologies need to be \\nassessed to ensure they are functioning as expected and account for bias (87). Although today many approaches are posting \\nequivalent or better-than-human scores on textual analysis tasks, \\nit is important not to equate high scores with true language understanding. It is, however, equally important not to view Table 1: Examples of existing and potential applications \\nof natural language processing in public health\\nAbbreviation: NLP , natural language processing Type of \\nactivityPublic health \\nobjectiveExample of NLP use\\nIdentification \\nof at-risk populations or conditions of interest To continuously measure the incidence and prevalence of diseases and disease risk factors (i.e. surveillance)Analysis of unstructured or semistructured text from electronic health records or social media (36–42)\\nTo identify vulnerable and at-risk populations Analysis of risk behaviours using social media (43–45)'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='Identification of health interventionsTo develop optimal recommendations/interventions Automated systematic review and analysis of the information contained in scientific publications and unpublished data (46–50)\\nTo identify best practices Identification of promising public health interventions through analysis of online grey and peer reviewed literature (51)\\nIdentification of health outcomes using real-world evidenceTo evaluate the benefits of health interventionsAnalysis of unstructured or semistructured text from electronic health records, online media and publications to determine the impact of public health recommendations and interventions (52,53)\\nTo identify unintended adverse outcomes related to interventions Analysis of unstructured or semistructured text from electronic health records, social media and publications to identify potential adverse events of interventions (54–58)\\nKnowledge generation and translationTo support public health research Analysis and extraction of information from electronic health records and scientific publications for knowledge generation (59–62)\\nTo support evidence-informed decision making Use of chatbots, question/answer systems and text summarizers to provide personalized information to individuals seeking advice to improve their health and prevent disease (63–65)'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='Environmental scanning and situational awareness To conduct public health risk assessments and provide situational awarenessAnalysis of online content for real-time critical event detection and mitigation (66–70)\\nTo monitor activities that may have an impact on public health decision makingAnalysis of decisions of international and national stakeholders (71) OVERVIEW\\nCCDR • June 4, 2020 • Vol. 46 No. 6 Page 164 \\na lack of true language understanding as a lack of usefulness. \\nModels with a “relatively poor” depth of understanding can still \\nbe highly effective at information extraction, classification and \\nprediction tasks, particularly with the increasing availability of labelled data.\\nNatural language processing and the \\ncoronavirus disease 2019 (COVID-19)\\nWith the emergence of the COVID-19, NLP has taken a \\nprominent role in the outbreak response efforts (88,89). NLP has \\nbeen rapidly employed to analyze the vast quantity of textual \\ninformation that has been made available through unrestricted access to peer-review journals, preprints and digital media (90). \\nNLP has been widely used to support the medical and scientific \\ncommunities in finding answers to key research questions, summarization of evidence, question answering, tracking \\nmisinformation and monitoring of population sentiment (91–97).\\nConclusion\\nNLP is creating extraordinary opportunities to improve evidence-\\ninformed decision making in public health. We anticipate that'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='informed decision making in public health. We anticipate that \\nbroader applications of NLP will lead to the creation of more \\nefficient surveillance systems that are able to identify diseases and at-risk conditions in real time. Similarly, with an ability to \\nanalyze and synthesize large volumes of information almost \\ninstantaneously, NLP is expected to facilitate targeted health promotion and disease prevention activities, potentially leading \\nto population-wide disease reduction and greater health equity. \\nHowever, these opportunities are not without risks: biased models, biased data, loss of data privacy and the need to \\nmaintain and update models to reflect the evolving language \\nand context of public communication are all existing challenges that will need to be addressed. We encourage the public health \\nand computer science communities to collaborate in order to \\nmitigate these risks, ensure that public health practice does not fall behind in these technologies or miss opportunities for health \\npromotion and disease surveillance and prevention in this rapidly \\nevolving landscape.\\nAuthors’ statement \\nOB — Writing – original draft, review & editing and \\nconceptualizationMT — Writing – original draft, review & editing and conceptualizationKY — Writing – review & editing, and conceptualizationCD — Writing – review & editingHS — Writing – review & editingJS — Writing – original draft, review & editing and conceptualizationConflict of interest\\nNone.\\nAcknowledgements'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='None.\\nAcknowledgements\\nWe thank J Nash and J Robertson who were kind enough to offer feedback and suggestions. \\nFunding\\nThis work is supported by the Public Health Agency of Canada. \\nThe research undertaken by JS was funded by the Canadian \\nfederal government’s Genomic Research and Development \\nInitiative.\\nReferences\\n1. Majkowska A, Mittal S, Steiner DF , Reicher JJ, McKinney \\nSM, Duggan GE, Eswaran K, Cameron Chen PH, Liu Y , Kalidindi\\n\\xa0SR, Ding A, Corrado GS, Tse D, Shetty S. Chest \\nradiograph interpretation with deep learning models: assessment with radiologist-adjudicated reference standards and population-adjusted evaluation. Radiology 2020;294(2):421–31. DOI PubMed\\n2. Liu X, Faes L, Kale A, Wagner SK, Fu DJ, Bruynseels A, Mahendiran T, Moraes G, Shamdas M, Kern C, Ledsam JR, Schmid MK, Balaskas K, Topol EJ, Bachmann LM, Keane PA, Denniston AK. A comparison of deep learning performance against health care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis. Lancet Digital Health 2019. DOI\\n3. Perez MV, Mahaffey KW, Hedlin H, Rumsfeld JS, Garcia A, \\nFerris T, Balasubramanian V, Russo AM, Rajmane A, Cheung \\nL, Hung G, Lee J, Kowey P , Talati N, Nag D, Gummidipundi SE, Beatty A, Hills MT, Desai S, Granger CB, Desai M, Turakhia MP; Apple Heart Study Investigators. Large-scale assessment of a smartwatch to identify atrial fibrillation. N Engl J Med 2019;381(20):1909–17. DOI PubMed'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='4. Feldman J, Thomas-Bachli A, Forsyth J, Patel ZH, Khan K. Development of a global infectious disease activity database using natural language processing, machine learning, and \\nhuman expertise. J Am Med Inform Assoc 2019;26(11):1355–\\n9. DOI PubMed\\n5. Poplin R, Varadarajan AV, Blumer K, Liu Y , McConnell\\xa0MV, \\nCorrado GS, Peng L, Webster DR. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning. Nat Biomed Eng 2018;2(3):158–64.  DOI PubMed\\n6. Vamathevan J, Clark D, Czodrowski P , Dunham I, Ferran\\xa0E, Lee G, Li B, Madabhushi A, Shah P , Spitzer M, Zhao S. Applications of machine learning in drug discovery and development. Nat Rev Drug Discov 2019;18(6):463–77.  \\nDOI PubMedCCDR • June 4, 2020 • Vol. 46 No. 6 Page 165 OVERVIEW\\n7. Corsello SM, Nagari RT, Spangler RD, Rossen J, Kocak\\xa0M, \\nBryan JG, Humeidi R, Peck D, Wu X, Tang AA, Wang\\xa0VM, Bender SA, Lemire E, Narayan R, Montgomery P , Ben-David\\xa0U, Garvie CW, Chen Y , Rees MG, Lyons NJ, McFarland JM, Wong BT, Wang L, Dumont N, O’Hearn\\xa0PJ, \\nStefan E, Doench JG, Harrington CN, Greulich\\xa0H, \\nMeyerson\\xa0M, Vazquez F , Subramanian A, Roth JA, Bittker JA, Boehm JS, Mader CC, Tsherniak A, Golub TR. Discovering the anticancer potential of non-oncology drugs by systematic viability profiling. Nat Can 2020;1:235–48. DOI\\n8. Topol EJ. High-performance medicine: the convergence \\nof human and artificial intelligence. Nat Med 2019 \\nJan;25(1):44–56. DOI PubMed'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='of human and artificial intelligence. Nat Med 2019 \\nJan;25(1):44–56. DOI PubMed\\n9. MEDLINE PubMed Production Statistics. Bethesda (MD): U.S. National Library of Medicine (updated 2019-11-19; accessed 2020-01-27). https://www.nlm.nih.gov/bsd/medline_pubmed_production_stats.html\\n10. Twitter usage statistics. Internet LiveStats.com (updated 2013-08-16; accessed 2020-01-27). https://www.internetlivestats.com/twitter-statistics/\\n11. Searching for health. Google News Lab, Schema; 2017 (accessed 2020-01-27). https://googlenewslab.gistapp.com/\\nsearching-for -health\\n12. Friedman C, Elhadad N. Natural language processing in \\nhealth care and biomedicine. In: Shortliffe E, Cimino J, editors. Biomed Informatics London: Springer; 2014. DOI\\n13. Ruder S. NLP-progress. London (UK): Sebastian Ruder (accessed 2020-01-18). https://nlpprogress.com/\\n14. Jurafsky D, Martin JH. Speech and language processing. \\nStanford (CA): Stanford University; 2019 (updated 2019-\\n11-16; accessed 2020-01-18). https://web.stanford.edu/~jurafsky/slp3/\\n15. Nadkarni PM, Ohno-Machado L, Chapman WW. Natural language processing: an introduction. J Am Med Inform Assoc 2011;18(5):544–51. DOI PubMed\\n16. Nilsson N. Introduction to machine learning. Stanford (CA): Robotic Library, Department of Computer Science, Stanford University; 1998. http://robotics.stanford.edu/people/nilsson/MLBOOK.pdf\\n17. Zhou M, Duan N, Liu S, Shum HY . Progress in neural \\nNLP: modeling, learning, and reasoning. Engineering \\n2020;6(3):275–90. DOI'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='NLP: modeling, learning, and reasoning. Engineering \\n2020;6(3):275–90. DOI\\n18. Tang B, Pan Z, Yin K, Khateeb A. Recent advances of deep learning in bioinformatics and computational biology. Front Genet 2019;10:214. DOI PubMed\\n19. Hirschberg J, Manning CD. Advances in natural language \\nprocessing. Science 2015;349(6245):261–6. DOI PubMed\\n20. Wang A, Singh A, Michael J, Hill F , Levy O, Bowman S. GLUE: a multi-task benchmark and analysis platform for \\nnatural language understanding. Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP . Brussels (BE): 2018 Nov; p. 353–5. DOI21. The Big Bad NLP Database. New York (NY): Quantum Stat; 2020 (updated 2020-01-21; accessed 2020-01-27). https://quantumstat.com/dataset/dataset.html\\n22. Jackson B, Huston P . Advancing health equity to improve health: the time is now. Health Promot Chronic Dis Prev Can 2016;36(2):17–20. DOI PubMed\\n23. Pan American Health Organization. Just societies: health equity and dignified lives. Report of the Commission of the Pan American Health Organization on Equity and Health Inequalities in the Americas. Washington (DC): Pan American Health Organization (updated 2019-11; accessed 2020-01-18). http://search.ebscohost.com/login.aspx?direct=true&site=eds-live&db=edsebk&AN=2329553\\n24. Marmot M, Allen J, Goldblatt P , Boyce T, McNeish D, Grady\\xa0M, Geddes I; The Marmot Review. Fair society, healthy lives: strategic review of health inequalities in'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='England post-2010. UCL Institute of Health Equity.  \\nhttp://www.parliament.uk/documents/fair-society-healthy-lives-full-report.pdf\\n25. Arcaya MC, Arcaya AL, Subramanian SV. Inequalities in health: definitions, concepts, and theories. Glob Health Action 2015;8:27106. DOI PubMed\\n26. Public Health Agency of Canada. The Chief Public Health Officer’s report on the state of public health in Canada: addressing health inequalities. Ottawa (ON): Public Health Agency of Canada; 2008. Report No.: HP2-10/2008E.  http://www.phac-aspc.gc.ca/cphorsphc-respcacsp/2008/fr-rc/index-eng.ph\\n27. Ndumbe-Eyoh S, Dyck L, Clement C. Common agenda for public health action on health equity. Antigonish (NS): \\nNational Collaborating Centre for Determinants of Health, \\nSt Francis Xavier University; 2016. http://nccdh.ca/images/uploads/comments/Common_Agenda_EN.pdf\\n28. Alonso-Coello P , Schünemann HJ, Moberg J, Brignardello-Petersen R, Akl EA, Davoli M, Treweek S, Mustafa RA, Rada G, Rosenbaum S, Morelli A, Guyatt GH, Oxman AD; GRADE Working Group. GRADE Evidence to Decision (EtD) frameworks: a systematic and transparent \\napproach to making well informed healthcare choices. 1: \\nIntroduction. BMJ 2016;353:i2016. DOI PubMed\\n29. Kim ES, James P , Zevon ES, Trudel-Fitzgerald C, Kubzansky\\xa0LD, Grodstein F . Social media as an emerging data resource for epidemiologic research: characteristics of social media users and non-users in the Nurses’ Health Study II. Am J Epidemiol 2020;189(2):156–61. DOI PubMed'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='30. Koleck TA, Dreisbach C, Bourne PE, Bakken S. Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review. J Am Med Inform Assoc 2019;26(4):364–79. DOI PubMed\\n31. Marshall IJ, Wallace BC. Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. Syst Rev 2019;8(1):163.  DOI PubMedOVERVIEW\\nCCDR • June 4, 2020 • Vol. 46 No. 6 Page 166 \\n32. Yin Z, Sulieman LM, Malin BA. A systematic literature review \\nof machine learning in online personal health data. J Am Med Inform Assoc 2019;26(6):561–76. DOI PubMed\\n33. Kreimeyer K, Foster M, Pandey A, Arya N, Halford G, Jones\\xa0SF , Forshee R, Walderhaug M, Botsis T. Natural language processing systems for capturing and standardizing unstructured clinical information: A systematic review. J Biomed Inform 2017;73:14–29. DOI PubMed\\n34. The Office of the National Coordinator for Health Information Technology. Health IT dashboard: Quick stats. Washington (DC): U.S. Department of Health and Human Services. https://dashboard.healthit.gov/quickstats/quickstats.php\\n35. Harris JK, Mansour R, Choucair B, Olson J, Nissen C, Bhatt\\xa0J; Centers for Disease Control and Prevention. Health department use of social media to identify foodborne illness \\n- Chicago, Illinois, 2013-2014. MMWR Morb Mortal Wkly \\nRep 2014;63(32):681–5. PubMed'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='- Chicago, Illinois, 2013-2014. MMWR Morb Mortal Wkly \\nRep 2014;63(32):681–5. PubMed\\n36. Gesualdo F , Stilo G, Agricola E, Gonfiantini MV, Pandolfi\\xa0E, Velardi P , Tozzi AE. Influenza-like illness surveillance on Twitter through automated learning of naïve language. PLoS One 2013;8(12):e82489. DOI PubMed\\n37. Eichstaedt JC, Smith RJ, Merchant RM, Ungar LH, Crutchley\\xa0P , Preo\\nţiuc-Pietro D, Asch DA, Schwartz HA. \\nFacebook language predicts depression in medical records. Proc Natl Acad Sci USA. 2018;115(44):11203–8. DOI\\n38. Şerban O, Thapen N, Maginnis B, Hankin C, Foot V. \\nReal-time processing of social media with SENTINEL: a syndromic surveillance system incorporating deep learning for health classification. Inf Process Manage 2019;56(3):1166–84. DOI\\n39. Edo-Osagie O, Smith G, Lake I, Edeghere O, De La Iglesia\\xa0B. Twitter mining using semi-supervised classification for relevance filtering in syndromic surveillance. PLoS One 2019;14(7):e0210689. DOI PubMed\\n40. Ford E, Carroll JA, Smith HE, Scott D, Cassell JA. Extracting information from the text of electronic medical records to improve case detection: a systematic review. J Am Med Inform Assoc 2016;23(5):1007–15. DOI PubMed\\n41. Dorr D, Bejan CA, Pizzimenti C, Singh S, Storer M, Quinones\\xa0A. Identifying patients with significant problems related to social determinants of health with natural \\nlanguage processing. Stud Health Technol Inform \\n2019;264:1456–7. DOI PubMed'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='language processing. Stud Health Technol Inform \\n2019;264:1456–7. DOI PubMed\\n42. Carrell DS, Cronkite D, Palmer RE, Saunders K, Gross DE, Masters ET, Hylan TR, Von Korff M. Using natural language processing to identify problem usage of prescription opioids. Int J Med Inform 2015;84(12):1057–64.  \\nDOI PubMed\\n43. Cacheda F , Fernandez D, Novoa FJ, Carneiro V. Early detection of depression: social network analysis and random \\nforest techniques. J Med Internet Res 2019;21(6):e12554. DOI PubMed44. Conway M, Hu M, Chapman WW. Recent advances in using natural language processing to address public health research questions using social media and consumer generated data. Yearb Med Inform 2019;28(1):208–17.  DOI PubMed\\n45. Coppersmith G, Dredze M, Harman C. Quantifying mental health signals in Twitter. Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: from linguistic signal to clinical reality. Baltimore (MA): 27 June 2014;p. 51–60. DOI\\n46. Gates A, Guitard S, Pillay J, Elliott SA, Dyson MP , Newton\\xa0AS, Hartling L. Performance and usability of machine learning for screening in systematic reviews: a comparative evaluation of three tools. Syst Rev \\n2019;8(1):278. DOI PubMed\\n47. Przybyła P , Soto AJ, Ananiadou S. Identifying personalised \\ntreatments and clinical trials for precision medicine using \\nsemantic search with Thalia. Manchester (UK): TREC; 2017.'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='semantic search with Thalia. Manchester (UK): TREC; 2017. \\nhttps://www.researchgate.net/publication/323629465_Identifying_Personalised_Treatments_and_Clinical_Trials_for_Precision_Medicine_using_Semantic_Search_with_Thalia\\n48. Bannach-Brown A, Przybyła P , Thomas J, Rice AS, Ananiadou\\xa0S, Liao J, Macleod MR. Machine learning \\nalgorithms for systematic review: reducing workload in a \\npreclinical review of animal studies and reducing human screening error. Syst Rev 2019;8(1):23. DOI PubMed\\n49. Norman C, Leeflang M, Spijker R, Kanoulas E, Névéol A. A distantly supervised dataset for automated data extraction from diagnostic studies. ACL Workshop on Biomedical Natural Language Processing, Florence (IT): 2019 Aug. DOI\\n50. Tsafnat G, Glasziou P , Karystianis G, Coiera E. Automated \\nscreening of research studies for systematic reviews using \\nstudy characteristics. Syst Rev 2018;7(1):64. DOI PubMed\\n51. Lerner I, Créquit P , Ravaud P , Atal I. Automatic screening using word embeddings achieved high sensitivity and workload reduction for updating living network meta-analyses. J Clin Epidemiol 2019;108:86–94.  DOI PubMed\\n52. Tucker TC, Durbin EB, McDowell JK, Huang B. Unlocking \\nthe potential of population-based cancer registries. Cancer \\n2019;125(21):3729–37. DOI PubMed\\n53. Mohammadhassanzadeh H, Sketris I, Traynor R, Alexander S, \\nWinquist B, Stewart SA. Using natural language processing'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='Winquist B, Stewart SA. Using natural language processing \\nto examine the uptake, content, and readability of media coverage of a pan-Canadian drug safety research project: cross-sectional observational study. JMIR Form Res 2020;4(1):e13296. DOI PubMed\\n54. Banerji A, Lai KH, Li Y , Saff RR, Camargo CA Jr, \\nBlumenthal\\xa0KG, Zhou L. Natural language processing \\ncombined with ICD-9-CM codes as a novel method to study the epidemiology of allergic drug reactions. J Allergy Clin Immunol Pract 2020;8(3):1032–1038.e1. DOI PubMedCCDR • June 4, 2020 • Vol. 46 No. 6 Page 167 OVERVIEW\\n55. Young IJ, Luz S, Lone N. A systematic review of natural \\nlanguage processing for classification tasks in the field of incident reporting and adverse event analysis. Int J Med Inform 2019;132:103971. DOI PubMed\\n56. Henry S, Buchan K, Filannino M, Stubbs A, Uzuner O. 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records. J Am Med Inform Assoc 2020;27(1):3–12. DOI PubMed\\n57. Fan B, Fan W, Smith C, Garner H. Adverse drug event detection and extraction from open data: a deep learning approach. Inf Process Manage 2020;57(1):102131. DOI\\n58. Yu W, Zheng C, Xie F , Chen W, Mercado C, Sy LS, Qian L, Glenn S, Tseng HF , Lee G, Duffy J, McNeil MM, Daley MF , Crane B, McLean HQ, Jackson LA, Jacobsen SJ. The use of natural language processing to identify vaccine-related anaphylaxis at five health care systems in the Vaccine Safety'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='Datalink. Pharmacoepidemiol Drug Saf 2020;29(2):182–8. \\nDOI PubMed\\n59. Liu F , Weng C, Yu H. Advancing clinical research through natural language processing on electronic health records: traditional machine learning meets deep learning. In: Richesson RL, Andrews JE, editors: Clinical Research \\nInformatics. Springer International Publishing; 2019.  \\np.\\xa0357–78. DOI\\n60. Chan L, Beers K, Yau AA, Chauhan K, Duffy Á, Chaudhary\\xa0K, Debnath N, Saha A, Pattharanitima P , Cho J, Kotanko P , Federman A, Coca SG, Van Vleck T, Nadkarni GN. Natural language processing of electronic health records is superior to billing codes to identify symptom burden in hemodialysis patients. Kidney Int 2020;97(2):383–92. DOI PubMed\\n61. Juhn Y , Liu H. Artificial intelligence approaches using natural language processing to advance EHR-based clinical research. J Allergy Clin Immunol 2020;145(2):463–9. DOI PubMed\\n62. Wang Y , Wang L, Rastegar-Mojarad M, Moon S, Shen F , Afzal N, Liu S, Zeng Y , Mehrabi S, Sohn S, Liu H. Clinical information extraction applications: A literature review. J Biomed Inform 2018;77:34–49. DOI PubMed\\n63. Laranjo L, Dunn AG, Tong HL, Kocaballi AB, Chen J, \\nBashir R, Surian D, Gallego B, Magrabi F , Lau AY , Coiera E. \\nConversational agents in healthcare: a systematic review. J Am Med Inform Assoc 2018;25(9):1248–58. DOI PubMed\\n64. Head to health. COVID-19 support. Department of Health; Australian Government (accessed 2020-01-27).  https://headtohealth.gov.au/sam-the-chatbot'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='65. Pereira J, Díaz Ó. Using health chatbots for behavior change: a mapping study. J Med Syst 2019;43(5):135.  \\nDOI PubMed\\n66. Dion M, AbdelMalik P , Mawudeku A. Big Data and the \\nGlobal Public Health Intelligence Network (GPHIN). Can \\nCommun Dis Rep 2015;41(9):209–14. DOI PubMed67. Ghosh S, Chakraborty P , Lewis BL, Majumder M, Cohn\\xa0E, \\nBrownstein JS, Marathe M, Ramakrishnan N. GELL: Automatic extraction of epidemiological line lists from open sources. Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining; \\n2017 Aug 13-17; Halifax (NS): Association for Computing \\nMachinery; 2017. p. 1477–86. DOI\\n68. Charles-Smith LE, Reynolds TL, Cameron MA, Conway M, Lau EH, Olsen JM, Pavlin JA, Shigematsu M, Streichert\\xa0LC, Suda KJ, Corley CD. Using social media for actionable disease surveillance and outbreak management: a systematic \\nliterature review. PLoS One 2015;10(10):e0139701.  \\nDOI PubMed\\n69. Jordan S, Hovet S, Fung I, Liang H, Fu KW, Tsz Ho Tse Z. Using Twitter for public health surveillance from monitoring and prediction to public response. Data (Basel) 2018;4(1):6. DOI\\n70. Abbood A, Ullrich A, Busche R, Ghozzi S. EventEpi—A natural language processing framework for event-based surveillance medRxiv 2019;19006395. DOI\\n71. Anglin K. Gather-narrow-extract: A framework for studying local policy variation using web-scraping and natural \\nlanguage processing. J Res Educ Eff 2019;12(4):685–706. \\nDOI'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='language processing. J Res Educ Eff 2019;12(4):685–706. \\nDOI\\n72. Tatman R, Conner K. Effects of talker dialect, gender & race on accuracy of Bing speech and YouTube automatic captions. Proc Interspeech 2017;934–8. DOI\\n73. Spasic I, Nenadic G. Clinical text data in machine learning: systematic review. JMIR Med Inform 2020;8(3):e17984.  DOI PubMed\\n74. Rajkomar A, Hardt M, Howell MD, Corrado G, Chin MH. Ensuring fairness in machine learning to advance health equity. Ann Intern Med 2018;169(12):866–72. DOI PubMed\\n75. Gramlich J. 10 facts about Americans and Facebook. Washington (DC): Pew Research Center (accessed 2020-01-27). https://www.pewresearch.org/fact-tank/2019/05/16/facts-about-americans-and-facebook/\\n76. Xu C, Doshi T. Fairness indicators: scalable infrastructure for fair ML system. Mountain View (CA): Google (accessed 2020-01-27). DOI\\n77. Holstein K, Vaughan JW, Daumé H, Dudík M, Wallach H. Improving fairness in machine learning systems: What do industry practitioners need? CHI ‹19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 2019 Paper No.: 600. p. 1–16. DOI\\n78. Wiens J, Price WN 2nd, Sjoding MW. Diagnosing bias in data-driven algorithms for healthcare. Nat Med 2020;26(1):25–6. DOI PubMed\\n79. Chen IY , Joshi S, Ghassemi M. Treating health disparities with artificial intelligence. Nat Med 2020;26(1):16–7.  DOI PubMedOVERVIEW\\nCCDR • June 4, 2020 • Vol. 46 No. 6 Page 168 \\n80. Montreal Declaration for a Responsible Development of'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='80. Montreal Declaration for a Responsible Development of \\nArtificial Intelligence. Forum on the Socially Responsible Development of AI: 2017 Nov 2-3: Montréal (QC) (accessed 2020-01-18). https://www.montrealdeclaration-responsibleai.com/the-declaration\\n81. Treasury Board Secretariat. Directive on automated decision-making. Ottawa (ON): Government of Canada (modified 2019-02-05; accessed 2020-01-27). https://www.tbs-sct.gc.ca/pol/doc-eng.aspx?id=32592\\n82. Friedman C, Rindflesch TC, Corn M. Natural language processing: state of the art and prospects for significant progress, a workshop sponsored by the National Library of Medicine. J Biomed Inform 2013;46(5):765–73. DOI PubMed\\n83. Sheikhalishahi S, Miotto R, Dudley JT, Lavelli A, Rinaldi F , Osmani V. Natural language processing of clinical notes on chronic diseases: systematic review. JMIR Med Inform \\n2019;7(2):e12239. DOI PubMed\\n84. Ford E, Curlewis K, Wongkoblap A, Curcin V. Public opinions \\non using social media content to identify users with depression and target mental health care advertising: mixed methods survey. JMIR Ment Health 2019;6(11):e12942.  DOI PubMed\\n85. Radebaugh C, Erlingsson U. Introducing tensorflow privacy: learning with differential privacy for training data. Medium.com (accessed 2020-01-27). https://medium.com/tensorflow/introducing-tensorflowprivacy-learning-with-differential-privacy-for-trainingdata-b143c5e801b6'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='86. Penning de Vries BB, van Smeden M, Rosendaal FR, Groenwold RH. Title, abstract, and keyword searching resulted in poor recovery of articles in systematic reviews of \\nepidemiologic practice. J Clin Epidemiol 2020;121:55–61. \\nDOI PubMed\\n87. Obermeyer Z, Powers B, Vogeli C, Mullainathan S. Dissecting racial bias in an algorithm used to manage the health of populations. Science 2019;366(6464):447–53.  DOI PubMed88. Coronavirus tech handbook: natural language processing. https://coronavirustechhandbook.com/nlp\\n89. COVID-19 Open Research Dataset Challenge (CORD-19): An AI challenge with AI2, CZI, MSR, Georgetown, NIH & The White House. San Francisco (CA): kaggle.com (accessed 2020-01-27). https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\\n90. PubMed Central. Public Health Emergency COVID-19 Initiative. Bethesda (MD): US National Library of Medicine (accessed 2020-01-27). PubMed\\n91. Bullock J, Luccioni A, Pham KH, Lam CS, Luengo-Oroz M. Mapping the landscape of artificial intelligence applications against COVID-19. arXiv:2003.11336 [cs.CY]. https://vectorinstitute.ai/wp-content/uploads/2020/03/arxiv-mappingai.pdf\\n92. Allen Institute for Artificial Intelligence (AI2). CORD-19 Explorer: explore the dataset. https://cord-19.apps.allenai.org/\\n93. Chen E, Lerman K, Ferrara E. COVID-19: the first public coronavirus Twitter dataset. Ithaca (NY): Cornell University (accessed 2020-01-27). https://arix.org/abs/2003.07372'),\n",
       " Document(metadata={'title': 'Challenges and opportunities for public health made possible by advances in natural language processing', 'year': 2020}, page_content='94. LitCovid. Bethesda (MD): U.S. National Library of Medicine (accessed 2020-01-27). PubMed\\n95. Coronafiles: Chatbots take strain off Denmark’s emergency helplines (accessed 2020-01-27). https://sifted.eu/articles/coronafiles-chatbots-helplines/\\n96. Kritikos M. At a glance: scientific foresight: What if we could fight coronavirus with artificial intelligence? Scientific Foresight Unit, European Parliament. https://www.europarl.\\neuropa.eu/RegData/etudes/ATAG/2020/641538/EPRS_\\nATA(2020)641538_EN.pdf\\n97. Against AI. COVID-19 Canada. About us. Toronto (ON): \\nCIFAR (accessed 2020-01-27). https://ai-against-covid.ca/'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='COGVIEW & INTELNET:\\nNuanced Energy-Based Knowledge Representation\\nAnd Integrated Cognitive-Conceptual Framework For\\nRealistic Culture, Values, and Concept-Affected\\nSystems Simulation\\nDaniel J. Olsher, Member, IEEE\\nCognitive Science Program\\nTemasek Laboratories, National University of Singapore\\ndan@intmind.com\\nAbstract —An increasingly important AI frontier is the ability\\nto represent worldviews, culture, values, and other nuanced\\nstructures, and to simulate the effects of these on perception,emotion/affect, judgment, and opinion formation. Such informa-\\ntion, however, is notably difﬁcult to model and represent, due\\nto its ﬁne-grained, diffuse nature. Reasoning is also highly chal-\\nlenging in these domains. This paper presents a novel ‘Energy-\\nBased’ Knowledge Representation formalism (INTELNET) ideal\\nfor modeling, fusing, and reasoning about nuanced semantics,\\ncultures, affects, and worldviews. It then introduces the inte-grated COGVIEW conscious/unconscious psychological simula-\\ntion framework operating on top of INTELNET and advances a\\ndetailed example within the suicide terrorism domain.\\nApplications include intelligent reasoning systems, human-\\nitarian missions, cultural simulations, knowledge engineering,\\nlanguage processing, anti-discrimination and prejudice reduction,\\nterrorism reduction, and norm change efforts, among others.\\nKeywords —Culture, Belief, Emotion Simulation, Norm Change,\\nEnergy-Based Knowledge Representation, Reasoning, Cognitive\\nPsychology'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='Energy-Based Knowledge Representation, Reasoning, Cognitive\\nPsychology\\nI. C ULTURES ,BELIEFS ,CONCEPTS ,AND MORE :\\nINTELNET E NERGY -BASED KR (EBKR) AND THE\\nCOGVIEW F RAMEWORK\\nThe ability to create agents that can reliably integrate and\\nsimulate information regarding worldviews, including culture,values, and other conceptual structures represents an increas-ingly important frontier in artiﬁcial intelligence. [1], [2], [3]\\nWorldviews mediate perception, but are notoriously difﬁ-\\ncult to model and to represent, in part due to their nuanced,diffuse natures. In computational systems, moreover, it isdifﬁcult to deﬁne, bound, and reason about whatever ‘it’ is\\nthat culture and worldviews consist of. Beyond this, effects of\\nhuman cognition add another layer of complexity.\\nThis paper presents a new knowledge representation for-\\nmalism, INTELNET , expressly targeted at nuanced data gen-\\nerated by systems grounded in and arising from the humancognitive capacity. INTELNET is ideally suited to worldviewmodeling and the nuanced semantics underpinning naturallanguage, enabling simulation of the effects of worldviews onreasoning, persuasion, and understanding.\\nIn order to facilitate enhanced reasoning capabilities,\\nINTELNET seeks to model nuanced semantics , representing\\ninformation with a symbolic opacity intermediate between thatof neural networks and typical symbolic systems. INTELNET\\nconcepts are represented as distributed, interconnected net-'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='concepts are represented as distributed, interconnected net-\\nworks wherein each part of a network operates in concert with\\nothers to deﬁne a concept and to model the meaning of aparticular semantic domain.\\nBeyond nuance, computer worldview models are faced with\\ncertain ‘quirks’ of human psychological processing which leadcomplex conceptual systems to behave differently than theyotherwise would if they were not being generated via humancognition. To this end, the paper presents a framework cover-ing speciﬁc psychological phenomena that integrate with theINTELNET formalism in order to enable detailed simulations.INTELNET, together with the integrated psychological models\\ndiscussed in this paper, are referred to as the COGVIEW\\nframework.\\nPotential applications of INTELNET and COGVIEW are\\nextremely wide, including advanced reasoning, perception,persuasion, social systems modeling, and natural languageprocessing. COGVIEW-based cultural networks represent theways in which speciﬁc cultures have concretized and reiﬁedmeaning components and how these components interact to\\nproduce culturally-mediated judgments and behaviors.\\nCOGVIEW/INTELNET is ideal for modeling cultures as'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='COGVIEW/INTELNET is ideal for modeling cultures as\\nit is able to represent the speciﬁc conceptual semantics presentin particular cultures at a much greater level of speciﬁcityand clarity than traditional ontologies, accurately preservinga major source of cultural inﬂuence on model outcomes.It is capable of representing essentially inﬁnite nuance byseamlessly combining smaller semantic representations whilepreserving key semantics.\\nMoreover, the representation can be made aware which\\nconcepts are highly valued and which are stigmatized, and\\nis able to make this information relevant throughout simula-\\ntion by modulating valences, magnitudes, and other energyaspects. When semantic components interact with previouslystigmatized or valorized semantics, the model is able to extendthis ‘colorization’ to connected components.\\nTaken together, the ability to model: 1) unique interconnec-\\ntion structures between concepts, 2) the nature and semanticsof concepts themselves, 3) judgments, 4) expectation/valueviolations, and 5) the degree to which stimuli generate energyoutcomes in accordance with culturally-demanded energy dis-tributions provides a powerful system for cultural simulation.\\nPrevious work ([4], [5], [6], [7]) points towards how graph'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='Previous work ([4], [5], [6], [7]) points towards how graph\\nstructures may be used to compute on these representations,identify likely areas of discord and moral questions, designnorm-changing communications, and make predictions abouthow various stimuli are likely to be viewed by members ofvarious cultures.\\nINTELNET nuance and ﬂexibility also allow for fusion ,\\nthe interconnection of information sources from various do-\\nmains. In [7], a cultural network is connected to an emotionmodel, and in ongoing work multiple commonsense knowledge\\nsources are fused and used together to enable reasoning.\\nII. W\\nH YAN E W KR MECHANISM ?\\nThe motivation for Energy-Based Knowledge Represen-\\ntation (INTELNET) and COGVIEW stems from the no-\\ntion that information originating from psychologically and\\nconceptually-mediated social patterns, principles, and pro-\\ncesses (such as cultures, worldviews and natural language se-\\nmantics ) possesses unique properties, requiring speciﬁc tools in\\norder to be modeled felicitously. In this paper, such information\\nis termed Cognitively Mediated Process Data , or CMPD.\\n82 978-1-4673-5923-8/13/$31.00 c/circlecopyrt2013 IEEE\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. ’Conceptually mediated’ refers to processes that unfold dif-\\nferently in practice depending on the speciﬁc concepts presentin some knowledge base and on the speciﬁc ways in which'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='those concepts are interconnected. An example would be moral\\nperception, in which the speciﬁc concepts one has regardingvirtue and vice and the implications attached to each of theseall play a major role in determining how one will view aparticular phenomenon. Similarly, ’psychologically-mediated’evokes phenomena which, in order to be successfully modeled,require reference to the functioning of speciﬁc psychologicalprocesses. A key example is word association; given theconcepts ’sun’, ’sand’, ’ocean’, ’water’, and ’waves’, a humanwould immediately reference and activate the concept ’beach’,while a computer would a priori have no reason to do so. In\\ncases like these, models with no capability to model semanticassociation would predict incorrect outcomes for incoming\\nstimuli.\\nDavis, Shrobe and Szolovits [8] suggest that knowledge\\nrepresentation formalisms fulﬁll a number of roles, includingthat of a world surrogate enabling reasoning instead of action,ontological commitments suggesting the terms in which weview the world, a theory of intelligent reasoning, a deﬁnitionof what is ‘natural’/easy to say, and a medium of human\\nexpression (we can only ‘say’ what our KR allows us to).\\nIn CMPD-mediated contexts, the goal of INTELNET and'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='In CMPD-mediated contexts, the goal of INTELNET and\\nCOGVIEW is to allow us to say what we need to in a nuanceddomain-appropriate manner, to take cognition into account,and to conduct complex reasoning. When modeling culturesand CMPD-heavy contexts, the simpliﬁcations required to usesymbol- and ontology-based KRs often force us to leave outmuch of the semantic detail making the system what it is.Standard relations are semantically opaque, generating frameproblems and making it difﬁcult to combine multiple piecesof information. Generalizing (repurposing, reconstruing) in-formation during reasoning become difﬁcult as well. Left-outdetail makes advanced reasoning difﬁcult indeed, contributingto the perception that deep cultural modeling is an intractableproblem.\\nCOGVIEW/INTELNET has also demonstrated the simu-\\nlation of emotional/affective outcomes [7], using knowledge\\nto generate likely appraisals and predict pleasantness, level ofemotional engagement, and other key parameters. (see also [9])\\nIn line with the Intrinsic Cognitive Models (ICM) [10] view\\nof mental representation, most cultural and CMPD-grounded\\ncontexts involve mostly implicit information. In INTELNETthe goal is to store information implicitly and extract it implic-itly when you need it, leading to “boundless” inferences ([10]).\\nINTELNET/COGVIEW networks’ contextual and dynamicability to build larger representations from smaller ones is idealfor making implicit knowledge useful during reasoning.'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='In CMPD contexts, locally and contextually-correct infer-\\nence often displace the need and desire for complete infer-ence (see, for example, [11]) in that information is widely\\ndistributed and conﬂicting and ‘truth’ becomes much more\\nrelative and contextual in these domains.\\nA. Nuance and Psychological ‘Quirks’\\nBecause it is created by humans operating within the\\ncomplex human semantic capacity (the capacity for making\\nmeaning), CMPD nearly always draws on signiﬁcantly inter-\\nconnected concepts within expansive knowledge bases and is\\ndeﬁned by subtle nuances and unlimited meaning gradations\\nor ‘shades of gray’ .\\nCMPD is ‘fractal’ in the sense that the concepts that\\ncomprise it have complex internal structures referencing mul-tiple other concepts, which again possess complex structures,and so on. CMPD is always highly contextualized; intelligent\\nreasoning and social processes take place within and are highly\\ndependent on the nuances of surrounding contexts (which arethemselves complex and nuanced, involving implicit referenceto knowledge about the world).\\nWith CMPD, the interconnection structure between con-\\nstituent concepts is a key part of meaning; the pattern ofrelationships between a concept and other concepts forms a\\nmajor part of the original concept’s semantics.\\nCMPD may only fully be modeled by systems capable'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='major part of the original concept’s semantics.\\nCMPD may only fully be modeled by systems capable\\nof handling ‘quirky’ behavior explicable only with referenceto human cognitive functioning, such as priming, associativememory for concepts, emotions, short-term memory limita-tions, and unconscious processing. Each such ‘quirk’ exhibitssigniﬁcant inﬂuence on the functioning and effectiveness ofdecision making, perception and persuasion; as an example, inmany contexts concepts contribute to perception formation in\\nproportion to the energy they receive. In order to model suchperceptions, modelers must pay attention to the (psychological)\\nprocesses by which various regions of knowledge structuresreceive energy and contribute to simulation outcomes, andmust know speciﬁcally how concepts are constructed andinterconnected.\\nA key goal of the COGVIEW architecture is to make\\nvarious effects of these ‘quirks’ more explicit by ﬁrst iden-tifying which psychological processes are most critical andthen demonstrating how these processes may be modeled ontop of INTELNET.\\nIII. N\\nOVEL KR M ECHANISM :ENERGY -BASED KR\\n(INTELNET)\\nUsing purely symbolic tools, it is difﬁcult to represent'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='(INTELNET)\\nUsing purely symbolic tools, it is difﬁcult to represent\\ndeeply nuanced, highly interconnected semantics; becausesymbols are highly granular, with bright-line separations be-tween them, symbolic KR often requires knowledge designersto abandon much of the information otherwise implicit inproblem domains because the KR does not offer an easy wayto represent it. As a consequence, purely symbolic systemsare often unable to perform beyond the original intention andmindset of the knowledge engineer - that is to say, they cannotreconstrue the world in new ways based on dynamic task\\ndemands. For example, a system which understands a\\nTABLE\\nonly as a piece of FURNITURE will not be able to reconstrue it\\nas being capable of serving as SHELTER (something one may\\nhide under) in a context which demands this.\\nSymbols are opaque, without internal semantics or infor-\\nmation about how various aspects could be reused or modiﬁed\\nin novel contexts. Neural networks, on the other hand, operate\\nat a level of abstraction too far below concepts to be able toeasily replace them in everyday use.\\nThe alternative presented here, ‘energy-based’ KR, uses\\nenergy ﬂows between various portions of the larger semantic\\ndeﬁnition of a concept to connect those components together,\\npermit them to interact and affect one another, and to worktogether in order to ‘build up‘ a concept deﬁnition.\\nDuring reasoning, various regions of the concept deﬁnition'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='During reasoning, various regions of the concept deﬁnition\\n‘space’ may be selected according to current needs, allowingcontextualized concept reconstrual. INTELNET allows formaximum extraction of the knowledge implicit in any givendomain, enabling systems to solve problems unlike those antic-ipated by the designer and/or that they may have seen before,and allowing for minimal ’pre-cognizing’ of problem domainsby knowledge engineers. The high ‘grain’ (capacity for ﬁne\\ndetail) of INTELNET representation permits the developmentof advanced reasoning techniques.\\nEnergy-based KR involves extended spreading combination\\nand recombination ofsemantic subcomponents within tradi-\\ntional concepts and other normally opaque semantic buildingblocks .\\nConcepts as assembled under INTELNET possess many\\ndesirable properties, mirroring the real-world functioning ofhuman concepts and cognitive processes and providing sufﬁ-cient nuance to serve as a base for highly advanced reasoning.\\nAny KR is only capable of answering certain questions,\\nespecially if it demands particular simpliﬁcations or is unableto represent a particularly salient part of a problem (such as thepsychological processes underlying social models). Systems\\nwith ﬁner representational grain are better able to adapt datato new uses as they have access to enough information to make\\n2013 IEEE Symposium on Computational Intellig ence for Human-like Intelligence (CIHLI) 83'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='2013 IEEE Symposium on Computational Intellig ence for Human-like Intelligence (CIHLI) 83\\nAuthorized licensed use limited to: Peking University. Downloaded on November 23,2024 at 04:56:56 UTC from IEEE Xplore.  Restrictions apply. informed decisions about what sorts of new conclusions may\\nbe drawn from the original data.\\nTo this end, in INTELNET all information (knowledge,\\nconcepts, interaction patterns, and more) is stored in networks ,\\ndeﬁned as augmented graphs. Each element may be ‘elabo-\\nrated’ with networks; even nodes, edges, and so on may all\\ncontain networks within them specifying the semantics of those\\nelements. Opaque symbols are avoided as far as possible, withnetworks ‘all the way down’.\\nINTELNET network edges channel energy, modify it, and\\nconnect disparate concept spaces, concepts, and other elements(as deﬁned below) together. Representations are built up byinserting certain types of energy at start nodes, permittingenergy to spread outwards, and collecting results once energyhas spread to a sufﬁciently wide radius or to particularly\\ninteresting nodes. The result is a collection of nodes, edges,and a record of the amount and type of energy arrivingat each node at each point in time. Analysis is made of\\nenergy ﬂows, especially ‘clashes’ (described below), which\\noccur when energy of one polarity meets energy of another.Clashes are highly meaningful and important, often pointingto semantically signiﬁcant areas within modeled domains (seeexamples below).\\nIV . R'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='IV . R\\nELATED WORK AND DISCUSSION :\\nWHAT INTELNET I S...\\n(Hint: It’s Not What You Think)\\nINTELNET and COGVIEW represent fundamentally new\\napproaches to questions of knowledge representation and beliefmodeling. To support this claim, this section discusses related\\nwork, brieﬂy considering key differences vis- `a-vis the present\\napproach.\\nA. Spreading Activation, Marker Passing\\nSpreading activation, in both its biologically-inspired [12],\\n[13] and semantic network-employed forms, “[serves] thefunction of quickly spreading an associative relevancy measure\\nover declarative memory”. [12].\\nIn COGVIEW/INTELNET, linking one concept to another\\ndoes not mean that one concept merely activates another\\n(raises its relevancy measure), however - it indicates that aconcept interacts with another to co-create semantic ﬁelds\\nand inﬂuence other concepts and energy ﬂows. This is a keydistinction.\\nEnergy binds smaller semantic components together, driv-\\ning a dynamic reiﬁcation process that in essence creates newconcepts as it goes. Energy ﬂows carry the action of variousinformation sources (and functions upon these) throughout thenetwork, using network links to distribute information andinteraction, binding the components of larger semantic ﬁeldstogether during traversal. Energy quanta include implicit se-mantic representations of upstream nodes and links, includingthe means by which they were generated and the judgmentsattached to the sources of that energy.'),\n",
       " Document(metadata={'title': 'COGVIEW & INTELNET: Nuanced Energy-Based Knowledge Representation And Integrated Cognitive-Conceptual Framework For Realistic Culture, Values, and Concept-Affected Systems Simulation', 'year': 2021}, page_content='Energy quanta do not represent activation; rather, they\\nreﬂect the summation of activities occurring at upper levels,representing a ‘motive force’ enabling nodes to participate insemantic simulation. Energy entering a node enables that nodeto ‘do’ things like modifying energy ﬂows and sending energyto other network elements. There is no decay; energy ﬂowsuntil it reaches leaf nodes and all loops have been exhausted.\\nWhen modeling cultures and beliefs, a key information\\nsource is emotional engagement - the level of positive or neg-\\native emotional ‘force’ attached to particular concepts (such asfamily, taboos, and so on.) Emotions drive signiﬁcant amountsof behavior, suggesting that simulations must be aware of howstimuli generate emotions and ‘colorize’ other stimuli. Themore emotional engagement attached to a particular stimulus,generally the more INTELNET energy it will be able to sendto other nodes. Negative energy reaching a node indicatesthat that node is part of a larger semantic ﬁeld viewed asunfavorable, impossible, incompatible, or inappropriate, andvice versa for positive.While spreading activation involves ontology traversal,\\nINTELNET energy spreading is a complex process of buildingup larger wholes from smaller pieces, keeping track of change'),\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedder) # Spacy Embeddings\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ThesisSearch(BaseModel):\n",
    "    \"\"\"Search and summarize over a corpus of thesis about a certain field or knowledge.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"The user's search query for academic content summarization.\",\n",
    "    )\n",
    "    \n",
    "    min_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive.\",\n",
    "    )\n",
    "    max_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        \"\"\"Prints all specified fields with their values.\"\"\"\n",
    "        for field in self.__fields__:\n",
    "            value = getattr(self, field)\n",
    "            if value is not None:\n",
    "                print(f\"{field}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 19\u001b[0m\n\u001b[0;32m      9\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m     10\u001b[0m     [\n\u001b[0;32m     11\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, system),\n\u001b[0;32m     12\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     13\u001b[0m     ]\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m llm \u001b[38;5;241m=\u001b[39m RedPillLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     17\u001b[0m                  api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRED_PILL_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     18\u001b[0m                  temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThesisSearch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_calling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m query_analyzer \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m structured_llm\n",
      "File \u001b[1;32mc:\\Users\\hengz\\.conda\\envs\\langchain\\lib\\site-packages\\langchain_core\\language_models\\base.py:241\u001b[0m, in \u001b[0;36mBaseLanguageModel.with_structured_output\u001b[1;34m(self, schema, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Not implemented on this class.\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Implement this on child class if there is a way of steering the model to\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# generate responses that match a given schema.\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tools.customllm import RedPillLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of corpus containing academic thesis. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = RedPillLLM(model=\"gpt-4o\", \n",
    "                 api_key=os.getenv(\"RED_PILL_API_KEY\"),\n",
    "                 temperature = 0)\n",
    "structured_llm = llm.with_structured_output(ThesisSearch, method=\"function_calling\", strict=True)\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":5},\n",
    "                                     search_type=\"similarity\")\n",
    "\n",
    "docs = retriever.invoke(\"What is Task Decomposition?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this way, the model is trained more evenly for different tasks towards the end of the training\n",
      "processto reduceinter-taskinterference.Wangetal. [ 138]d e fi n eαas\n",
      "α(e)=min/parenleftBig\n",
      "αm,(e−1)αm−α0\n",
      "M+α0/parenrightBig\n",
      ",\n",
      "whereα0andαmdenote initial and maximum values of α. The noise level of the self-supervised\n",
      "denoising autoencoding task is scheduled similarly, increasing difficulty after a warm-up period.\n",
      "In both works, temperature αincreases during training which encourages up-sampling of low-\n",
      "resourcetasksand alleviates overfitting.\n",
      "3.4 Task Scheduling\n",
      "Task scheduling determines the order of tasks on which an MTL model is trained. A naive way\n",
      "is to train all tasks together. Zhang et al. [ 161] take this way to train an MTL model, where data\n",
      "batches are organized as four-dimensional tensors of size N×M×T×d,w h e r eNdenotes the\n",
      "number of samples, Mdenotes the number of tasks, Tdenotes sequence length, and drepresents\n",
      "embedding dimensions. Similarly, Zalmout and Habash [ 156] putlabeled data and unlabeled data\n",
      "together to form a batch and Xia et al. [ 146] learned the dependency parsing and semantic role\n",
      "labelingtaskstogether.InthecaseofauxiliaryMTL,AugensteinandSøgaard[ 4]traintheprimary\n",
      "taskandoneoftheauxiliarytaskstogetherateachstep.Conversely,Songetal.[ 120]trainoneof\n",
      "theprimarytasksandtheauxiliary tasktogetherand shufflesbetweenthetwoprimarytasks.\n",
      "Alternatively, we can train an MTL model on different tasks at different steps. Similar to data\n",
      "\n",
      "\n",
      "Figure 4 A UIMA pipeline. An input task is sequentially put through\n",
      "a series of tasks, with intermediate results at each step and ﬁnal output atthe end. Generally, the output of a task is the input of its successor, butexceptionally, a particular task may provide feedback to a previous one (as\n",
      "in task 4 providing input to task 1). Intermediate results (eg, successive\n",
      "transformations of the original bus) are read from/written to the CAS,which contains metadata deﬁning the formats of the data required at everystep, the intermediate results, and annotations that link to these results.\n",
      "548 J Am Med Inform Assoc 2011; 18:544e551. doi:10.1136/amiajnl-2011-000464ReviewDownloaded from https://academic.oup.com/jamia/article/18/5/544/829676 by Peking University user on 23 November 2024\n",
      "however, applies to NLP in general: it would occur even if the\n",
      "individual tasks were all combined into a single body of code.\n",
      "One way to address it (adopted in some commercial systems) is\n",
      "to use alternative algorithms (in multiple or branching pipelines)and contrast the ﬁnal results obtained. This allows tuning the\n",
      "output to trade-offs (high precision versus high recall, etc).\n",
      "A LOOK INTO THE FUTURE\n",
      "Recent advances in arti ﬁcial intelligence (eg, computer chess)\n",
      "have shown that effective approaches utilize the strengths ofelectronic circuitry dhigh speed and large memory/disk capacity,\n",
      "problem-speci ﬁc data-compression techniques and evaluation\n",
      "\n",
      "\n",
      "the rest of the system.\n",
      "This is the intention behind pipelined NLP frameworks, such\n",
      "as GATE\n",
      "77and IBM (now Apache) Unstructured Information\n",
      "Management Architecture (UIMA).78UIMA ’s scope goes beyond\n",
      "NLP: one could integrate structured-format databases, images,\n",
      "and multi-media, and any arbitrary technology. In UIMA, eachanalytical task transforms (a copy of) its input by adding XML-\n",
      "based markup and/or reading/writing external data. A task\n",
      "operates on Common Analysis Structure (CAS), which containsthe data (possibly in multiple formats, eg, audio, HTML),\n",
      "a schema describing the analysis structure (ie, the details of the\n",
      "markup/external formats), the analysis results, and links(indexes) to the portions of the source data that they refer to.UIMA does not dictate the design of the analytical tasks\n",
      "themselves: they interact with the UIMA pipeline only through\n",
      "the CAS, and can be treated as black boxes: thus, different taskscould be written in different programming languages.\n",
      "The schema for a particular CAS is developer-de ﬁned\n",
      " because\n",
      "it is usually problem-speci ﬁc. (Currently, no standard schemas\n",
      "exist for tasks such as POS tagging, although this may change.)\n",
      "Deﬁnition is performed using XMI (XML Metadata Inter-\n",
      "change), the XML-interchange equivalent of the Uni ﬁed\n",
      "Modeling Language (UML). XMI, however, is ‘programmer-\n",
      "hostile ’: it is easier to use a commercial UML tool to design\n",
      "a UML model visually and then generate XMI from it.79\n",
      "\n",
      "\n",
      "i=1exp(Qe,i/τ),\n",
      "Qe,t=(1−α)eQ0,t+e/summationdisplay.1\n",
      "k=1α(1−α)e−kRk,\n",
      "whereτdenotesthetemperature, αisthedecayrate,and Rkistheobservedrewardatstep kthatis\n",
      "definedasthenegativevalidationlossoftheprimarytask.Analysisshowsthatthebanditassigns\n",
      "a higher probability to the primary task at first and then more evenly switches between all tasks,\n",
      "whichechoesthedynamic datasamplingtechniquesintroducedin Section 3.3.\n",
      "ACM Comput.Surv., Vol. 56,No.12,Article 295.Publicationdate:July 2024.295:14 S. Chen et al.\n",
      "Besides probabilistic approaches, task scheduling could also use heuristics based on certain\n",
      "performance metrics. By optimizing the Tchebycheff loss, Mao et al. [ 81] learn from the task\n",
      "which has the worst validation performance at each step. The CA-MTL model [ 101] introduces\n",
      "anuncertainty-basedsamplingstrategybasedonShannonentropyforjointlearningofclassifica-\n",
      "tion tasks. Specifically, given a batch size bandMtasks, a pool of b×Ms a m p l e si sfi r s ts a m p l e d .\n",
      "Then,theuncertaintymeasure U(x)fora sample xfromtask iis definedas\n",
      "U(x)=Si(x)\n",
      "ˆS×S/prime,\n",
      "whereSdenotes the Shannon entropy of the model’s prediction on x,ˆSis the model’s maximum\n",
      "averageentropyoverthe bsamplesfromeachtask. S/primedenotestheentropyofauniformdistribution\n",
      "and is used to normalize the variance of the number of classes in each task. At last, bsamples\n",
      "withthehighestuncertaintymeasuresareusedfortrainingatthecurrentstep.Experimentsshow\n",
      "\n",
      "\n",
      "61,73–75,78,80,89,105,111,113,115,120,134,145,146,151,158,159,159,160,163,165,169,\n",
      "170]. For example, to prevent large datasets from dominating training, Perera et al. [ 98] set the\n",
      "weightsas\n",
      "λt∝1\n",
      "|Dt|,\n",
      "where|Dt|denotes the size of the training dataset for task t. The weights can also be adjusted\n",
      "dynamically during the training process based on certain metrics. Through adjusting weights,\n",
      "we can purposely emphasize different tasks in different training stages. For instance, since dy-\n",
      "namically assigning smaller weights to more uncertain tasks usually leads to good performance\n",
      "for MTL [ 19,62] assigns weights based on the homoscedasticity of training losses from different\n",
      "ACMComput.Surv., Vol. 56,No. 12,Article 295.Publicationdate:July 2024.Multi-Task LearninginNatural Language Processing: AnOverview 295:11\n",
      "tasksas\n",
      "λt=1\n",
      "2σ2\n",
      "t,\n",
      "whereσtmeasuresthevarianceofthetraininglossfortask t.In[70],theweightofanunsupervised\n",
      "taskissettoaconfidencescorethatmeasureshowmuchapredictionresemblesthecorresponding\n",
      "self-supervised label. To ensure that a student model could receive enough supervision during\n",
      "knowledge distillation, BAM! [ 20] combines the supervised loss Lsupwith the distillation loss\n",
      "Ldissas\n",
      "L=λLdiss+(1−λ)Lsup,\n",
      "whereλincreaseslinearlyfrom0to1inthetrainingprocess.In[ 121],threetasksarejointlyopti-\n",
      "mized, including the primary essay OE task as well as the auxiliary sentence function identifi-\n",
      "cation(SFI)a n dparagraph function identification (PFI) tasks. The two lower-level auxiliary\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.customllm import RedPillLLM\n",
    "\n",
    "llm = RedPillLLM(model=\"gpt-4o\", \n",
    "                 api_key=os.getenv(\"RED_PILL_API_KEY\"),\n",
    "                 temperature = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rag Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not explicitly define \"Task Decomposition.\" However, based on the context of multi-task learning in natural language processing, task decomposition generally refers to the process of breaking down a complex task into smaller, more manageable sub-tasks. This approach can help in organizing and scheduling tasks more effectively, allowing models to focus on specific aspects of a task at different stages of training. Task decomposition can also facilitate the integration of various tasks within a multi-task learning framework, potentially improving the overall performance by reducing inter-task interference and optimizing task scheduling.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
